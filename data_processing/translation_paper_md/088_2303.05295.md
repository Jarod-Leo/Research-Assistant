# Dynamic Stashing Quantization for Efficient Transformer Training

链接: http://arxiv.org/abs/2303.05295v1

原文摘要:
Large Language Models (LLMs) have demonstrated impressive performance on a
range of Natural Language Processing (NLP) tasks. Unfortunately, the immense
amount of computations and memory accesses required for LLM training makes them
prohibitively expensive in terms of hardware cost, and thus challenging to
deploy in use cases such as on-device learning. In this paper, motivated by the
observation that LLM training is memory-bound, we propose a novel dynamic
quantization strategy, termed Dynamic Stashing Quantization (DSQ), that puts a
special focus on reducing the memory operations, but also enjoys the other
benefits of low precision training, such as the reduced arithmetic cost. We
conduct a thorough study on two translation tasks (trained-from-scratch) and
three classification tasks (fine-tuning). DSQ reduces the amount of arithmetic
operations by $20.95\times$ and the number of DRAM operations by $2.55\times$
on IWSLT17 compared to the standard 16-bit fixed-point, which is widely used in
on-device learning.

中文翻译:
大型语言模型（LLMs）在一系列自然语言处理（NLP）任务中展现出卓越性能。然而，LLM训练所需的庞大计算量与内存访问量导致硬件成本极其高昂，这使得其在设备端学习等应用场景中的部署面临重大挑战。本文基于"LLM训练具有内存瓶颈"这一关键发现，提出了一种新型动态量化策略——动态暂存量（Dynamic Stashing Quantization, DSQ），该策略不仅通过降低内存操作实现核心优化，同时兼具低精度训练的其他优势（如减少算术运算成本）。我们在两个翻译任务（从头训练）和三个分类任务（微调）上进行了系统研究。实验表明，在IWSLT17数据集上，与设备端学习广泛采用的16位定点标准方法相比，DSQ将算术运算量降低20.95倍，并将DRAM操作次数减少2.55倍。
