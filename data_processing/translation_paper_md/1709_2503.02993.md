# Zero-Shot Multi-Label Classification of Bangla Documents: Large Decoders Vs. Classic Encoders

链接: http://arxiv.org/abs/2503.02993v1

原文摘要:
Bangla, a language spoken by over 300 million native speakers and ranked as
the sixth most spoken language worldwide, presents unique challenges in natural
language processing (NLP) due to its complex morphological characteristics and
limited resources. While recent Large Decoder Based models (LLMs), such as GPT,
LLaMA, and DeepSeek, have demonstrated excellent performance across many NLP
tasks, their effectiveness in Bangla remains largely unexplored. In this paper,
we establish the first benchmark comparing decoder-based LLMs with classic
encoder-based models for Zero-Shot Multi-Label Classification (Zero-Shot-MLC)
task in Bangla. Our evaluation of 32 state-of-the-art models reveals that,
existing so-called powerful encoders and decoders still struggle to achieve
high accuracy on the Bangla Zero-Shot-MLC task, suggesting a need for more
research and resources for Bangla NLP.

中文翻译:
孟加拉语作为全球第六大语言，拥有超过3亿母语使用者，其复杂的形态学特征和有限的资源为自然语言处理（NLP）领域带来了独特挑战。尽管当前基于解码器的大型语言模型（如GPT、LLaMA和DeepSeek）在多数NLP任务中展现出卓越性能，但其在孟加拉语中的应用效果仍属未知领域。本文首次建立了基于解码器的LLM模型与经典编码器模型在孟加拉语零样本多标签分类任务（Zero-Shot-MLC）中的性能基准。通过对32个前沿模型的评估发现，现有所谓强大的编码器和解码器模型在孟加拉语Zero-Shot-MLC任务中仍难以实现高准确率，这表明孟加拉语NLP领域亟需更多研究投入与资源支持。

（翻译说明：采用学术论文摘要的规范表述，通过以下处理实现专业性与可读性平衡：
1. 术语统一："decoder-based LLMs"统一译为"基于解码器的LLM模型"，"Zero-Shot-MLC"首次出现标注英文全称
2. 句式重构：将原文复合句拆分为符合中文表达习惯的短句，如将"due to..."处理为因果句式
3. 专业表达："state-of-the-art models"译为"前沿模型"而非字面直译
4. 文化适配："Bangla"根据中国语言学界惯例译为"孟加拉语"而非"孟加拉文"
5. 逻辑显化：通过"这表明"等连接词明确研究结论与数据间的推导关系）
