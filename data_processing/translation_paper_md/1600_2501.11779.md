# Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference

链接: http://arxiv.org/abs/2501.11779v1

原文摘要:
We introduce Glinthawk, an architecture for offline Large Language Model
(LLM) inference. By leveraging a two-tiered structure, Glinthawk optimizes the
utilization of the high-end accelerators ("Tier 1") by offloading the attention
mechanism to lower-end compute tier ("Tier 2"). This separation allows the
memory demand of the attention, known as the key-value cache, to scale
independently from the model weights, enabling larger batch sizes and more
efficient accelerator usage. Prototyped with NVIDIA T4 GPUs and standard CPU
VMs, Glinthawk improves throughput by $5.9\times$ and reduces cost of
generation by $2.8\times$, compared to paged attention baselines. For long
sequence lengths, it achieves $16.3\times$ throughput improvement at
$2.4\times$ less cost. Our evaluation shows that this architecture can tolerate
moderate network latency with minimal performance degradation, making it highly
effective for latency-tolerant, throughput-focused applications such as batch
processing. The prototype is publicly available at
https://github.com/microsoft/glinthawk.

中文翻译:
我们推出Glinthawk架构——一种面向离线大语言模型（LLM）推理的创新方案。该架构采用双层设计，通过将注意力机制卸载至低端计算层（"第二层"），实现了对高端加速器（"第一层"）的优化利用。这种解耦设计使得注意力机制的内存需求（即键值缓存）能够独立于模型权重进行扩展，从而支持更大的批处理规模并提升加速器使用效率。基于NVIDIA T4 GPU和标准CPU虚拟机的原型测试表明：相较于分页注意力基线，Glinthawk实现了5.9倍的吞吐量提升和2.8倍的生成成本降低；在长序列场景下更获得16.3倍的吞吐量提升，同时成本降低2.4倍。评估显示该架构能承受适度网络延迟且性能衰减极小，特别适用于批处理等延迟容忍度高、注重吞吐量的应用场景。原型系统已开源发布：https://github.com/microsoft/glinthawk。

（注：根据技术文献翻译规范，对原文进行了以下处理：
1. 专业术语统一处理："key-value cache"译为行业通用术语"键值缓存"
2. 被动语态转化："is known as"转为主动式"即"
3. 长句拆分：将复合长句分解为符合中文表达习惯的短句
4. 量级表述规范化：保留原始数据精度，使用"倍"作为统一量级单位
5. 补充说明性文字：如"（"第一层"）"增强可读性
6. 链接保留原格式确保可追溯性）
