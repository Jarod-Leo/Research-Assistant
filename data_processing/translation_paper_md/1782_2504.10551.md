# MiMu: Mitigating Multiple Shortcut Learning Behavior of Transformers

链接: http://arxiv.org/abs/2504.10551v1

原文摘要:
Empirical Risk Minimization (ERM) models often rely on spurious correlations
between features and labels during the learning process, leading to shortcut
learning behavior that undermines robustness generalization performance.
Current research mainly targets identifying or mitigating a single shortcut;
however, in real-world scenarios, cues within the data are diverse and unknown.
In empirical studies, we reveal that the models rely to varying extents on
different shortcuts. Compared to weak shortcuts, models depend more heavily on
strong shortcuts, resulting in their poor generalization ability. To address
these challenges, we propose MiMu, a novel method integrated with
Transformer-based ERMs designed to Mitigate Multiple shortcut learning
behavior, which incorporates self-calibration strategy and self-improvement
strategy. In the source model, we preliminarily propose the self-calibration
strategy to prevent the model from relying on shortcuts and make overconfident
predictions. Then, we further design self-improvement strategy in target model
to reduce the reliance on multiple shortcuts. The random mask strategy involves
randomly masking partial attention positions to diversify the focus of target
model other than concentrating on a fixed region. Meanwhile, the adaptive
attention alignment module facilitates the alignment of attention weights to
the calibrated source model, without the need for post-hoc attention maps or
supervision. Finally, extensive experiments conducted on Natural Language
Processing (NLP) and Computer Vision (CV) demonstrate the effectiveness of MiMu
in improving robustness generalization abilities.

中文翻译:
以下是您提供的英文论文摘要的中文翻译：

【经验风险最小化模型中的多捷径学习缓解方法】

经验风险最小化（ERM）模型在学习过程中常依赖特征与标签间的伪相关性，导致产生损害鲁棒性泛化性能的捷径学习行为。现有研究主要针对识别或消除单一捷径，然而现实场景中数据内部的提示线索具有多样性和未知性。通过实证研究，我们发现模型会不同程度地依赖不同捷径——相较于弱捷径，模型对强捷径的过度依赖会导致其泛化能力显著下降。

为应对这些挑战，我们提出MiMu（Mitigate Multiple shortcuts），这是一种与基于Transformer的ERM模型集成的新方法，旨在缓解多捷径学习行为。该方法包含双重策略：自校准策略与自优化策略。在源模型中，我们初步引入自校准策略以防止模型依赖捷径做出过度自信的预测；进而在目标模型中设计自优化策略，通过两种机制降低对多捷径的依赖：1）随机掩码策略通过遮蔽部分注意力位置，促使目标模型关注区域多样化而非固定区域；2）自适应注意力对齐模块无需事后注意力图或监督信号，即可实现与校准源模型的注意力权重对齐。最终，在自然语言处理（NLP）和计算机视觉（CV）领域的广泛实验证明，MiMu能有效提升模型的鲁棒性泛化能力。

（翻译说明：采用学术论文的规范表述，保留ERM、Transformer等技术术语的英文原名；将"shortcut learning"译为"捷径学习"以符合领域术语；通过分号与破折号重构英文长句为符合中文阅读习惯的短句；"self-calibration/self-improvement"统一译为"自校准/自优化"保持策略命名一致性；调整被动语态为主动表述，如"are diverse and unknown"译为"具有多样性和未知性"）
