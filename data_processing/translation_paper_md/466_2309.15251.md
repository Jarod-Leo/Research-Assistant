# VPA: Fully Test-Time Visual Prompt Adaptation

链接: http://arxiv.org/abs/2309.15251v1

原文摘要:
Textual prompt tuning has demonstrated significant performance improvements
in adapting natural language processing models to a variety of downstream tasks
by treating hand-engineered prompts as trainable parameters. Inspired by the
success of textual prompting, several studies have investigated the efficacy of
visual prompt tuning. In this work, we present Visual Prompt Adaptation (VPA),
the first framework that generalizes visual prompting with test-time
adaptation. VPA introduces a small number of learnable tokens, enabling fully
test-time and storage-efficient adaptation without necessitating source-domain
information. We examine our VPA design under diverse adaptation settings,
encompassing single-image, batched-image, and pseudo-label adaptation. We
evaluate VPA on multiple tasks, including out-of-distribution (OOD)
generalization, corruption robustness, and domain adaptation. Experimental
results reveal that VPA effectively enhances OOD generalization by 3.3% across
various models, surpassing previous test-time approaches. Furthermore, we show
that VPA improves corruption robustness by 6.5% compared to strong baselines.
Finally, we demonstrate that VPA also boosts domain adaptation performance by
relatively 5.2%. Our VPA also exhibits marked effectiveness in improving the
robustness of zero-shot recognition for vision-language models.

中文翻译:
文本提示调优通过将人工设计的提示作为可训练参数，在使自然语言处理模型适应各类下游任务方面展现出显著性能提升。受文本提示成功实践的启发，多项研究开始探索视觉提示调优的有效性。本研究提出视觉提示自适应框架（VPA），这是首个将视觉提示与测试时自适应相结合的通用框架。VPA通过引入少量可学习标记，实现了完全基于测试时的高效自适应，且无需源域信息支持。我们在单图像、批图像及伪标签自适应等多种设置下验证了VPA的设计方案，并在分布外泛化（OOD）、抗干扰鲁棒性和领域自适应等任务上进行了全面评估。实验结果表明：VPA将各类模型的OOD泛化能力平均提升3.3%，超越现有测试时自适应方法；相较于强基线模型，其抗干扰鲁棒性提高6.5%；在领域自适应任务中相对性能提升达5.2%。研究还发现VPA能显著增强视觉-语言模型的零样本识别鲁棒性。
