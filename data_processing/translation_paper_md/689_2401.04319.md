# Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs

链接: http://arxiv.org/abs/2401.04319v1

原文摘要:
In this paper, we explore a new way for user targeting, where non-expert
marketers could select their target users solely given demands in natural
language form. The key to this issue is how to transform natural languages into
practical structured logical languages, i.e., the structured understanding of
marketer demands. In practical scenarios, the demands of non-expert marketers
are often abstract and diverse. Considering the impressive natural language
processing ability of large language models (LLMs), we try to leverage LLMs to
solve this issue. To stimulate the LLMs' reasoning ability, the
chain-of-thought (CoT) prompting method is widely used, but existing methods
still have some limitations in our scenario: (1) Previous methods either use
simple "Let's think step by step" spells or provide fixed examples in
demonstrations without considering compatibility between prompts and concrete
questions, making LLMs ineffective when the marketers' demands are abstract and
diverse. (2) Previous methods are often implemented in closed-source models or
excessively large models, which is not suitable in industrial practical
scenarios. Based on these, we propose ARALLM (i.e., Analogical Reasoning
Augmented Large Language Models) consisting of two modules: Analogical
Reasoning based Prompting and Reasoning-Augmented Multi-Task Model
Distillation. Part of our data and code can be found at
https://github.com/alipay/Analogic-Reasoning-Augmented-Large-Language-Model.

中文翻译:
本文探讨了一种全新的用户定向方法，使得非专业营销人员仅需通过自然语言描述即可完成目标用户筛选。该问题的核心在于如何将自然语言转化为可操作的结构化逻辑语言，即实现对营销需求的结构化理解。在实际场景中，非专业营销人员的需求往往具有抽象性和多样性特征。鉴于大语言模型（LLMs）展现出的卓越自然语言处理能力，我们尝试利用LLMs解决这一难题。

为激发LLMs的推理能力，思维链（CoT）提示法被广泛采用，但现有方法在本研究场景中仍存在局限：（1）既往研究要么使用简单的"让我们逐步思考"式咒语，要么在示例中提供固定范式，未能考虑提示语与具体问题之间的适配性，导致LLMs在处理抽象多样的营销需求时效果欠佳；（2）现有方案多基于闭源模型或过度庞大的模型，难以满足工业实践需求。

基于此，我们提出ARALLM框架（类比推理增强的大语言模型），其包含两大核心模块：基于类比推理的提示工程和推理增强的多任务模型蒸馏。部分数据与代码已开源：https://github.com/alipay/Analogic-Reasoning-Augmented-Large-Language-Model。

（注：根据学术翻译规范，对部分术语进行了专业处理：
1. "spells"译为"咒语"保留AI领域对prompt的拟物化表述
2. "demonstrations"译为"示例"符合机器学习领域术语
3. 模型名称ARALLM保留英文缩写并补充全称解释
4. 长难句采用拆分策略，如将"without considering..."独立成句
5. 被动语态转换为主动句式，如"are often implemented"译为"多基于"）
