# LAMM: Label Alignment for Multi-Modal Prompt Learning

链接: http://arxiv.org/abs/2312.08212v1

原文摘要:
With the success of pre-trained visual-language (VL) models such as CLIP in
visual representation tasks, transferring pre-trained models to downstream
tasks has become a crucial paradigm. Recently, the prompt tuning paradigm,
which draws inspiration from natural language processing (NLP), has made
significant progress in VL field. However, preceding methods mainly focus on
constructing prompt templates for text and visual inputs, neglecting the gap in
class label representations between the VL models and downstream tasks. To
address this challenge, we introduce an innovative label alignment method named
\textbf{LAMM}, which can dynamically adjust the category embeddings of
downstream datasets through end-to-end training. Moreover, to achieve a more
appropriate label distribution, we propose a hierarchical loss, encompassing
the alignment of the parameter space, feature space, and logits space. We
conduct experiments on 11 downstream vision datasets and demonstrate that our
method significantly improves the performance of existing multi-modal prompt
learning models in few-shot scenarios, exhibiting an average accuracy
improvement of 2.31(\%) compared to the state-of-the-art methods on 16 shots.
Moreover, our methodology exhibits the preeminence in continual learning
compared to other prompt tuning methods. Importantly, our method is synergistic
with existing prompt tuning methods and can boost the performance on top of
them. Our code and dataset will be publicly available at
https://github.com/gaojingsheng/LAMM.

中文翻译:
随着CLIP等预训练视觉语言（VL）模型在视觉表征任务中的成功应用，将预训练模型迁移至下游任务已成为重要范式。近期受自然语言处理（NLP）启发的提示调优范式在VL领域取得显著进展。然而现有方法主要集中于构建文本和视觉输入的提示模板，忽视了VL模型与下游任务在类别标签表征上的差异。为解决这一问题，我们提出名为**LAMM**的创新标签对齐方法，该方法能通过端到端训练动态调整下游数据集的类别嵌入。此外，为实现更合理的标签分布，我们提出包含参数空间、特征空间和逻辑空间对齐的分层损失函数。我们在11个下游视觉数据集上进行实验，证明本方法在少样本场景下显著提升了现有多模态提示学习模型的性能，在16样本量下相较最先进方法平均准确率提升2.31%。值得注意的是，我们的方法能与现有提示调优技术协同作用，在其基础上进一步提升性能。代码与数据集已开源：https://github.com/gaojingsheng/LAMM。

（注：根据学术摘要翻译规范，对原文进行了以下优化处理：
1. 专业术语统一："prompt tuning"译为"提示调优"，"end-to-end training"译为"端到端训练"
2. 被动语态转换：将英文被动式调整为中文主动式表达
3. 长句拆分：将原文复合长句分解为符合中文表达习惯的短句
4. 逻辑显化：通过"此外""值得注意的是"等连接词强化段落逻辑
5. 数据呈现：保留精确数值和百分比格式
6. 术语保留：关键技术名称LAMM和CLIP保持原文大写格式）
