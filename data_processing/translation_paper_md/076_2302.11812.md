# Teacher Intervention: Improving Convergence of Quantization Aware Training for Ultra-Low Precision Transformers

链接: http://arxiv.org/abs/2302.11812v1

原文摘要:
Pre-trained Transformer models such as BERT have shown great success in a
wide range of applications, but at the cost of substantial increases in model
complexity. Quantization-aware training (QAT) is a promising method to lower
the implementation cost and energy consumption. However, aggressive
quantization below 2-bit causes considerable accuracy degradation due to
unstable convergence, especially when the downstream dataset is not abundant.
This work proposes a proactive knowledge distillation method called Teacher
Intervention (TI) for fast converging QAT of ultra-low precision pre-trained
Transformers. TI intervenes layer-wise signal propagation with the intact
signal from the teacher to remove the interference of propagated quantization
errors, smoothing loss surface of QAT and expediting the convergence.
Furthermore, we propose a gradual intervention mechanism to stabilize the
recovery of subsections of Transformer layers from quantization. The proposed
schemes enable fast convergence of QAT and improve the model accuracy
regardless of the diverse characteristics of downstream fine-tuning tasks. We
demonstrate that TI consistently achieves superior accuracy with significantly
lower fine-tuning iterations on well-known Transformers of natural language
processing as well as computer vision compared to the state-of-the-art QAT
methods.

中文翻译:
预训练Transformer模型（如BERT）已在众多应用中展现出卓越性能，但其代价是模型复杂度的大幅提升。量化感知训练（QAT）作为一种降低实现成本与能耗的有效方法备受关注。然而，当量化位数低于2比特时，由于训练过程的不稳定收敛（尤其在目标数据集规模有限时），会导致模型精度显著下降。本研究提出一种称为"教师干预"（TI）的主动知识蒸馏方法，用于超低精度预训练Transformer的快速收敛QAT。该方法通过逐层注入教师模型的完整信号来干预前向传播，从而消除量化误差传播的干扰，平滑QAT的损失曲面并加速收敛。此外，我们提出渐进式干预机制，以稳定恢复Transformer各层子结构在量化过程中的表征能力。所提方案能实现QAT的快速收敛，并在各类下游微调任务中普遍提升模型精度。实验表明：相较于现有最优QAT方法，TI在自然语言处理和计算机视觉领域的经典Transformer模型上，均能以显著更少的微调迭代次数持续获得更高的精度。

