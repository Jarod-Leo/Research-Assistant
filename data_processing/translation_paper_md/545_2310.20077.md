# Partial Tensorized Transformers for Natural Language Processing

链接: http://arxiv.org/abs/2310.20077v1

原文摘要:
The transformer architecture has revolutionized Natural Language Processing
(NLP) and other machine-learning tasks, due to its unprecedented accuracy.
However, their extensive memory and parameter requirements often hinder their
practical applications. In this work, we study the effect of tensor-train
decomposition to improve the accuracy and compress transformer vision-language
neural networks, namely BERT and ViT. We focus both on embedding-layer
compression and partial tensorization of neural networks (PTNN) through an
algorithmic approach. Our novel PTNN approach significantly improves the
accuracy of existing models by up to 5%, all without the need for post-training
adjustments, breaking new ground in the field of tensor decomposition.

中文翻译:
以下是符合要求的学术中文翻译：

变压器架构以前所未有的准确性彻底改变了自然语言处理（NLP）及其他机器学习任务，但其庞大的内存与参数需求往往阻碍实际应用。本研究探讨了张量链分解对提升视觉语言神经网络（BERT与ViT）精度与压缩性能的影响。我们通过算法路径，重点研究嵌入层压缩与神经网络部分张量化（PTNN）技术。所提出的新型PTNN方法将现有模型精度最高提升5%，且无需训练后调整，在张量分解领域实现了突破性进展。

注：翻译过程中严格遵循以下学术规范：
1. 专业术语统一（如transformer译作"变压器架构"，tensor-train decomposition译作"张量链分解"）
2. 被动语态转化（如"is studied"译为"探讨"）
3. 长句拆分重组（将原文复合句按中文表达习惯分解为多个短句）
4. 概念准确传达（如"post-training adjustments"译为"训练后调整"而非字面直译）
5. 保持学术严谨性（如"up to 5%"译为"最高提升5%"而非模糊表述）
