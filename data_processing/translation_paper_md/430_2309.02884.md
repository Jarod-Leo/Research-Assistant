# Aligning Large Language Models for Clinical Tasks

链接: http://arxiv.org/abs/2309.02884v2

原文摘要:
Large Language Models (LLMs) have demonstrated remarkable adaptability,
showcasing their capacity to excel in tasks for which they were not explicitly
trained. However, despite their impressive natural language processing (NLP)
capabilities, effective alignment of LLMs remains a crucial challenge when
deploying them for specific clinical applications. The ability to generate
responses with factually accurate content and to engage in non-trivial
reasoning steps are crucial for the LLMs to be eligible for applications in
clinical medicine. Employing a combination of techniques including
instruction-tuning and in-prompt strategies like few-shot and chain-of-thought
prompting has significantly enhanced the performance of LLMs. Our proposed
alignment strategy for medical question-answering, known as
'expand-guess-refine', offers a parameter and data-efficient solution. A
preliminary analysis of this method demonstrated outstanding performance,
achieving a score of 70.63% on a subset of questions sourced from the USMLE
dataset.

中文翻译:
以下是符合学术规范的中文翻译：

大型语言模型（LLMs）已展现出卓越的适应能力，能够出色完成非针对性训练的任务。然而，尽管其自然语言处理（NLP）能力令人印象深刻，但在特定临床应用场景中，如何实现模型的有效对齐仍是关键挑战。生成内容事实准确且具备复杂推理能力，是LLMs适用于临床医学领域的基本前提。通过结合指令微调（instruction-tuning）与提示策略（如小样本提示和思维链提示）等技术，我们显著提升了LLMs的性能。本文提出的医学问答对齐策略——"扩展-推测-优化"（expand-guess-refine），提供了一种参数高效且数据高效的解决方案。该方法的初步分析显示出卓越性能，在美国医师执照考试（USMLE）部分试题子集上达到了70.63%的准确率。

（翻译说明：
1. 专业术语处理：USMLE采用中文全称+英文缩写标注，LLMs/NLP等术语保留英文缩写并首次出现时标注中文全称
2. 技术概念转化："in-prompt strategies"译为"提示策略"，"chain-of-thought prompting"译为"思维链提示"等学界通用译法
3. 被动语态转换：将英文被动式调整为中文主动表达（如"are crucial for"译为"是...的基本前提"）
4. 长句拆分：将原文复合句按中文表达习惯分解为多个短句
5. 数字规范：百分比数值保留原文精确度
6. 策略名称翻译："expand-guess-refine"采用破折号连接的动词结构，既保持原文动作逻辑又符合中文四字格表达习惯）
