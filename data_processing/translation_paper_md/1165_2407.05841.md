# An Empirical Comparison of Vocabulary Expansion and Initialization Approaches for Language Models

链接: http://arxiv.org/abs/2407.05841v1

原文摘要:
Language Models (LMs) excel in natural language processing tasks for English
but show reduced performance in most other languages. This problem is commonly
tackled by continually pre-training and fine-tuning these models for said
languages. A significant issue in this process is the limited vocabulary
coverage in the original model's tokenizer, leading to inadequate
representation of new languages and necessitating an expansion of the
tokenizer. The initialization of the embeddings corresponding to new vocabulary
items presents a further challenge. Current strategies require cross-lingual
embeddings and lack a solid theoretical foundation as well as comparisons with
strong baselines. In this paper, we first establish theoretically that
initializing within the convex hull of existing embeddings is a good
initialization, followed by a novel but simple approach, Constrained Word2Vec
(CW2V), which does not require cross-lingual embeddings. Our study evaluates
different initialization methods for expanding RoBERTa and LLaMA 2 across four
languages and five tasks. The results show that CW2V performs equally well or
even better than more advanced techniques. Additionally, simpler approaches
like multivariate initialization perform on par with these advanced methods
indicating that efficient large-scale multilingual continued pretraining can be
achieved even with simpler initialization methods. We release our code publicly
(https://github.com/AI4Bharat/VocabAdaptation_LLM/tree/CW2V).

中文翻译:
语言模型（LMs）在英语自然语言处理任务中表现卓越，但在大多数其他语言中性能有所下降。针对这一问题，通常采用持续预训练和微调这些模型以适应目标语言的方法。该过程中的一个关键问题是原始模型分词器的词汇覆盖范围有限，导致对新语言的表征不足，因此需要扩展分词器。而新增词汇对应嵌入向量的初始化则构成了进一步挑战。现有策略需要依赖跨语言嵌入向量，既缺乏坚实的理论基础，也未能与强基线方法进行充分比较。本文首先从理论上证明：在现有嵌入向量的凸包内进行初始化是一种优良策略，随后提出一种新颖而简洁的方法——约束型Word2Vec（CW2V），该方法无需跨语言嵌入向量支持。我们通过四项语言和五项任务的实验，评估了RoBERTa和LLaMA 2模型扩展时的不同初始化方法。结果表明CW2V的表现与更先进的技术相当甚至更优。此外，多元正态分布初始化等简单方法的性能与这些先进方法持平，这表明即使采用简单的初始化方法也能实现高效的大规模多语言持续预训练。相关代码已开源发布（https://github.com/AI4Bharat/VocabAdaptation_LLM/tree/CW2V）。

（注：根据学术论文翻译规范，对部分术语进行了标准化处理：
1. "convex hull"译为"凸包"（数学专业术语）
2. "multivariate initialization"译为"多元正态分布初始化"（结合上下文补充统计含义）
3. 保持"RoBERTa"和"LLaMA 2"等模型名称原文
4. 长难句按中文习惯拆分重组，如将"leading to..."独立成句译为"导致..."
5. 被动语态转换为主动句式，如"is commonly tackled by..."译为"通常采用..."）
