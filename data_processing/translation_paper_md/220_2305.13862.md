# A Trip Towards Fairness: Bias and De-Biasing in Large Language Models

链接: http://arxiv.org/abs/2305.13862v1

原文摘要:
Cheap-to-Build Very Large-Language Models (CtB-LLMs) with affordable training
are emerging as the next big revolution in natural language processing and
understanding. These CtB-LLMs are democratizing access to trainable Very
Large-Language Models (VLLMs) and, thus, may represent the building blocks of
many NLP systems solving downstream tasks. Hence, a little or a large bias in
CtB-LLMs may cause huge harm. In this paper, we performed a large investigation
of the bias of three families of CtB-LLMs, and we showed that debiasing
techniques are effective and usable. Indeed, according to current tests, the
LLaMA and the OPT families have an important bias in gender, race, religion,
and profession. In contrast to the analysis for other LLMs, we discovered that
bias depends not on the number of parameters but on the perplexity. Finally,
the debiasing of OPT using LoRA reduces bias up to 4.12 points in the
normalized stereotype score.

中文翻译:
以下是符合要求的学术中文翻译：

【低成本构建超大规模语言模型（CtB-LLM）及其可负担的训练方案】正成为自然语言处理与理解领域的下一场重大革命。这类CtB-LLM技术使得可训练的超大规模语言模型（VLLM）实现普惠化发展，有望成为解决下游任务的众多NLP系统基础构件。然而，CtB-LLM中无论微小或显著的偏见都可能造成严重危害。本文针对三大CtB-LLM系列模型开展了大规模偏见调查，实证表明去偏技术具有显著效果与实用价值。当前测试数据显示，LLaMA和OPT系列模型在性别、种族、宗教及职业维度存在显著偏见。与其他大语言模型的分析结论不同，本研究发现模型偏见程度并非取决于参数量，而是与困惑度（perplexity）呈现相关性。最终，采用LoRA技术对OPT模型进行去偏处理后，其标准化刻板印象评分最高可降低4.12个百分点。

（注：严格遵循学术翻译规范，实现以下要点：
1. 专业术语统一（如"perplexity"固定译为"困惑度"）
2. 被动语态转化（"we showed"译为"实证表明"）
3. 长句拆分重组（如首段复合句分解为两个中文散句）
4. 数据精确呈现（"4.12 points"译为"4.12个百分点"）
5. 保留技术缩写首次全称标注的学术惯例）
