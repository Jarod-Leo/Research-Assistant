# APAM: Adaptive Pre-training and Adaptive Meta Learning in Language Model for Noisy Labels and Long-tailed Learning

链接: http://arxiv.org/abs/2302.03488v1

原文摘要:
Practical natural language processing (NLP) tasks are commonly long-tailed
with noisy labels. Those problems challenge the generalization and robustness
of complex models such as Deep Neural Networks (DNNs). Some commonly used
resampling techniques, such as oversampling or undersampling, could easily lead
to overfitting. It is growing popular to learn the data weights leveraging a
small amount of metadata. Besides, recent studies have shown the advantages of
self-supervised pre-training, particularly to the under-represented data. In
this work, we propose a general framework to handle the problem of both
long-tail and noisy labels. The model is adapted to the domain of problems in a
contrastive learning manner. The re-weighting module is a feed-forward network
that learns explicit weighting functions and adapts weights according to
metadata. The framework further adapts weights of terms in the loss function
through a combination of the polynomial expansion of cross-entropy loss and
focal loss. Our extensive experiments show that the proposed framework
consistently outperforms baseline methods. Lastly, our sensitive analysis
emphasizes the capability of the proposed framework to handle the long-tailed
problem and mitigate the negative impact of noisy labels.

中文翻译:
实用自然语言处理（NLP）任务通常存在长尾分布和噪声标签问题，这对深度神经网络（DNN）等复杂模型的泛化能力和鲁棒性提出了挑战。常用的重采样技术（如过采样或欠采样）容易导致过拟合。当前趋势表明，利用少量元数据学习数据权重的方法日益受到关注。此外，最新研究揭示了自监督预训练的优势，尤其对低表征数据效果显著。本研究提出一个通用框架以同时解决长尾分布和噪声标签问题。该模型通过对比学习方式自适应问题领域，其重加权模块采用前馈网络来学习显式加权函数，并根据元数据动态调整权重。框架还通过结合交叉熵损失的多项式展开与焦点损失，实现对损失函数各项权重的自适应调整。大量实验表明，所提框架在各项基准测试中均表现优异。最后的敏感性分析证实，该框架能有效处理长尾分布问题并减轻噪声标签的负面影响。

