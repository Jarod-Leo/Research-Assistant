# What do Language Models know about word senses? Zero-Shot WSD with Language Models and Domain Inventories

链接: http://arxiv.org/abs/2302.03353v1

原文摘要:
Language Models are the core for almost any Natural Language Processing
system nowadays. One of their particularities is their contextualized
representations, a game changer feature when a disambiguation between word
senses is necessary. In this paper we aim to explore to what extent language
models are capable of discerning among senses at inference time. We performed
this analysis by prompting commonly used Languages Models such as BERT or
RoBERTa to perform the task of Word Sense Disambiguation (WSD). We leverage the
relation between word senses and domains, and cast WSD as a textual entailment
problem, where the different hypothesis refer to the domains of the word
senses. Our results show that this approach is indeed effective, close to
supervised systems.

中文翻译:
语言模型是当今几乎所有自然语言处理系统的核心组件。其独特优势在于能生成语境化表征，这一特性在需要消除词语歧义时具有颠覆性意义。本文旨在探究语言模型在推理阶段对词义辨别的能力边界。我们通过提示BERT、RoBERTa等常用语言模型执行词义消歧（WSD）任务展开分析，利用词义与领域之间的关联性，将WSD任务重构为文本蕴含问题——其中不同假设对应词语义项所属的领域。实验结果表明，该方法效果显著，性能接近监督式系统水平。

