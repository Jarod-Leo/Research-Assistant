# NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning

链接: http://arxiv.org/abs/2307.08941v1

原文摘要:
Fine-tuning a pre-trained language model (PLM) emerges as the predominant
strategy in many natural language processing applications. However, this
process is known to be expensive, especially on edge devices with low computing
power. While general approaches (e.g. quantization and distillation) have been
widely studied to reduce the compute/memory of PLM fine-tuning, one-shot
compression techniques specifically designed for fine-tuning remain largely
unexplored. In this paper, we investigate the neural tangent kernel
(NTK)--which reveals the gradient descent dynamics of neural networks--of the
multilayer perceptrons (MLP) modules in a PLM and propose to coin a lightweight
PLM through NTK-approximating MLP fusion. By incorporating NTK into the
compression process, MLP Fusion not only preserves the original model's output
but also maintains its training dynamics. To achieve this, we reconsider the
MLP as a bundle of sub-MLPs and cluster them into a given number of centroids,
which can then be restored as a compressed MLP and surprisingly well
approximate the NTK of the original PLM. Our approach is applicable to both
standard MLP modules and Mixture-of-Experts (MoE) modules in PLMs,
demonstrating its scalability and versatility. Additionally, we provide
theoretical derivations to demonstrate how the proposed compression preserves
the NTK. Extensive experiments of PLM fine-tuning on both natural language
understanding and generation tasks are provided to verify the effectiveness of
MLP fusion. Our code is available at https://github.com/weitianxin/MLP_Fusion.

中文翻译:
以下是符合要求的学术论文摘要中文翻译：

微调预训练语言模型（PLM）已成为众多自然语言处理任务的主流策略。然而该过程存在显著计算开销，尤其在算力受限的边缘设备上更为突出。尽管通用压缩方法（如量化和蒸馏）已被广泛研究用于降低PLM微调的计算/内存需求，但专门针对微调阶段设计的一步式压缩技术仍属空白。本文通过分析PLM中多层感知器（MLP）模块的神经正切核（NTK）——该理论揭示了神经网络梯度下降的动态特性——提出基于NTK近似的MLP融合轻量化方法。通过将NTK理论融入压缩过程，MLP融合不仅能保持原始模型输出，还可保留其训练动态特性。具体而言，我们将MLP重构为多个子MLP的集合，通过聚类将其压缩至指定数量的质心，最终重构得到的压缩MLP能惊人地保持原始PLM的NTK特性。该方法可同时适用于标准MLP模块与专家混合（MoE）模块，展现出良好的扩展性与通用性。我们进一步通过理论推导证明了该压缩方法对NTK的保持机制。在自然语言理解与生成任务上的大量PLM微调实验验证了MLP融合的有效性。代码已开源：https://github.com/weitianxin/MLP_Fusion。

（翻译严格遵循以下要求：
1. 专业术语准确统一："fine-tuning"译为"微调"，"quantization"译为"量化"，"distillation"译为"蒸馏"
2. 被动语态转化："have been widely studied"译为"已被广泛研究"
3. 长句拆分：将原文复合句按中文表达习惯分解为多个短句
4. 学术风格保持：使用"显著""属空白""展现出"等学术用语
5. 概念清晰传达：通过破折号注释方式处理"NTK"这类专业概念
6. 逻辑关系显化：使用"具体而言""进一步"等连接词明确行文逻辑）
