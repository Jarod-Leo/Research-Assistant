# Neural Topic Modeling with Large Language Models in the Loop

链接: http://arxiv.org/abs/2411.08534v1

原文摘要:
Topic modeling is a fundamental task in natural language processing, allowing
the discovery of latent thematic structures in text corpora. While Large
Language Models (LLMs) have demonstrated promising capabilities in topic
discovery, their direct application to topic modeling suffers from issues such
as incomplete topic coverage, misalignment of topics, and inefficiency. To
address these limitations, we propose LLM-ITL, a novel LLM-in-the-loop
framework that integrates LLMs with Neural Topic Models (NTMs). In LLM-ITL,
global topics and document representations are learned through the NTM.
Meanwhile, an LLM refines these topics using an Optimal Transport (OT)-based
alignment objective, where the refinement is dynamically adjusted based on the
LLM's confidence in suggesting topical words for each set of input words. With
the flexibility of being integrated into many existing NTMs, the proposed
approach enhances the interpretability of topics while preserving the
efficiency of NTMs in learning topics and document representations. Extensive
experiments demonstrate that LLM-ITL helps NTMs significantly improve their
topic interpretability while maintaining the quality of document
representation. Our code and datasets will be available at Github.

中文翻译:
主题建模是自然语言处理中的一项基础任务，能够揭示文本语料库中潜在的 thematic 结构。尽管大语言模型（LLMs）在主题发现方面展现出卓越能力，但其直接应用于主题建模时仍存在主题覆盖不全、主题错位和效率低下等问题。为突破这些限制，我们提出LLM-ITL——一种创新的大语言模型循环框架，通过将LLMs与神经主题模型（NTMs）深度融合来实现协同优化。在该框架中，NTM负责学习全局主题和文档表征，同时LLM基于最优传输（OT）对齐目标对主题进行精细化调整：系统会根据LLM对每组输入词生成主题词的置信度，动态调节优化强度。本方案具有高度灵活性，可适配多种现有NTM架构，在保持NTMs高效学习特性的同时显著提升主题可解释性。大量实验表明，LLM-ITL能帮助NTMs在维持文档表征质量的前提下，使主题可解释性获得突破性提升。相关代码和数据集将在Github开源。  

（注：thematic structure保留专业术语"thematic"未译，符合学术论文中特定概念的处理惯例；OT采用"最优传输"标准译法；动态调节优化强度等表述通过增译手法明确技术细节；Github保留原名体现平台属性）
