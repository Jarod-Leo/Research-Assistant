# Low-Cost Language Models: Survey and Performance Evaluation on Python Code Generation

链接: http://arxiv.org/abs/2404.11160v1

原文摘要:
Large Language Models (LLMs) have become a popular choice for many Natural
Language Processing (NLP) tasks due to their versatility and ability to produce
high-quality results. Specifically, they are increasingly used for automatic
code generation to help developers tackle repetitive coding tasks. However,
LLMs' substantial computational and memory requirements often make them
inaccessible to users with limited resources. This paper focuses on very
low-cost models which offer a more accessible alternative to resource-intensive
LLMs. We notably: (1) propose a thorough semi-manual evaluation of their
performance in generating Python code, (2) introduce a Chain-of-Thought (CoT)
prompting strategy to improve model reasoning and code quality, and (3) propose
a new dataset of 60 programming problems, with varied difficulty levels,
designed to extend existing benchmarks like HumanEval and EvalPlus. Our
findings show that some low-cost compatible models achieve competitive results
compared to larger models like ChatGPT despite using significantly fewer
resources. We will make our dataset and prompts publicly available to support
further research.

中文翻译:
大型语言模型（LLMs）因其多功能性和生成高质量结果的能力，已成为众多自然语言处理（NLP）任务的热门选择。特别是在自动代码生成领域，它们正被越来越多地用于帮助开发者处理重复性编码任务。然而，LLMs庞大的计算和内存需求往往使资源有限的用户难以企及。本文聚焦于极低成本模型，为资源密集型LLMs提供了一种更易获取的替代方案。我们重点实现了以下工作：（1）提出了一套详尽的半人工评估方法，用于衡量这些模型在生成Python代码时的性能；（2）引入思维链（Chain-of-Thought, CoT）提示策略以提升模型推理能力和代码质量；（3）构建包含60个编程问题的新数据集，问题难度梯度分明，旨在扩展HumanEval、EvalPlus等现有基准。研究结果表明，部分低成本兼容模型尽管资源消耗显著降低，其表现仍可与ChatGPT等大型模型媲美。我们将公开数据集和提示模板以支持后续研究。  

（注：翻译过程中进行了以下专业处理：  
1. 术语统一："Chain-of-Thought"采用学界通用译法"思维链"，"HumanEval/EvalPlus"保留原名  
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句，如将"notably"引导的列举项转换为中文分号结构  
3. 技术概念显化："semi-manual evaluation"译为"半人工评估"以突出人机协同特性  
4. 逻辑显性化：通过"旨在"等连接词明示数据集构建目的与现有基准的关系  
5. 被动语态转化：将"are increasingly used"等被动式转为中文主动表述）
