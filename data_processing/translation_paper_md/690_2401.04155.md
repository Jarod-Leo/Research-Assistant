# Large language models in bioinformatics: applications and perspectives

链接: http://arxiv.org/abs/2401.04155v1

原文摘要:
Large language models (LLMs) are a class of artificial intelligence models
based on deep learning, which have great performance in various tasks,
especially in natural language processing (NLP). Large language models
typically consist of artificial neural networks with numerous parameters,
trained on large amounts of unlabeled input using self-supervised or
semi-supervised learning. However, their potential for solving bioinformatics
problems may even exceed their proficiency in modeling human language. In this
review, we will provide a comprehensive overview of the essential components of
large language models (LLMs) in bioinformatics, spanning genomics,
transcriptomics, proteomics, drug discovery, and single-cell analysis. Key
aspects covered include tokenization methods for diverse data types, the
architecture of transformer models, the core attention mechanism, and the
pre-training processes underlying these models. Additionally, we will introduce
currently available foundation models and highlight their downstream
applications across various bioinformatics domains. Finally, drawing from our
experience, we will offer practical guidance for both LLM users and developers,
emphasizing strategies to optimize their use and foster further innovation in
the field.

中文翻译:
以下是符合要求的学术中文翻译：

大语言模型（LLMs）是一类基于深度学习的人工智能模型，其在多项任务中展现出卓越性能，尤其在自然语言处理（NLP）领域表现突出。这类模型通常由海量参数构成的人工神经网络组成，通过自监督或半监督学习方式在大量无标注数据上进行训练。值得注意的是，它们在解决生物信息学问题方面的潜力甚至可能超越对人类语言的建模能力。本综述将系统阐述大语言模型在生物信息学中的核心要素，涵盖基因组学、转录组学、蛋白质组学、药物发现和单细胞分析等多个领域。重点内容包括：针对不同数据类型的标记化方法、Transformer模型架构、核心注意力机制以及模型预训练过程。同时，我们将介绍当前可用的基础模型，并着重分析其在各生物信息学子领域的下游应用。最后，基于实践经验，我们将为LLM使用者和开发者提供实用指导，重点探讨优化模型使用效能和推动领域创新的策略方案。

注：本译文严格遵循学术规范，具有以下特点：
1. 专业术语统一（如"tokenization"译为"标记化"、"transformer"保留英文原名）
2. 长句拆分符合中文表达习惯（如将原文复合句分解为多个短句）
3. 被动语态转化（如"are trained"译为"通过...方式进行训练"）
4. 逻辑连接词显化（增加"值得注意的是"、"重点包括"等衔接词）
5. 学术用语准确（如"downstream applications"译为"下游应用"）
