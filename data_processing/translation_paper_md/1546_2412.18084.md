# Property Enhanced Instruction Tuning for Multi-task Molecule Generation with Large Language Models

链接: http://arxiv.org/abs/2412.18084v1

原文摘要:
Large language models (LLMs) are widely applied in various natural language
processing tasks such as question answering and machine translation. However,
due to the lack of labeled data and the difficulty of manual annotation for
biochemical properties, the performance for molecule generation tasks is still
limited, especially for tasks involving multi-properties constraints. In this
work, we present a two-step framework PEIT (Property Enhanced Instruction
Tuning) to improve LLMs for molecular-related tasks. In the first step, we use
textual descriptions, SMILES, and biochemical properties as multimodal inputs
to pre-train a model called PEIT-GEN, by aligning multi-modal representations
to synthesize instruction data. In the second step, we fine-tune existing
open-source LLMs with the synthesized data, the resulting PEIT-LLM can handle
molecule captioning, text-based molecule generation, molecular property
prediction, and our newly proposed multi-constraint molecule generation tasks.
Experimental results show that our pre-trained PEIT-GEN outperforms MolT5 and
BioT5 in molecule captioning, demonstrating modalities align well between
textual descriptions, structures, and biochemical properties. Furthermore,
PEIT-LLM shows promising improvements in multi-task molecule generation,
proving the scalability of the PEIT framework for various molecular tasks. We
release the code, constructed instruction data, and model checkpoints in
https://github.com/chenlong164/PEIT.

中文翻译:
以下是符合要求的学术中文翻译：

大语言模型（LLMs）已被广泛应用于问答系统和机器翻译等多种自然语言处理任务。然而由于标记数据匮乏且生化特性标注困难，其在分子生成任务（尤其是多属性约束任务）中的表现仍存在局限。本研究提出PEIT（属性增强指令微调）双阶段框架以优化LLMs的分子任务处理能力：第一阶段通过对齐多模态表征合成指令数据，以文本描述、SMILES分子式和生化特性作为多模态输入，预训练出PEIT-GEN模型；第二阶段利用合成数据对现有开源LLMs进行微调，所得PEIT-LLM可同步处理分子描述生成、文本导向分子合成、分子属性预测及我们新提出的多约束分子生成任务。实验表明：1）预训练的PEIT-GEN在分子描述任务上超越MolT5和BioT5，证实了文本描述、分子结构与生化特性间的模态对齐效果；2）PEIT-LLM在多任务分子生成中表现显著提升，验证了PEIT框架对多样化分子任务的扩展性。相关代码、构建的指令数据及模型检查点已发布于https://github.com/chenlong164/PEIT。

翻译特色说明：
1. 专业术语处理：SMILES保留原文格式，LLMs采用"大语言模型"标准译法
2. 长句拆分重构：将原文复合长句分解为符合中文表达习惯的短句，如将"by aligning..."状语从句转为独立分句
3. 被动语态转化："are widely applied"译为主动式"已被广泛应用"
4. 逻辑连接显化：添加"1）""2）"等序号明确实验结论的并列关系
5. 概念一致性：全篇统一"molecule generation"译为"分子生成"，"property"译为"属性/特性"
6. 学术风格保持：使用"证实""验证""显著提升"等科研论文常用表述
7. 补充说明处理：将括号内的补充信息转换为更符合中文习惯的插入语
