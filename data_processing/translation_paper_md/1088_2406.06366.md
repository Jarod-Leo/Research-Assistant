# Symmetric Dot-Product Attention for Efficient Training of BERT Language Models

链接: http://arxiv.org/abs/2406.06366v1

原文摘要:
Initially introduced as a machine translation model, the Transformer
architecture has now become the foundation for modern deep learning
architecture, with applications in a wide range of fields, from computer vision
to natural language processing. Nowadays, to tackle increasingly more complex
tasks, Transformer-based models are stretched to enormous sizes, requiring
increasingly larger training datasets, and unsustainable amount of compute
resources. The ubiquitous nature of the Transformer and its core component, the
attention mechanism, are thus prime targets for efficiency research. In this
work, we propose an alternative compatibility function for the self-attention
mechanism introduced by the Transformer architecture. This compatibility
function exploits an overlap in the learned representation of the traditional
scaled dot-product attention, leading to a symmetric with pairwise coefficient
dot-product attention. When applied to the pre-training of BERT-like models,
this new symmetric attention mechanism reaches a score of 79.36 on the GLUE
benchmark against 78.74 for the traditional implementation, leads to a
reduction of 6% in the number of trainable parameters, and reduces the number
of training steps required before convergence by half.

中文翻译:
最初作为机器翻译模型提出的Transformer架构，现已成为现代深度学习的基础框架，其应用领域涵盖计算机视觉到自然语言处理等诸多方向。当前，为应对日益复杂的任务，基于Transformer的模型规模不断膨胀，不仅需要越来越庞大的训练数据集，还消耗着不可持续的计算资源。因此，这种无处不在的Transformer架构及其核心组件——注意力机制，自然成为效率优化的重点研究对象。本研究提出了一种替代性的兼容性函数，用于改进Transformer架构中的自注意力机制。该函数通过利用传统缩放点积注意力在学习表征中的重叠特性，构建出具有成对系数的对称点积注意力机制。在类似BERT模型的预训练任务中，这种新型对称注意力机制在GLUE基准测试中达到79.36分（传统实现为78.74分），同时使可训练参数量减少6%，并将模型收敛所需的训练步数缩减一半。
