# FltLM: An Intergrated Long-Context Large Language Model for Effective Context Filtering and Understanding

链接: http://arxiv.org/abs/2410.06886v1

原文摘要:
The development of Long-Context Large Language Models (LLMs) has markedly
advanced natural language processing by facilitating the process of textual
data across long documents and multiple corpora. However, Long-Context LLMs
still face two critical challenges: The lost in the middle phenomenon, where
crucial middle-context information is likely to be missed, and the distraction
issue that the models lose focus due to overly extended contexts. To address
these challenges, we propose the Context Filtering Language Model (FltLM), a
novel integrated Long-Context LLM which enhances the ability of the model on
multi-document question-answering (QA) tasks. Specifically, FltLM innovatively
incorporates a context filter with a soft mask mechanism, identifying and
dynamically excluding irrelevant content to concentrate on pertinent
information for better comprehension and reasoning. Our approach not only
mitigates these two challenges, but also enables the model to operate
conveniently in a single forward pass. Experimental results demonstrate that
FltLM significantly outperforms supervised fine-tuning and retrieval-based
methods in complex QA scenarios, suggesting a promising solution for more
accurate and reliable long-context natural language understanding applications.

中文翻译:
**长文本大语言模型（LLMs）的发展**通过提升对长文档和多语料库文本数据的处理能力，显著推动了自然语言处理的进步。然而，当前长文本LLMs仍面临两大关键挑战：一是**中间信息丢失现象**（即模型容易忽略文本中部的关键信息），二是**注意力分散问题**（因上下文过长导致模型聚焦能力下降）。  

为解决这些问题，我们提出**上下文过滤语言模型（FltLM）**——一种新型集成式长文本LLM，可显著提升模型在多文档问答（QA）任务中的表现。FltLM创新性地引入带有软掩码机制的上下文过滤器，能够动态识别并屏蔽无关内容，使模型集中处理相关信息以增强理解与推理能力。该方法不仅有效缓解上述挑战，还能通过单次前向传播实现高效运算。实验表明，在复杂QA场景中，FltLM的表现显著优于监督微调和基于检索的方法，为长文本自然语言理解应用提供了更精准可靠的解决方案。  

（注：翻译严格遵循学术规范，术语如"lost in the middle"译为专业表述"中间信息丢失现象"，"soft mask mechanism"译为"软掩码机制"；长句按中文习惯拆分，被动语态转为主动表述；关键概念首次出现标注英文缩写，确保专业性。）
