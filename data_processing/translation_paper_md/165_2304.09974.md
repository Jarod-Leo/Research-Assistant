# SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery

链接: http://arxiv.org/abs/2304.09974v1

原文摘要:
Advances in GPT-based large language models (LLMs) are revolutionizing
natural language processing, exponentially increasing its use across various
domains. Incorporating uni-directional attention, these autoregressive LLMs can
generate long and coherent paragraphs. However, for visual question answering
(VQA) tasks that require both vision and language processing, models with
bi-directional attention or models employing fusion techniques are often
employed to capture the context of multiple modalities all at once. As GPT does
not natively process vision tokens, to exploit the advancements in GPT models
for VQA in robotic surgery, we design an end-to-end trainable Language-Vision
GPT (LV-GPT) model that expands the GPT2 model to include vision input (image).
The proposed LV-GPT incorporates a feature extractor (vision tokenizer) and
vision token embedding (token type and pose). Given the limitations of
unidirectional attention in GPT models and their ability to generate coherent
long paragraphs, we carefully sequence the word tokens before vision tokens,
mimicking the human thought process of understanding the question to infer an
answer from an image. Quantitatively, we prove that the LV-GPT model
outperforms other state-of-the-art VQA models on two publically available
surgical-VQA datasets (based on endoscopic vision challenge robotic scene
segmentation 2018 and CholecTriplet2021) and on our newly annotated dataset
(based on the holistic surgical scene dataset). We further annotate all three
datasets to include question-type annotations to allow sub-type analysis.
Furthermore, we extensively study and present the effects of token sequencing,
token type and pose embedding for vision tokens in the LV-GPT model.

中文翻译:
基于GPT的大型语言模型（LLM）的突破性进展正在彻底改变自然语言处理领域，其跨领域应用呈现指数级增长。这类自回归LLM采用单向注意力机制，能够生成连贯的长段落文本。然而，对于需要视觉与语言协同处理的视觉问答（VQA）任务，研究者通常采用双向注意力模型或特征融合技术来同时捕捉多模态上下文信息。由于GPT本身不具备视觉标记处理能力，为将GPT模型的先进特性应用于机器人手术VQA任务，我们设计了一个端到端可训练的语言-视觉GPT（LV-GPT）模型，通过扩展GPT2架构使其支持图像输入。该LV-GPT模型包含特征提取器（视觉标记生成器）和视觉标记嵌入（标记类型与位置编码）。针对GPT单向注意力的局限性及其长文本生成优势，我们通过精心设计标记序列——将文字标记置于视觉标记之前，模拟人类"理解问题后从图像推理答案"的认知流程。定量实验表明，在三个手术VQA数据集（基于2018年内窥镜视觉挑战赛机器人场景分割数据集、CholecTriplet2021数据集及我们新标注的全景手术场景数据集）上，LV-GPT模型性能均超越现有最优VQA模型。我们进一步为所有数据集添加问题类型标注以支持子类分析，并深入研究了视觉标记的序列编排、类型编码与位置嵌入对LV-GPT模型的影响机制。
