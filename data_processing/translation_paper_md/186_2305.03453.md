# T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering

链接: http://arxiv.org/abs/2305.03453v1

原文摘要:
Large Language Models (LLMs) have recently demonstrated exceptional
performance in various Natural Language Processing (NLP) tasks. They have also
shown the ability to perform chain-of-thought (CoT) reasoning to solve complex
problems. Recent studies have explored CoT reasoning in complex multimodal
scenarios, such as the science question answering task, by fine-tuning
multimodal models with high-quality human-annotated CoT rationales. However,
collecting high-quality COT rationales is usually time-consuming and costly.
Besides, the annotated rationales are hardly accurate due to the external
essential information missed. To address these issues, we propose a novel
method termed T-SciQ that aims at teaching science question answering with LLM
signals. The T-SciQ approach generates high-quality CoT rationales as teaching
signals and is advanced to train much smaller models to perform CoT reasoning
in complex modalities. Additionally, we introduce a novel data mixing strategy
to produce more effective teaching data samples for simple and complex science
question answer problems. Extensive experimental results show that our T-SciQ
method achieves a new state-of-the-art performance on the ScienceQA benchmark,
with an accuracy of 96.18%. Moreover, our approach outperforms the most
powerful fine-tuned baseline by 4.5%. The code is publicly available at
https://github.com/T-SciQ/T-SciQ.

中文翻译:
以下是符合学术规范的中文翻译：

大语言模型（LLMs）近期在各类自然语言处理（NLP）任务中展现出卓越性能，并表现出通过思维链（CoT）推理解决复杂问题的能力。最新研究通过使用高质量人工标注的CoT原理对多模态模型进行微调，探索了复杂多模态场景（如科学问答任务）中的CoT推理。然而，收集高质量的CoT原理通常耗时且成本高昂，且由于遗漏外部关键信息，标注原理往往难以保证准确性。针对这些问题，我们提出名为T-SciQ的创新方法，旨在利用LLM信号进行科学问答教学。该方法通过生成高质量CoT原理作为教学信号，并进一步训练更小规模的模型以执行复杂模态下的CoT推理。此外，我们引入了一种新颖的数据混合策略，可为简单和复杂科学问答问题生成更有效的教学数据样本。大量实验结果表明，我们的T-SciQ方法在ScienceQA基准测试中以96.18%的准确率创造了最新性能记录，相较性能最强的微调基线模型提升了4.5%。相关代码已开源：https://github.com/T-SciQ/T-SciQ。

（翻译说明：
1. 专业术语采用学术界通用译法，如"chain-of-thought reasoning"译为"思维链推理"
2. 被动语态转换为中文主动句式，如"have been explored"译为"探索了"
3. 长难句进行合理切分，如将原文最后两句拆分为三个中文短句
4. 保留技术术语首字母缩写（CoT/LLM）及专业平台名称（ScienceQA）的英文原名
5. 数字及百分比的表达符合中文科技论文规范
6. 项目网址等专有信息完整保留）
