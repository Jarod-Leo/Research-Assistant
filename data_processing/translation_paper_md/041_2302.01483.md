# SPADE: Self-supervised Pretraining for Acoustic DisEntanglement

链接: http://arxiv.org/abs/2302.01483v1

原文摘要:
Self-supervised representation learning approaches have grown in popularity
due to the ability to train models on large amounts of unlabeled data and have
demonstrated success in diverse fields such as natural language processing,
computer vision, and speech. Previous self-supervised work in the speech domain
has disentangled multiple attributes of speech such as linguistic content,
speaker identity, and rhythm. In this work, we introduce a self-supervised
approach to disentangle room acoustics from speech and use the acoustic
representation on the downstream task of device arbitration. Our results
demonstrate that our proposed approach significantly improves performance over
a baseline when labeled training data is scarce, indicating that our
pretraining scheme learns to encode room acoustic information while remaining
invariant to other attributes of the speech signal.

中文翻译:
自监督表征学习方法因能利用大量无标注数据训练模型而日益流行，并在自然语言处理、计算机视觉和语音等多个领域展现出卓越成效。既往语音领域的自监督研究已成功解耦语音的多种属性，如语言内容、说话人身份和节奏韵律。本研究提出一种自监督方法，旨在从语音信号中分离房间混响特征，并将所得声学表征应用于设备仲裁的下游任务。实验结果表明，在标注训练数据稀缺的情况下，本方法相较基线模型具有显著性能提升，这证明我们的预训练方案能有效编码房间声学信息，同时保持对语音信号其他属性的不变性。

