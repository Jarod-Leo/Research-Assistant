# Uncovering Latent Memories: Assessing Data Leakage and Memorization Patterns in Large Language Models

链接: http://arxiv.org/abs/2406.14549v1

原文摘要:
Frontier AI systems are making transformative impacts across society, but
such benefits are not without costs: models trained on web-scale datasets
containing personal and private data raise profound concerns about data privacy
and security. Language models are trained on extensive corpora including
potentially sensitive or proprietary information, and the risk of data leakage
- where the model response reveals pieces of such information - remains
inadequately understood. Prior work has investigated what factors drive
memorization and have identified that sequence complexity and the number of
repetitions drive memorization. Here, we focus on the evolution of memorization
over training. We begin by reproducing findings that the probability of
memorizing a sequence scales logarithmically with the number of times it is
present in the data. We next show that sequences which are apparently not
memorized after the first encounter can be "uncovered" throughout the course of
training even without subsequent encounters, a phenomenon we term "latent
memorization". The presence of latent memorization presents a challenge for
data privacy as memorized sequences may be hidden at the final checkpoint of
the model but remain easily recoverable. To this end, we develop a diagnostic
test relying on the cross entropy loss to uncover latent memorized sequences
with high accuracy.

中文翻译:
前沿人工智能系统正在对社会产生变革性影响，但这种效益并非没有代价：基于包含个人隐私数据的网络规模数据集训练的模型，引发了关于数据隐私与安全的深刻忧虑。语言模型通过包含潜在敏感或专有信息的海量语料库进行训练，而数据泄露风险（即模型响应暴露出此类信息片段）至今仍未得到充分认知。既往研究已探讨了驱动记忆效应的因素，发现序列复杂度和重复次数会促进记忆形成。本研究聚焦于记忆现象在训练过程中的动态演变。我们首先复现了关键发现：序列被记忆的概率与其在数据中出现次数呈对数关系。随后揭示了一个新现象——初次接触后看似未被记忆的序列，即便后续不再出现，也能在训练过程中被"唤醒"，我们称之为"潜在记忆"。这种潜在记忆的存在对数据隐私构成挑战，因为被记忆的序列可能在模型最终检查点处于隐匿状态，但仍可被轻易复原。为此，我们开发了一种基于交叉熵损失的诊断测试方法，能高精度地检测出潜在记忆序列。
