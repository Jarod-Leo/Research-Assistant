# Comparing Human-Centered Language Modeling: Is it Better to Model Groups, Individual Traits, or Both?

链接: http://arxiv.org/abs/2401.12492v1

原文摘要:
Pre-trained language models consider the context of neighboring words and
documents but lack any author context of the human generating the text.
However, language depends on the author's states, traits, social, situational,
and environmental attributes, collectively referred to as human context (Soni
et al., 2024). Human-centered natural language processing requires
incorporating human context into language models. Currently, two methods exist:
pre-training with 1) group-wise attributes (e.g., over-45-year-olds) or 2)
individual traits. Group attributes are simple but coarse -- not all
45-year-olds write the same way -- while individual traits allow for more
personalized representations, but require more complex modeling and data. It is
unclear which approach benefits what tasks. We compare pre-training models with
human context via 1) group attributes, 2) individual users, and 3) a combined
approach on five user- and document-level tasks. Our results show that there is
no best approach, but that human-centered language modeling holds avenues for
different methods.

中文翻译:
预训练语言模型能够考虑邻近词语及文档的上下文，却忽视了文本生成者——人类作者的相关背景。事实上，语言表达本质上受到作者状态、个性特征、社会关系、情境因素和环境属性等人类语境要素（统称为"人类上下文"，参见Soni等人2024年研究）的深刻影响。要实现以人为中心的自然语言处理，就必须将人类上下文整合到语言模型中。目前存在两种主要方法：1）基于群体属性（如45岁以上人群）的预训练；2）基于个体特征的预训练。群体属性方法虽简单但过于粗粒度——并非所有45岁人群都有相同的表达方式；而个体特征虽能实现更个性化的表征，却需要更复杂的建模和数据支持。这两种方法在不同任务中的优劣尚不明确。本研究通过五种用户及文档级任务，对比了三种预训练方式：1）群体属性注入、2）个体用户建模、3）混合方法。结果表明不存在普适的最佳方案，但以人为中心的语言建模确实为不同方法提供了发展空间。

（翻译说明：
1. 专业术语处理："human context"译为"人类上下文"并保留原文文献标注
2. 长句拆分：将原文复合句按中文表达习惯分解为多个短句
3. 被动语态转换："it is unclear"转为主动句式"优劣尚不明确"
4. 概念显化："holds avenues for different methods"意译为"为不同方法提供了发展空间"
5. 数字规范：保持"45岁"等数据表述与原文一致
6. 学术风格：使用"表征""建模"等符合计算机领域术语体系的表达）
