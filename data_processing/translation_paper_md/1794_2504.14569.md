# NoWag: A Unified Framework for Shape Preserving Compression of Large Language Models

链接: http://arxiv.org/abs/2504.14569v1

原文摘要:
Large language models (LLMs) exhibit remarkable performance across various
natural language processing tasks but suffer from immense computational and
memory demands, limiting their deployment in resource-constrained environments.
To address this challenge, we propose NoWag: (Normalized Weight and Activation
Guided Compression), a unified framework for zero-shot shape preserving
compression algorithms. We compressed Llama-2 7B/13B/70B and Llama-3 8/70BB
models, using two popular forms of shape-preserving compression, vector
quantization NoWag-VQ (NoWag for Vector Quantization), and
unstructured/semi-structured pruning NoWag-P (NoWag for Pruning). We found that
NoWag-VQ significantly outperforms state-of-the-art zero shot VQ, and that
NoWag-P performs competitively against state-of-the-art methods. These results
suggest commonalities between these compression paradigms that could inspire
future work. Our code is available at https://github.com/LawrenceRLiu/NoWag

中文翻译:
以下是符合要求的学术中文翻译：

大语言模型（LLMs）在各类自然语言处理任务中展现出卓越性能，但其巨大的计算与内存需求限制了在资源受限环境中的部署。为应对这一挑战，我们提出NoWag框架（标准化权重与激活引导的压缩技术），这是一种零样本形状保持压缩算法的统一框架。我们采用两种主流形状保持压缩方法——向量量化（NoWag-VQ）和非结构化/半结构化剪枝（NoWag-P），对Llama-2 7B/13B/70B及Llama-3 8B/70B模型进行压缩。实验表明：NoWag-VQ在零样本向量量化方面显著优于当前最优方法，NoWag-P与前沿剪枝方法相比也具竞争力。这些发现揭示了不同压缩范式间的共性特征，可为未来研究提供启示。项目代码已开源：https://github.com/LawrenceRLiu/NoWag

（注：根据学术规范，关键术语首次出现时保留英文缩写并标注全称；模型名称Llama-2/3保持原格式；技术术语如"zero-shot"译为"零样本"符合NLP领域惯例；长句按中文习惯切分为短句；被动语态转换为主动表述；URL信息完整保留）
