# Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models

链接: http://arxiv.org/abs/2502.13533v1

原文摘要:
Large Language Models (LLMs) have significantly advanced natural language
processing with exceptional task generalization capabilities. Low-Rank Adaption
(LoRA) offers a cost-effective fine-tuning solution, freezing the original
model parameters and training only lightweight, low-rank adapter matrices.
However, the memory footprint of LoRA is largely dominated by the original
model parameters. To mitigate this, we propose LoRAM, a memory-efficient LoRA
training scheme founded on the intuition that many neurons in
over-parameterized LLMs have low training utility but are essential for
inference. LoRAM presents a unique twist: it trains on a pruned (small) model
to obtain pruned low-rank matrices, which are then recovered and utilized with
the original (large) model for inference. Additionally, minimal-cost continual
pre-training, performed by the model publishers in advance, aligns the
knowledge discrepancy between pruned and original models. Our extensive
experiments demonstrate the efficacy of LoRAM across various pruning strategies
and downstream tasks. For a model with 70 billion parameters, LoRAM enables
training on a GPU with only 20G HBM, replacing an A100-80G GPU for LoRA
training and 15 GPUs for full fine-tuning. Specifically, QLoRAM implemented by
structured pruning combined with 4-bit quantization, for LLaMA-3.1-70B
(LLaMA-2-70B), reduces the parameter storage cost that dominates the memory
usage in low-rank matrix training by 15.81$\times$ (16.95$\times$), while
achieving dominant performance gains over both the original LLaMA-3.1-70B
(LLaMA-2-70B) and LoRA-trained LLaMA-3.1-8B (LLaMA-2-13B). Code is available at
https://github.com/junzhang-zj/LoRAM.

中文翻译:
以下是符合要求的学术中文翻译：

大型语言模型（LLMs）凭借卓越的任务泛化能力显著推动了自然语言处理的发展。低秩自适应（LoRA）提供了一种经济高效的微调方案，通过冻结原始模型参数仅训练轻量化的低秩适配矩阵。然而LoRA的内存占用仍主要受原始模型参数支配。为此，我们提出LoRAM——一种基于"过参数化LLMs中大量神经元具有低训练效用但推理必需"这一洞见的内存高效LoRA训练方案。LoRAM的创新在于：在剪枝后的小模型上训练获得剪枝低秩矩阵，随后将其恢复并与原始大模型共同用于推理。此外，模型发布方可预先进行极小成本持续预训练，以消除剪枝模型与原始模型间的知识差异。大量实验证明LoRAM在不同剪枝策略和下游任务中均具优越性。对于700亿参数模型，LoRAM仅需20G HBM显存的GPU即可完成训练，替代了LoRA训练所需的A100-80G显卡和全参数微调所需的15块GPU。具体而言，QLoRAM通过结构化剪枝结合4-bit量化，在LLaMA-3.1-70B（LLaMA-2-70B）上将主导低秩矩阵训练内存消耗的参数存储成本降低15.81倍（16.95倍），同时性能显著超越原始LLaMA-3.1-70B（LLaMA-2-70B）和经LoRA训练的LLaMA-3.1-8B（LLaMA-2-13B）。代码已开源：https://github.com/junzhang-zj/LoRAM。

（注：译文严格遵循以下规范：
1. 专业术语统一（如LoRA/LoRAM不译，LLMs保留英文缩写）
2. 被动语态转换（如"are frozen"译为"通过冻结"）
3. 长句拆分重组（如将复合从句分解为多个短句）
4. 数字格式标准化（保持阿拉伯数字与倍数表示）
5. 学术表达（使用"泛化能力""微调方案"等术语）
6. 技术细节准确传达（如HBM显存、结构化剪枝等））
