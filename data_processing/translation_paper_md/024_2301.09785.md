# Transformer-Patcher: One Mistake worth One Neuron

链接: http://arxiv.org/abs/2301.09785v1

原文摘要:
Large Transformer-based Pretrained Language Models (PLMs) dominate almost all
Natural Language Processing (NLP) tasks. Nevertheless, they still make mistakes
from time to time. For a model deployed in an industrial environment, fixing
these mistakes quickly and robustly is vital to improve user experiences.
Previous works formalize such problems as Model Editing (ME) and mostly focus
on fixing one mistake. However, the one-mistake-fixing scenario is not an
accurate abstraction of the real-world challenge. In the deployment of AI
services, there are ever-emerging mistakes, and the same mistake may recur if
not corrected in time. Thus a preferable solution is to rectify the mistakes as
soon as they appear nonstop. Therefore, we extend the existing ME into
Sequential Model Editing (SME) to help develop more practical editing methods.
Our study shows that most current ME methods could yield unsatisfying results
in this scenario. We then introduce Transformer-Patcher, a novel model editor
that can shift the behavior of transformer-based models by simply adding and
training a few neurons in the last Feed-Forward Network layer. Experimental
results on both classification and generation tasks show that
Transformer-Patcher can successively correct up to thousands of errors
(Reliability) and generalize to their equivalent inputs (Generality) while
retaining the model's accuracy on irrelevant inputs (Locality). Our method
outperforms previous fine-tuning and HyperNetwork-based methods and achieves
state-of-the-art performance for Sequential Model Editing (SME). The code is
available at https://github.com/ZeroYuHuang/Transformer-Patcher.

中文翻译:
基于Transformer的大型预训练语言模型（PLM）在自然语言处理（NLP）领域占据主导地位，但仍会不时产生错误。对于工业级部署的模型而言，快速稳健地修正这些错误对提升用户体验至关重要。现有研究将此类问题形式化为"模型编辑"（Model Editing, ME），但主要聚焦于单一错误的修正。然而，单次纠错场景并不能准确反映现实挑战——AI服务部署中错误会持续涌现，若未及时修正，同类错误可能反复发生。因此更理想的解决方案是实时持续修正错误。为此，我们将现有ME框架扩展为"序列化模型编辑"（Sequential Model Editing, SME），以推动更具实用性的编辑方法研发。研究发现，当前多数ME方法在此场景下表现欠佳。我们提出创新模型编辑器Transformer-Patcher，其仅需在Transformer最后一层前馈网络中添加并训练少量神经元即可改变模型行为。在分类和生成任务的实验表明，该方法能连续修正数千个错误（可靠性），泛化至等效输入（泛化性），同时保持模型对无关输入的原有精度（局部性）。相较基于微调和超网络的方法，本方案在SME任务中取得最先进性能。代码已开源：https://github.com/ZeroYuHuang/Transformer-Patcher。

