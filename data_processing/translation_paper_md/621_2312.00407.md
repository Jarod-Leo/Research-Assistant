# CoLLiE: Collaborative Training of Large Language Models in an Efficient Way

链接: http://arxiv.org/abs/2312.00407v1

原文摘要:
Large language models (LLMs) are increasingly pivotal in a wide range of
natural language processing tasks. Access to pre-trained models, courtesy of
the open-source community, has made it possible to adapt these models to
specific applications for enhanced performance. However, the substantial
resources required for training these models necessitate efficient solutions.
This paper introduces CoLLiE, an efficient library that facilitates
collaborative training of large language models using 3D parallelism,
parameter-efficient fine-tuning (PEFT) methods, and optimizers such as Lion,
Adan, Sophia, LOMO and AdaLomo. With its modular design and comprehensive
functionality, CoLLiE offers a balanced blend of efficiency, ease of use, and
customization. CoLLiE has proven superior training efficiency in comparison
with prevalent solutions in pre-training and fine-tuning scenarios.
Furthermore, we provide an empirical evaluation of the correlation between
model size and GPU memory consumption under different optimization methods, as
well as an analysis of the throughput. Lastly, we carry out a comprehensive
comparison of various optimizers and PEFT methods within the instruction-tuning
context. CoLLiE is available at https://github.com/OpenLMLab/collie.

中文翻译:
以下是符合要求的学术中文翻译：

大语言模型（LLMs）在自然语言处理任务中正发挥日益关键的作用。开源社区提供的预训练模型使得针对特定应用场景的模型适配与性能提升成为可能，然而其训练过程所需的庞大计算资源亟需高效解决方案。本文提出CoLLiE——一个基于三维并行策略、参数高效微调（PEFT）方法以及Lion/Adan/Sophia/LOMO/AdaLomo等优化器的高效大模型协同训练框架。该库采用模块化设计，在训练效率、易用性和可定制性之间实现了良好平衡。实验表明，CoLLiE在预训练和微调场景下均展现出优于主流方案的训练效率。此外，我们通过实证研究量化了不同优化方法下模型规模与GPU显存占用的关联性，并进行了吞吐量分析。最后，我们在指令微调场景下对各类优化器与PEFT方法进行了系统性对比。CoLLiE已开源发布于https://github.com/OpenLMLab/collie。

（翻译严格遵循以下原则：
1. 专业术语准确统一（如3D parallelism→三维并行策略，PEFT→参数高效微调）
2. 被动语态转换（"has been proven"→"实验表明"）
3. 长句拆分重组（将原文复合句按中文表达习惯分解为多个短句）
4. 学术风格保持（使用"亟需""量化""系统性"等学术用语）
5. 重要概念首次出现标注英文缩写
6. 链接信息完整保留）
