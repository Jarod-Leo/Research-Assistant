# LLM2KB: Constructing Knowledge Bases using instruction tuned context aware Large Language Models

链接: http://arxiv.org/abs/2308.13207v1

原文摘要:
The advent of Large Language Models (LLM) has revolutionized the field of
natural language processing, enabling significant progress in various
applications. One key area of interest is the construction of Knowledge Bases
(KB) using these powerful models. Knowledge bases serve as repositories of
structured information, facilitating information retrieval and inference tasks.
Our paper proposes LLM2KB, a system for constructing knowledge bases using
large language models, with a focus on the Llama 2 architecture and the
Wikipedia dataset. We perform parameter efficient instruction tuning for
Llama-2-13b-chat and StableBeluga-13B by training small injection models that
have only 0.05 % of the parameters of the base models using the Low Rank
Adaptation (LoRA) technique. These injection models have been trained with
prompts that are engineered to utilize Wikipedia page contexts of subject
entities fetched using a Dense Passage Retrieval (DPR) algorithm, to answer
relevant object entities for a given subject entity and relation. Our best
performing model achieved an average F1 score of 0.6185 across 21 relations in
the LM-KBC challenge held at the ISWC 2023 conference.

中文翻译:
大型语言模型（LLM）的出现为自然语言处理领域带来了革命性变革，推动各类应用取得显著进展。其中，利用这类强大模型构建知识库（KB）成为关键研究方向。知识库作为结构化信息的存储库，能够有效支持信息检索与推理任务。本文提出LLM2KB系统，专注于基于Llama 2架构和维基百科数据集的大模型知识库构建方案。我们采用参数高效指令微调策略，通过低秩自适应（LoRA）技术为Llama-2-13b-chat和StableBeluga-13B训练仅含基础模型0.05%参数的小型注入模型。这些注入模型使用经过特殊设计的提示模板进行训练，该模板能结合基于稠密段落检索（DPR）算法获取的主题实体维基百科上下文，针对给定主题实体与关系生成相关客体实体。在ISWC 2023会议举办的LM-KBC挑战赛中，我们的最佳模型在21个关系类别上平均F1值达到0.6185。
