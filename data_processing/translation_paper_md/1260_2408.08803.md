# Leveraging FourierKAN Classification Head for Pre-Trained Transformer-based Text Classification

链接: http://arxiv.org/abs/2408.08803v1

原文摘要:
In resource constraint settings, adaptation to downstream classification
tasks involves fine-tuning the final layer of a classifier (i.e. classification
head) while keeping rest of the model weights frozen. Multi-Layer Perceptron
(MLP) heads fine-tuned with pre-trained transformer backbones have long been
the de facto standard for text classification head fine-tuning. However, the
fixed non-linearity of MLPs often struggles to fully capture the nuances of
contextual embeddings produced by pre-trained models, while also being
computationally expensive. In our work, we investigate the efficacy of KAN and
its variant, Fourier KAN (FR-KAN), as alternative text classification heads.
Our experiments reveal that FR-KAN significantly outperforms MLPs with an
average improvement of 10% in accuracy and 11% in F1-score across seven
pre-trained transformer models and four text classification tasks. Beyond
performance gains, FR-KAN is more computationally efficient and trains faster
with fewer parameters. These results underscore the potential of FR-KAN to
serve as a lightweight classification head, with broader implications for
advancing other Natural Language Processing (NLP) tasks.

中文翻译:
在资源受限的环境中，下游分类任务的适配通常采用固定模型其余权重、仅微调分类器最后一层（即分类头）的策略。长期以来，基于预训练Transformer架构的多层感知机（MLP）分类头微调一直是文本分类任务的事实标准。然而，MLP固有的固定非线性特性往往难以充分捕捉预训练模型生成的上下文嵌入的细微差异，同时存在计算成本较高的问题。本研究探讨了KAN及其变体傅里叶KAN（FR-KAN）作为替代性文本分类头的有效性。实验表明，在七种预训练Transformer模型和四项文本分类任务中，FR-KAN以平均10%的准确率提升和11%的F1分数提升显著优于MLP。除性能优势外，FR-KAN还具有更高的计算效率，能以更少的参数实现更快的训练速度。这些结果凸显了FR-KAN作为轻量级分类头的潜力，对推动其他自然语言处理（NLP）任务的发展具有更广泛的启示意义。

（翻译说明：采用学术论文的规范表述方式，对专业术语如"fine-tuning"统一译为"微调"；将被动语态转换为中文主动句式；对长难句进行合理切分；保留"Transformer/MLP/KAN"等专业缩写首次出现时的全称；通过"即"字结构处理英文括号中的解释性内容；使用"本研究/实验表明"等符合中文论文摘要习惯的表述。）
