# A Heterogeneous Chiplet Architecture for Accelerating End-to-End Transformer Models

链接: http://arxiv.org/abs/2312.11750v1

原文摘要:
Transformers have revolutionized deep learning and generative modeling,
enabling advancements in natural language processing tasks. However, the size
of transformer models is increasing continuously, driven by enhanced
capabilities across various deep learning tasks. This trend of ever-increasing
model size has given rise to new challenges in terms of memory and compute
requirements. Conventional computing platforms, including GPUs, suffer from
suboptimal performance due to the memory demands imposed by models with
millions/billions of parameters. The emerging chiplet-based platforms provide a
new avenue for compute- and data-intensive machine learning (ML) applications
enabled by a Network-on-Interposer (NoI). However, designing suitable hardware
accelerators for executing Transformer inference workloads is challenging due
to a wide variety of complex computing kernels in the Transformer architecture.
In this paper, we leverage chiplet-based heterogeneous integration (HI) to
design a high-performance and energy-efficient multi-chiplet platform to
accelerate transformer workloads. We demonstrate that the proposed NoI
architecture caters to the data access patterns inherent in a transformer
model. The optimized placement of the chiplets and the associated NoI links and
routers enable superior performance compared to the state-of-the-art hardware
accelerators. The proposed NoI-based architecture demonstrates scalability
across varying transformer models and improves latency and energy efficiency by
up to 11.8x and 2.36x, respectively when compared with the existing
state-of-the-art architecture HAIMA.

中文翻译:
Transformer模型已经彻底改变了深度学习和生成式建模领域，推动了自然语言处理任务的发展。然而随着各类深度学习任务性能需求的提升，Transformer模型的规模持续扩大，这种不断增长的模型趋势引发了内存和计算资源需求方面的新挑战。由于百万/十亿级参数模型的内存需求，包括GPU在内的传统计算平台都面临着性能瓶颈。基于小芯片（chiplet）的新兴平台通过中介层网络（NoI）为计算密集型和数据密集型的机器学习应用提供了新途径。然而，由于Transformer架构中包含多种复杂计算核心，为其设计合适的硬件加速器仍具挑战性。本文利用基于小芯片的异质集成技术（HI），设计了一个高性能、高能效的多小芯片平台来加速Transformer工作负载。我们证明所提出的NoI架构能有效适配Transformer模型固有的数据访问模式。通过优化小芯片布局及相应的NoI链路与路由器配置，该方案相比现有最先进的硬件加速器展现出更优异的性能。与当前最优架构HAIMA相比，这种基于NoI的架构在不同规模Transformer模型上均表现出良好的可扩展性，延迟和能效分别最高提升11.8倍和2.36倍。

（注：根据学术文献翻译规范，对部分术语进行了标准化处理：
1. "chiplet"统一译为"小芯片"（业界通用译法）
2. "Network-on-Interposer"采用"中介层网络"的学术标准译法
3. 保持"Transformer"作为专业术语不翻译
4. 技术指标"11.8x/2.36x"译为"11.8倍/2.36倍"符合中文科技文献表述习惯
5. 复杂长句按中文表达习惯进行了合理切分，如将"due to..."因果状语从句转换为前置分句）
