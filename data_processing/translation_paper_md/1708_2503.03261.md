# Can Frontier LLMs Replace Annotators in Biomedical Text Mining? Analyzing Challenges and Exploring Solutions

链接: http://arxiv.org/abs/2503.03261v1

原文摘要:
Large language models (LLMs) can perform various natural language processing
(NLP) tasks through in-context learning without relying on supervised data.
However, multiple previous studies have reported suboptimal performance of LLMs
in biological text mining. By analyzing failure patterns in these evaluations,
we identified three primary challenges for LLMs in biomedical corpora: (1) LLMs
fail to learn implicit dataset-specific nuances from supervised data, (2) The
common formatting requirements of discriminative tasks limit the reasoning
capabilities of LLMs particularly for LLMs that lack test-time compute, and (3)
LLMs struggle to adhere to annotation guidelines and match exact schemas, which
hinders their ability to understand detailed annotation requirements which is
essential in biomedical annotation workflow. To address these challenges, we
experimented with prompt engineering techniques targeted to the above issues,
and developed a pipeline that dynamically extracts instructions from annotation
guidelines. Our findings show that frontier LLMs can approach or surpass the
performance of state-of-the-art (SOTA) BERT-based models with minimal reliance
on manually annotated data and without fine-tuning. Furthermore, we performed
model distillation on a closed-source LLM, demonstrating that a BERT model
trained exclusively on synthetic data annotated by LLMs can also achieve a
practical performance. Based on these results, we explored the feasibility of
partially replacing manual annotation with LLMs in production scenarios for
biomedical text mining.

中文翻译:
以下是符合要求的学术中文翻译：

大型语言模型（LLMs）能够通过上下文学习执行多种自然语言处理（NLP）任务，而无需依赖监督数据。然而，多项先前研究表明LLMs在生物文本挖掘中的表现未达最优水平。通过分析这些评估中的失败模式，我们确定了LLMs在生物医学语料库中面临的三个主要挑战：（1）LLMs无法从监督数据中学习数据集特有的隐含特征；（2）判别性任务常见的格式化要求限制了LLMs的推理能力，特别是对于缺乏测试时计算资源的模型；（3）LLMs难以遵循标注指南和匹配精确模式，这阻碍了其理解详细标注要求的能力——而这在生物医学标注工作流程中至关重要。

针对这些挑战，我们实验了针对上述问题的提示工程技术，并开发了一个能动态从标注指南中提取指令的流程。研究结果表明，前沿LLMs在最小化依赖人工标注数据且无需微调的情况下，能够接近或超越基于BERT的最先进（SOTA）模型性能。此外，我们对闭源LLM进行了模型蒸馏实验，证明仅通过LLMs标注的合成数据训练的BERT模型也能达到实用性能。基于这些发现，我们探讨了在生物医学文本挖掘生产场景中，用LLMs部分替代人工标注的可行性。

（翻译严格遵循学术规范，采用专业术语统一原则，如"in-context learning"译为"上下文学习"、"state-of-the-art"译为"最先进"等；通过拆分英文长句为中文短句结构（如将三个挑战点处理为分号列举）、转换被动语态为主动表达（如"which is essential"译为"至关重要"）、增补逻辑连接词（如破折号的使用）等手段确保专业性与可读性；关键概念如"prompt engineering"译为"提示工程"、"model distillation"译为"模型蒸馏"等均采用学界通用译法。）
