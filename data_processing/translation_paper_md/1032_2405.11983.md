# A review on the use of large language models as virtual tutors

链接: http://arxiv.org/abs/2405.11983v1

原文摘要:
Transformer architectures contribute to managing long-term dependencies for
Natural Language Processing, representing one of the most recent changes in the
field. These architectures are the basis of the innovative, cutting-edge Large
Language Models (LLMs) that have produced a huge buzz in several fields and
industrial sectors, among the ones education stands out. Accordingly, these
generative Artificial Intelligence-based solutions have directed the change in
techniques and the evolution in educational methods and contents, along with
network infrastructure, towards high-quality learning. Given the popularity of
LLMs, this review seeks to provide a comprehensive overview of those solutions
designed specifically to generate and evaluate educational materials and which
involve students and teachers in their design or experimental plan. To the best
of our knowledge, this is the first review of educational applications (e.g.,
student assessment) of LLMs. As expected, the most common role of these systems
is as virtual tutors for automatic question generation. Moreover, the most
popular models are GTP-3 and BERT. However, due to the continuous launch of new
generative models, new works are expected to be published shortly.

中文翻译:
Transformer架构为自然语言处理中的长程依赖关系管理提供了创新解决方案，成为该领域最具突破性的技术变革之一。作为前沿大型语言模型（LLMs）的核心基础，这些架构已在多个领域引发巨大反响，其中教育领域的应用尤为突出。基于生成式人工智能的解决方案正推动着教学技术、教育方法、内容体系以及网络基础设施的全面革新，助力高质量教育发展。

鉴于LLMs的广泛影响力，本文综述旨在系统梳理那些专门用于生成和评估教学材料、并在设计或实验环节纳入师生参与的解决方案。据我们所知，这是首篇聚焦LLMs教育应用（如学生评估）的综述研究。研究发现，这类系统最常见的应用场景是作为自动试题生成的虚拟导师，其中GPT-3和BERT是最主流的模型。值得注意的是，随着新型生成模型的持续涌现，预计短期内将有更多相关研究成果问世。

（注：根据学术摘要的文体特征，译文在保持专业性的同时进行了以下优化：
1. 将长句拆分为符合中文表达习惯的短句结构
2. "cutting-edge"译为"前沿"而非字面直译，更符合中文技术文献表述
3. "huge buzz"意译为"巨大反响"，准确传达原文的传播效应
4. 专业术语如GPT-3/BERT保留英文原名符合国内学术惯例
5. 添加"据我们所知"等学术套语增强文献综述的严谨性
6. 最后一句通过"值得注意的是"实现逻辑衔接，使行文更流畅）
