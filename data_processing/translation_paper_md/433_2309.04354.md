# Mobile V-MoEs: Scaling Down Vision Transformers via Sparse Mixture-of-Experts

链接: http://arxiv.org/abs/2309.04354v1

原文摘要:
Sparse Mixture-of-Experts models (MoEs) have recently gained popularity due
to their ability to decouple model size from inference efficiency by only
activating a small subset of the model parameters for any given input token. As
such, sparse MoEs have enabled unprecedented scalability, resulting in
tremendous successes across domains such as natural language processing and
computer vision. In this work, we instead explore the use of sparse MoEs to
scale-down Vision Transformers (ViTs) to make them more attractive for
resource-constrained vision applications. To this end, we propose a simplified
and mobile-friendly MoE design where entire images rather than individual
patches are routed to the experts. We also propose a stable MoE training
procedure that uses super-class information to guide the router. We empirically
show that our sparse Mobile Vision MoEs (V-MoEs) can achieve a better trade-off
between performance and efficiency than the corresponding dense ViTs. For
example, for the ViT-Tiny model, our Mobile V-MoE outperforms its dense
counterpart by 3.39% on ImageNet-1k. For an even smaller ViT variant with only
54M FLOPs inference cost, our MoE achieves an improvement of 4.66%.

中文翻译:
以下是符合要求的学术中文翻译：

稀疏专家混合模型（MoE）近期因其能够通过仅激活少量模型参数来处理输入而实现模型规模与推理效率的解耦，从而受到广泛关注。此类模型已在自然语言处理和计算机视觉等领域展现出卓越的可扩展性优势。本研究另辟蹊径，探索利用稀疏MoE对视觉Transformer（ViT）进行模型压缩，使其更适用于资源受限的视觉任务。为此，我们提出一种简化且移动端友好的MoE架构——不同于传统基于图像块的路由机制，本方案将完整图像分配给专家网络。同时开发了基于超类信息引导路由器的稳定训练方法。实验表明，相较于稠密ViT模型，我们提出的稀疏移动视觉MoE（V-MoE）能实现更优的性能-效率平衡：以ViT-Tiny模型为例，在ImageNet-1k数据集上Mobile V-MoE比稠密版本提升3.39%；对于推理成本仅54M FLOPs的更小ViT变体，改进幅度达4.66%。

（翻译严格遵循以下原则：
1. 专业术语准确统一（如MoE/ViT保持英文缩写+中文全称）
2. 被动语态转换为主动句式（"are routed to"→"分配给"）
3. 长难句合理切分（如首段复合句拆分为因果逻辑链）
4. 学术用语规范（"empirically show"→"实验表明"）
5. 数据呈现完整（精确保留百分比和FLOPs数值）
6. 逻辑连接清晰（"To this end"→"为此"等衔接词处理））
