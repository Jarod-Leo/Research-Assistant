# ULLME: A Unified Framework for Large Language Model Embeddings with Generation-Augmented Learning

链接: http://arxiv.org/abs/2408.03402v1

原文摘要:
Large Language Models (LLMs) excel in various natural language processing
tasks, but leveraging them for dense passage embedding remains challenging.
This is due to their causal attention mechanism and the misalignment between
their pre-training objectives and the text ranking tasks. Despite some recent
efforts to address these issues, existing frameworks for LLM-based text
embeddings have been limited by their support for only a limited range of LLM
architectures and fine-tuning strategies, limiting their practical application
and versatility. In this work, we introduce the Unified framework for Large
Language Model Embedding (ULLME), a flexible, plug-and-play implementation that
enables bidirectional attention across various LLMs and supports a range of
fine-tuning strategies. We also propose Generation-augmented Representation
Learning (GRL), a novel fine-tuning method to boost LLMs for text embedding
tasks. GRL enforces consistency between representation-based and
generation-based relevance scores, leveraging LLMs' powerful generative
abilities for learning passage embeddings. To showcase our framework's
flexibility and effectiveness, we release three pre-trained models from ULLME
with different backbone architectures, ranging from 1.5B to 8B parameters, all
of which demonstrate strong performance on the Massive Text Embedding
Benchmark. Our framework is publicly available at:
https://github.com/nlp-uoregon/ullme. A demo video for ULLME can also be found
at https://rb.gy/ws1ile.

中文翻译:
以下是符合您要求的中文翻译：

大型语言模型（LLMs）在各类自然语言处理任务中表现卓越，但将其应用于密集段落嵌入仍面临挑战。这主要归因于其因果注意力机制，以及模型预训练目标与文本排序任务之间的不匹配性。尽管近期已有研究尝试解决这些问题，但现有基于LLM的文本嵌入框架普遍存在局限性：仅支持有限范围的LLM架构和微调策略，制约了其实用性和多功能性。

本研究提出统一化大型语言模型嵌入框架（ULLME），该即插即用式灵活解决方案实现了跨LLM模型的双向注意力机制，并支持多种微调策略。我们同时创新性地提出生成增强表示学习法（GRL），这种新型微调方法通过强制表示相关性与生成相关性分数的一致性，利用LLMs强大的生成能力来优化段落嵌入学习。

为验证框架的灵活性与有效性，我们发布了三个基于不同主干架构的ULLME预训练模型（参数量1.5B至8B），这些模型在Massive文本嵌入基准测试中均展现出强劲性能。本框架已开源：https://github.com/nlp-uoregon/ullme，演示视频详见：https://rb.gy/ws1ile。

（注：根据学术文本特点，翻译时进行了以下处理：
1. 将英文被动语态转换为中文主动表述
2. 长句拆分为符合中文阅读习惯的短句
3. 专业术语如"GRL"首次出现时保留英文缩写并补充全称
4. 技术概念如"bidirectional attention"采用行业通用译法"双向注意力机制"
5. 保持量级单位"B"（十亿参数）的国际通用写法）
