# BBT-Fin: Comprehensive Construction of Chinese Financial Domain Pre-trained Language Model, Corpus and Benchmark

链接: http://arxiv.org/abs/2302.09432v1

原文摘要:
To advance Chinese financial natural language processing (NLP), we introduce
BBT-FinT5, a new Chinese financial pre-training language model based on the T5
model. To support this effort, we have built BBT-FinCorpus, a large-scale
financial corpus with approximately 300GB of raw text from four different
sources. In general domain NLP, comprehensive benchmarks like GLUE and
SuperGLUE have driven significant advancements in language model pre-training
by enabling head-to-head comparisons among models. Drawing inspiration from
these benchmarks, we propose BBT-CFLEB, a Chinese Financial Language
understanding and generation Evaluation Benchmark, which includes six datasets
covering both understanding and generation tasks. Our aim is to facilitate
research in the development of NLP within the Chinese financial domain. Our
model, corpus and benchmark are released at
https://github.com/ssymmetry/BBT-FinCUGE-Applications. Our work belongs to the
Big Bang Transformer (BBT), a large-scale pre-trained language model project.

中文翻译:
为推动中文金融自然语言处理（NLP）的发展，我们推出了基于T5架构的新型中文金融预训练语言模型BBT-FinT5。为支持此项研究，我们构建了BBT-FinCorpus大规模金融语料库，该语料库整合了来自四个不同来源、总量约300GB的原始文本数据。在通用领域NLP中，GLUE和SuperGLUE等综合性基准测试通过实现模型间的直接性能对比，显著推动了语言模型预训练技术的进步。受此启发，我们提出了中文金融语言理解与生成评估基准BBT-CFLEB，该基准包含六个数据集，覆盖理解与生成两类任务。我们的目标是促进中文金融领域NLP发展的相关研究。模型、语料库及评估基准已发布于https://github.com/ssymmetry/BBT-FinCUGE-Applications。本项工作隶属于大规模预训练语言模型项目"天工大模型"（Big Bang Transformer, BBT）。
