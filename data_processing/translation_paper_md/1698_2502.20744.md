# Comparative study of the ansätze in quantum language models

链接: http://arxiv.org/abs/2502.20744v1

原文摘要:
Quantum language models are the alternative to classical language models,
which borrow concepts and methods from quantum machine learning and
computational linguistics. While several quantum natural language processing
(QNLP) methods and frameworks exist for text classification and generation,
there is a lack of systematic study to compare the performance across various
ans\"atze, in terms of their hyperparameters and classical and quantum methods
to implement them. Here, we evaluate the performance of quantum natural
language processing models based on these ans\"atze at different levels in text
classification tasks. We perform a comparative study and optimize the QNLP
models by fine-tuning several critical hyperparameters. Our results demonstrate
how the balance between simplification and expressivity affects model
performance. This study provides extensive data to improve our understanding of
QNLP models and opens the possibility of developing better QNLP algorithms.

中文翻译:
量子语言模型是经典语言模型的替代方案，其借鉴了量子机器学习和计算语言学中的概念与方法。尽管目前已存在多种用于文本分类与生成的量子自然语言处理（QNLP）方法和框架，但针对不同理论方案（ansätze）的性能比较仍缺乏系统性研究——包括其超参数设置以及经典与量子实现方法的差异。本研究基于不同层级的文本分类任务，评估了这些理论方案构建的量子自然语言处理模型性能。我们通过精细调节多个关键超参数开展对比研究并优化QNLP模型。实验结果表明模型简化度与表达力之间的平衡如何影响性能表现。本研究提供了丰富数据以深化对QNLP模型的理解，并为开发更优QNLP算法开辟了可能性。

（注：ansätze为德语词汇，在量子计算领域特指"理论框架"或"解决方案"，保留原文术语并采用括号标注处理；QNLP作为专业缩写首次出现时标注全称；通过增补连接词"其"、"这些"等保持中文语流连贯性；将英语长句合理切分为符合中文表达习惯的短句；"fine-tuning"译为"精细调节"以准确传达参数优化过程的渐进特性）
