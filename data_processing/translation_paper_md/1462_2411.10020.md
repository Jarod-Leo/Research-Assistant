# Information Extraction from Clinical Notes: Are We Ready to Switch to Large Language Models?

链接: http://arxiv.org/abs/2411.10020v1

原文摘要:
Backgrounds: Information extraction (IE) is critical in clinical natural
language processing (NLP). While large language models (LLMs) excel on
generative tasks, their performance on extractive tasks remains debated.
Methods: We investigated Named Entity Recognition (NER) and Relation Extraction
(RE) using 1,588 clinical notes from four sources (UT Physicians, MTSamples,
MIMIC-III, and i2b2). We developed an annotated corpus covering 4 clinical
entities and 16 modifiers, and compared instruction-tuned LLaMA-2 and LLaMA-3
against BERT in terms of performance, generalizability, computational
resources, and throughput to BERT. Results: LLaMA models outperformed BERT
across datasets. With sufficient training data, LLaMA showed modest
improvements (1% on NER, 1.5-3.7% on RE); improvements were larger with limited
training data. On unseen i2b2 data, LLaMA-3-70B outperformed BERT by 7% (F1) on
NER and 4% on RE. However, LLaMA models required more computing resources and
ran up to 28 times slower. We implemented "Kiwi," a clinical IE package
featuring both models, available at https://kiwi.clinicalnlp.org/. Conclusion:
This study is among the first to develop and evaluate a comprehensive clinical
IE system using open-source LLMs. Results indicate that LLaMA models outperform
BERT for clinical NER and RE but with higher computational costs and lower
throughputs. These findings highlight that choosing between LLMs and
traditional deep learning methods for clinical IE applications should remain
task-specific, taking into account both performance metrics and practical
considerations such as available computing resources and the intended use case
scenarios.

中文翻译:
背景：信息抽取（IE）在临床自然语言处理（NLP）中至关重要。尽管大语言模型（LLMs）在生成任务上表现优异，但其在抽取任务中的性能仍存在争议。  
方法：我们使用来自四个数据源（UT Physicians、MTSamples、MIMIC-III和i2b2）的1,588份临床记录，研究了命名实体识别（NER）和关系抽取（RE）。我们构建了一个标注语料库，涵盖4类临床实体和16种修饰属性，并对比了指令调优的LLaMA-2、LLaMA-3与BERT在性能、泛化能力、计算资源和吞吐量方面的表现。  
结果：LLaMA模型在所有数据集上均优于BERT。当训练数据充足时，LLaMA仅表现小幅提升（NER提升1%，RE提升1.5-3.7%）；而在训练数据有限时提升更显著。在未见过的i2b2数据上，LLaMA-3-70B的NER（F1值）和RE分别比BERT高出7%和4%。但LLaMA模型需要更多计算资源，运行速度最多慢28倍。我们开发了集成两种模型的临床IE工具包"Kiwi"，可通过https://kiwi.clinicalnlp.org/获取。  
结论：本研究是首批使用开源LLMs构建并评估综合性临床IE系统的尝试之一。结果表明，LLaMA模型在临床NER和RE任务上优于BERT，但需付出更高计算成本和更低吞吐量。这些发现强调，在临床IE应用中，选择LLMs还是传统深度学习方法应基于具体任务需求，同时权衡性能指标与计算资源可用性、应用场景等实际因素。  

（注：翻译严格遵循学术论文摘要的规范表述，在保持专业术语准确性的前提下优化了句式结构。例如将"modifiers"译为"修饰属性"而非字面的"修饰符"，将"throughput"译为"吞吐量"而非"处理量"，并采用"指令调优"等NLP领域通用译法。长难句如最后一句通过拆分和语序调整实现符合中文表达习惯的转换。）
