# Will Affective Computing Emerge from Foundation Models and General AI? A First Evaluation on ChatGPT

链接: http://arxiv.org/abs/2303.03186v1

原文摘要:
ChatGPT has shown the potential of emerging general artificial intelligence
capabilities, as it has demonstrated competent performance across many natural
language processing tasks. In this work, we evaluate the capabilities of
ChatGPT to perform text classification on three affective computing problems,
namely, big-five personality prediction, sentiment analysis, and suicide
tendency detection. We utilise three baselines, a robust language model
(RoBERTa-base), a legacy word model with pretrained embeddings (Word2Vec), and
a simple bag-of-words baseline (BoW). Results show that the RoBERTa trained for
a specific downstream task generally has a superior performance. On the other
hand, ChatGPT provides decent results, and is relatively comparable to the
Word2Vec and BoW baselines. ChatGPT further shows robustness against noisy
data, where Word2Vec models achieve worse results due to noise. Results
indicate that ChatGPT is a good generalist model that is capable of achieving
good results across various problems without any specialised training, however,
it is not as good as a specialised model for a downstream task.

中文翻译:
ChatGPT展现了新兴通用人工智能技术的潜力，其在多项自然语言处理任务中均表现出色。本研究评估了ChatGPT在三个情感计算问题上的文本分类能力：大五人格预测、情感分析和自杀倾向检测。我们采用三个基线模型进行对比：鲁棒语言模型（RoBERTa-base）、预训练词嵌入的传统词模型（Word2Vec）以及简单的词袋基线模型（BoW）。结果表明，针对特定下游任务微调的RoBERTa模型普遍具有最优性能；而ChatGPT虽能取得尚可的结果，其表现仅与Word2Vec和BoW基线模型相当。值得注意的是，ChatGPT在噪声数据环境下展现出较强鲁棒性——相同条件下Word2Vec模型会因噪声干扰导致性能下降。研究证明ChatGPT是一种优秀的通用模型，无需专门训练即可在多种任务中获得良好效果，但其性能仍逊色于针对下游任务专门优化的特定模型。
