# Language Model Analysis for Ontology Subsumption Inference

链接: http://arxiv.org/abs/2302.06761v1

原文摘要:
Investigating whether pre-trained language models (LMs) can function as
knowledge bases (KBs) has raised wide research interests recently. However,
existing works focus on simple, triple-based, relational KBs, but omit more
sophisticated, logic-based, conceptualised KBs such as OWL ontologies. To
investigate an LM's knowledge of ontologies, we propose OntoLAMA, a set of
inference-based probing tasks and datasets from ontology subsumption axioms
involving both atomic and complex concepts. We conduct extensive experiments on
ontologies of different domains and scales, and our results demonstrate that
LMs encode relatively less background knowledge of Subsumption Inference (SI)
than traditional Natural Language Inference (NLI) but can improve on SI
significantly when a small number of samples are given. We will open-source our
code and datasets.

中文翻译:
近年来，关于预训练语言模型（LMs）能否作为知识库（KBs）使用的研究引发了广泛关注。然而现有工作主要聚焦于简单的三元组关系型知识库，却忽略了更复杂的、基于逻辑的概念化知识库（如OWL本体）。为探究语言模型对本体的认知能力，我们提出OntoLAMA——一套基于推理的探测任务及数据集，其构建来源为同时包含原子概念与复合概念的本体包含公理。通过对不同领域、不同规模本体的广泛实验，我们发现：与传统自然语言推理（NLI）相比，语言模型对包含推理（SI）的背景知识编码相对不足，但在提供少量样本后其SI能力可获得显著提升。我们将公开相关代码与数据集。

