# DrivingWorld: ConstructingWorld Model for Autonomous Driving via Video GPT

链接: http://arxiv.org/abs/2412.19505v1

原文摘要:
Recent successes in autoregressive (AR) generation models, such as the GPT
series in natural language processing, have motivated efforts to replicate this
success in visual tasks. Some works attempt to extend this approach to
autonomous driving by building video-based world models capable of generating
realistic future video sequences and predicting ego states. However, prior
works tend to produce unsatisfactory results, as the classic GPT framework is
designed to handle 1D contextual information, such as text, and lacks the
inherent ability to model the spatial and temporal dynamics essential for video
generation. In this paper, we present DrivingWorld, a GPT-style world model for
autonomous driving, featuring several spatial-temporal fusion mechanisms. This
design enables effective modeling of both spatial and temporal dynamics,
facilitating high-fidelity, long-duration video generation. Specifically, we
propose a next-state prediction strategy to model temporal coherence between
consecutive frames and apply a next-token prediction strategy to capture
spatial information within each frame. To further enhance generalization
ability, we propose a novel masking strategy and reweighting strategy for token
prediction to mitigate long-term drifting issues and enable precise control.
Our work demonstrates the ability to produce high-fidelity and consistent video
clips of over 40 seconds in duration, which is over 2 times longer than
state-of-the-art driving world models. Experiments show that, in contrast to
prior works, our method achieves superior visual quality and significantly more
accurate controllable future video generation. Our code is available at
https://github.com/YvanYin/DrivingWorld.

中文翻译:
以下是符合学术规范的中文翻译：

自回归生成模型（如自然语言处理领域的GPT系列）近年取得的成功，激发了将其应用于视觉任务的探索。部分研究尝试将该方法扩展至自动驾驶领域，通过构建基于视频的世界模型来生成逼真的未来视频序列并预测自车状态。然而，由于经典GPT框架专为处理文本等一维上下文信息设计，缺乏对视频生成至关重要的时空动态建模能力，现有研究往往难以取得理想效果。本文提出DrivingWorld——一种具有时空融合机制的GPT风格自动驾驶世界模型，该设计能有效建模空间与时间动态关系，实现高保真度的长时视频生成。具体而言，我们提出"下一状态预测"策略建模帧间时序连贯性，并采用"下一标记预测"策略捕捉帧内空间信息。为增强泛化能力，我们进一步提出创新的标记预测掩码策略和重加权策略，以缓解长期漂移问题并实现精准控制。实验表明，本方法能生成持续超过40秒的高保真连贯视频片段，时长达到现有最优驾驶世界模型的2倍以上。与先前工作相比，我们的方法在视觉质量和可控未来视频生成的准确性上均有显著提升。代码已开源：https://github.com/YvanYin/DrivingWorld。

（翻译严格遵循以下原则：
1. 专业术语统一处理（如"autoregressive"译为"自回归"，"ego states"译为"自车状态"）
2. 长句按中文习惯拆分重组（如将"featureing..."独立译为分句）
3. 被动语态主动化（如"are designed to"译为"专为"）
4. 保留技术概念准确性（如"next-token prediction"译为"下一标记预测"）
5. 数字单位符合中文表述规范（"40 seconds"译为"40秒"）
6. 学术用语规范化（如"state-of-the-art"译为"现有最优"））
