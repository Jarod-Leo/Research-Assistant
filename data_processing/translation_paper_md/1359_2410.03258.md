# Adaptive BPE Tokenization for Enhanced Vocabulary Adaptation in Finetuning Pretrained Language Models

链接: http://arxiv.org/abs/2410.03258v1

原文摘要:
In this work, we show a fundamental limitation in vocabulary adaptation
approaches that use Byte-Pair Encoding (BPE) tokenization scheme for
fine-tuning pretrained language models (PLMs) to expert domains. Current
approaches trivially append the target domain-specific vocabulary at the end of
the PLM vocabulary. This approach leads to a lower priority score and causes
sub-optimal tokenization in BPE that iteratively uses merge rules to tokenize a
given text. To mitigate this issue, we propose AdaptBPE where the BPE
tokenization initialization phase is modified to first perform the longest
string matching on the added (target) vocabulary before tokenizing at the
character level. We perform an extensive evaluation of AdaptBPE versus the
standard BPE over various classification and summarization tasks; AdaptBPE
improves by 3.57% (in terms of accuracy) and 1.87% (in terms of Rouge-L),
respectively. AdaptBPE for MEDVOC works particularly well when reference
summaries have high OOV concentration or are longer in length. We also conduct
a human evaluation, revealing that AdaptBPE generates more relevant and more
faithful summaries as compared to MEDVOC. We make our codebase publicly
available at https://github.com/gb-kgp/adaptbpe.

中文翻译:
本研究揭示了基于字节对编码（BPE）的词汇适配方法在微调预训练语言模型（PLM）至专业领域时存在的根本性局限。现有方法简单地将目标领域词汇追加至PLM词表末端，导致这些词汇在BPE的优先级评分降低——该算法通过迭代应用合并规则进行分词，从而引发次优分词效果。为解决该问题，我们提出AdaptBPE方法，通过改进BPE分词初始化阶段：在字符级分词前，优先对新增（目标领域）词汇执行最长字符串匹配。我们在多类分类与摘要任务上对AdaptBPE与标准BPE进行广泛评测，结果显示AdaptBPE分别以3.57%（准确率）和1.87%（Rouge-L）的优势胜出。当参考摘要含有大量未登录词或篇幅较长时，针对MEDVOC的AdaptBPE表现尤为突出。人工评估进一步表明，相较于MEDVOC，AdaptBPE生成的摘要更具相关性与忠实度。代码已开源：https://github.com/gb-kgp/adaptbpe。

（注：根据学术翻译规范，对部分术语进行了标准化处理：
1. "Byte-Pair Encoding"统一译为"字节对编码"（BPE为通用缩写保留）
2. "PLM"首次出现时译为"预训练语言模型"并标注英文缩写
3. "OOV"译为"未登录词"（自然语言处理领域标准译法）
4. 技术表述采用"分词"而非"标记化"以符合中文NLP领域习惯
5. 保留算法名称"AdaptBPE"和数据集名称"MEDVOC"的英文原名）
