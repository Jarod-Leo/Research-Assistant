# A Survey of Techniques for Optimizing Transformer Inference

链接: http://arxiv.org/abs/2307.07982v1

原文摘要:
Recent years have seen a phenomenal rise in performance and applications of
transformer neural networks. The family of transformer networks, including
Bidirectional Encoder Representations from Transformer (BERT), Generative
Pretrained Transformer (GPT) and Vision Transformer (ViT), have shown their
effectiveness across Natural Language Processing (NLP) and Computer Vision (CV)
domains. Transformer-based networks such as ChatGPT have impacted the lives of
common men. However, the quest for high predictive performance has led to an
exponential increase in transformers' memory and compute footprint. Researchers
have proposed techniques to optimize transformer inference at all levels of
abstraction. This paper presents a comprehensive survey of techniques for
optimizing the inference phase of transformer networks. We survey techniques
such as knowledge distillation, pruning, quantization, neural architecture
search and lightweight network design at the algorithmic level. We further
review hardware-level optimization techniques and the design of novel hardware
accelerators for transformers. We summarize the quantitative results on the
number of parameters/FLOPs and accuracy of several models/techniques to
showcase the tradeoff exercised by them. We also outline future directions in
this rapidly evolving field of research. We believe that this survey will
educate both novice and seasoned researchers and also spark a plethora of
research efforts in this field.

中文翻译:
近年来，Transformer神经网络在性能与应用领域取得了显著突破。以双向编码器表示模型（BERT）、生成式预训练模型（GPT）和视觉Transformer（ViT）为代表的Transformer家族，已在自然语言处理（NLP）和计算机视觉（CV）领域展现出卓越效能。以ChatGPT为代表的Transformer网络甚至深刻影响了普通大众的生活。然而，对高预测性能的追求导致Transformer模型的内存占用和计算需求呈指数级增长。研究者们提出了不同抽象层次的Transformer推理优化技术。本文系统综述了Transformer网络推理阶段的优化方法：在算法层面梳理了知识蒸馏、剪枝、量化、神经架构搜索和轻量化网络设计等技术；在硬件层面探讨了优化技术及专用加速器设计；通过参数量/浮点运算次数（FLOPs）与模型精度的量化数据对比，揭示了不同技术方案的权衡策略；最后展望了这一快速发展领域的研究方向。本综述旨在为初阶与资深研究者提供知识参考，并激发该领域的后续研究热潮。

（翻译说明：采用学术论文摘要的标准表述方式，通过以下处理实现专业性与可读性的平衡：
1. 专业术语保留英文缩写并首次标注全称（如BERT/GPT）
2. 长句拆分重组（如将原文复合句拆分为三个短句说明Transformer影响）
3. 被动语态转换（如"techniques have been proposed"转为主动式"研究者们提出"）
4. 概念显化处理（如"common men"译为"普通大众"更符合中文语境）
5. 量化指标统一规范（FLOPs采用中文标准译法"浮点运算次数"）
6. 保持学术文本特征（使用"旨在""探讨""揭示"等学术动词））
