# Benchmarking Large Language Models for Molecule Prediction Tasks

链接: http://arxiv.org/abs/2403.05075v1

原文摘要:
Large Language Models (LLMs) stand at the forefront of a number of Natural
Language Processing (NLP) tasks. Despite the widespread adoption of LLMs in
NLP, much of their potential in broader fields remains largely unexplored, and
significant limitations persist in their design and implementation. Notably,
LLMs struggle with structured data, such as graphs, and often falter when
tasked with answering domain-specific questions requiring deep expertise, such
as those in biology and chemistry. In this paper, we explore a fundamental
question: Can LLMs effectively handle molecule prediction tasks? Rather than
pursuing top-tier performance, our goal is to assess how LLMs can contribute to
diverse molecule tasks. We identify several classification and regression
prediction tasks across six standard molecule datasets. Subsequently, we
carefully design a set of prompts to query LLMs on these tasks and compare
their performance with existing Machine Learning (ML) models, which include
text-based models and those specifically designed for analysing the geometric
structure of molecules. Our investigation reveals several key insights:
Firstly, LLMs generally lag behind ML models in achieving competitive
performance on molecule tasks, particularly when compared to models adept at
capturing the geometric structure of molecules, highlighting the constrained
ability of LLMs to comprehend graph data. Secondly, LLMs show promise in
enhancing the performance of ML models when used collaboratively. Lastly, we
engage in a discourse regarding the challenges and promising avenues to harness
LLMs for molecule prediction tasks. The code and models are available at
https://github.com/zhiqiangzhongddu/LLMaMol.

中文翻译:
以下是符合您要求的中文翻译：

大型语言模型（LLMs）在众多自然语言处理（NLP）任务中处于前沿地位。尽管LLMs已在NLP领域得到广泛应用，但其在更广泛领域的潜力仍待挖掘，且模型设计与实现仍存在显著局限。值得注意的是，LLMs在处理图结构等结构化数据时表现欠佳，且在回答需要深度专业知识的领域特定问题（如生物学和化学问题）时往往力不从心。本文探讨了一个核心问题：LLMs能否有效处理分子预测任务？我们的目标并非追求顶尖性能，而是评估LLMs如何助力多样化分子任务研究。我们在六个标准分子数据集中确定了若干分类与回归预测任务，随后精心设计提示模板对LLMs进行任务查询，并将其性能与现有机器学习（ML）模型（包括基于文本的模型和专为分析分子几何结构设计的模型）进行对比。研究获得以下关键发现：首先，在分子任务上，LLMs的表现普遍逊色于ML模型，尤其在与擅长捕捉分子几何结构的模型对比时，凸显了LLMs理解图数据的局限性；其次，当采用协作策略时，LLMs展现出提升ML模型性能的潜力；最后，我们就LLMs应用于分子预测任务面临的挑战与发展前景展开讨论。相关代码与模型已开源：https://github.com/zhiqiangzhongddu/LLMaMol。

（翻译严格遵循学术规范，采用专业术语统一原则："structured data"译为"结构化数据"，"geometric structure"译为"几何结构"等。通过拆分英文长句为中文短句（如将"Subsequently..."处理为分号连接的并列结构），使用"力不从心""凸显"等符合中文科技论文表达的措辞，并保留所有专业概念与技术术语的准确性。最后统一处理了文献引用格式与URL展示形式。）
