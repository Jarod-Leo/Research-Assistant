# Analyzing the Role of Semantic Representations in the Era of Large Language Models

链接: http://arxiv.org/abs/2405.01502v1

原文摘要:
Traditionally, natural language processing (NLP) models often use a rich set
of features created by linguistic expertise, such as semantic representations.
However, in the era of large language models (LLMs), more and more tasks are
turned into generic, end-to-end sequence generation problems. In this paper, we
investigate the question: what is the role of semantic representations in the
era of LLMs? Specifically, we investigate the effect of Abstract Meaning
Representation (AMR) across five diverse NLP tasks. We propose an AMR-driven
chain-of-thought prompting method, which we call AMRCoT, and find that it
generally hurts performance more than it helps. To investigate what AMR may
have to offer on these tasks, we conduct a series of analysis experiments. We
find that it is difficult to predict which input examples AMR may help or hurt
on, but errors tend to arise with multi-word expressions, named entities, and
in the final inference step where the LLM must connect its reasoning over the
AMR to its prediction. We recommend focusing on these areas for future work in
semantic representations for LLMs. Our code:
https://github.com/causalNLP/amr_llm.

中文翻译:
传统自然语言处理（NLP）模型通常依赖由语言学专业知识构建的丰富特征集（如语义表示）。但在大语言模型（LLM）时代，越来越多任务被转化为通用的端到端序列生成问题。本文探讨核心问题：LLM时代下语义表示究竟扮演何种角色？我们通过五项多样化NLP任务系统研究了抽象意义表示（AMR）的作用，提出名为AMRCoT的AMR驱动思维链提示方法，发现该方法对性能的负面影响普遍大于增益。为探究AMR可能的价值，我们开展系列分析实验，发现难以预判AMR对具体输入样例的助益或损害，但其错误往往集中于多词表达、命名实体识别，以及LLM需将AMR推理与最终预测相衔接的关键推断环节。我们建议未来LLM语义表示研究应重点关注这些领域。代码已开源：https://github.com/causalNLP/amr_llm。

（翻译说明：采用学术论文摘要的标准表述方式，通过以下处理实现专业性与可读性平衡：
1. 专业术语规范处理："Abstract Meaning Representation"统一译为"抽象意义表示"，"chain-of-thought prompting"译为"思维链提示"
2. 长句拆分重组：将原文复合句按中文表达习惯分解为多个短句，如将"we investigate..."复杂句拆分为两个独立分句
3. 逻辑显化：添加"核心问题"等过渡词明确研究主线，使用"究竟"加强探讨语气
4. 被动语态转化："it is difficult to predict..."转换为主动句式"难以预判..."
5. 概念一致性：全篇统一"大语言模型"与"LLM"的混用表述，保持术语一致性
6. 技术细节保留：完整保留AMRCoT方法名称及GitHub链接等关键信息）
