# Large Language Models Meet NLP: A Survey

链接: http://arxiv.org/abs/2405.12819v1

原文摘要:
While large language models (LLMs) like ChatGPT have shown impressive
capabilities in Natural Language Processing (NLP) tasks, a systematic
investigation of their potential in this field remains largely unexplored. This
study aims to address this gap by exploring the following questions: (1) How
are LLMs currently applied to NLP tasks in the literature? (2) Have traditional
NLP tasks already been solved with LLMs? (3) What is the future of the LLMs for
NLP? To answer these questions, we take the first step to provide a
comprehensive overview of LLMs in NLP. Specifically, we first introduce a
unified taxonomy including (1) parameter-frozen application and (2)
parameter-tuning application to offer a unified perspective for understanding
the current progress of LLMs in NLP. Furthermore, we summarize the new
frontiers and the associated challenges, aiming to inspire further
groundbreaking advancements. We hope this work offers valuable insights into
the {potential and limitations} of LLMs in NLP, while also serving as a
practical guide for building effective LLMs in NLP.

中文翻译:
虽然以ChatGPT为代表的大规模语言模型（LLMs）在自然语言处理（NLP）任务中展现出卓越能力，但学界对其在该领域的应用潜力仍缺乏系统性探究。本研究旨在通过探讨以下问题填补这一空白：（1）现有文献中LLMs如何应用于NLP任务？（2）传统NLP任务是否已被LLMs解决？（3）LLMs在NLP领域的未来发展趋势？为回答这些问题，我们首次对NLP领域的LLMs研究进行全面综述。具体而言，我们首先提出包含（1）参数冻结应用与（2）参数调优应用的统一分类框架，为理解LLMs在NLP中的研究进展提供整合视角。此外，我们梳理了新兴研究前沿及相应挑战，以期激发突破性进展。本研究不仅揭示了LLMs在NLP领域的潜力与局限，更致力于为构建高效NLP语言模型提供实践指南。

（注：{potential and limitations}的翻译采用"潜力与局限"这一学术常用表述，既保持专业性与简洁性，又符合中文表达习惯。全文通过拆分英文长句、调整语序、补充连接词等方式实现学术文本的本土化转换，同时严格保留专业术语的准确性。）
