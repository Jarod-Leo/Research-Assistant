# Diffusion Models for Non-autoregressive Text Generation: A Survey

链接: http://arxiv.org/abs/2303.06574v1

原文摘要:
Non-autoregressive (NAR) text generation has attracted much attention in the
field of natural language processing, which greatly reduces the inference
latency but has to sacrifice the generation accuracy. Recently, diffusion
models, a class of latent variable generative models, have been introduced into
NAR text generation, showing an improved text generation quality. In this
survey, we review the recent progress in diffusion models for NAR text
generation. As the background, we first present the general definition of
diffusion models and the text diffusion models, and then discuss their merits
for NAR generation. As the core content, we further introduce two mainstream
diffusion models in existing work of text diffusion, and review the key designs
of the diffusion process. Moreover, we discuss the utilization of pre-trained
language models (PLMs) for text diffusion models and introduce optimization
techniques for text data. Finally, we discuss several promising directions and
conclude this paper. Our survey aims to provide researchers with a systematic
reference of related research on text diffusion models for NAR generation. We
present our collection of text diffusion models at
https://github.com/RUCAIBox/Awesome-Text-Diffusion-Models.

中文翻译:
非自回归（NAR）文本生成在自然语言处理领域备受关注，该方法虽能显著降低推理延迟，却不得不牺牲生成准确性。近年来，作为一类潜变量生成模型的扩散模型被引入NAR文本生成领域，展现出提升文本生成质量的潜力。本文系统综述了扩散模型在NAR文本生成中的最新进展：首先作为背景知识，阐述扩散模型与文本扩散模型的一般定义，并探讨其适用于NAR生成的优势；作为核心内容，详细分析现有文本扩散研究中的两类主流模型框架，梳理扩散过程的关键设计；进而讨论预训练语言模型（PLMs）在文本扩散中的运用方式，并介绍面向文本数据的优化技术；最后展望若干具有前景的研究方向。

