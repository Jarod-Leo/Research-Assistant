# Deconstructing Recurrence, Attention, and Gating: Investigating the transferability of Transformers and Gated Recurrent Neural Networks in forecasting of dynamical systems

链接: http://arxiv.org/abs/2410.02654v1

原文摘要:
Machine learning architectures, including transformers and recurrent neural
networks (RNNs) have revolutionized forecasting in applications ranging from
text processing to extreme weather. Notably, advanced network architectures,
tuned for applications such as natural language processing, are transferable to
other tasks such as spatiotemporal forecasting tasks. However, there is a
scarcity of ablation studies to illustrate the key components that enable this
forecasting accuracy. The absence of such studies, although explainable due to
the associated computational cost, intensifies the belief that these models
ought to be considered as black boxes. In this work, we decompose the key
architectural components of the most powerful neural architectures, namely
gating and recurrence in RNNs, and attention mechanisms in transformers. Then,
we synthesize and build novel hybrid architectures from the standard blocks,
performing ablation studies to identify which mechanisms are effective for each
task. The importance of considering these components as hyper-parameters that
can augment the standard architectures is exhibited on various forecasting
datasets, from the spatiotemporal chaotic dynamics of the multiscale Lorenz 96
system, the Kuramoto-Sivashinsky equation, as well as standard real world
time-series benchmarks. A key finding is that neural gating and attention
improves the performance of all standard RNNs in most tasks, while the addition
of a notion of recurrence in transformers is detrimental. Furthermore, our
study reveals that a novel, sparsely used, architecture which integrates
Recurrent Highway Networks with neural gating and attention mechanisms, emerges
as the best performing architecture in high-dimensional spatiotemporal
forecasting of dynamical systems.

中文翻译:
以下是符合您要求的中文翻译：

机器学习架构（包括Transformer和循环神经网络RNN）已彻底改变了从文本处理到极端天气预测等众多领域的预测技术。值得注意的是，为自然语言处理等任务优化的先进网络架构，可迁移应用于时空预测等其他任务。然而，目前缺乏消融研究来阐明实现高预测精度的关键组件。尽管高计算成本可以解释这类研究的缺失，但这种情况强化了"这些模型应被视为黑箱"的普遍认知。本研究系统解构了最强大神经架构的核心组件：RNN中的门控循环机制，以及Transformer中的注意力机制。通过将这些标准模块重新组合构建新型混合架构，我们开展消融实验以确定不同任务的有效机制。在多个预测数据集（包括多尺度Lorenz 96系统的时空混沌动力学、Kuramoto-Sivashinsky方程以及标准现实世界时间序列基准）上的实验表明，将这些组件视为可增强标准架构的超参数至关重要。关键发现是：神经门控和注意力机制能提升所有标准RNN在多数任务中的表现，而在Transformer中添加循环概念反而有害。此外，研究揭示了一种新型稀疏架构——将循环高速公路网络与神经门控及注意力机制相融合，在动态系统的高维时空预测中展现出最优性能。

翻译说明：
1. 专业术语处理：准确翻译"gating/recurrence/attention mechanisms"等核心概念，保留"Transformer/RNN"等专有名词
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句，如将"notably..."处理为独立强调句
3. 逻辑显化：通过"然而/此外"等连接词明确原文隐含的转折递进关系
4. 学术规范：保持"消融研究/时空混沌动力学"等学术表述的准确性
5. 动态对应："emerges as"译为"展现出"更符合中文论文表述习惯
6. 术语统一：全篇保持"架构/组件/机制"等核心概念翻译一致性
