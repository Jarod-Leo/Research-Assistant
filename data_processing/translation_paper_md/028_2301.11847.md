# A Comparative Study of Pretrained Language Models for Long Clinical Text

链接: http://arxiv.org/abs/2301.11847v1

原文摘要:
Objective: Clinical knowledge enriched transformer models (e.g.,
ClinicalBERT) have state-of-the-art results on clinical NLP (natural language
processing) tasks. One of the core limitations of these transformer models is
the substantial memory consumption due to their full self-attention mechanism,
which leads to the performance degradation in long clinical texts. To overcome
this, we propose to leverage long-sequence transformer models (e.g., Longformer
and BigBird), which extend the maximum input sequence length from 512 to 4096,
to enhance the ability to model long-term dependencies in long clinical texts.
  Materials and Methods: Inspired by the success of long sequence transformer
models and the fact that clinical notes are mostly long, we introduce two
domain enriched language models, Clinical-Longformer and Clinical-BigBird,
which are pre-trained on a large-scale clinical corpus. We evaluate both
language models using 10 baseline tasks including named entity recognition,
question answering, natural language inference, and document classification
tasks.
  Results: The results demonstrate that Clinical-Longformer and
Clinical-BigBird consistently and significantly outperform ClinicalBERT and
other short-sequence transformers in all 10 downstream tasks and achieve new
state-of-the-art results.
  Discussion: Our pre-trained language models provide the bedrock for clinical
NLP using long texts. We have made our source code available at
https://github.com/luoyuanlab/Clinical-Longformer, and the pre-trained models
available for public download at:
https://huggingface.co/yikuan8/Clinical-Longformer.
  Conclusion: This study demonstrates that clinical knowledge enriched
long-sequence transformers are able to learn long-term dependencies in long
clinical text. Our methods can also inspire the development of other
domain-enriched long-sequence transformers.

中文翻译:
目的：临床知识增强型Transformer模型（如ClinicalBERT）在临床自然语言处理任务中具有最先进的性能。这类模型的核心局限在于其全自注意力机制导致显存占用过高，致使长文本处理性能下降。为此，我们提出采用长序列Transformer模型（如Longformer和BigBird），将最大输入序列长度从512扩展到4096，以增强对长临床文本中长期依赖关系的建模能力。
材料与方法：基于长序列Transformer模型的技术突破和临床文本普遍较长的特性，我们提出了两种领域增强语言模型——Clinical-Longformer和Clinical-BigBird，并在大规模临床语料库上进行预训练。通过10项基准任务（包括命名实体识别、问答系统、自然语言推理和文档分类）对模型性能进行评估。
结果：实验结果表明，Clinical-Longformer和Clinical-BigBird在所有10项下游任务中均显著优于ClinicalBERT等短序列Transformer模型，创造了新的性能标杆。
讨论：本研究提供的预训练模型为长文本临床自然语言处理奠定了基础。源代码已开源（GitHub仓库：luoyuanlab/Clinical-Longformer），预训练模型可通过Hugging Face平台（yikuan8/Clinical-Longformer）公开获取。
结论：本研究证实临床知识增强的长序列Transformer能有效学习长临床文本中的长期依赖关系。该方法对其他领域的长序列Transformer开发具有借鉴意义。

