# On the importance of Data Scale in Pretraining Arabic Language Models

链接: http://arxiv.org/abs/2401.07760v1

原文摘要:
Pretraining monolingual language models have been proven to be vital for
performance in Arabic Natural Language Processing (NLP) tasks. In this paper,
we conduct a comprehensive study on the role of data in Arabic Pretrained
Language Models (PLMs). More precisely, we reassess the performance of a suite
of state-of-the-art Arabic PLMs by retraining them on massive-scale,
high-quality Arabic corpora. We have significantly improved the performance of
the leading Arabic encoder-only BERT-base and encoder-decoder T5-base models on
the ALUE and ORCA leaderboards, thereby reporting state-of-the-art results in
their respective model categories. In addition, our analysis strongly suggests
that pretraining data by far is the primary contributor to performance,
surpassing other factors. Our models and source code are publicly available at
https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/JABER-PyTorch.

中文翻译:
以下是符合要求的学术中文翻译：

单语预训练语言模型已被证明对阿拉伯语自然语言处理（NLP）任务性能至关重要。本文针对阿拉伯语预训练语言模型（PLMs）中数据的作用展开系统性研究。具体而言，我们通过在大规模高质量阿拉伯语语料库上重新训练，对一系列前沿阿拉伯语PLMs的性能进行重新评估。实验显著提升了当前最优阿拉伯语编码器（BERT-base）和编码器-解码器（T5-base）模型在ALUE和ORCA评测基准上的表现，从而在各自模型类别中实现了最先进的性能指标。此外，我们的分析充分表明：预训练数据质量是目前影响模型性能的首要因素，其重要性远超其他变量因素。相关模型与源代码已开源：https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/JABER-PyTorch。

（翻译说明：
1. 专业术语规范处理："state-of-the-art"译为"最先进的"，"leaderboards"译为"评测基准"
2. 句式结构调整：将英文被动语态转换为中文主动表述（如"have been proven"译为"已被证明"）
3. 技术概念准确传达："encoder-only/encoder-decoder"保留专业表述同时添加括号说明
4. 长句拆分重组：将复合长句分解为符合中文表达习惯的短句
5. 学术风格保持：使用"展开系统性研究""显著提升"等符合论文摘要的规范表述
6. 链接信息完整保留并采用中文标点规范）
