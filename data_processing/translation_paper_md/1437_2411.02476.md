# A Comparative Analysis of Instruction Fine-Tuning LLMs for Financial Text Classification

链接: http://arxiv.org/abs/2411.02476v1

原文摘要:
Large Language Models (LLMs) have demonstrated impressive capabilities across
diverse Natural Language Processing (NLP) tasks, including language
understanding, reasoning, and generation. However, general-domain LLMs often
struggle with financial tasks due to the technical and specialized nature of
financial texts. This study investigates the efficacy of instruction
fine-tuning smaller-scale LLMs, including Mistral-7B, Llama3-8B, and Phi3-mini,
to enhance their performance in financial text classification tasks. We
fine-tuned both instruction-tuned and base models across four financial
classification tasks, achieving significant improvements in task-specific
performance. Furthermore, we evaluated the zero-shot capabilities of these
fine-tuned models on three unseen complex financial tasks, including argument
classification, deal completeness classification, and causal classification.
Our results indicate while base model fine-tuning led to greater degradation,
instruction-tuned models maintained more robust performance. To address this
degradation, we employed model merging techniques, integrating single-task
domain-specific fine-tuned models with the base model. Using this merging
method resulted in significant enhancements in zero-shot performance, even
exceeding the original model's accuracy on certain datasets. Our findings
underscore the effectiveness of instruction fine-tuning and model merging for
adapting LLMs to specialized financial text classification tasks.

中文翻译:
以下是符合您要求的中文翻译：

大型语言模型（LLMs）在自然语言处理（NLP）领域展现出卓越的多任务处理能力，涵盖语言理解、逻辑推理与文本生成等任务。然而，由于金融文本具有高度专业性和技术性特征，通用领域的大型语言模型在金融任务中往往表现欠佳。本研究探讨了通过指令微调中小规模模型（包括Mistral-7B、Llama3-8B和Phi3-mini）来提升金融文本分类任务性能的有效性。我们在四项金融分类任务上对基础模型和指令微调模型分别进行优化，均实现了任务特定性能的显著提升。进一步地，我们在三项未见过的复杂金融任务（包括论点分类、交易完整性分类和因果分类）上评估了这些微调模型的零样本能力。实验结果表明：基础模型微调后性能衰减较大，而指令微调模型则保持了更强的鲁棒性。为缓解性能衰减问题，我们采用模型融合技术，将单任务领域微调模型与基础模型进行整合。这种融合方法显著提升了零样本性能，在某些数据集上甚至超越了原始模型的准确率。本研究证实了指令微调与模型融合技术在使LLMs适应专业化金融文本分类任务方面具有显著效果。

翻译说明：
1. 专业术语处理：LLMs/NLP等专业缩写首次出现时保留英文全称+中文译名，后续使用中文简称
2. 长句拆分：将原文复合长句按中文表达习惯分解为多个短句，如将"including..."处理为括号补充说明
3. 被动语态转换："were fine-tuned"等被动式转为中文主动语态"进行优化"
4. 概念显化："unseen tasks"译为"未见过的任务"而非直译"不可见任务"
5. 技术表述统一："model merging"全篇统一译为"模型融合"而非"合并"
6. 逻辑连接优化：添加"进一步地"等连接词增强段落连贯性
7. 数据呈现方式：保留具体模型名称和准确率等关键数据不变
