# SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks

链接: http://arxiv.org/abs/2402.09025v1

原文摘要:
Large language models (LLMs) have proven to be highly effective across
various natural language processing tasks. However, their large number of
parameters poses significant challenges for practical deployment. Pruning, a
technique aimed at reducing the size and complexity of LLMs, offers a potential
solution by removing redundant components from the network. Despite the promise
of pruning, existing methods often struggle to achieve substantial end-to-end
LLM inference speedup. In this paper, we introduce SLEB, a novel approach
designed to streamline LLMs by eliminating redundant transformer blocks. We
choose the transformer block as the fundamental unit for pruning, because LLMs
exhibit block-level redundancy with high similarity between the outputs of
neighboring blocks. This choice allows us to effectively enhance the processing
speed of LLMs. Our experimental results demonstrate that SLEB outperforms
previous LLM pruning methods in accelerating LLM inference while also
maintaining superior perplexity and accuracy, making SLEB as a promising
technique for enhancing the efficiency of LLMs. The code is available at:
https://github.com/jiwonsong-dev/SLEB.

中文翻译:
以下是符合要求的学术论文摘要中文翻译：

【译文】
大语言模型（LLMs）已被证实在各类自然语言处理任务中表现卓越，但其庞大的参数量给实际部署带来了重大挑战。剪枝技术通过移除网络中的冗余组件，为缩减LLMs规模与复杂度提供了可行方案。尽管剪枝技术前景广阔，现有方法往往难以实现显著的端到端LLM推理加速。本文提出SLEB——一种通过消除冗余Transformer模块来精简LLMs的创新方法。我们选择Transformer模块作为剪枝的基本单元，因为LLMs表现出模块级冗余特性，相邻模块的输出具有高度相似性。这种设计使我们能有效提升LLMs的处理速度。实验结果表明，SLEB在加速LLM推理方面优于现有剪枝方法，同时保持了更优的困惑度与准确率，使其成为提升LLM效率的前沿技术。代码已开源：https://github.com/jiwonsong-dev/SLEB。

【翻译要点说明】
1. 专业术语处理：
- "pruning"统一译为"剪枝技术/剪枝"（符合计算机领域术语规范）
- "transformer blocks"译为"Transformer模块"（学术界通用译法）
- "perplexity"保留专业术语"困惑度"

2. 句式重构：
- 将原文复合长句拆分为符合中文表达习惯的短句（如第二句拆分）
- 被动语态转换为主动表述（如"have proven to be"译为"已被证实"）

3. 学术风格保持：
- 使用"本文""结果表明"等学术论文惯用表述
- 保留技术细节的精确性（如"block-level redundancy"译为"模块级冗余特性"）

4. 格式规范：
- 首字母缩略词LLMs首次出现标注全称
- 保留原文超链接格式及技术术语大小写（如Transformer）

5. 流畅性优化：
- 添加连接词"因为"使逻辑更清晰
- 使用四字格"表现卓越""前景广阔"增强可读性
