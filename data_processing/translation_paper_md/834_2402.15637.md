# Addressing Order Sensitivity of In-Context Demonstration Examples in Causal Language Models

链接: http://arxiv.org/abs/2402.15637v1

原文摘要:
In-context learning has become a popular paradigm in natural language
processing. However, its performance can be significantly influenced by the
order of in-context demonstration examples. In this paper, we found that causal
language models (CausalLMs) are more sensitive to this order compared to prefix
language models (PrefixLMs). We attribute this phenomenon to the
auto-regressive attention masks within CausalLMs, which restrict each token
from accessing information from subsequent tokens. This results in different
receptive fields for samples at different positions, thereby leading to
representation disparities across positions. To tackle this challenge, we
introduce an unsupervised fine-tuning method, termed the Information-Augmented
and Consistency-Enhanced approach. This approach utilizes contrastive learning
to align representations of in-context examples across different positions and
introduces a consistency loss to ensure similar representations for inputs with
different permutations. This enhances the model's predictive consistency across
permutations. Experimental results on five benchmarks suggest that our proposed
method can reduce the sensitivity of CausalLMs to the order of in-context
examples and exhibit robust generalizability, particularly when demonstrations
are sourced from a candidate pool different from that used in the training
phase, or when the number of in-context examples differs from what is used
during training.

中文翻译:
以下是符合学术规范的中文翻译：

情境学习已成为自然语言处理领域的流行范式。然而，其性能表现会显著受到情境示例排列顺序的影响。本文发现，相较于前缀语言模型（PrefixLMs），因果语言模型（CausalLMs）对此顺序表现出更强的敏感性。我们将此现象归因于因果语言模型中的自回归注意力掩码机制——该机制限制每个词元获取后续词元的信息，导致不同位置的样本具有不同的感受野，从而产生位置间的表征差异。为应对这一挑战，我们提出了一种名为"信息增强与一致性优化"的无监督微调方法。该方法通过对比学习对齐不同位置情境示例的表征，并引入一致性损失函数来确保不同排列组合的输入能获得相似表征，从而增强模型在排列组合间的预测一致性。在五个基准测试上的实验结果表明：我们提出的方法能有效降低因果语言模型对情境示例顺序的敏感性，并展现出强大的泛化能力——尤其当演示样本来自与训练阶段不同的候选池，或情境示例数量与训练时不一致时仍能保持稳健性能。

（翻译说明：
1. 专业术语采用学界通用译法，如"in-context learning"译为"情境学习"、"auto-regressive"译为"自回归"
2. 长难句进行合理切分，如将定语从句转换为破折号补充说明
3. 被动语态转换为中文主动表达，如"can be influenced"译为"会显著受到"
4. 关键方法名称采用引号标注，保持术语一致性
5. 补充连接词提升行文流畅性，如"从而"、"尤其当"等
6. 保留英文缩写首次出现时的全称标注，符合学术规范）
