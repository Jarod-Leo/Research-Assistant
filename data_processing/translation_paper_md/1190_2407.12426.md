# Sharif-STR at SemEval-2024 Task 1: Transformer as a Regression Model for Fine-Grained Scoring of Textual Semantic Relations

链接: http://arxiv.org/abs/2407.12426v1

原文摘要:
Semantic Textual Relatedness holds significant relevance in Natural Language
Processing, finding applications across various domains. Traditionally,
approaches to STR have relied on knowledge-based and statistical methods.
However, with the emergence of Large Language Models, there has been a paradigm
shift, ushering in new methodologies. In this paper, we delve into the
investigation of sentence-level STR within Track A (Supervised) by leveraging
fine-tuning techniques on the RoBERTa transformer. Our study focuses on
assessing the efficacy of this approach across different languages. Notably,
our findings indicate promising advancements in STR performance, particularly
in Latin languages. Specifically, our results demonstrate notable improvements
in English, achieving a correlation of 0.82 and securing a commendable 19th
rank. Similarly, in Spanish, we achieved a correlation of 0.67, securing the
15th position. However, our approach encounters challenges in languages like
Arabic, where we observed a correlation of only 0.38, resulting in a 20th rank.

中文翻译:
语义文本关联性在自然语言处理领域具有重要意义，其应用涵盖多个方向。传统研究方法主要依赖知识库和统计手段，但随着大语言模型的出现，该领域正经历范式转变并催生新方法。本文通过微调RoBERTa转换器模型，深入研究了Track A（监督学习）中的句子级语义文本关联性，重点评估该方法在不同语言中的有效性。研究结果显示该方法性能显著提升，尤其在拉丁语系中表现突出：英语任务获得0.82相关系数（排名第19位），西班牙语达到0.67相关系数（排名第15位）。但在阿拉伯语等语言中面临挑战，仅取得0.38相关系数（排名第20位）。
