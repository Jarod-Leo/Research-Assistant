# idT5: Indonesian Version of Multilingual T5 Transformer

链接: http://arxiv.org/abs/2302.00856v1

原文摘要:
Indonesian language is spoken by almost 200 million people and is the 10th
most spoken language in the world, but it is under-represented in NLP (Natural
Language Processing) research. A sparsity of language resources has hampered
previous work on Indonesian. The Transformer is a new architecture rapidly
becoming dominant for NLP, surpassing alternatives like convolutional and
recurrent neural networks. T5 (Text-to-Text Transfer Transformer) is a
Transformer model that converts all text-based language problems to
text-to-text format for English. The multilingual variant is mT5 (multilingual
T5) which has shown promising results on many NLP tasks across languages.
However, the size of this multilingual model is a drawback for its application
in real production applications, which sometimes require only one language. In
this study, the mT5 model was adapted for only one language, Indonesian,
resulting in a pre-trained T5 model that was specific only for Indonesian with
a smaller size. For performance comparison, we fine-tuned this model and the
mT5 model to the Sentiment Analysis (SA), Question Generation (QG), and
Question Answering (QA) tasks with the exact mechanism and dataset. Fine-tuned
model based on our model achieved 77.18% accuracy on SA, 8% higher than the
mT5-based model, and obtained nearly the same score as the mT5-based model on
QG and QA. The results confirm that it is possible to produce a smaller
pre-trained model that maintains comparable yields while reducing the model
size by up to 58%. In addition, the resulting model requires less memory, loads
faster, and inference times faster.

中文翻译:
印度尼西亚语使用者近2亿人，是全球使用人数排名第十的语言，但在自然语言处理（NLP）研究领域却长期处于代表性不足的状态。语言资源的匮乏一直制约着该语言的NLP研究进展。Transformer作为一种新兴架构正快速成为NLP领域的主导技术，其性能已超越卷积神经网络和循环神经网络等传统方案。T5（文本到文本迁移转换器）是基于Transformer的模型，可将所有基于文本的英语语言问题转化为文本到文本格式。其多语言变体mT5（多语言T5）在跨语言NLP任务中展现出优异性能。然而该多语言模型的规模限制了其在实际生产环境中的应用——特别是当应用场景仅需处理单一语言时。本研究将mT5模型适配为印度尼西亚语专用版本，最终获得一个体积更小、仅针对印度尼西亚语的预训练T5模型。为评估性能，我们采用相同机制和数据集，分别对本模型与mT5模型进行情感分析（SA）、问题生成（QG）和问答（QA）任务的微调。基于本模型的微调结果在SA任务中达到77.18%准确率，较mT5基础模型提升8%；在QG和QA任务中则与mT5基础模型表现相当。实验证实：通过最高58%的模型体积压缩，仍可保持具有可比性的性能产出。此外，所得模型具有内存占用更低、加载速度更快、推理耗时更短等优势。
