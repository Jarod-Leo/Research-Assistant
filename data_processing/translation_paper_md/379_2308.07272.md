# Dialogue for Prompting: a Policy-Gradient-Based Discrete Prompt Optimization for Few-shot Learning

链接: http://arxiv.org/abs/2308.07272v1

原文摘要:
Prompt-based pre-trained language models (PLMs) paradigm have succeeded
substantially in few-shot natural language processing (NLP) tasks. However,
prior discrete prompt optimization methods require expert knowledge to design
the base prompt set and identify high-quality prompts, which is costly,
inefficient, and subjective. Meanwhile, existing continuous prompt optimization
methods improve the performance by learning the ideal prompts through the
gradient information of PLMs, whose high computational cost, and low
readability and generalizability are often concerning. To address the research
gap, we propose a Dialogue-comprised Policy-gradient-based Discrete Prompt
Optimization ($DP_2O$) method. We first design a multi-round dialogue alignment
strategy for readability prompt set generation based on GPT-4. Furthermore, we
propose an efficient prompt screening metric to identify high-quality prompts
with linear complexity. Finally, we construct a reinforcement learning (RL)
framework based on policy gradients to match the prompts to inputs optimally.
By training a policy network with only 0.67% of the PLM parameter size on the
tasks in the few-shot setting, $DP_2O$ outperforms the state-of-the-art (SOTA)
method by 1.52% in accuracy on average on four open-source datasets. Moreover,
subsequent experiments also demonstrate that $DP_2O$ has good universality,
robustness, and generalization ability.

中文翻译:
基于提示词（prompt）的预训练语言模型（PLMs）范式在少样本自然语言处理（NLP）任务中取得了显著成功。然而，现有离散式提示词优化方法需要依赖专家知识设计基础提示词集并筛选高质量提示词，存在成本高、效率低且主观性强的问题；而主流连续式提示词优化方法虽通过PLMs的梯度信息学习理想提示词来提升性能，但其计算开销大、可读性与泛化性差的缺陷始终存在。为填补这一研究空白，本文提出一种融合对话策略与策略梯度的离散提示词优化方法（$DP_2O$）。我们首先基于GPT-4设计多轮对话对齐策略生成可读性提示词集；进而提出线性复杂度的提示词高效筛选指标；最终构建基于策略梯度的强化学习（RL）框架实现提示词与输入样本的最优匹配。在少样本场景下，仅需训练参数量为PLM 0.67%的策略网络，$DP_2O$在四个开源数据集上的平均准确率即超越现有最优方法1.52%。后续实验进一步验证了该方法具有优异的普适性、鲁棒性和泛化能力。

（注：根据学术论文翻译规范，对部分术语进行了标准化处理：
1. "prompt"统一译为"提示词"（NLP领域通用译法）
2. "policy gradient"译为"策略梯度"（强化学习标准术语）
3. 数学符号$DP_2O$保留原文格式
4. 长难句按中文表达习惯拆分重组，如将定语从句转换为分句
5. 专业表述如"少样本"（few-shot）、"强化学习框架"（RL framework）等采用学界通用译法）
