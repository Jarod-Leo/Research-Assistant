# MixLoRA: Enhancing Large Language Models Fine-Tuning with LoRA based Mixture of Experts

链接: http://arxiv.org/abs/2404.15159v1

原文摘要:
Fine-tuning Large Language Models (LLMs) is a common practice to adapt
pre-trained models for specific applications. While methods like LoRA have
effectively addressed GPU memory constraints during fine-tuning, their
performance often falls short, especially in multi-task scenarios. In contrast,
Mixture-of-Expert (MoE) models, such as Mixtral 8x7B, demonstrate remarkable
performance in multi-task learning scenarios while maintaining a reduced
parameter count. However, the resource requirements of these MoEs remain
challenging, particularly for consumer-grade GPUs with less than 24GB memory.
To tackle these challenges, we propose MixLoRA, an approach to construct a
resource-efficient sparse MoE model based on LoRA. MixLoRA inserts multiple
LoRA-based experts within the feed-forward network block of a frozen
pre-trained dense model and employs a commonly used top-k router. Unlike other
LoRA-based MoE methods, MixLoRA enhances model performance by utilizing
independent attention-layer LoRA adapters. Additionally, an auxiliary load
balance loss is employed to address the imbalance problem of the router. Our
evaluations show that MixLoRA improves about 9% accuracy compared to
state-of-the-art PEFT methods in multi-task learning scenarios. We also propose
a new high-throughput framework to alleviate the computation and memory
bottlenecks during the training and inference of MOE models. This framework
reduces GPU memory consumption by 40% and token computation latency by 30%
during both training and inference.

中文翻译:
以下是符合要求的学术中文翻译：

微调大语言模型（LLMs）是将预训练模型适配特定应用的常见方法。虽然LoRA等方法有效缓解了微调过程中的GPU内存限制，但其性能表现往往欠佳，尤其在多任务场景下。相比之下，混合专家模型（MoE，如Mixtral 8x7B）在保持参数量精简的同时，展现出卓越的多任务学习能力。然而这类MoE模型的资源需求仍具挑战性，特别是对于显存小于24GB的消费级GPU。

为应对这些挑战，我们提出MixLoRA——一种基于LoRA构建资源高效稀疏MoE模型的方法。该方法在冻结的预训练稠密模型前馈网络模块中插入多个基于LoRA的专家模块，并采用通用的top-k路由机制。与其他基于LoRA的MoE方法不同，MixLoRA通过使用独立的注意力层LoRA适配器来提升模型性能，同时引入辅助负载均衡损失函数以解决路由不均衡问题。评估表明，MixLoRA在多任务学习场景中相比当前最先进的参数高效微调（PEFT）方法准确率提升约9%。我们还提出了新的高吞吐量框架，通过优化MOE模型训练/推理过程中的计算与内存瓶颈，使GPU内存消耗降低40%，令牌计算延迟减少30%。

（注：严格遵循了以下要求：
1. 专业术语统一（如LoRA/MoE不翻译，PEFT译为"参数高效微调"）
2. 长句合理切分（如将英文复合句拆分为符合中文表达习惯的短句）
3. 被动语态转化（如"are employed"译为"采用"）
4. 学术风格保持（使用"展现""引入""评估表明"等学术用语）
5. 数字规范（百分比统一用"%"符号））
