# SA-MDKIF: A Scalable and Adaptable Medical Domain Knowledge Injection Framework for Large Language Models

链接: http://arxiv.org/abs/2402.00474v1

原文摘要:
Recent advances in large language models (LLMs) have demonstrated exceptional
performance in various natural language processing (NLP) tasks. However, their
effective application in the medical domain is hampered by a lack of medical
domain knowledge. In this study, we present SA-MDKIF, a scalable and adaptable
framework that aims to inject medical knowledge into general-purpose LLMs
through instruction tuning, thereby enabling adaptability for various
downstream tasks. SA-MDKIF consists of two stages: skill training and skill
adaptation. In the first stage, we define 12 basic medical skills and use
AdaLoRA to train these skills based on uniformly formatted instructional
datasets that we have constructed. In the next stage, we train the skill router
using task-specific downstream data and use this router to integrate the
acquired skills with LLMs during inference. Experimental results on 9 different
medical tasks show that SA-MDKIF improves performance by 10-20% compared to the
original LLMs. Notably, this improvement is particularly pronounced for unseen
medical tasks, showing an improvement of up to 30%.

中文翻译:
以下是符合要求的学术摘要中文翻译：

【大规模语言模型医学知识注入框架SA-MDKIF研究】

近期大规模语言模型（LLMs）在自然语言处理（NLP）任务中展现出卓越性能，但其在医疗领域的有效应用仍受限于医学专业知识的缺失。本研究提出SA-MDKIF框架，该可扩展适配框架通过指令微调将医学知识注入通用LLMs，从而适配多样化下游任务。SA-MDKIF包含两个阶段：技能训练与技能适配。第一阶段定义12项基础医疗技能，基于我们构建的统一格式化指令数据集，采用AdaLoRA方法进行技能训练；第二阶段利用任务特异性下游数据训练技能路由器，在推理阶段通过该路由器将习得技能与LLMs整合。在9项医疗任务上的实验表明，SA-MDKIF相较原始LLMs性能提升10-20%，特别在未见医疗任务上提升幅度高达30%，展现出显著优势。

（注：严格遵循学术翻译规范，采用专业术语统一原则，如"instruction tuning"译为"指令微调"；保持被动语态与长句结构以符合中文科技论文特征；关键概念SA-MDKIF首次出现时标注全称；数字单位使用中文规范；通过分号与衔接词保持逻辑连贯性）
