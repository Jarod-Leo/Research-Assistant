# Binary Neural Networks for Large Language Model: A Survey

链接: http://arxiv.org/abs/2502.19008v1

原文摘要:
Large language models (LLMs) have wide applications in the field of natural
language processing(NLP), such as GPT-4 and Llama. However, with the
exponential growth of model parameter sizes, LLMs bring significant resource
overheads. Low-bit quantization, as a key technique, reduces memory usage and
computational demands by decreasing the bit-width of model parameters,
activations, and gradients. Previous quantization methods for LLMs have largely
employed Post-Training Quantization (PTQ) and Quantization-Aware Training
(QAT). PTQ does not require any retraining of the original model, while QAT
involves optimizing precision during training to achieve the best quantization
parameters. The BitNet team proposed a radically different approach, where
quantization is performed from the start of model training, utilizing
low-precision binary weights during the training process. This approach has led
to the emergence of many binary quantization techniques for large language
models. This paper provides a comprehensive review of these binary quantization
techniques. Specifically, we will introduce binary quantization techniques in
deep neural networks and further explore their application to LLMs, reviewing
their various contributions, implementations, and applications.

中文翻译:
以下是您提供的英文论文摘要的中文翻译：

大语言模型（LLMs）在自然语言处理（NLP）领域具有广泛应用，例如GPT-4和Llama。然而随着模型参数规模的指数级增长，LLMs带来了显著的资源开销。低比特量化作为关键技术，通过降低模型参数、激活值和梯度的位宽来减少内存占用和计算需求。现有LLM量化方法主要采用训练后量化（PTQ）和量化感知训练（QAT）：PTQ无需对原始模型进行任何重训练，而QAT则通过在训练过程中优化精度来获得最佳量化参数。BitNet团队提出了一种截然不同的范式——从模型训练伊始就进行量化，在训练过程中直接使用低精度二值权重。这种创新方法催生了许多面向大语言模型的二值量化技术。本文系统梳理了这些二值量化技术：首先介绍深度神经网络中的二值量化方法，进而深入探讨其在LLMs中的应用，详细评述各类技术的核心贡献、实现方式与应用场景。

（翻译说明：
1. 专业术语处理：LLMs统一译为"大语言模型"，PTQ/QAT等首现时保留英文缩写并标注全称
2. 长句拆分重构：将原文复合句按中文表达习惯拆分为多个短句，如将"where quantization..."从句独立为中文短句
3. 逻辑显化：通过"首先...进而..."等连接词强化技术路线的递进关系
4. 动态对等："radically different approach"译为"截然不同的范式"既保留学术性又符合中文表达
5. 术语统一性："binary quantization"全篇统一译为"二值量化"保持概念一致性）
