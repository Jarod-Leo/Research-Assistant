# OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection for Memory-Efficient LLM Fine-tuning

链接: http://arxiv.org/abs/2405.18380v1

原文摘要:
The rapid advancements in Large Language Models (LLMs) have revolutionized
various natural language processing tasks. However, the substantial size of
LLMs presents significant challenges in training or fine-tuning. While
parameter-efficient approaches such as low-rank adaptation (LoRA) have gained
popularity, they often compromise performance compared to full-rank
fine-tuning. In this paper, we propose Outlier-weighed Layerwise Sampled
Low-Rank Projection (OwLore), a new memory-efficient fine-tuning approach,
inspired by the layerwise outlier distribution of LLMs. Unlike LoRA, which adds
extra adapters to all layers, OwLore strategically assigns higher sampling
probabilities to layers with more outliers, selectively sampling only a few
layers and fine-tuning their pre-trained weights. To further increase the
number of fine-tuned layers without a proportional rise in memory costs, we
incorporate gradient low-rank projection, further boosting the approach's
performance. Our extensive experiments across various architectures, including
LLaMa2, LLaMa3, and Mistral, demonstrate that OwLore consistently outperforms
baseline approaches, including full fine-tuning. Specifically, it achieves up
to a 1.1% average accuracy gain on the Commonsense Reasoning benchmark, a 3.0%
improvement on MMLU, and a notable 10% boost on MT-Bench, while being more
memory efficient. OwLore allows us to fine-tune LLaMa2-7B with only 21GB of
memory. Code is available at https://github.com/pixeli99/OwLore.

中文翻译:
以下为符合学术规范的中文翻译：

大语言模型（LLMs）的快速发展为自然语言处理任务带来了革命性变革。然而，LLMs庞大的参数量给训练或微调带来了巨大挑战。虽然低秩自适应（LoRA）等参数高效方法广受欢迎，但与全秩微调相比，其性能往往有所折损。本文提出一种新型内存高效微调方法——异常值加权分层采样低秩投影（OwLore），其设计灵感源自LLMs的层间异常值分布特征。与LoRA在所有层级添加适配器的策略不同，OwLore通过智能分配采样概率（对异常值较多的层级赋予更高概率），仅选择性采样部分层级并微调其预训练权重。为进一步增加可微调层数而不显著增加内存开销，我们引入梯度低秩投影技术，从而进一步提升方法性能。基于LLaMa2、LLaMa3和Mistral等架构的大规模实验表明，OwLore在包括全参数微调在内的基线方法中均表现优异：在常识推理基准测试中平均准确率提升1.1%，在MMLU上提升3.0%，在MT-Bench上显著提升10%，同时保持更高内存效率。OwLore仅需21GB内存即可完成LLaMa2-7B的微调。代码已开源：https://github.com/pixeli99/OwLore。

（翻译严格遵循以下原则：
1. 专业术语统一处理（如"fine-tuning"统一译为"微调"）
2. 被动语态转换为中文主动句式（如"are incorporated"译为"我们引入"）
3. 长难句拆分重组（如将原文复合从句分解为多个短句）
4. 数字与单位格式规范转换（如"21GB"保留原格式）
5. 学术用语准确（如"benchmark"译为"基准测试"而非"标杆"）
6. 技术概念保留英文缩写同时提供中文全称（如首次出现"LoRA"时标注"低秩自适应"））
