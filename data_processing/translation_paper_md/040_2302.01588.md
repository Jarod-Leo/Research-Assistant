# Bioformer: an efficient transformer language model for biomedical text mining

链接: http://arxiv.org/abs/2302.01588v1

原文摘要:
Pretrained language models such as Bidirectional Encoder Representations from
Transformers (BERT) have achieved state-of-the-art performance in natural
language processing (NLP) tasks. Recently, BERT has been adapted to the
biomedical domain. Despite the effectiveness, these models have hundreds of
millions of parameters and are computationally expensive when applied to
large-scale NLP applications. We hypothesized that the number of parameters of
the original BERT can be dramatically reduced with minor impact on performance.
In this study, we present Bioformer, a compact BERT model for biomedical text
mining. We pretrained two Bioformer models (named Bioformer8L and Bioformer16L)
which reduced the model size by 60% compared to BERTBase. Bioformer uses a
biomedical vocabulary and was pre-trained from scratch on PubMed abstracts and
PubMed Central full-text articles. We thoroughly evaluated the performance of
Bioformer as well as existing biomedical BERT models including BioBERT and
PubMedBERT on 15 benchmark datasets of four different biomedical NLP tasks:
named entity recognition, relation extraction, question answering and document
classification. The results show that with 60% fewer parameters, Bioformer16L
is only 0.1% less accurate than PubMedBERT while Bioformer8L is 0.9% less
accurate than PubMedBERT. Both Bioformer16L and Bioformer8L outperformed
BioBERTBase-v1.1. In addition, Bioformer16L and Bioformer8L are 2-3 fold as
fast as PubMedBERT/BioBERTBase-v1.1. Bioformer has been successfully deployed
to PubTator Central providing gene annotations over 35 million PubMed abstracts
and 5 million PubMed Central full-text articles. We make Bioformer publicly
available via https://github.com/WGLab/bioformer, including pre-trained models,
datasets, and instructions for downstream use.

中文翻译:
以下是符合要求的学术中文翻译：

基于Transformer的双向编码器表征模型（BERT）等预训练语言模型已在自然语言处理任务中取得最优性能。近期BERT被成功应用于生物医学领域。尽管效果显著，这类模型通常具有数亿参数，在大规模自然语言处理应用中计算成本高昂。我们提出假设：在性能影响最小化的前提下，原始BERT模型的参数量可大幅削减。本研究介绍Bioformer——一种面向生物医学文本挖掘的轻量化BERT模型。我们预训练了Bioformer8L和Bioformer16L两个版本，相较BERTBase模型尺寸减少60%。该模型采用生物医学专业词表，并在PubMed摘要及PubMed Central全文数据上从头训练。我们在15个基准数据集上系统评估了Bioformer与现有生物医学BERT模型（包括BioBERT和PubMedBERT）在四大类任务中的表现：命名实体识别、关系抽取、问答系统和文档分类。实验表明：参数量减少60%的情况下，Bioformer16L准确率仅比PubMedBERT低0.1%，Bioformer8L低0.9%。两个版本均优于BioBERTBase-v1.1，且推理速度达到PubMedBERT/BioBERTBase-v1.1的2-3倍。目前Bioformer已成功部署于PubTator Central平台，为超过3500万篇PubMed摘要和500万篇PMC全文提供基因注释。我们通过https://github.com/WGLab/bioformer公开了预训练模型、数据集及下游应用指南。

