# Continuum Attention for Neural Operators

链接: http://arxiv.org/abs/2406.06486v1

原文摘要:
Transformers, and the attention mechanism in particular, have become
ubiquitous in machine learning. Their success in modeling nonlocal, long-range
correlations has led to their widespread adoption in natural language
processing, computer vision, and time-series problems. Neural operators, which
map spaces of functions into spaces of functions, are necessarily both
nonlinear and nonlocal if they are universal; it is thus natural to ask whether
the attention mechanism can be used in the design of neural operators.
Motivated by this, we study transformers in the function space setting. We
formulate attention as a map between infinite dimensional function spaces and
prove that the attention mechanism as implemented in practice is a Monte Carlo
or finite difference approximation of this operator. The function space
formulation allows for the design of transformer neural operators, a class of
architectures designed to learn mappings between function spaces, for which we
prove a universal approximation result. The prohibitive cost of applying the
attention operator to functions defined on multi-dimensional domains leads to
the need for more efficient attention-based architectures. For this reason we
also introduce a function space generalization of the patching strategy from
computer vision, and introduce a class of associated neural operators.
Numerical results, on an array of operator learning problems, demonstrate the
promise of our approaches to function space formulations of attention and their
use in neural operators.

中文翻译:
以下是符合您要求的学术中文翻译：

Transformer架构（尤其是其中的注意力机制）已在机器学习领域得到广泛应用。该架构在建模非局部性长程关联方面的成功，使其被广泛采纳于自然语言处理、计算机视觉和时间序列分析等领域。神经算子作为函数空间到函数空间的映射，若要具备通用性则必须兼具非线性和非局部特性——这自然引出一个核心问题：注意力机制能否用于神经算子的设计？

基于此研究动机，我们在函数空间框架下对Transformer进行理论探索。首先将注意力机制构建为无限维函数空间之间的映射，并证明实际应用中的注意力机制是该算子的蒙特卡洛或有限差分近似。这种函数空间表述使我们能够构建"Transformer神经算子"这一新型架构，专门用于学习函数空间之间的映射关系，我们已为此类架构证明了通用逼近定理。

由于将注意力算子应用于多维域定义函数时存在计算复杂度爆炸的问题，我们进一步提出了更高效的注意力架构方案：一方面将计算机视觉中的分块策略推广到函数空间，另一方面构建了相应的新型神经算子类别。在多种算子学习任务上的数值实验表明，我们所提出的函数空间注意力表述方法及其在神经算子中的应用具有显著优势。

（注：本译文严格遵循学术规范，采用术语统一原则，如"nonlocal"译为"非局部性"而非"非本地"；通过拆分英文长句为中文短句结构；保留"universal approximation theorem"等专业概念的准确译法；对数学概念如"Monte Carlo approximation"采用学界通用译名"蒙特卡洛近似"）
