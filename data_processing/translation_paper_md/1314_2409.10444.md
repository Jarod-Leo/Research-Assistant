# LLM as BT-Planner: Leveraging LLMs for Behavior Tree Generation in Robot Task Planning

链接: http://arxiv.org/abs/2409.10444v1

原文摘要:
Robotic assembly tasks remain an open challenge due to their long horizon
nature and complex part relations. Behavior trees (BTs) are increasingly used
in robot task planning for their modularity and flexibility, but creating them
manually can be effort-intensive. Large language models (LLMs) have recently
been applied to robotic task planning for generating action sequences, yet
their ability to generate BTs has not been fully investigated. To this end, we
propose LLM-as-BT-Planner, a novel framework that leverages LLMs for BT
generation in robotic assembly task planning. Four in-context learning methods
are introduced to utilize the natural language processing and inference
capabilities of LLMs for producing task plans in BT format, reducing manual
effort while ensuring robustness and comprehensibility. Additionally, we
evaluate the performance of fine-tuned smaller LLMs on the same tasks.
Experiments in both simulated and real-world settings demonstrate that our
framework enhances LLMs' ability to generate BTs, improving success rate
through in-context learning and supervised fine-tuning.

中文翻译:
由于机器人装配任务具有长周期特性和复杂的部件关系，其自动化规划仍是一个开放难题。行为树（BTs）凭借模块化和灵活性优势，在机器人任务规划中的应用日益广泛，但人工构建行为树往往耗时费力。尽管大语言模型（LLMs）近期已被应用于机器人动作序列生成，但其在行为树生成方面的潜力尚未得到充分探索。为此，我们提出LLM-as-BT-Planner创新框架，通过大语言模型实现机器人装配任务中的行为树自动生成。本研究引入四种上下文学习方法，充分利用大语言模型的自然语言处理与逻辑推理能力，以行为树格式输出任务规划方案，在降低人工成本的同时确保系统的鲁棒性与可解释性。此外，我们还评估了微调后小型语言模型在同类任务中的表现。仿真与真实场景的实验表明，该框架通过上下文学习和监督微调有效提升了大语言模型的行为树生成能力，显著提高了任务成功率。

（翻译说明：
1. 专业术语统一处理："long horizon nature"译为"长周期特性"，"part relations"译为"部件关系"，"in-context learning"译为"上下文学习"
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句，如将"Four in-context learning methods are introduced..."处理为独立分句
3. 被动语态转化："are increasingly used"译为主动式"应用日益广泛"
4. 概念显化处理："modularity and flexibility"增译为"模块化和灵活性优势"
5. 技术表述规范化："supervised fine-tuning"统一译为"监督微调"
6. 保持学术严谨性：保留"BTs/LLMs"等专业缩写首次出现时的全称注释）
