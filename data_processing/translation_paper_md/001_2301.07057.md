# Transformer Based Implementation for Automatic Book Summarization

链接: http://arxiv.org/abs/2301.07057v1

原文摘要:
Document Summarization is the procedure of generating a meaningful and
concise summary of a given document with the inclusion of relevant and
topic-important points. There are two approaches: one is picking up the most
relevant statements from the document itself and adding it to the Summary known
as Extractive and the other is generating sentences for the Summary known as
Abstractive Summarization. Training a machine learning model to perform tasks
that are time-consuming or very difficult for humans to evaluate is a major
challenge. Book Abstract generation is one of such complex tasks. Traditional
machine learning models are getting modified with pre-trained transformers.
Transformer based language models trained in a self-supervised fashion are
gaining a lot of attention; when fine-tuned for Natural Language
Processing(NLP) downstream task like text summarization. This work is an
attempt to use Transformer based techniques for Abstract generation.

中文翻译:
文档摘要是指通过提取与主题相关的关键信息，生成既简洁又能准确反映原文内容的概括性文本。目前主要存在两种技术路径：其一是从原文直接抽取最具代表性的语句组成摘要（抽取式摘要），其二是通过语义理解生成全新语句形成摘要（生成式摘要）。在机器学习领域，如何训练模型完成那些耗时且人工评估困难的任务始终是重大挑战，书籍摘要生成正是此类复杂任务的典型代表。随着预训练Transformer模型的发展，传统机器学习方法正在经历革新。基于Transformer架构、采用自监督训练方式的语言模型，经过自然语言处理（NLP）下游任务（如文本摘要）的微调后，已展现出显著优势。本研究旨在探索基于Transformer技术的摘要生成方法。  

