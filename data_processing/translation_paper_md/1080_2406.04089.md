# On Limitation of Transformer for Learning HMMs

链接: http://arxiv.org/abs/2406.04089v1

原文摘要:
Despite the remarkable success of Transformer-based architectures in various
sequential modeling tasks, such as natural language processing, computer
vision, and robotics, their ability to learn basic sequential models, like
Hidden Markov Models (HMMs), is still unclear. This paper investigates the
performance of Transformers in learning HMMs and their variants through
extensive experimentation and compares them to Recurrent Neural Networks
(RNNs). We show that Transformers consistently underperform RNNs in both
training speed and testing accuracy across all tested HMM models. There are
even challenging HMM instances where Transformers struggle to learn, while RNNs
can successfully do so. Our experiments further reveal the relation between the
depth of Transformers and the longest sequence length it can effectively learn,
based on the types and the complexity of HMMs. To address the limitation of
transformers in modeling HMMs, we demonstrate that a variant of the
Chain-of-Thought (CoT), called $\textit{block CoT}$ in the training phase, can
help transformers to reduce the evaluation error and to learn longer sequences
at a cost of increasing the training time. Finally, we complement our empirical
findings by theoretical results proving the expressiveness of transformers in
approximating HMMs with logarithmic depth.

中文翻译:
尽管基于Transformer的架构在自然语言处理、计算机视觉和机器人技术等序列建模任务中取得了显著成功，但其学习隐马尔可夫模型（HMMs）等基础序列模型的能力仍不明确。本文通过大量实验研究了Transformer在学习HMMs及其变体时的表现，并与循环神经网络（RNNs）进行对比。研究表明，在所有测试的HMM模型中，Transformer在训练速度和测试准确率上始终逊于RNNs。某些具有挑战性的HMM实例中，Transformer甚至完全无法有效学习，而RNNs却能成功掌握。实验进一步揭示了Transformer的深度与其能有效学习的最长序列长度之间的关系，这种关系取决于HMM的类型和复杂度。为克服Transformer在建模HMMs时的局限性，我们提出一种训练阶段的思维链（CoT）变体——$\textit{分块思维链}$（block CoT），该方案能帮助Transformer降低评估误差并学习更长序列，但会相应增加训练时间。最后，我们通过理论证明补充了实验发现：具有对数深度的Transformer架构在逼近HMMs时具备足够的表达能力。
