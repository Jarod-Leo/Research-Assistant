# MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders

链接: http://arxiv.org/abs/2409.06635v1

原文摘要:
The rapid advancements in large language models (LLMs) have significantly
enhanced natural language processing capabilities, facilitating the development
of AudioLLMs that process and understand speech and audio inputs alongside
text. Existing AudioLLMs typically combine a pre-trained audio encoder with a
pre-trained LLM, which are subsequently finetuned on specific audio tasks.
However, the pre-trained audio encoder has constrained capacity to capture
features for new tasks and datasets. To address this, we propose to incorporate
mixtures of `weak' encoders (MoWE) into the AudioLLM framework. MoWE
supplements a base encoder with a pool of relatively light weight encoders,
selectively activated based on the audio input to enhance feature extraction
without significantly increasing model size. Our empirical results demonstrate
that MoWE effectively improves multi-task performance, broadening the
applicability of AudioLLMs to more diverse audio tasks.

中文翻译:
大型语言模型（LLM）的快速发展显著提升了自然语言处理能力，推动了能够同时处理语音、音频输入与文本的AudioLLM技术发展。现有AudioLLM通常将预训练音频编码器与预训练LLM结合，并在特定音频任务上进行微调。然而，预训练音频编码器在捕捉新任务和数据集特征方面存在能力局限。为此，我们提出在AudioLLM框架中引入"弱编码器混合"（MoWE）机制。该方案通过基础编码器与一组轻量化编码器协同工作，根据音频输入动态激活特定编码器，在不显著增加模型规模的前提下增强特征提取能力。实验结果表明，MoWE能有效提升多任务性能，使AudioLLM适用于更广泛的音频处理任务。

（翻译说明：
1. 专业术语处理：LLM/AudioLLM/MoWE等首字母缩略词保留英文缩写形式，符合学术惯例
2. 技术概念转化："weak encoders"译为"弱编码器"并加引号强调其特殊含义
3. 句式重构：将英文长句拆分为符合中文表达习惯的短句，如将"which are subsequently..."独立成句
4. 被动语态转换："are selectively activated"译为主动式"动态激活"
5. 学术风格保持：使用"为此""该方案""实验结果表明"等学术用语
6. 逻辑显化：补充"机制""方案"等范畴词使技术描述更清晰）
