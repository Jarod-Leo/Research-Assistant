# Enhance Reasoning by Learning from Mistakes: Peer-Review Knowledge Distillation from Multiple Large Language Models

链接: http://arxiv.org/abs/2410.03663v1

原文摘要:
While reasoning capabilities typically emerge in large language models (LLMs)
with tens of billions of parameters, recent research focuses on improving
smaller open-source models through knowledge distillation (KD) from commercial
LLMs. However, many of these studies rely solely on responses from a single LLM
as the gold rationale, unlike the natural human learning process, which
involves understanding both the correct answers and the reasons behind
mistakes. In this paper, we introduce a novel Fault-Aware DistIllation via
Peer-Review (FAIR) approach: 1) Instead of merely obtaining rationales from
teachers, our method asks teachers to identify and explain the student's
mistakes, providing customized instruction learning data. 2) We design a
simulated peer-review process between teacher LLMs, which selects only the
generated rationales above the acceptance threshold. This reduces the chance of
teachers guessing correctly with flawed rationale, improving instructional data
quality. Comprehensive experiments and analysis on mathematical, commonsense,
and logical reasoning tasks demonstrate the effectiveness of our method.

中文翻译:
以下是符合要求的学术中文翻译：

虽然推理能力通常只在具有数百亿参数的大语言模型（LLMs）中显现，但近期研究致力于通过从商用LLMs进行知识蒸馏（KD）来改进小型开源模型。然而，与人类自然学习过程（需要同时理解正确答案和错误背后的原因）不同，这些研究大多仅依赖单一LLM的响应作为黄金标准逻辑链。本文提出了一种新颖的"基于同行评审的容错蒸馏"（FAIR）方法：1）不同于仅从教师模型获取标准解释，我们的方法要求教师模型识别并解释学生模型的错误，从而提供定制化的指导学习数据；2）我们设计了教师LLMs之间的模拟同行评审流程，仅筛选超过接受阈值的生成逻辑链。这降低了教师模型通过错误逻辑链猜测出正确答案的概率，从而提升教学数据质量。在数学推理、常识推理和逻辑推理任务上的全面实验与分析验证了本方法的有效性。

翻译说明：
1. 专业术语处理：保持"knowledge distillation"为"知识蒸馏"、"peer-review"为"同行评审"等标准译法
2. 被动语态转换：将英文被动结构转换为中文主动表达（如"are obtained"译为"获取"）
3. 长句拆分：将原文复合长句按中文习惯分解为多个短句
4. 概念显化：将"gold rationale"意译为"黄金标准逻辑链"以明确其指代
5. 技术表述规范：严格保持"LLMs"、"FAIR"等专业缩写形式
6. 逻辑连接词处理：使用"从而"、"因此"等符合中文论文表达的连接词
7. 术语一致性：全文统一"rationale"译为"逻辑链"、"teacher/student"译为"教师/学生模型"
