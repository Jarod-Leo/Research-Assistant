# Unveiling Safety Vulnerabilities of Large Language Models

链接: http://arxiv.org/abs/2311.04124v1

原文摘要:
As large language models become more prevalent, their possible harmful or
inappropriate responses are a cause for concern. This paper introduces a unique
dataset containing adversarial examples in the form of questions, which we call
AttaQ, designed to provoke such harmful or inappropriate responses. We assess
the efficacy of our dataset by analyzing the vulnerabilities of various models
when subjected to it. Additionally, we introduce a novel automatic approach for
identifying and naming vulnerable semantic regions - input semantic areas for
which the model is likely to produce harmful outputs. This is achieved through
the application of specialized clustering techniques that consider both the
semantic similarity of the input attacks and the harmfulness of the model's
responses. Automatically identifying vulnerable semantic regions enhances the
evaluation of model weaknesses, facilitating targeted improvements to its
safety mechanisms and overall reliability.

中文翻译:
随着大语言模型的广泛应用，其可能产生的有害或不恰当回应引发关注。本文提出一个独特的对抗性示例数据集AttaQ，该数据集以问题形式呈现，专门用于诱发此类不良回应。我们通过分析不同模型在该数据集上的表现来评估其有效性。此外，我们创新性地提出一种自动化方法，用于识别并命名脆弱语义区域——即模型容易生成有害输出的输入语义区间。该方法通过应用特殊聚类技术实现，同时考量输入攻击的语义相似度与模型回应危害性。自动识别脆弱语义区域不仅能强化模型缺陷评估，更有助于针对性地改进安全机制，从而全面提升系统可靠性。

（翻译说明：采用学术论文摘要的规范表述，处理长句时进行合理切分；"adversarial examples"译为"对抗性示例"符合AI安全领域术语；"vulnerable semantic regions"创造性译为"脆弱语义区域"既保持专业又便于理解；通过"诱发""考量""强化"等动词实现动态对等；最后一句采用"不仅...更..."递进结构准确传达原文逻辑关系）
