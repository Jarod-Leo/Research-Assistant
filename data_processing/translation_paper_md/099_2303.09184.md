# Block-wise Bit-Compression of Transformer-based Models

链接: http://arxiv.org/abs/2303.09184v1

原文摘要:
With the popularity of the recent Transformer-based models represented by
BERT, GPT-3 and ChatGPT, there has been state-of-the-art performance in a range
of natural language processing tasks. However, the massive computations, huge
memory footprint, and thus high latency of Transformer-based models is an
inevitable challenge for the cloud with high real-time requirement. To tackle
the issue, we propose BBCT, a method of block-wise bit-compression for
transformer without retraining. Our method achieves more fine-grained
compression of the whole transformer, including embedding, matrix
multiplication, GELU, softmax, layer normalization, and all the intermediate
results. As a case, we compress an efficient BERT with the method of BBCT. Our
benchmark test results on General Language Understanding Evaluation (GLUE) show
that BBCT can achieve less than 1% accuracy drop in most tasks.

中文翻译:
随着以BERT、GPT-3和ChatGPT为代表的Transformer模型近期广受欢迎，该架构在一系列自然语言处理任务中展现出最先进的性能。然而基于Transformer模型的海量计算、巨大内存占用及由此产生的高延迟，对于具有高实时性要求的云端部署而言仍是不可避免的挑战。为解决这一问题，我们提出BBCT（分块位压缩Transformer）方法，该方法无需重新训练即可实现Transformer模型的压缩。我们的技术实现了对整个Transformer模型更细粒度的压缩，涵盖词嵌入层、矩阵乘法、GELU激活函数、softmax归一化、层归一化以及所有中间计算结果。以高效版BERT模型为例，我们应用BBCT方法进行压缩。在通用语言理解评估基准（GLUE）上的测试结果表明，该方法在多数任务中能实现准确率下降不足1%的压缩效果。
