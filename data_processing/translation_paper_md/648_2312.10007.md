# Faithful Persona-based Conversational Dataset Generation with Large Language Models

链接: http://arxiv.org/abs/2312.10007v1

原文摘要:
High-quality conversational datasets are essential for developing AI models
that can communicate with users. One way to foster deeper interactions between
a chatbot and its user is through personas, aspects of the user's character
that provide insights into their personality, motivations, and behaviors.
Training Natural Language Processing (NLP) models on a diverse and
comprehensive persona-based dataset can lead to conversational models that
create a deeper connection with the user, and maintain their engagement. In
this paper, we leverage the power of Large Language Models (LLMs) to create a
large, high-quality conversational dataset from a seed dataset. We propose a
Generator-Critic architecture framework to expand the initial dataset, while
improving the quality of its conversations. The Generator is an LLM prompted to
output conversations. The Critic consists of a mixture of expert LLMs that
control the quality of the generated conversations. These experts select the
best generated conversations, which we then use to improve the Generator. We
release Synthetic-Persona-Chat, consisting of 20k conversations seeded from
Persona-Chat. We evaluate the quality of Synthetic-Persona-Chat and our
generation framework on different dimensions through extensive experiments, and
observe that the losing rate of Synthetic-Persona-Chat against Persona-Chat
during Turing test decreases from 17.2% to 8.8% over three iterations.

中文翻译:
高质量的对话数据集对于开发能与用户交流的AI模型至关重要。促进聊天机器人与用户深度互动的一种方式是通过"人物角色"——即反映用户性格特征、动机和行为的个性化要素。基于多样化、全面的人物角色数据集来训练自然语言处理（NLP）模型，可以构建出与用户建立深层联系并保持互动黏性的对话模型。本文利用大语言模型（LLMs）的强大能力，从种子数据集创建大规模高质量的对话数据集。我们提出生成器-评判器架构框架，在扩展初始数据集的同时提升对话质量：生成器是由提示驱动的LLM负责输出对话；评判器则由混合专家LLM组成，负责控制生成对话的质量。这些专家模型筛选最优生成对话，进而用于改进生成器。我们发布了包含2万条对话的Synthetic-Persona-Chat数据集（源自Persona-Chat种子数据）。通过多维度实验评估，发现经过三次迭代后，Synthetic-Persona-Chat在图灵测试中相对于Persona-Chat的失败率从17.2%降至8.8%，验证了数据集质量和生成框架的有效性。
