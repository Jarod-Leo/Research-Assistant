# Feed-Forward Blocks Control Contextualization in Masked Language Models

链接: http://arxiv.org/abs/2302.00456v1

原文摘要:
Transformers are ubiquitous in wide tasks. Interpreting their internals is a
pivotal goal. Nevertheless, their particular components, feed-forward (FF)
blocks, have typically been less analyzed despite their substantial parameter
amounts. We analyze the input contextualization effects of FF blocks by
rendering them in the attention maps as a human-friendly visualization scheme.
Our experiments with both masked- and causal-language models reveal that FF
networks modify the input contextualization to emphasize specific types of
linguistic compositions. In addition, FF and its surrounding components tend to
cancel out each other's effects, suggesting potential redundancy in the
processing of the Transformer layer.

中文翻译:
Transformer模型已广泛应用于各类任务中，理解其内部机制成为关键目标。然而，尽管前馈网络（FF）模块参数量庞大，却鲜有深入研究。本研究通过将FF模块在注意力图谱中进行可视化呈现，以人类可读的方式解析其对输入语境化的影响。基于掩码语言模型和因果语言模型的实验表明：前馈网络通过调整输入语境化来强化特定类型的语言组合模式。此外，FF模块与其相邻组件往往产生相互抵消的效果，这暗示了Transformer层在处理过程中可能存在冗余结构。

