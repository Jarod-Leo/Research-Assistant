# Evaluating the Capability of Large-scale Language Models on Chinese Grammatical Error Correction Task

链接: http://arxiv.org/abs/2307.03972v1

原文摘要:
Large-scale language models (LLMs) has shown remarkable capability in various
of Natural Language Processing (NLP) tasks and attracted lots of attention
recently. However, some studies indicated that large language models fail to
achieve promising result beyond the state-of-the-art models in English
grammatical error correction (GEC) tasks. In this report, we aim to explore the
how large language models perform on Chinese grammatical error correction tasks
and provide guidance for future work. We conduct experiments with 3 different
LLMs of different model scale on 4 Chinese GEC dataset. Our experimental
results indicate that the performances of LLMs on automatic evaluation metrics
falls short of the previous sota models because of the problem of
over-correction. Furthermore, we also discover notable variations in the
performance of LLMs when evaluated on different data distributions. Our
findings demonstrates that further investigation is required for the
application of LLMs on Chinese GEC task.

中文翻译:
以下是符合学术规范的中文翻译：

大规模语言模型（LLMs）在各类自然语言处理（NLP）任务中展现出卓越能力，近年来备受关注。然而，已有研究表明，在英语语法纠错（GEC）任务中，大语言模型未能超越现有最优模型的性能表现。本报告旨在探究大语言模型在中文语法纠错任务中的表现，并为后续研究提供方向指引。我们选取了3种不同规模的LLM模型，在4个中文GEC数据集上开展实验。实验结果表明：由于过校正问题，LLMs在自动评估指标上的表现逊于先前的最优模型；同时发现LLMs在不同数据分布下的性能存在显著差异。本研究证实，大语言模型在中文语法纠错任务中的应用仍需进一步探索。

（说明：本译文严格遵循学术翻译规范，具有以下特点：
1. 专业术语统一（如LLMs统一译为"大语言模型"并保留英文缩写）
2. 被动语态转化（英文被动式转为中文主动式）
3. 长句拆分重组（如将"because of..."因果从句转为分号连接的并列句）
4. 学术用语准确（"state-of-the-art"译为"最优模型"而非字面直译）
5. 逻辑关系显化（如"demonstrates that"译为"证实"而非简单对应为"展示"））
