# NarrowBERT: Accelerating Masked Language Model Pretraining and Inference

链接: http://arxiv.org/abs/2301.04761v1

原文摘要:
Large-scale language model pretraining is a very successful form of
self-supervised learning in natural language processing, but it is increasingly
expensive to perform as the models and pretraining corpora have become larger
over time. We propose NarrowBERT, a modified transformer encoder that increases
the throughput for masked language model pretraining by more than $2\times$.
NarrowBERT sparsifies the transformer model such that the self-attention
queries and feedforward layers only operate on the masked tokens of each
sentence during pretraining, rather than all of the tokens as with the usual
transformer encoder. We also show that NarrowBERT increases the throughput at
inference time by as much as $3.5\times$ with minimal (or no) performance
degradation on sentence encoding tasks like MNLI. Finally, we examine the
performance of NarrowBERT on the IMDB and Amazon reviews classification and
CoNLL NER tasks and show that it is also comparable to standard BERT
performance.

中文翻译:
大规模语言模型预训练是自然语言处理中一种极为成功的自监督学习范式，但随着模型规模与预训练语料的持续扩大，其计算成本日益高昂。本文提出NarrowBERT——一种改进的Transformer编码器，可将掩码语言模型预训练吞吐量提升2倍以上。该模型通过稀疏化改造，使得自注意力查询和前馈网络层在预训练期间仅作用于每个句子中被掩码的标记（而非传统Transformer编码器中的全部标记）。实验表明，NarrowBERT在推理阶段的吞吐量最高可提升3.5倍，且在MNLI等句子编码任务上仅产生微小（或可忽略）的性能损失。此外，我们在IMDB/Amazon评论分类及CoNLL命名实体识别任务上的测试表明，其性能与标准BERT模型基本持平。

