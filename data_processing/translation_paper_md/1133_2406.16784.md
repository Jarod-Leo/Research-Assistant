# The Progression of Transformers from Language to Vision to MOT: A Literature Review on Multi-Object Tracking with Transformers

链接: http://arxiv.org/abs/2406.16784v1

原文摘要:
The transformer neural network architecture allows for autoregressive
sequence-to-sequence modeling through the use of attention layers. It was
originally created with the application of machine translation but has
revolutionized natural language processing. Recently, transformers have also
been applied across a wide variety of pattern recognition tasks, particularly
in computer vision. In this literature review, we describe major advances in
computer vision utilizing transformers. We then focus specifically on
Multi-Object Tracking (MOT) and discuss how transformers are increasingly
becoming competitive in state-of-the-art MOT works, yet still lag behind
traditional deep learning methods.

中文翻译:
以下是符合要求的学术中文翻译：

基于注意力机制的变压器神经网络架构能够实现自回归的序列到序列建模。该架构最初是为机器翻译任务设计的，如今已彻底革新了自然语言处理领域。近年来，变压器模型更被广泛应用于各类模式识别任务，尤其在计算机视觉领域表现突出。本文献综述首先系统阐述了变压器模型推动计算机视觉发展的重大突破，继而聚焦多目标跟踪（MOT）这一具体方向，深入探讨了当前研究现状：尽管变压器方法在顶尖MOT研究中正展现出越来越强的竞争力，但其性能仍稍逊于传统深度学习方法。

（翻译说明：）
1. 专业术语处理：
- "transformer"译为行业通用术语"变压器"而非直译"变形金刚"
- "autoregressive"采用计算机领域标准译法"自回归"
- "state-of-the-art"译为"顶尖/最先进"符合学术惯例

2. 句式重构：
- 将原文复合句拆分为符合中文表达习惯的短句
- "has revolutionized"译为"彻底革新"增强动词表现力
- "yet still lag behind"转化为"尽管...但..."的转折句式

3. 学术规范：
- 保持被动语态与主动语态的合理转换
- 专业缩写"MOT"首次出现时标注全称
- 使用"阐述""聚焦""探讨"等学术动词

4. 风格统一：
- 全文采用客观中立的学术语体
- 避免口语化表达，如"最近"改为"近年来"
- 关键概念前后译名保持一致
