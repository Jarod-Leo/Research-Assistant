# Learning from Predictions: Fusing Training and Autoregressive Inference for Long-Term Spatiotemporal Forecasts

链接: http://arxiv.org/abs/2302.11101v1

原文摘要:
Recurrent Neural Networks (RNNs) have become an integral part of modeling and
forecasting frameworks in areas like natural language processing and
high-dimensional dynamical systems such as turbulent fluid flows. To improve
the accuracy of predictions, RNNs are trained using the Backpropagation Through
Time (BPTT) method to minimize prediction loss. During testing, RNNs are often
used in autoregressive scenarios where the output of the network is fed back
into the input. However, this can lead to the exposure bias effect, as the
network was trained to receive ground-truth data instead of its own
predictions. This mismatch between training and testing is compounded when the
state distributions are different, and the train and test losses are measured.
To address this, previous studies have proposed solutions for language
processing networks with probabilistic predictions. Building on these advances,
we propose the Scheduled Autoregressive BPTT (BPTT-SA) algorithm for predicting
complex systems. Our results show that BPTT-SA effectively reduces iterative
error propagation in Convolutional RNNs and Convolutional Autoencoder RNNs, and
demonstrate its capabilities in long-term prediction of high-dimensional fluid
flows.

中文翻译:
循环神经网络（RNN）已成为自然语言处理和高维动态系统（如湍流流体）建模与预测框架的核心组成部分。为提升预测精度，研究者通常采用时间反向传播算法（BPTT）训练RNN以最小化预测误差。然而在测试阶段，当网络以自回归模式运行时（即将输出反馈作为输入），会出现暴露偏差问题——因为网络训练时接收的是真实数据而非自身预测结果。当状态分布存在差异且需对比训练/测试损失时，这种训练与测试的不匹配会进一步加剧。针对该问题，先前研究已为概率预测型语言处理网络提出了解决方案。基于这些进展，我们提出适用于复杂系统预测的"计划性自回归BPTT算法"（BPTT-SA）。实验表明，该算法能有效抑制卷积RNN和卷积自编码器RNN中的迭代误差传播，并在高维流体流动的长期预测中展现出卓越性能。

