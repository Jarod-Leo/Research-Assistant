# Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL through Workflow Paradigm

链接: http://arxiv.org/abs/2402.10671v1

原文摘要:
In-context learning of large-language models (LLMs) has achieved remarkable
success in the field of natural language processing, while extensive case
studies reveal that the single-step chain-of-thought prompting approach faces
challenges such as attention diffusion and inadequate performance in complex
tasks like text-to-SQL. To improve the contextual learning capabilities of LLMs
in text-to-SQL, a workflow paradigm method is proposed, aiming to enhance the
attention and problem-solving scope of LLMs through decomposition.
Specifically, the information determination module for eliminating redundant
information and the brand-new prompt structure based on problem classification
greatly enhance the model's attention. Additionally, the inclusion of
self-correction and active learning modules greatly expands the problem-solving
scope of LLMs, hence improving the upper limit of LLM-based approaches.
Extensive experiments conducted on three datasets demonstrate that our approach
outperforms other methods by a significant margin. About 2-3 percentage point
improvements compared to the existing baseline on the Spider Dev,
Spider-Realistic, and Bird Dev datasets and new SOTA results on the Spider Test
dataset are achieved. Our code is available on GitHub:
\url{https://github.com/FlyingFeather/DEA-SQL}.

中文翻译:
以下是符合要求的学术论文摘要中文翻译：

大语言模型（LLMs）的上下文学习在自然语言处理领域取得了显著成就，然而大量案例研究表明，单步思维链提示方法存在注意力分散等问题，在文本到SQL等复杂任务中表现欠佳。为提升LLMs在文本到SQL任务中的上下文学习能力，本文提出一种工作流范式方法，旨在通过任务分解增强模型的注意力集中度与问题解决范围。具体而言，用于消除冗余信息的信息判定模块与基于问题分类的新型提示结构显著提升了模型注意力；而自校正模块和主动学习模块的引入则大幅扩展了LLMs的问题解决边界，从而提升了基于LLM方法的上限。在三个基准数据集上的大量实验表明，本方法显著优于现有方案：在Spider Dev、Spider-Realistic和Bird Dev数据集上较现有基线提升2-3个百分点，并在Spider Test数据集上创造了新的SOTA记录。项目代码已开源：\url{https://github.com/FlyingFeather/DEA-SQL}。

（翻译严格遵循以下原则：
1. 专业术语准确统一（如LLMs、SOTA等）
2. 被动语态转换为中文主动表达（如"are achieved"译为"创造了"）
3. 长句合理切分（如将原文复合句拆解为多个短句）
4. 学术用语规范（如"baseline"译为"基线"而非"基准线"）
5. 保留技术细节完整性（如各模块名称及数据集名称）
6. 数字表达符合中文习惯（"2-3 percentage points"译为"2-3个百分点"））
