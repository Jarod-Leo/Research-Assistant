# From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient LLMs

链接: http://arxiv.org/abs/2504.13471v1

原文摘要:
In recent years, Large Language Models (LLMs) have significantly advanced
artificial intelligence by optimizing traditional Natural Language Processing
(NLP) pipelines, improving performance and generalization. This has spurred
their integration into various systems. Many NLP systems, including ours,
employ a "one-stage" pipeline directly incorporating LLMs. While effective,
this approach incurs substantial costs and latency due to the need for large
model parameters to achieve satisfactory outcomes. This paper introduces a
three-stage cost-efficient end-to-end LLM deployment pipeline-including
prototyping, knowledge transfer, and model compression-to tackle the
cost-performance dilemma in LLM-based frameworks. Our approach yields a super
tiny model optimized for cost and performance in online systems, simplifying
the system architecture. Initially, by transforming complex tasks into a
function call-based LLM-driven pipeline, an optimal performance prototype
system is constructed to produce high-quality data as a teacher model. The
second stage combines techniques like rejection fine-tuning, reinforcement
learning, and knowledge distillation to transfer knowledge to a smaller 0.5B
student model, delivering effective performance at minimal cost. The final
stage applies quantization and pruning to extremely compress models to 0.4B,
achieving ultra-low latency and cost. The framework's modular design and
cross-domain capabilities suggest potential applicability in other NLP areas.

中文翻译:
近年来，大型语言模型（LLMs）通过优化传统自然语言处理（NLP）流程，显著提升了人工智能的性能与泛化能力，这推动了其在各类系统中的整合应用。包括本系统在内的许多NLP系统采用直接整合LLMs的"单阶段"流程，虽然有效，但为获得理想效果所需的大规模模型参数会导致高昂成本与延迟。本文提出一种三阶段高性价比端到端LLM部署方案——包含原型构建、知识迁移和模型压缩——以解决基于LLM框架的成本与性能矛盾。我们的方法最终生成一个为在线系统成本与性能优化的超小型模型，同时简化了系统架构。

具体而言：首先通过将复杂任务转化为基于函数调用的LLM驱动流程，构建具有最佳性能的原型系统作为教师模型，用于生成高质量数据；第二阶段结合拒绝微调、强化学习和知识蒸馏等技术，将知识迁移至0.5B参数量的小型学生模型，以最低成本实现有效性能；最终阶段采用量化和剪枝技术将模型极致压缩至0.4B，达成超低延迟与成本。该框架的模块化设计和跨领域能力表明其在其他NLP领域也具有潜在适用性。

（注：B表示"十亿"参数单位，根据学术惯例保留原计量单位；技术术语如"fine-tuning/微调"、"quantization/量化"等采用NLP领域通用译法；长句按中文表达习惯拆分为短句，同时保留原文的技术严谨性；被动语态转换为主动表述以符合中文科技文献风格）
