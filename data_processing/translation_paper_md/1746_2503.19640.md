# An Efficient Data Reuse with Tile-Based Adaptive Stationary for Transformer Accelerators

链接: http://arxiv.org/abs/2503.19640v1

原文摘要:
Transformer-based models have become the \textit{de facto} backbone across
many fields, such as computer vision and natural language processing. However,
as these models scale in size, external memory access (EMA) for weight and
activations becomes a critical bottleneck due to its significantly higher
energy consumption compared to internal computations. While most prior work has
focused on optimizing the self-attention mechanism, little attention has been
given to optimizing data transfer during linear projections, where EMA costs
are equally important. In this paper, we propose the Tile-based Adaptive
Stationary (TAS) scheme that selects the input or weight stationary in a tile
granularity, based on the input sequence length. Our experimental results
demonstrate that TAS can significantly reduce EMA by more than 97\% compared to
traditional stationary schemes, while being compatible with various attention
optimization techniques and hardware accelerators.

中文翻译:
基于Transformer的模型已成为计算机视觉和自然语言处理等诸多领域\textit{事实上的}核心架构。然而随着模型规模扩大，权重和激活值的外部存储器访问（EMA）因其能耗远高于内部计算，逐渐成为关键性能瓶颈。现有研究多聚焦于自注意力机制优化，却鲜少关注线性投影阶段的数据传输优化——该环节的EMA开销同样不可忽视。本文提出基于分块的动态驻留策略（TAS），该方案能根据输入序列长度，以分块粒度自适应选择输入驻留或权重驻留模式。实验表明，相较于传统驻留方案，TAS在兼容各类注意力优化技术与硬件加速器的同时，可降低97%以上的EMA开销。  

（注：译文严格遵循以下处理原则：  
1. 专业术语如"EMA"保留英文缩写并添加中文注释  
2. 技术概念"input/weight stationary"译为"输入驻留/权重驻留"保持学术惯例  
3. 被动语态转换为中文主动表述（如"has been focused on"→"多聚焦于"）  
4. 长难句拆分重组（如"where EMA costs..."处理为破折号补充说明）  
5. 保持数值精确性（97%→97%）  
6. 学术用语统一（"scheme"→"方案/策略"）  
7. 文化适配处理（"de facto"→"事实上的"符合中文技术文献表达））
