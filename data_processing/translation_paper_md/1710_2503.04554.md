# Compositional Translation: A Novel LLM-based Approach for Low-resource Machine Translation

链接: http://arxiv.org/abs/2503.04554v1

原文摘要:
The ability of generative large language models (LLMs) to perform in-context
learning has given rise to a large body of research into how best to prompt
models for various natural language processing tasks. Machine Translation (MT)
has been shown to benefit from in-context examples, in particular when they are
semantically similar to the sentence to translate. In this paper, we propose a
new LLM-based translation paradigm, compositional translation, to replace naive
few-shot MT with similarity-based demonstrations. An LLM is used to decompose a
sentence into simpler phrases, and then to translate each phrase with the help
of retrieved demonstrations. Finally, the LLM is prompted to translate the
initial sentence with the help of the self-generated phrase-translation pairs.
Our intuition is that this approach should improve translation because these
shorter phrases should be intrinsically easier to translate and easier to match
with relevant examples. This is especially beneficial in low-resource
scenarios, and more generally whenever the selection pool is small or out of
domain. We show that compositional translation boosts LLM translation
performance on a wide range of popular MT benchmarks, including FLORES 200,
NTREX 128 and TICO-19. Code and outputs are available at
https://github.com/ArmelRandy/compositional-translation

中文翻译:
以下是符合要求的学术化中文翻译：

生成式大语言模型（LLMs）的上下文学习能力引发了大量关于如何为不同自然语言处理任务设计最优提示的研究。研究表明，机器翻译（MT）能够受益于上下文示例，特别是当示例与待翻译句子语义相似时。本文提出一种基于LLM的新型翻译范式——组合式翻译，用以取代传统的基于相似性示例的少样本机器翻译方法。该范式首先利用LLM将句子分解为更简单的短语，然后借助检索到的示例翻译每个短语，最终提示LLM利用自生成的短语-翻译对完成整句翻译。我们的核心假设是：由于短短语本身更易翻译且更易匹配相关示例，这种方法能有效提升翻译质量。该方法在资源匮乏场景（特别是候选池规模有限或领域外数据时）优势尤为显著。实验证明，组合式翻译在FLORES 200、NTREX 128和TICO-19等多个主流机器翻译基准上显著提升了LLM的翻译性能。代码与输出结果详见https://github.com/ArmelRandy/compositional-translation

（注：根据学术规范，专业术语如LLMs/MT首次出现时保留英文缩写并添加中文全称；技术术语如"in-context learning"译为"上下文学习"符合NLP领域共识；长难句按中文表达习惯拆分重组；项目名称FLORES等保留英文原名；URL信息完整保留）
