# TabuLa: Harnessing Language Models for Tabular Data Synthesis

链接: http://arxiv.org/abs/2310.12746v1

原文摘要:
Tabular data synthesis is crucial for addressing privacy and security
concerns in industries reliant on tabular data. While recent advancements adopt
large language models (LLMs) for realistic tabular data generation, their long
training times and limited reusability hinder practical applications. In this
paper, we propose Tabula, a tabular data synthesizer that leverages the
structure of LLM. Unlike state-of-the-art (SOTA) LLM-based tabular data
synthesizers that rely on pre-trained LLMs, Tabula discards the pre-trained
weights originally designed for natural language tasks, focusing instead on a
tailored approach for tabular data. In addition, Tabula introduces a token
sequence compression strategy that significantly reduces training time while
maintaining data quality, alongside a novel token padding method that improves
sequence alignment across training batches. Experiments on six datasets show
that Tabula achieves superior synthetic data utility compared to current SOTA
methods. Additionally, the results demonstrate that Tabula model trained on
tabular datasets serves effectively as a foundational model for synthesizing
new tabular datasets. Furthermore, the proposed padding method outperforms the
conventional left and right padding strategies. Finally, the results highlight
that Tabula averagely reduces training time per epoch by 46.2% compared to
state-of-the-art LLM approaches while achieving higher data utility. Our code
is available at https://github.com/zhao-zilong/Tabula

中文翻译:
表格数据合成技术对于解决依赖表格数据的行业所面临的隐私与安全问题至关重要。尽管近期研究采用大语言模型（LLM）来生成逼真的表格数据，但其冗长的训练周期和有限的可复用性制约了实际应用。本文提出Tabula——一种基于LLM架构优化的表格数据合成器。与当前依赖预训练LLM的先进表格数据合成方案不同，Tabula摒弃了原本为自然语言任务设计的预训练权重，转而采用针对表格数据定制的全新方法。此外，Tabula创新性地提出：（1）通过令牌序列压缩策略在保证数据质量的同时显著缩短训练时间；（2）新型令牌填充方法提升训练批次间的序列对齐效果。在六个数据集上的实验表明：Tabula生成的合成数据效用优于现有最优方法；验证了基于表格数据训练的Tabula模型能有效作为合成新表格数据集的基础模型；所提填充方法性能超越传统左右填充策略；最终结果显示，Tabula在实现更高数据效用的同时，较当前最优LLM方案平均减少46.2%的单轮训练耗时。代码已开源：https://github.com/zhao-zilong/Tabula

（翻译说明：
1. 专业术语处理："tabular data"统一译为"表格数据"，"token"译为计算机领域通用译法"令牌"
2. 技术概念显化：将"structure of LLM"具体化为"LLM架构优化"，避免直译造成的理解模糊
3. 长句拆分：将原文复合长句按中文表达习惯分解为多个短句，如将实验结论部分拆分为四个并列发现
4. 被动语态转换：将"is crucial for"等被动表达转为"对于...至关重要"的主动句式
5. 逻辑连接显化：添加"此外"、"最终"等连接词强化段落衔接
6. 数字规范：保留精确百分比"46.2%"，符合科技论文表述规范
7. 补充说明：对"foundational model"增加"基础模型"的行业标准译法）
