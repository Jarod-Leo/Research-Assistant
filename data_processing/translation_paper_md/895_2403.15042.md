# LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement

链接: http://arxiv.org/abs/2403.15042v1

原文摘要:
Pretrained large language models (LLMs) are currently state-of-the-art for
solving the vast majority of natural language processing tasks. While many
real-world applications still require fine-tuning to reach satisfactory levels
of performance, many of them are in the low-data regime, making fine-tuning
challenging. To address this, we propose LLM2LLM, a targeted and iterative data
augmentation strategy that uses a teacher LLM to enhance a small seed dataset
by augmenting additional data that can be used for fine-tuning on a specific
task. LLM2LLM (1) fine-tunes a baseline student LLM on the initial seed data,
(2) evaluates and extracts data points that the model gets wrong, and (3) uses
a teacher LLM to generate synthetic data based on these incorrect data points,
which are then added back into the training data. This approach amplifies the
signal from incorrectly predicted data points by the LLM during training and
reintegrates them into the dataset to focus on more challenging examples for
the LLM. Our results show that LLM2LLM significantly enhances the performance
of LLMs in the low-data regime, outperforming both traditional fine-tuning and
other data augmentation baselines. LLM2LLM reduces the dependence on
labor-intensive data curation and paves the way for more scalable and
performant LLM solutions, allowing us to tackle data-constrained domains and
tasks. We achieve improvements up to 24.2% on the GSM8K dataset, 32.6% on
CaseHOLD, 32.0% on SNIPS, 52.6% on TREC and 39.8% on SST-2 over regular
fine-tuning in the low-data regime using a Llama-2-7B student model. Our code
is available at https://github.com/SqueezeAILab/LLM2LLM .

中文翻译:
以下是符合要求的学术中文翻译：

预训练大语言模型（LLMs）当前是解决绝大多数自然语言处理任务的最先进技术。尽管许多实际应用仍需微调以达到满意性能，但多数场景处于低数据状态，这使得微调面临挑战。为此，我们提出LLM2LLM——一种基于教师模型的定向迭代数据增强策略，通过增强适用于特定任务微调的数据来扩展小型种子数据集。该方法具体分为三步：(1) 在初始种子数据上微调基线学生LLM；(2) 评估并提取模型预测错误的数据点；(3) 利用教师LLM基于这些错误样本生成合成数据并重新注入训练集。该策略通过放大训练过程中LLM错误预测数据点的信号，并将其重新整合至数据集，使模型聚焦于更具挑战性的样本。实验结果表明，在低数据条件下，LLM2LLM显著提升模型性能，其表现优于传统微调方法及其他数据增强基线。该方法降低了对人工密集型数据标注的依赖，为构建更具扩展性和高性能的LLM解决方案开辟了新途径，使数据受限领域和任务的处理成为可能。使用Llama-2-7B学生模型时，在GSM8K数据集上相对常规微调提升24.2%，CaseHOLD提升32.6%，SNIPS提升32.0%，TREC提升52.6%，SST-2提升39.8%。代码已开源：https://github.com/SqueezeAILab/LLM2LLM。

（注：根据学术规范，技术术语如"fine-tuning"统一译为"微调"，"low-data regime"译为"低数据状态/条件"，模型名称Llama-2-7B保留原名。数据集名称GSM8K/CaseHOLD等保持英文大写形式，长数字保留阿拉伯数字格式，百分比符号使用中文全角规范。）
