# ArtGPT-4: Artistic Vision-Language Understanding with Adapter-enhanced MiniGPT-4

链接: http://arxiv.org/abs/2305.07490v1

原文摘要:
The success of large language models (LLMs) has inspired an emerging research
field of multimodal learning. However, a grand challenge of exploiting LLMs for
multimodal learning is the size of pre-trained LLMs which are always with
billions of parameters. To tackle this challenge, models such as MiniGPT-4 and
LLaVA have been developed to fine-tune the pre-trained models using fewer
parameters. Despite their promising performance, these models remain limited in
their understanding of artistic imagery. To facilitate better
artistic-understanding, in this paper, we propose ArtGPT-4, a pioneering large
vision-language model tailored to address the limitations of existing models in
artistic comprehension. The key innovation of ArtGPT-4 lies in its craft for
the sophisticated challenge of artistic image comprehension, setting it apart
from other models that overlook fine details for broader themes. Specifically,
it works by integrating some specialized adapter layers into the LLM, enabling
the model to more efficiently and effectively parse and interpret complex
visual tokens, instead of fine-tuning the whole LLM as in the existing method.
ArtGPT-4 has demonstrated its outstanding performance on the efficiency:
utilizing a Tesla A100 device, its training can be completed in mere 2 hours
with an image-text pair dataset comprising approximately 0.52M entries.
Additionally, ArtGPT-4 has also achieved state-of-the-art performance on the
ArtEmis and ArtEmis-v2.0 datasets as well as the benchmarks established in this
work, lagging behind professional artists' descriptions by a negligible 0.15
points on a 6-point scale. The outstanding performance of ArtGPT-4 shows that
it can render images with an artistic-understanding and convey the emotions
they inspire, mirroring human interpretation. The code and the pre-trained
model are accessible in \url{https://github.com/DLYuanGod/ArtGPT-4}.

中文翻译:
大型语言模型（LLM）的成功激发了多模态学习这一新兴研究领域的兴起。然而，如何利用参数量动辄数十亿的预训练LLM进行多模态学习仍面临重大挑战。为应对这一挑战，MiniGPT-4和LLaVA等模型通过减少参数量对预训练模型进行微调。尽管这些模型表现出色，但其对艺术图像的理解能力仍存在局限。为提升艺术理解能力，本文提出ArtGPT-4——这是首个针对现有模型艺术理解缺陷而定制的大型视觉语言模型。该模型的核心创新在于攻克了艺术图像解析这一复杂难题，其能捕捉其他模型为突出主题而忽略的精细细节。具体而言，ArtGPT-4通过向LLM注入专用适配层（而非如现有方法般微调整个LLM），使模型能更高效精准地解析复杂视觉表征。实验证明ArtGPT-4具有卓越效率：在Tesla A100设备上，仅需2小时即可完成约52万图文对数据集的训练。此外，该模型在ArtEmis、ArtEmis-v2.0数据集及本文构建的基准测试中均达到最先进水平，在6分制评估中仅落后专业艺术家描述0.15分。ArtGPT-4的优异表现证明其能实现类人化的艺术理解与情感传达。代码与预训练模型已开源于\url{https://github.com/DLYuanGod/ArtGPT-4}。

（翻译说明：采用学术论文摘要的标准结构，通过以下处理确保专业性：
1. 术语统一："adapter layers"译为"适配层"，"fine-tuning"统一为"微调"
2. 被动语态转换：将英文被动式转换为中文主动式（如"has been developed"译为"通过...进行"）
3. 长句拆分：将原文复合句分解为符合中文表达习惯的短句
4. 数字规范：统一使用阿拉伯数字表示具体数值
5. 专业表述："state-of-the-art"译为"最先进水平"，"benchmarks"译为"基准测试"
6. 保留技术细节：准确翻译模型架构创新点，确保技术严谨性）
