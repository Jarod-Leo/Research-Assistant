# Mixed-Distil-BERT: Code-mixed Language Modeling for Bangla, English, and Hindi

链接: http://arxiv.org/abs/2309.10272v1

原文摘要:
One of the most popular downstream tasks in the field of Natural Language
Processing is text classification. Text classification tasks have become more
daunting when the texts are code-mixed. Though they are not exposed to such
text during pre-training, different BERT models have demonstrated success in
tackling Code-Mixed NLP challenges. Again, in order to enhance their
performance, Code-Mixed NLP models have depended on combining synthetic data
with real-world data. It is crucial to understand how the BERT models'
performance is impacted when they are pretrained using corresponding code-mixed
languages. In this paper, we introduce Tri-Distil-BERT, a multilingual model
pre-trained on Bangla, English, and Hindi, and Mixed-Distil-BERT, a model
fine-tuned on code-mixed data. Both models are evaluated across multiple NLP
tasks and demonstrate competitive performance against larger models like mBERT
and XLM-R. Our two-tiered pre-training approach offers efficient alternatives
for multilingual and code-mixed language understanding, contributing to
advancements in the field.

中文翻译:
以下是符合学术规范的中文翻译：

自然语言处理领域最受欢迎的下游任务之一是文本分类。当文本呈现语码混合现象时，文本分类任务变得更具挑战性。尽管在预训练阶段未接触过此类文本，但不同BERT模型已成功应对语码混合的自然语言处理难题。值得注意的是，为提升模型性能，现有语码混合NLP模型普遍采用合成数据与真实数据相结合的策略。在此背景下，探究BERT模型使用对应语码混合语言进行预训练时的性能变化至关重要。本文提出Tri-Distil-BERT（基于孟加拉语、英语和印地语预训练的多语言模型）与Mixed-Distil-BERT（经语码混合数据微调的模型）。实验表明，两种模型在多项NLP任务中均展现出与mBERT、XLM-R等大型模型相当的竞争力。我们提出的双层预训练策略为多语言及语码混合语言理解提供了高效解决方案，推动了该领域的技术发展。

（翻译说明：
1. 专业术语处理："code-mixed"统一译为"语码混合"，"fine-tuned"译为"微调"，符合NLP领域规范
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句，如将"Though they..."从句转化为转折句式
3. 概念显化："two-tiered pre-training approach"译为"双层预训练策略"以突出方法论特征
4. 被动语态转换：将"have demonstrated success"等被动表达转为主动语态
5. 文化适配：保留"BERT/mBERT/XLM-R"等专业模型名称不翻译，维持学术严谨性
6. 逻辑衔接：通过"在此背景下"、"实验表明"等短语保持论证连贯性）
