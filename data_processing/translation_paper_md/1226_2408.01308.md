# Reconsidering Token Embeddings with the Definitions for Pre-trained Language Models

链接: http://arxiv.org/abs/2408.01308v1

原文摘要:
Learning token embeddings based on token co-occurrence statistics has proven
effective for both pre-training and fine-tuning in natural language processing.
However, recent studies have pointed out that the distribution of learned
embeddings degenerates into anisotropy (i.e., non-uniform distribution), and
even pre-trained language models (PLMs) suffer from a loss of semantics-related
information in embeddings for low-frequency tokens. This study first analyzes
the fine-tuning dynamics of encoder-based PLMs and demonstrates their
robustness against degeneration. On the basis of this analysis, we propose
DefinitionEMB, a method that utilizes definitions to re-construct isotropically
distributed and semantics-related token embeddings for encoder-based PLMs while
maintaining original robustness during fine-tuning. Our experiments demonstrate
the effectiveness of leveraging definitions from Wiktionary to re-construct
such embeddings for two encoder-based PLMs: RoBERTa-base and BART-large.
Furthermore, the re-constructed embeddings for low-frequency tokens improve the
performance of these models across various GLUE and four text summarization
datasets.

中文翻译:
基于词元共现统计信息学习词元嵌入的方法，已被证明在自然语言处理的预训练与微调阶段均具有显著效果。然而近期研究指出，习得嵌入的分布会退化为各向异性（即非均匀分布），即便是预训练语言模型（PLM）也面临低频词元嵌入中语义信息丢失的问题。本研究首先分析了基于编码器的PLM在微调过程中的动态特性，证实其对于分布退化具有较强鲁棒性。基于此发现，我们提出DefinitionEMB方法：通过利用词语定义来重构各向同性分布且保留语义关联的词元嵌入，同时保持模型在微调阶段的原始鲁棒性。实验表明，借助维基词典的定义信息，该方法能有效重构RoBERTa-base和BART-large两种编码器PLM的嵌入表征。特别值得注意的是，重构后的低频词元嵌入显著提升了这两个模型在GLUE基准测试和四个文本摘要数据集上的性能表现。

（译文特点说明：
1. 专业术语准确对应："anisotropy"译为"各向异性"，"fine-tuning"统一为"微调"
2. 长句拆分重构：将原文复合长句按中文表达习惯分解为多个短句，如将"demonstrates their robustness..."独立成句
3. 被动语态转化："has proven effective"处理为主动式"已被证明"
4. 概念显化处理："low-frequency tokens"译为"低频词元"而非直译"低频率标记"
5. 逻辑连接强化：添加"特别值得注意的是"等衔接词增强行文连贯性
6. 术语统一性：全文保持"词元嵌入"、"预训练语言模型"等核心术语的一致性）
