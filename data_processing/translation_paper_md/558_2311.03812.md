# Conversations in Galician: a Large Language Model for an Underrepresented Language

链接: http://arxiv.org/abs/2311.03812v1

原文摘要:
The recent proliferation of Large Conversation Language Models has
highlighted the economic significance of widespread access to this type of AI
technologies in the current information age. Nevertheless, prevailing models
have primarily been trained on corpora consisting of documents written in
popular languages. The dearth of such cutting-edge tools for low-resource
languages further exacerbates their underrepresentation in the current economic
landscape, thereby impacting their native speakers. This paper introduces two
novel resources designed to enhance Natural Language Processing (NLP) for the
Galician language. We present a Galician adaptation of the Alpaca dataset,
comprising 52,000 instructions and demonstrations. This dataset proves
invaluable for enhancing language models by fine-tuning them to more accurately
adhere to provided instructions. Additionally, as a demonstration of the
dataset utility, we fine-tuned LLaMA-7B to comprehend and respond in Galician,
a language not originally supported by the model, by following the Alpaca
format. This work contributes to the research on multilingual models tailored
for low-resource settings, a crucial endeavor in ensuring the inclusion of all
linguistic communities in the development of Large Language Models. Another
noteworthy aspect of this research is the exploration of how knowledge of a
closely related language, in this case, Portuguese, can assist in generating
coherent text when training resources are scarce. Both the Galician Alpaca
dataset and Cabuxa-7B are publicly accessible on our Huggingface Hub, and we
have made the source code available to facilitate replication of this
experiment and encourage further advancements for underrepresented languages.

中文翻译:
近年来大规模对话语言模型的激增，凸显了在当今信息时代普及此类人工智能技术的经济价值。然而，现有模型主要基于主流语言文档构成的语料库进行训练。针对资源稀缺语言的前沿工具匮乏，进一步加剧了这些语言在当前经济格局中的边缘化地位，从而影响了其母语使用群体。本文介绍两种旨在增强加利西亚语自然语言处理能力的新型资源：首先推出基于Alpaca数据集改造的加利西亚语版本，包含52,000条指令与示例，该数据集通过微调显著提升语言模型遵循指令的精准度；其次作为数据集效用的实证，我们采用Alpaca格式对LLaMA-7B进行微调，使其能够理解并输出该模型原本不支持的加利西亚语。本研究不仅推动了面向资源稀缺环境的多语言模型研发——这对确保所有语言社群参与大语言模型发展至关重要，还特别探索了在训练资源有限时，如何利用亲缘语言（如葡萄牙语）的知识来生成连贯文本。加利西亚语Alpaca数据集与Cabuxa-7B模型已公开于Huggingface Hub平台，相关源代码亦已开放，旨在促进实验复现并推动弱势语言技术的持续发展。
