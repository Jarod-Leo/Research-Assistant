# An Overview on Language Models: Recent Developments and Outlook

链接: http://arxiv.org/abs/2303.05759v1

原文摘要:
Language modeling studies the probability distributions over strings of
texts. It is one of the most fundamental tasks in natural language processing
(NLP). It has been widely used in text generation, speech recognition, machine
translation, etc. Conventional language models (CLMs) aim to predict the
probability of linguistic sequences in a causal manner, while pre-trained
language models (PLMs) cover broader concepts and can be used in both causal
sequential modeling and fine-tuning for downstream applications. PLMs have
their own training paradigms (usually self-supervised) and serve as foundation
models in modern NLP systems. This overview paper provides an introduction to
both CLMs and PLMs from five aspects, i.e., linguistic units, architectures,
training methods, evaluation methods, and applications. Furthermore, we discuss
the relationship between CLMs and PLMs and shed light on the future directions
of language modeling in the pre-trained era.

中文翻译:
语言建模研究文本字符串的概率分布，是自然语言处理（NLP）中最基础的任务之一，被广泛应用于文本生成、语音识别、机器翻译等领域。传统语言模型（CLM）以因果方式预测语言序列的概率，而预训练语言模型（PLM）涵盖更广泛的概念，既能用于因果序列建模，也可通过微调应用于下游任务。预训练语言模型具有独特的训练范式（通常采用自监督学习），已成为现代NLP系统的基础模型。本综述论文从五个维度——语言单元、模型架构、训练方法、评估方法及应用场景——系统介绍传统与预训练语言模型，并深入探讨二者的关联，进而为预训练时代的语言建模发展方向提供前瞻性思考。  

