# I Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders

链接: http://arxiv.org/abs/2503.18878v1

原文摘要:
Large Language Models (LLMs) have achieved remarkable success in natural
language processing. Recent advances have led to the developing of a new class
of reasoning LLMs; for example, open-source DeepSeek-R1 has achieved
state-of-the-art performance by integrating deep thinking and complex
reasoning. Despite these impressive capabilities, the internal reasoning
mechanisms of such models remain unexplored. In this work, we employ Sparse
Autoencoders (SAEs), a method to learn a sparse decomposition of latent
representations of a neural network into interpretable features, to identify
features that drive reasoning in the DeepSeek-R1 series of models. First, we
propose an approach to extract candidate ''reasoning features'' from SAE
representations. We validate these features through empirical analysis and
interpretability methods, demonstrating their direct correlation with the
model's reasoning abilities. Crucially, we demonstrate that steering these
features systematically enhances reasoning performance, offering the first
mechanistic account of reasoning in LLMs. Code available at
https://github.com/AIRI-Institute/SAE-Reasoning

中文翻译:
以下是符合学术规范的中文翻译：

大语言模型（LLMs）在自然语言处理领域取得了显著成就。最新进展催生了一类新型推理大模型，例如开源的DeepSeek-R1通过整合深度思考与复杂推理能力，实现了最先进的性能表现。尽管这些模型展现出令人印象深刻的能力，其内部推理机制仍未被充分探索。本研究采用稀疏自编码器（SAEs）方法——一种将神经网络潜在表征分解为可解释稀疏特征的技术——来识别DeepSeek-R1系列模型中驱动推理的关键特征。首先，我们提出从SAE表征中提取候选"推理特征"的方法，并通过实证分析与可解释性技术验证这些特征与模型推理能力的直接关联。最关键的是，我们证明定向调控这些特征能系统性提升推理性能，首次为大语言模型的推理机制提供了机理层面的解释。代码详见https://github.com/AIRI-Institute/SAE-Reasoning

（翻译说明：
1. 专业术语采用学界通用译法，如"sparse autoencoders"译为"稀疏自编码器"
2. 被动语态转换为中文主动表述，如"remain unexplored"处理为"未被充分探索"
3. 长难句进行合理切分，如"demonstrating..."独立成句并添加"通过"衔接
4. 保持学术严谨性，关键概念如"mechanistic account"译为"机理层面的解释"
5. 技术路径描述使用"采用...方法"的规范表达
6. 代码链接等专有名词保留原始格式）
