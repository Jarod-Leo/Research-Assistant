# Evaluating Named Entity Recognition: Comparative Analysis of Mono- and Multilingual Transformer Models on Brazilian Corporate Earnings Call Transcriptions

链接: http://arxiv.org/abs/2403.12212v1

原文摘要:
Since 2018, when the Transformer architecture was introduced, Natural
Language Processing has gained significant momentum with pre-trained
Transformer-based models that can be fine-tuned for various tasks. Most models
are pre-trained on large English corpora, making them less applicable to other
languages, such as Brazilian Portuguese. In our research, we identified two
models pre-trained in Brazilian Portuguese (BERTimbau and PTT5) and two
multilingual models (mBERT and mT5). BERTimbau and mBERT use only the Encoder
module, while PTT5 and mT5 use both the Encoder and Decoder. Our study aimed to
evaluate their performance on a financial Named Entity Recognition (NER) task
and determine the computational requirements for fine-tuning and inference. To
this end, we developed the Brazilian Financial NER (BraFiNER) dataset,
comprising sentences from Brazilian banks' earnings calls transcripts annotated
using a weakly supervised approach. Additionally, we introduced a novel
approach that reframes the token classification task as a text generation
problem. After fine-tuning the models, we evaluated them using performance and
error metrics. Our findings reveal that BERT-based models consistently
outperform T5-based models. While the multilingual models exhibit comparable
macro F1-scores, BERTimbau demonstrates superior performance over PTT5. In
terms of error metrics, BERTimbau outperforms the other models. We also
observed that PTT5 and mT5 generated sentences with changes in monetary and
percentage values, highlighting the importance of accuracy and consistency in
the financial domain. Our findings provide insights into the differing
performance of BERT- and T5-based models for the NER task.

中文翻译:
自2018年Transformer架构问世以来，基于预训练Transformer模型（可针对各类任务进行微调）的自然语言处理技术获得了显著发展。当前大多数模型都在大型英语语料库上进行预训练，这导致其在巴西葡萄牙语等其他语言中的应用受限。本研究聚焦于两个巴西葡萄牙语预训练模型（BERTimbau和PTT5）和两个多语言模型（mBERT与mT5），其中BERTimbau和mBERT仅使用编码器模块，而PTT5和mT5同时采用编码器-解码器架构。我们旨在评估这些模型在金融领域命名实体识别（NER）任务中的表现，并测算其微调与推理阶段的算力需求。

为此，我们构建了巴西金融NER数据集（BraFiNER），该数据集包含采用弱监督方法标注的巴西银行财报电话会议记录语句。同时，我们创新性地将标记分类任务重构为文本生成问题。模型微调完成后，我们通过性能指标和误差指标进行评估。研究发现：基于BERT的模型始终优于T5系列模型；虽然多语言模型的宏观F1分数相近，但BERTimbau表现显著优于PTT5；在误差指标方面，BERTimbau同样展现最佳性能。值得注意的是，PTT5和mT5生成的语句会出现货币与百分比数值的篡改现象，这凸显了金融领域对准确性与一致性的严苛要求。本研究为理解BERT与T5架构在NER任务中的性能差异提供了重要洞见。

（注：根据学术翻译规范，对部分术语进行了统一处理：
1. "fine-tuned"译为"微调"而非"调优"
2. "weakly supervised approach"译为"弱监督方法"而非"弱监督方式"
3. "macro F1-scores"译为"宏观F1分数"而非"宏F1分数"
4. 保持"BERTimbau"等模型名称原文不变
5. 将长句合理切分为符合中文表达习惯的短句）
