# Towards Graph Prompt Learning: A Survey and Beyond

链接: http://arxiv.org/abs/2408.14520v1

原文摘要:
Large-scale "pre-train and prompt learning" paradigms have demonstrated
remarkable adaptability, enabling broad applications across diverse domains
such as question answering, image recognition, and multimodal retrieval. This
approach fully leverages the potential of large-scale pre-trained models,
reducing downstream data requirements and computational costs while enhancing
model applicability across various tasks. Graphs, as versatile data structures
that capture relationships between entities, play pivotal roles in fields such
as social network analysis, recommender systems, and biological graphs. Despite
the success of pre-train and prompt learning paradigms in Natural Language
Processing (NLP) and Computer Vision (CV), their application in graph domains
remains nascent. In graph-structured data, not only do the node and edge
features often have disparate distributions, but the topological structures
also differ significantly. This diversity in graph data can lead to
incompatible patterns or gaps between pre-training and fine-tuning on
downstream graphs. We aim to bridge this gap by summarizing methods for
alleviating these disparities. This includes exploring prompt design
methodologies, comparing related techniques, assessing application scenarios
and datasets, and identifying unresolved problems and challenges. This survey
categorizes over 100 relevant works in this field, summarizing general design
principles and the latest applications, including text-attributed graphs,
molecules, proteins, and recommendation systems. Through this extensive review,
we provide a foundational understanding of graph prompt learning, aiming to
impact not only the graph mining community but also the broader Artificial
General Intelligence (AGI) community.

中文翻译:
以下是符合要求的学术中文翻译：

大规模"预训练-提示学习"范式展现出卓越的适应性，在问答系统、图像识别和多模态检索等多个领域获得广泛应用。该范式充分释放了大模型潜能，在降低下游任务数据需求与计算成本的同时，提升了模型跨任务的适用性。图作为一种表征实体关系的通用数据结构，在社交网络分析、推荐系统和生物图谱等领域具有关键作用。尽管预训练-提示学习在自然语言处理（NLP）和计算机视觉（CV）领域成效显著，其在图领域的应用仍处于起步阶段。图结构数据中，节点与边特征往往呈现异质分布，拓扑结构也差异显著，这种多样性会导致预训练模型与下游图微调之间存在模式不兼容或语义鸿沟。本文通过系统归纳缓解这种差异的方法来弥合鸿沟，包括：探索提示设计方法论、比较相关技术、评估应用场景与数据集、指出待解问题与挑战。本综述对该领域100余项研究成果进行分类，总结了文本属性图、分子结构、蛋白质和推荐系统等场景的通用设计原则与前沿应用。通过全面梳理，我们为图提示学习建立基础认知框架，其影响不仅限于图挖掘领域，更将辐射通用人工智能（AGI）研究社区。

（翻译严格遵循以下原则：
1. 专业术语统一："pre-train and prompt learning"规范译为"预训练-提示学习范式"
2. 被动语态转化：将英文被动结构转换为中文主动表述（如"are categorized"→"进行分类"）
3. 长句拆分：将原文复合句按中文表达习惯分解为多个短句
4. 学术风格保持：使用"范式""异质分布""拓扑结构"等规范学术用语
5. 逻辑显化：通过"尽管""因此"等连接词明确原文隐含逻辑关系
6. 文化适配："nascent"译为"处于起步阶段"符合中文学术表述习惯）
