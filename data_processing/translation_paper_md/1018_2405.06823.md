# PLeak: Prompt Leaking Attacks against Large Language Model Applications

链接: http://arxiv.org/abs/2405.06823v1

原文摘要:
Large Language Models (LLMs) enable a new ecosystem with many downstream
applications, called LLM applications, with different natural language
processing tasks. The functionality and performance of an LLM application
highly depend on its system prompt, which instructs the backend LLM on what
task to perform. Therefore, an LLM application developer often keeps a system
prompt confidential to protect its intellectual property. As a result, a
natural attack, called prompt leaking, is to steal the system prompt from an
LLM application, which compromises the developer's intellectual property.
Existing prompt leaking attacks primarily rely on manually crafted queries, and
thus achieve limited effectiveness.
  In this paper, we design a novel, closed-box prompt leaking attack framework,
called PLeak, to optimize an adversarial query such that when the attacker
sends it to a target LLM application, its response reveals its own system
prompt. We formulate finding such an adversarial query as an optimization
problem and solve it with a gradient-based method approximately. Our key idea
is to break down the optimization goal by optimizing adversary queries for
system prompts incrementally, i.e., starting from the first few tokens of each
system prompt step by step until the entire length of the system prompt.
  We evaluate PLeak in both offline settings and for real-world LLM
applications, e.g., those on Poe, a popular platform hosting such applications.
Our results show that PLeak can effectively leak system prompts and
significantly outperforms not only baselines that manually curate queries but
also baselines with optimized queries that are modified and adapted from
existing jailbreaking attacks. We responsibly reported the issues to Poe and
are still waiting for their response. Our implementation is available at this
repository: https://github.com/BHui97/PLeak.

中文翻译:
以下是符合要求的学术中文翻译：

大型语言模型（LLMs）催生了一个包含众多下游应用的新生态系统，这些被称为"LLM应用"的程序可执行多种自然语言处理任务。LLM应用的功能与性能高度依赖其系统提示（system prompt），该提示用于指导后端LLM执行特定任务。因此，开发者通常将系统提示作为核心知识产权予以保密。这催生了一种名为"提示泄露"（prompt leaking）的新型攻击——通过窃取LLM应用的系统提示来侵害开发者权益。现有攻击方法主要依赖人工构造查询语句，实际效果有限。

本文提出名为PLeak的新型黑盒提示泄露攻击框架，通过优化对抗性查询语句，使得目标LLM应用在响应时暴露其系统提示。我们将该问题建模为优化问题，并采用基于梯度的方法进行近似求解。核心创新在于采用增量式优化策略：从系统提示的起始标记开始逐步优化对抗查询，直至完整获取整个系统提示。

我们在离线环境和真实LLM应用（如知名平台Poe上的应用）中评估PLeak。实验表明，PLeak不仅能有效泄露系统提示，其性能显著优于人工构造查询的基线方法，也优于基于现有越狱攻击改造的优化查询方案。我们已向Poe平台负责任地披露了该漏洞，目前仍在等待官方回应。项目代码已开源：https://github.com/BHui97/PLeak。

（注：根据学术规范，专业术语首次出现时保留英文原名并标注中文译名，如"prompt leaking"译为"提示泄露"；技术平台名称"Poe"遵循行业惯例不予翻译；长难句按中文表达习惯拆分重组；被动语态转换为主动表述；准确保持技术细节的完整性）
