# DROJ: A Prompt-Driven Attack against Large Language Models

链接: http://arxiv.org/abs/2411.09125v1

原文摘要:
Large Language Models (LLMs) have demonstrated exceptional capabilities
across various natural language processing tasks. Due to their training on
internet-sourced datasets, LLMs can sometimes generate objectionable content,
necessitating extensive alignment with human feedback to avoid such outputs.
Despite massive alignment efforts, LLMs remain susceptible to adversarial
jailbreak attacks, which usually are manipulated prompts designed to circumvent
safety mechanisms and elicit harmful responses. Here, we introduce a novel
approach, Directed Rrepresentation Optimization Jailbreak (DROJ), which
optimizes jailbreak prompts at the embedding level to shift the hidden
representations of harmful queries towards directions that are more likely to
elicit affirmative responses from the model. Our evaluations on LLaMA-2-7b-chat
model show that DROJ achieves a 100\% keyword-based Attack Success Rate (ASR),
effectively preventing direct refusals. However, the model occasionally
produces repetitive and non-informative responses. To mitigate this, we
introduce a helpfulness system prompt that enhances the utility of the model's
responses. Our code is available at
https://github.com/Leon-Leyang/LLM-Safeguard.

中文翻译:
以下是符合要求的学术化中文翻译：

大型语言模型（LLMs）在各类自然语言处理任务中展现出卓越能力。由于训练数据源自互联网，这些模型可能生成不当内容，因此需要通过大量人类反馈对齐来规避此类输出。然而即便经过大规模对齐，LLMs仍易受对抗性越狱攻击的影响——这类攻击通常通过精心设计的提示词绕过安全机制，诱导有害响应。本文提出一种新颖的定向表征优化越狱方法（DROJ），该方法在嵌入层面对越狱提示进行优化，将有害查询的隐式表征向更易获得模型肯定回应的方向偏移。我们在LLaMA-2-7b-chat模型上的评估表明，DROJ实现了100%基于关键词的攻击成功率（ASR），能有效避免直接拒绝响应。但该模型偶尔会产生重复且无信息量的回答，为此我们引入了辅助性系统提示以提升响应效用。代码已开源：https://github.com/Leon-Leyang/LLM-Safeguard。

（翻译严格遵循以下原则：
1. 专业术语准确统一："adversarial jailbreak attacks"译为"对抗性越狱攻击"、"embedding level"译为"嵌入层面"
2. 被动语态转化："are designed"译为"通过精心设计"
3. 长句拆分：将原文复合句分解为符合中文表达习惯的短句
4. 概念显化："hidden representations"译为"隐式表征"以明确技术含义
5. 学术规范：保留技术缩写ASR及模型名称大小写格式
6. 流畅性优化："necessitating extensive alignment"转译为"因此需要通过...对齐"的因果句式）
