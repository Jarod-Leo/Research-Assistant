# Impact of Position Bias on Language Models in Token Classification

链接: http://arxiv.org/abs/2304.13567v1

原文摘要:
Language Models (LMs) have shown state-of-the-art performance in Natural
Language Processing (NLP) tasks. Downstream tasks such as Named Entity
Recognition (NER) or Part-of-Speech (POS) tagging are known to suffer from data
imbalance issues, particularly regarding the ratio of positive to negative
examples and class disparities. This paper investigates an often-overlooked
issue of encoder models, specifically the position bias of positive examples in
token classification tasks. For completeness, we also include decoders in the
evaluation. We evaluate the impact of position bias using different position
embedding techniques, focusing on BERT with Absolute Position Embedding (APE),
Relative Position Embedding (RPE), and Rotary Position Embedding (RoPE).
Therefore, we conduct an in-depth evaluation of the impact of position bias on
the performance of LMs when fine-tuned on token classification benchmarks. Our
study includes CoNLL03 and OntoNote5.0 for NER, English Tree Bank UD\_en, and
TweeBank for POS tagging. We propose an evaluation approach to investigate
position bias in transformer models. We show that LMs can suffer from this bias
with an average drop ranging from 3\% to 9\% in their performance. To mitigate
this effect, we propose two methods: Random Position Shifting and Context
Perturbation, that we apply on batches during the training process. The results
show an improvement of $\approx$ 2\% in the performance of the model on
CoNLL03, UD\_en, and TweeBank.

中文翻译:
语言模型（LMs）在自然语言处理（NLP）任务中展现出最先进的性能。诸如命名实体识别（NER）或词性标注（POS）等下游任务普遍存在数据不平衡问题，特别是正负样本比例及类别分布不均的情况。本文研究了一个常被忽视的编码器模型问题——即标记分类任务中正样本的位置偏差现象，为全面起见，评估范围也包含解码器模型。我们通过不同位置嵌入技术（重点考察采用绝对位置嵌入APE、相对位置嵌入RPE和旋转位置嵌入RoPE的BERT模型）系统评估了位置偏差的影响，进而深入研究了微调过程中位置偏差对语言模型在标记分类基准任务性能的作用。实验涵盖CoNLL03和OntoNote5.0（NER任务）、English Tree Bank UD_en及TweeBank（POS标注任务）。我们提出了一种检测Transformer模型位置偏差的评估方法，证明该偏差会导致语言模型性能平均下降3%至9%。为缓解此效应，我们提出两种应用于训练过程的批量处理方法：随机位置偏移和上下文扰动。实验结果表明，这些方法使模型在CoNLL03、UD_en和TweeBank上的性能提升约2%。  

（注：根据学术翻译规范，对部分术语进行了统一处理：  
1. "state-of-the-art"译为"最先进的"而非"尖端"以符合中文论文表述习惯  
2. "fine-tuned"译为"微调"保持NLP领域术语一致性  
3. 长句拆分重组，如将"focusing on..."处理为括号补充说明  
4. 数学符号$\approx$保留并译为"约"以保持精确性  
5. 数据集名称保留英文原名符合计算机领域惯例）
