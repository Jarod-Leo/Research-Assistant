# Towards Cross-Lingual LLM Evaluation for European Languages

链接: http://arxiv.org/abs/2410.08928v1

原文摘要:
The rise of Large Language Models (LLMs) has revolutionized natural language
processing across numerous languages and tasks. However, evaluating LLM
performance in a consistent and meaningful way across multiple European
languages remains challenging, especially due to the scarcity of
language-parallel multilingual benchmarks. We introduce a multilingual
evaluation approach tailored for European languages. We employ translated
versions of five widely-used benchmarks to assess the capabilities of 40 LLMs
across 21 European languages. Our contributions include examining the
effectiveness of translated benchmarks, assessing the impact of different
translation services, and offering a multilingual evaluation framework for LLMs
that includes newly created datasets: EU20-MMLU, EU20-HellaSwag, EU20-ARC,
EU20-TruthfulQA, and EU20-GSM8K. The benchmarks and results are made publicly
available to encourage further research in multilingual LLM evaluation.

中文翻译:
大型语言模型（LLM）的崛起为多语言自然语言处理领域带来了革命性变革。然而，针对欧洲多语种环境下LLM性能的标准化评估仍存在挑战，这主要源于语言平行多语种基准数据的稀缺性。本研究提出了一种面向欧洲语言的定制化多语种评估方案：通过五个广受认可基准测试的翻译版本，系统评估了40个LLM在21种欧洲语言中的表现。创新性工作包括：验证翻译基准的有效性、分析不同翻译服务的影响、构建包含新开发数据集（EU20-MMLU、EU20-HellaSwag、EU20-ARC、EU20-TruthfulQA和EU20-GSM8K）的多语种LLM评估框架。所有基准数据与评估结果均已公开，以期推动多语种LLM评估领域的深入研究。

（翻译说明：采用学术论文摘要的标准结构，在保持专业性的同时提升中文表达流畅度。关键术语如"benchmark"统一译为"基准测试/数据"，"translation services"译为"翻译服务"以保持概念一致性。长句按中文习惯切分为短句，如将原文最后复合句拆分为两个独立句。补充"本研究"作为主语使表述更完整，符合中文科技论文写作规范。）
