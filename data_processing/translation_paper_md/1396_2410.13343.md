# Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models

链接: http://arxiv.org/abs/2410.13343v1

原文摘要:
Large Language Models (LLMs) have shown remarkable capabilities in various
natural language processing tasks. However, LLMs may rely on dataset biases as
shortcuts for prediction, which can significantly impair their robustness and
generalization capabilities. This paper presents Shortcut Suite, a
comprehensive test suite designed to evaluate the impact of shortcuts on LLMs'
performance, incorporating six shortcut types, five evaluation metrics, and
four prompting strategies. Our extensive experiments yield several key
findings: 1) LLMs demonstrate varying reliance on shortcuts for downstream
tasks, significantly impairing their performance. 2) Larger LLMs are more
likely to utilize shortcuts under zero-shot and few-shot in-context learning
prompts. 3) Chain-of-thought prompting notably reduces shortcut reliance and
outperforms other prompting strategies, while few-shot prompts generally
underperform compared to zero-shot prompts. 4) LLMs often exhibit
overconfidence in their predictions, especially when dealing with datasets that
contain shortcuts. 5) LLMs generally have a lower explanation quality in
shortcut-laden datasets, with errors falling into three types: distraction,
disguised comprehension, and logical fallacy. Our findings offer new insights
for evaluating robustness and generalization in LLMs and suggest potential
directions for mitigating the reliance on shortcuts. The code is available at
\url {https://github.com/yyhappier/ShortcutSuite.git}.

中文翻译:
以下是符合要求的学术论文摘要中文翻译：

大语言模型（LLMs）在各类自然语言处理任务中展现出卓越能力。然而，这些模型可能依赖数据集偏差作为预测捷径，这会严重损害其鲁棒性与泛化能力。本文提出Shortcut Suite——一个包含六种捷径类型、五项评估指标和四种提示策略的综合测试套件，用于系统评估捷径对LLMs性能的影响。通过大量实验，我们获得以下关键发现：1）LLMs在下游任务中对捷径的依赖程度存在差异，这种依赖会显著降低模型性能；2）在零样本和少样本上下文学习提示下，规模更大的LLMs更倾向于使用捷径；3）思维链提示能显著降低捷径依赖并优于其他提示策略，而少样本提示整体表现通常不及零样本提示；4）LLMs在处理含捷径的数据集时经常表现出预测过度自信；5）LLMs在捷径数据集上的解释质量普遍较低，其错误可分为三类：注意力分散型、理解伪装型和逻辑谬误型。本研究为评估LLMs的鲁棒性与泛化能力提供了新视角，并为减少捷径依赖指明了潜在改进方向。代码已开源于\url{https://github.com/yyhappier/ShortcutSuite.git}。

（注：根据学术规范，专业术语如"zero-shot/few-shot"采用"零样本/少样本"译法；"Chain-of-thought"译为"思维链"是领域通用译名；被动语态转换为中文主动表述；长难句按中文习惯拆分；URL保留原始格式；项目名称Shortcut Suite保留英文原名以保持可检索性）
