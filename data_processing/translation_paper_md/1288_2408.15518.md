# Dolphin: Long Context as a New Modality for Energy-Efficient On-Device Language Models

链接: http://arxiv.org/abs/2408.15518v1

原文摘要:
This paper presents Dolphin, a novel decoder-decoder architecture for
energy-efficient processing of long contexts in language models. Our approach
addresses the significant energy consumption and latency challenges inherent in
on-device models. Dolphin employs a compact 0.5B parameter decoder to distill
extensive contextual information into a memory embedding, substantially
reducing the input length for the primary 7B parameter decoder model. Inspired
by vision-language models, we repurpose the image embedding projector to encode
long textual contexts, effectively treating extended context as a distinct
modality. This innovative method enables processing of substantially longer
contexts without the typical computational overhead associated with extended
input sequences. Empirical evaluations demonstrate a 10-fold improvement in
energy efficiency and a 5-fold reduction in latency compared to conventional
full-length context processing methods without losing quality of the response.
Our work contributes to the development of more sustainable and scalable
language models for on-device applications, addressing the critical need for
energy-efficient and responsive AI technologies in resource-constrained
environments while maintaining the accuracy to understand long contexts. This
research has implications for the broader field of natural language processing,
particularly in the domain of efficient model design for resource-limited
settings. By enabling more sophisticated AI capabilities on edge devices,
Dolphin paves the way for advanced language processing in a wide range of
applications where computational resources are at a premium. The Dolphin model
is publicly available at https://huggingface.co/NexaAIDev/Dolphin.

中文翻译:
本文提出Dolphin——一种创新的解码器-解码器架构，专为语言模型长上下文处理的能效优化而设计。该方案有效解决了设备端模型固有的高能耗与延迟挑战。Dolphin采用0.5B参数的紧凑型解码器将庞杂上下文信息蒸馏为记忆嵌入向量，使主7B参数解码器模型的输入长度大幅缩减。受视觉语言模型启发，我们创新性地复用图像嵌入投影器来编码长文本上下文，将扩展上下文视为独立模态进行处理。这种突破性方法能在不增加扩展输入序列典型计算开销的前提下，处理显著更长的上下文。实证评估表明，相较传统全长上下文处理方法，本方案在保持响应质量不变的同时，能效提升10倍，延迟降低5倍。本研究为开发更可持续、可扩展的设备端语言模型做出贡献，在资源受限环境中满足对高能效、快速响应AI技术的迫切需求，同时保持长上下文理解精度。该成果对自然语言处理领域具有广泛意义，特别是在资源受限场景的高效模型设计方向。通过赋能边缘设备更复杂的AI能力，Dolphin为计算资源受限场景下的高级语言处理开辟了新路径。模型已开源发布于https://huggingface.co/NexaAIDev/Dolphin。

（翻译说明：
1. 专业术语处理："decoder-decoder architecture"译为"解码器-解码器架构"，"memory embedding"译为"记忆嵌入向量"
2. 技术概念本地化："image embedding projector"译为"图像嵌入投影器"，符合计算机视觉领域术语习惯
3. 长句拆分重构：将原文复合长句分解为符合中文表达习惯的短句，如将"Empirical evaluations..."整句拆分为两个逻辑分句
4. 被动语态转换："is publicly available"主动化为"已开源发布"
5. 学术风格保持：使用"本方案""该成果"等学术用语，保持论文摘要的严谨性
6. 数字规范处理："0.5B/7B"保留阿拉伯数字形式，符合中文科技文献数字使用规范）
