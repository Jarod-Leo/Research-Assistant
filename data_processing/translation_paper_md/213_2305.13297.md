# Parallel Attention and Feed-Forward Net Design for Pre-training and Inference on Transformers

链接: http://arxiv.org/abs/2305.13297v1

原文摘要:
This paper investigates the key role of Feed-Forward Networks (FFNs) in
transformer models by utilizing the Parallel Attention and Feed-Forward Net
Design (PAF) architecture, and comparing it to their Series Attention and
Feed-Forward Net Design (SAF) counterparts. Central to the effectiveness of PAF
are two main assumptions regarding the FFN block and the attention block within
a layer: 1) the primary function of the FFN block is to maintain isotropy among
token embeddings and prevent their degeneration, and 2) the residual norm
computed in the attention block is substantially smaller than the input token
embedding norm. To empirically validate these assumptions, we train PAF
variants of two large language models (RoBERTa-large and bert-large-uncased).
Our results demonstrate that both assumptions hold true in the PAF design. This
study contributes to a deeper understanding of the roles and interactions
between FFNs and self-attention mechanisms in transformer architectures.

中文翻译:
本文通过采用并行注意力与前馈网络设计（PAF）架构，并与传统的串行注意力与前馈网络设计（SAF）进行对比，深入探究了前馈网络（FFN）在Transformer模型中的关键作用。PAF架构的有效性基于对层内FFN模块和注意力模块的两个核心假设：1）FFN模块的主要功能是保持词元嵌入的各向同性并防止其退化；2）注意力模块中计算得到的残差范数远小于输入词元嵌入的范数。为验证这些假设，我们基于两种大型语言模型（RoBERTa-large和bert-large-uncased）训练了PAF变体模型。实验结果表明，在PAF设计中这两个假设均成立。本研究为深入理解Transformer架构中前馈网络与自注意力机制的作用及交互关系提供了新的理论依据。

（翻译说明：1. 专业术语如FFN/SAF/PAF等保留英文缩写并首次出现时标注全称；2. 将"isotropy among token embeddings"译为"词元嵌入的各向同性"符合机器学习领域术语规范；3. "degeneration"译为"退化"准确表达原文指征的数学特性衰减现象；4. 通过拆分英文长句为中文短句结构，如将假设条件句转换为冒号列举式；5. 保持学术论文摘要的客观严谨风格，使用"探究""验证""表明"等科研常用动词。）
