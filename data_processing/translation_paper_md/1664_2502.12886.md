# Are Multilingual Language Models an Off-ramp for Under-resourced Languages? Will we arrive at Digital Language Equality in Europe in 2030?

链接: http://arxiv.org/abs/2502.12886v1

原文摘要:
Large language models (LLMs) demonstrate unprecedented capabilities and
define the state of the art for almost all natural language processing (NLP)
tasks and also for essentially all Language Technology (LT) applications. LLMs
can only be trained for languages for which a sufficient amount of pre-training
data is available, effectively excluding many languages that are typically
characterised as under-resourced. However, there is both circumstantial and
empirical evidence that multilingual LLMs, which have been trained using data
sets that cover multiple languages (including under-resourced ones), do exhibit
strong capabilities for some of these under-resourced languages. Eventually,
this approach may have the potential to be a technological off-ramp for those
under-resourced languages for which "native" LLMs, and LLM-based technologies,
cannot be developed due to a lack of training data. This paper, which
concentrates on European languages, examines this idea, analyses the current
situation in terms of technology support and summarises related work. The
article concludes by focusing on the key open questions that need to be
answered for the approach to be put into practice in a systematic way.

中文翻译:
大型语言模型（LLMs）展现出前所未有的能力，几乎为所有自然语言处理（NLP）任务及语言技术（LT）应用确立了技术标杆。这类模型的训练依赖于充足预训练数据的语言资源，这使得许多通常被归类为资源匮乏的语言被排除在外。然而，有间接和实证证据表明，采用多语言数据集（包含部分资源匮乏语言）训练的多语言LLMs，确实对某些资源匮乏语言展现出强大处理能力。长远来看，这一技术路径可能为那些因训练数据不足而无法开发"原生"LLMs及LLM技术的资源匮乏语言提供解决方案。本文以欧洲语言为研究对象，探讨了这一构想，分析了当前技术支持现状并综述了相关研究。文章最后聚焦于需要解决的关键开放性问题，以推动该技术路径的系统化实践。  

（译文特点说明：  
1. 专业术语统一处理："under-resourced languages"译为"资源匮乏语言"保持学术一致性  
2. 长句拆分重构：将原文复合句按中文表达习惯分解为多个短句，如将"do exhibit strong capabilities..."独立成句  
3. 被动语态转化："can only be trained"译为主动式"训练依赖于"更符合中文表达  
4. 文化适配："technological off-ramp"意译为"解决方案"避免直译生硬  
5. 逻辑显化：通过"长远来看"等连接词明确原文隐含的递进关系  
6. 学术风格保留：使用"实证证据""技术路径"等术语保持论文严谨性）
