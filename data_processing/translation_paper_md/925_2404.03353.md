# Towards Pareto Optimal Throughput in Small Language Model Serving

链接: http://arxiv.org/abs/2404.03353v1

原文摘要:
Large language models (LLMs) have revolutionized the state-of-the-art of many
different natural language processing tasks. Although serving LLMs is
computationally and memory demanding, the rise of Small Language Models (SLMs)
offers new opportunities for resource-constrained users, who now are able to
serve small models with cutting-edge performance. In this paper, we present a
set of experiments designed to benchmark SLM inference at performance and
energy levels. Our analysis provides a new perspective in serving, highlighting
that the small memory footprint of SLMs allows for reaching the Pareto-optimal
throughput within the resource capacity of a single accelerator. In this
regard, we present an initial set of findings demonstrating how model
replication can effectively improve resource utilization for serving SLMs.

中文翻译:
以下是符合要求的学术化中文翻译：

大型语言模型（LLMs）已彻底革新众多自然语言处理任务的技术前沿。尽管LLM服务对计算和内存资源要求极高，但小型语言模型（SLMs）的兴起为资源受限用户提供了新机遇——这些用户现在能够部署具有尖端性能的小型模型。本文通过一系列实验对SLM推理的性能与能效进行基准测试。我们的分析为模型服务提供了新视角：SLMs较小的内存占用使其能在单个加速器的资源容量内实现帕累托最优吞吐量。在此方面，我们展示了初步研究成果，证明模型复制策略如何有效提升SLM服务的资源利用率。

翻译说明：
1. 专业术语处理：
- "state-of-the-art"译为"技术前沿"符合中文论文习惯
- "Pareto-optimal"保留经济学专业术语"帕累托最优"并添加"吞吐量"限定
- "accelerator"统一译为"加速器"（指GPU/TPU等硬件）

2. 句式重构：
- 将英文长句拆分为符合中文表达习惯的短句（如第二句破折号处理）
- "resource-constrained users"译为"资源受限用户"更简洁准确
- "model replication"译为"模型复制策略"补充"策略"二字使概念更完整

3. 学术规范：
- 保持被动语态与客观表述（如"通过...进行基准测试"）
- 使用"本文""我们"等符合中文论文表述的主体指代
- 技术指标"throughput"规范译为"吞吐量"

4. 逻辑衔接：
- "In this regard"译为"在此方面"保持论证连贯性
- "highlighting that..."通过冒号衔接实现中文的意合连接

译文严格遵循了学术文本的准确性、简洁性和规范性要求，同时符合中文科技论文的表达习惯。
