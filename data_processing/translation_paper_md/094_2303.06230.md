# Generating Query Focused Summaries without Fine-tuning the Transformer-based Pre-trained Models

链接: http://arxiv.org/abs/2303.06230v1

原文摘要:
Fine-tuning the Natural Language Processing (NLP) models for each new data
set requires higher computational time associated with increased carbon
footprint and cost. However, fine-tuning helps the pre-trained models adapt to
the latest data sets; what if we avoid the fine-tuning steps and attempt to
generate summaries using just the pre-trained models to reduce computational
time and cost. In this paper, we tried to omit the fine-tuning steps and
investigate whether the Marginal Maximum Relevance (MMR)-based approach can
help the pre-trained models to obtain query-focused summaries directly from a
new data set that was not used to pre-train the models. First, we used topic
modelling on Wikipedia Current Events Portal (WCEP) and Debatepedia datasets to
generate queries for summarization tasks. Then, using MMR, we ranked the
sentences of the documents according to the queries. Next, we passed the ranked
sentences to seven transformer-based pre-trained models to perform the
summarization tasks. Finally, we used the MMR approach again to select the
query relevant sentences from the generated summaries of individual pre-trained
models and constructed the final summary. As indicated by the experimental
results, our MMR-based approach successfully ranked and selected the most
relevant sentences as summaries and showed better performance than the
individual pre-trained models.

中文翻译:
为每个新数据集微调自然语言处理(NLP)模型会导致计算时间增加，同时伴随碳足迹和成本的上升。尽管微调能帮助预训练模型适配最新数据集，但若跳过微调步骤、仅利用预训练模型生成摘要，能否有效降低计算时间和成本？本文尝试省略微调步骤，探究基于边际最大相关性(MMR)的方法能否使预训练模型直接从非预训练来源的新数据集中获取查询聚焦的摘要。具体流程为：首先在维基百科当前事件门户(WCEP)和Debatepedia数据集上应用主题建模生成摘要任务查询；随后通过MMR方法根据查询对文档句子进行排序；接着将排序后的句子输入七个基于Transformer的预训练模型执行摘要生成；最后再次运用MMR方法从各预训练模型生成的摘要中筛选查询相关句子以构建最终摘要。实验结果表明，基于MMR的方法能成功排序并选择最具相关性的句子作为摘要，其性能优于单独使用预训练模型。
