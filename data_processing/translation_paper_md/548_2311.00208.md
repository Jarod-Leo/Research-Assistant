# Transformers as Recognizers of Formal Languages: A Survey on Expressivity

链接: http://arxiv.org/abs/2311.00208v1

原文摘要:
As transformers have gained prominence in natural language processing, some
researchers have investigated theoretically what problems they can and cannot
solve, by treating problems as formal languages. Exploring such questions can
help clarify the power of transformers relative to other models of computation,
their fundamental capabilities and limits, and the impact of architectural
choices. Work in this subarea has made considerable progress in recent years.
Here, we undertake a comprehensive survey of this work, documenting the diverse
assumptions that underlie different results and providing a unified framework
for harmonizing seemingly contradictory findings.

中文翻译:
随着Transformer模型在自然语言处理领域崭露头角，部分研究者开始从形式语言的理论视角，系统探究这类模型能够解决与无法解决的问题。此类研究有助于厘清Transformer相较于其他计算模型的优势所在、其核心能力与固有局限，以及架构设计对模型性能的影响。近年来该细分领域已取得显著进展。本文对这一研究方向进行全面综述，梳理不同结论背后的多样化理论假设，并通过统一框架协调表面矛盾的研究发现。

（译文特点说明：
1. 专业术语准确处理："formal languages"译为"形式语言"，"architectural choices"译为"架构设计"
2. 长句拆分重构：将原文复合句按中文表达习惯分解为多个短句，如第一句拆分为两个逻辑单元
3. 学术语体保持："undertake a comprehensive survey"译为"进行全面综述"，"harmonizing findings"译为"协调研究发现"
4. 被动语态转化："has been made"转为主动式"已取得"
5. 概念显化处理："diverse assumptions"增译为"不同结论背后的多样化理论假设"，使学术表述更清晰）
