# Adapting Pre-trained Language Models for Quantum Natural Language Processing

链接: http://arxiv.org/abs/2302.13812v1

原文摘要:
The emerging classical-quantum transfer learning paradigm has brought a
decent performance to quantum computational models in many tasks, such as
computer vision, by enabling a combination of quantum models and classical
pre-trained neural networks. However, using quantum computing with pre-trained
models has yet to be explored in natural language processing (NLP). Due to the
high linearity constraints of the underlying quantum computing infrastructures,
existing Quantum NLP models are limited in performance on real tasks. We fill
this gap by pre-training a sentence state with complex-valued BERT-like
architecture, and adapting it to the classical-quantum transfer learning scheme
for sentence classification. On quantum simulation experiments, the pre-trained
representation can bring 50\% to 60\% increases to the capacity of end-to-end
quantum models.

中文翻译:
新兴的经典-量子迁移学习范式通过将量子模型与经典预训练神经网络相结合，在计算机视觉等众多任务中为量子计算模型带来了卓越性能。然而在自然语言处理（NLP）领域，量子计算与预训练模型的结合仍属空白。由于底层量子计算基础设施的高度线性约束限制，现有量子NLP模型在真实任务中的表现始终受限。本研究通过预训练具有复数形式类BERT架构的句子状态，并将其适配于句子分类的经典-量子迁移学习框架，成功填补了这一空白。量子模拟实验表明，预训练表征能使端到端量子模型的性能提升50%至60%。

