# A Measure-Theoretic Characterization of Tight Language Models

链接: http://arxiv.org/abs/2212.10502v1

原文摘要:
Language modeling, a central task in natural language processing, involves
estimating a probability distribution over strings. In most cases, the
estimated distribution sums to 1 over all finite strings. However, in some
pathological cases, probability mass can ``leak'' onto the set of infinite
sequences. In order to characterize the notion of leakage more precisely, this
paper offers a measure-theoretic treatment of language modeling. We prove that
many popular language model families are in fact tight, meaning that they will
not leak in this sense. We also generalize characterizations of tightness
proposed in previous works.

中文翻译:
语言建模作为自然语言处理的核心任务，其本质是对字符串的概率分布进行估计。在大多数情况下，这种估计的概率分布会在所有有限字符串上归一化为1。然而在某些特殊情况下，概率质量可能会"渗漏"到无限序列集合中。为了更精确地描述这种渗漏现象，本文从测度论的视角对语言建模进行了系统性研究。我们证明了许多主流语言模型族实际上具有紧致性特质，这意味着它们不会发生此类概率渗漏。同时，我们对前人研究中提出的紧致性特征进行了理论推广。

