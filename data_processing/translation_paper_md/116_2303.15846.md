# Soft-prompt tuning to predict lung cancer using primary care free-text Dutch medical notes

链接: http://arxiv.org/abs/2303.15846v1

原文摘要:
We investigate different natural language processing (NLP) approaches based
on contextualised word representations for the problem of early prediction of
lung cancer using free-text patient medical notes of Dutch primary care
physicians. Because lung cancer has a low prevalence in primary care, we also
address the problem of classification under highly imbalanced classes.
Specifically, we use large Transformer-based pretrained language models (PLMs)
and investigate: 1) how \textit{soft prompt-tuning} -- an NLP technique used to
adapt PLMs using small amounts of training data -- compares to standard model
fine-tuning; 2) whether simpler static word embedding models (WEMs) can be more
robust compared to PLMs in highly imbalanced settings; and 3) how models fare
when trained on notes from a small number of patients. We find that 1)
soft-prompt tuning is an efficient alternative to standard model fine-tuning;
2) PLMs show better discrimination but worse calibration compared to simpler
static word embedding models as the classification problem becomes more
imbalanced; and 3) results when training models on small number of patients are
mixed and show no clear differences between PLMs and WEMs. All our code is
available open source in
\url{https://bitbucket.org/aumc-kik/prompt_tuning_cancer_prediction/}.

中文翻译:
本研究基于语境化词表征技术，探索不同自然语言处理方法在肺癌早期预测中的应用。研究数据来源于荷兰全科医生的自由文本医疗记录。针对初级诊疗中肺癌低发病率的特点，我们重点解决了高度不平衡分类场景下的预测难题。具体而言，本研究采用基于Transformer架构的预训练语言模型（PLMs），重点考察：1）\textit{软提示调参}（一种利用少量训练数据适配PLMs的NLP技术）与标准模型微调的效能对比；2）在极端不平衡数据中，简单静态词嵌入模型（WEMs）是否比PLMs更具鲁棒性；3）模型在少量患者数据训练下的表现。研究发现：1）软提示调参可作为标准微调的有效替代方案；2）随着分类不平衡程度加剧，PLMs展现出更优的区分度但校准性弱于静态词嵌入模型；3）小样本训练时PLMs与WEMs表现差异不显著。全部代码已开源发布于\url{https://bitbucket.org/aumc-kik/prompt_tuning_cancer_prediction/}。

（注：根据学术摘要的文体特点，翻译时进行了以下处理：
1. 将英文被动语态转换为中文主动表述
2. 专业术语采用学界通用译法（如"prompt-tuning"译作"提示调参"）
3. 长难句拆解为符合中文表达习惯的短句结构
4. 保留技术术语首字母缩写并在首次出现时标注全称
5. 统一计量表述方式（如"small number of"译为"少量"）
6. 网页链接按原文格式完整保留）
