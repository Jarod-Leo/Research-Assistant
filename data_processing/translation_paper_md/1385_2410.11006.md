# Effective Self-Mining of In-Context Examples for Unsupervised Machine Translation with LLMs

链接: http://arxiv.org/abs/2410.11006v1

原文摘要:
Large Language Models (LLMs) have demonstrated impressive performance on a
wide range of natural language processing (NLP) tasks, primarily through
in-context learning (ICL). In ICL, the LLM is provided with examples that
represent a given task such that it learns to generate answers for test inputs.
However, access to these in-context examples is not guaranteed especially for
low-resource or massively multilingual tasks. In this work, we propose an
unsupervised approach to mine in-context examples for machine translation (MT),
enabling unsupervised MT (UMT) across different languages. Our approach begins
with word-level mining to acquire word translations that are then used to
perform sentence-level mining. As the quality of mined parallel pairs may not
be optimal due to noise or mistakes, we introduce a filtering criterion to
select the optimal in-context examples from a pool of unsupervised parallel
sentences. We evaluate our approach using two multilingual LLMs on 288
directions from the FLORES-200 dataset and analyze the impact of various
linguistic features on performance. Our findings demonstrate the effectiveness
of our unsupervised approach in mining in-context examples for MT, leading to
better or comparable translation performance as translation with regular
in-context samples (extracted from human-annotated data), while also
outperforming the other state-of-the-art UMT methods by an average of $7$ BLEU
points.

中文翻译:
以下是符合要求的学术中文翻译：

大型语言模型（LLMs）通过上下文学习（ICL）在众多自然语言处理（NLP）任务中展现出卓越性能。在ICL范式下，模型通过接收特定任务的示例样本，从而学会为测试输入生成相应答案。然而，获取高质量的上下文示例并非易事，尤其在低资源或大规模多语言任务场景中。本研究提出一种无监督的机器翻译（MT）上下文示例挖掘方法，实现跨语言的无监督机器翻译（UMT）。该方法首先进行词级对齐挖掘以获取单词翻译对，继而完成句子级对齐挖掘。考虑到噪声或错误可能导致挖掘的平行句对质量欠佳，我们提出过滤准则以从无监督平行语句池中筛选最优上下文示例。基于FLORES-200数据集的288个翻译方向，我们使用两种多语言LLMs进行评估，并分析了不同语言特征对性能的影响。实验结果表明：本方法能有效挖掘适用于MT的上下文示例，其翻译性能优于或持平于采用人工标注数据的常规ICL方法，同时以平均7个BLEU分的优势显著超越现有最先进的UMT方法。

（注：翻译严格遵循以下技术规范：
1. 专业术语统一（如LLMs/ICL等首现时标注英文缩写）
2. 被动语态转化（如"are provided"译为主动式"通过接收"）
3. 长句拆分重组（如原文最后长句分解为三个中文短句）
4. 数量单位规范（保留BLEU分等学术指标标准表述）
5. 逻辑连接显化（添加"考虑到"等衔接词确保行文流畅））
