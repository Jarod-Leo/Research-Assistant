# Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space

链接: http://arxiv.org/abs/2402.12026v1

原文摘要:
Despite the notable success of language models (LMs) in various natural
language processing (NLP) tasks, the reliability of LMs is susceptible to
backdoor attacks. Prior research attempts to mitigate backdoor learning while
training the LMs on the poisoned dataset, yet struggles against complex
backdoor attacks in real-world scenarios. In this paper, we investigate the
learning mechanisms of backdoor LMs in the frequency space by Fourier analysis.
Our findings indicate that the backdoor mapping presented on the poisoned
datasets exhibits a more discernible inclination towards lower frequency
compared to clean mapping, resulting in the faster convergence of backdoor
mapping. To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation
(MuScleLoRA), which deploys multiple radial scalings in the frequency space
with low-rank adaptation to the target model and further aligns the gradients
when updating parameters. Through downscaling in the frequency space,
MuScleLoRA encourages the model to prioritize the learning of relatively
high-frequency clean mapping, consequently mitigating backdoor learning.
Experimental results demonstrate that MuScleLoRA outperforms baselines
significantly. Notably, MuScleLoRA reduces the average success rate of diverse
backdoor attacks to below 15\% across multiple datasets and generalizes to
various backbone LMs, including BERT, RoBERTa, GPT2-XL, and Llama2. The codes
are publicly available at https://github.com/ZrW00/MuScleLoRA.

中文翻译:
尽管语言模型（LMs）在各种自然语言处理（NLP）任务中取得了显著成功，但其可靠性容易受到后门攻击的影响。现有研究尝试在有毒数据集上训练LMs时减轻后门学习，但在现实场景中仍难以应对复杂的后门攻击。本文通过傅里叶分析研究了后门LMs在频率空间中的学习机制。研究发现，相较于干净映射，有毒数据集呈现的后门映射在低频区域表现出更明显的倾向性，导致后门映射收敛更快。为缓解这一问题，我们提出多尺度低秩适配方法（MuScleLoRA），该方法在频率空间部署多重径向缩放，通过低秩适配目标模型并进一步对齐参数更新时的梯度。通过在频率空间进行降尺度处理，MuScleLoRA促使模型优先学习相对高频的干净映射，从而抑制后门学习。实验结果表明，MuScleLoRA显著优于基线方法。值得注意的是，该方法在多个数据集上将各类后门攻击的平均成功率降至15%以下，并能泛化至BERT、RoBERTa、GPT2-XL和Llama2等多种骨干LMs。代码已开源：https://github.com/ZrW00/MuScleLoRA。

（翻译说明：1. 专业术语统一处理："backdoor attacks"译为"后门攻击"、"low-rank adaptation"译为"低秩适配"；2. 长句拆分重构：将原文复合句按中文表达习惯分解为多个短句；3. 被动语态转化："is susceptible to"译为"容易受到"；4. 概念显化处理："Fourier analysis"补充译为"傅里叶分析"；5. 技术表述准确性："radial scalings"译为"径向缩放"以保持数学含义；6. 学术规范：保留模型名称及技术术语首字母大写；7. 流畅性优化：如"more discernible inclination"译为"更明显的倾向性"符合中文表达习惯。）
