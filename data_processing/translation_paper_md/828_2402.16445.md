# ProLLaMA: A Protein Large Language Model for Multi-Task Protein Language Processing

链接: http://arxiv.org/abs/2402.16445v1

原文摘要:
Large Language Models (LLMs) have achieved remarkable performance in multiple
Natural Language Processing (NLP) tasks. Under the premise that protein
sequences constitute the protein language, Protein Language Models(PLMs) have
advanced the field of protein engineering. However, as of now, unlike LLMs in
NLP, PLMs cannot handle the protein understanding task and the protein
generation task simultaneously in the Protein Language Processing (PLP) field.
This prompts us to delineate the inherent limitations in current PLMs: (i) the
lack of natural language capabilities, (ii) insufficient instruction
understanding, and (iii) high training resource demands. To address these
challenges, we introduce a training framework to transform any general LLM into
a PLM capable of handling multiple PLP tasks. To improve training efficiency,
we propose Protein Vocabulary Pruning (PVP) for general LLMs. We construct a
multi-task instruction dataset containing 13 million samples with superfamily
information, facilitating better modeling of protein sequence-function
landscapes. Through these methods, we develop the ProLLaMA model, the first
known PLM to handle multiple PLP tasks simultaneously. Experiments show that
ProLLaMA achieves state-of-the-art results in the unconditional protein
sequence generation task. In the controllable protein sequence generation task,
ProLLaMA can design novel proteins with desired functionalities. As for the
protein understanding task, ProLLaMA achieves a 62\% exact match rate in
superfamily prediction. Codes, model weights, and datasets are available at
\url{https://github.com/PKU-YuanGroup/ProLLaMA} and
\url{https://huggingface.co/GreatCaptainNemo}.

中文翻译:
以下是符合要求的学术中文翻译：

大语言模型（LLMs）在多项自然语言处理（NLP）任务中展现出卓越性能。基于蛋白质序列构成"蛋白质语言"的前提，蛋白质语言模型（PLMs）推动了蛋白质工程领域的发展。然而目前，与NLP领域的LLMs不同，PLMs尚无法在蛋白质语言处理（PLP）领域同时胜任蛋白质理解与生成任务。这促使我们揭示当前PLMs的固有局限：（1）缺乏自然语言能力；（2）指令理解不足；（3）训练资源需求过高。为解决这些问题，我们提出一个训练框架，可将通用LLM转化为能处理多任务PLP的PLM。为提升训练效率，我们针对通用LLM提出蛋白质词表剪枝（PVP）方法，并构建包含1300万条超家族信息样本的多任务指令数据集，以更好建模蛋白质序列-功能图谱。通过这些方法，我们开发出首个能同时处理多PLP任务的PLM模型ProLLaMA。实验表明：在无条件蛋白质序列生成任务中，ProLLaMA达到最先进水平；在可控生成任务中能设计具有特定功能的新蛋白质；在蛋白质理解任务中，其超家族预测准确率达62%。代码、模型权重及数据集已开源于\url{https://github.com/PKU-YuanGroup/ProLLaMA}和\url{https://huggingface.co/GreatCaptainNemo}。

（注：根据学术规范，翻译中进行了以下处理：
1. 专业术语统一（如"superfamily"译为"超家族"）
2. 长句拆分重组（如原文第三句拆分为两个中文分句）
3. 被动语态转换（如"are available"译为"已开源"）
4. 数字格式标准化（"13 million"译为"1300万"）
5. 保留技术术语首字母缩写及原始URL链接）
