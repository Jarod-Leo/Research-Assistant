# Extracting Sentence Embeddings from Pretrained Transformer Models

链接: http://arxiv.org/abs/2408.08073v1

原文摘要:
Pre-trained transformer models shine in many natural language processing
tasks and therefore are expected to bear the representation of the input
sentence or text meaning. These sentence-level embeddings are also important in
retrieval-augmented generation. But do commonly used plain averaging or prompt
templates sufficiently capture and represent the underlying meaning? After
providing a comprehensive review of existing sentence embedding extraction and
refinement methods, we thoroughly test different combinations and our original
extensions of the most promising ones on pretrained models. Namely, given 110 M
parameters, BERT's hidden representations from multiple layers, and many
tokens, we try diverse ways to extract optimal sentence embeddings. We test
various token aggregation and representation post-processing techniques. We
also test multiple ways of using a general Wikitext dataset to complement
BERT's sentence embeddings. All methods are tested on eight Semantic Textual
Similarity (STS), six short text clustering, and twelve classification tasks.
We also evaluate our representation-shaping techniques on other static models,
including random token representations. Proposed representation extraction
methods improve the performance on STS and clustering tasks for all models
considered. Very high improvements for static token-based models, especially
random embeddings for STS tasks, almost reach the performance of BERT-derived
representations. Our work shows that the representation-shaping techniques
significantly improve sentence embeddings extracted from BERT-based and simple
baseline models.

中文翻译:
预训练Transformer模型在众多自然语言处理任务中表现卓越，因此被认为能够有效承载输入句子或文本的语义表征。这类句子级嵌入在检索增强生成任务中同样至关重要。但常用的简单平均法或提示模板是否足以捕捉并呈现深层语义？本文在系统梳理现有句子嵌入提取与优化方法的基础上，对预训练模型上最具潜力的多种组合方案及我们提出的原创扩展方法进行了全面测试。具体而言，针对参数量达1.1亿的BERT模型，我们通过其多隐层输出的丰富token表征，探索了多种最优句子嵌入提取方案：测试了不同的token聚合与表征后处理技术；尝试了利用通用Wikitext数据集增强BERT句子嵌入的多种途径。所有方法均在8项语义文本相似度（STS）评测、6项短文本聚类及12项分类任务上进行验证。我们还评估了这些表征塑造技术在其他静态模型（包括随机token表征模型）上的适用性。实验表明，提出的表征提取方法能全面提升所有测试模型在STS和聚类任务中的表现——特别是基于静态token的模型获得显著提升，其中随机嵌入在STS任务中的改进幅度几乎达到BERT衍生表征的水平。本研究证实，表征塑造技术能显著提升基于BERT的模型及简单基线模型的句子嵌入质量。
