# Revisiting Offline Compression: Going Beyond Factorization-based Methods for Transformer Language Models

链接: http://arxiv.org/abs/2302.04045v1

原文摘要:
Recent transformer language models achieve outstanding results in many
natural language processing (NLP) tasks. However, their enormous size often
makes them impractical on memory-constrained devices, requiring practitioners
to compress them to smaller networks. In this paper, we explore offline
compression methods, meaning computationally-cheap approaches that do not
require further fine-tuning of the compressed model. We challenge the classical
matrix factorization methods by proposing a novel, better-performing
autoencoder-based framework. We perform a comprehensive ablation study of our
approach, examining its different aspects over a diverse set of evaluation
settings. Moreover, we show that enabling collaboration between modules across
layers by compressing certain modules together positively impacts the final
model performance. Experiments on various NLP tasks demonstrate that our
approach significantly outperforms commonly used factorization-based offline
compression methods.

中文翻译:
近年来，基于Transformer架构的语言模型在众多自然语言处理（NLP）任务中取得了卓越成果。然而，其庞大的参数量往往使得这些模型难以在内存受限的设备上部署，迫使研究者必须将其压缩为更小的网络。本文重点研究离线压缩方法——即无需对压缩后模型进行微调的低计算成本方案。我们通过提出一种新型的、性能更优的基于自动编码器的框架，对传统矩阵分解方法提出了挑战。我们通过多组不同评估场景下的实验，对本方法进行了全面的消融研究，深入剖析了其各个层面的表现。此外，研究发现：通过跨层模块协同压缩（即将特定模块联合压缩）能够有效提升最终模型性能。在多项NLP任务上的实验表明，我们所提出的方法显著优于目前主流的基于矩阵分解的离线压缩方案。
