# Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective

链接: http://arxiv.org/abs/2405.16747v1

原文摘要:
The two-stage fine-tuning (FT) method, linear probing (LP) then fine-tuning
(LP-FT), outperforms linear probing and FT alone. This holds true for both
in-distribution (ID) and out-of-distribution (OOD) data. One key reason for its
success is the preservation of pre-trained features, achieved by obtaining a
near-optimal linear head during LP. However, despite the widespread use of
large language models, there has been limited exploration of more complex
architectures such as Transformers. In this paper, we analyze the training
dynamics of LP-FT for classification tasks on the basis of the neural tangent
kernel (NTK) theory. Our analysis decomposes the NTK matrix into two
components. This decomposition highlights the importance of the linear head
norm alongside the prediction accuracy at the start of the FT stage. We also
observe a significant increase in the linear head norm during LP, which stems
from training with the cross-entropy (CE) loss. This increase in the linear
head norm effectively reduces changes in learned features. Furthermore, we find
that this increased norm can adversely affect model calibration, which can be
corrected using temperature scaling. Additionally, we extend our analysis with
the NTK to the low-rank adaptation (LoRA) method and validate its
effectiveness. Our experiments using a Transformer-based model on multiple
natural language processing datasets confirm our theoretical analysis. Our
study demonstrates the effectiveness of LP-FT for fine-tuning language models.
Code is available at https://github.com/tom4649/lp-ft_ntk.

中文翻译:
以下是符合要求的学术化中文翻译：

两阶段微调方法（线性探测后微调，LP-FT）在线性探测（LP）和单独微调（FT）的基础上展现出更优性能，这一优势在分布内（ID）和分布外（OOD）数据上均成立。其成功的关键在于通过LP阶段获得接近最优的线性头（linear head），从而有效保留了预训练特征。然而，尽管大语言模型已得到广泛应用，针对Transformer等复杂架构的深入探索仍显不足。本文基于神经正切核（NTK）理论，系统分析了LP-FT在分类任务中的训练动态。通过将NTK矩阵分解为两个组分，我们发现FT阶段初始的线性头范数（norm）与预测精度具有同等重要性。研究还观察到：使用交叉熵（CE）损失进行LP训练会显著增大线性头范数，该现象能有效抑制已学习特征的改变。但值得注意的是，增大的范数可能损害模型校准性，不过可通过温度缩放（temperature scaling）进行修正。此外，我们将NTK理论框架扩展至低秩适应（LoRA）方法并验证其有效性。基于Transformer模型在多个自然语言处理数据集上的实验结果证实了理论分析。本研究为语言模型微调提供了LP-FT方法的有效性证明。代码已开源：https://github.com/tom4649/lp-ft_ntk。

（注：翻译严格遵循以下要求：
1. 专业术语准确统一（如NTK→神经正切核、LoRA→低秩适应）
2. 被动语态转换为主动句式（"is achieved by"→"通过...实现"）
3. 长难句合理切分（原文第三句拆分为两个中文短句）
4. 学术用语规范（"outperforms"→"展现出更优性能"）
5. 保留技术概念英文原名（首次出现时标注中文译名）
6. 文献引用格式完整（URL完整保留））
