# Self-Data Distillation for Recovering Quality in Pruned Large Language Models

链接: http://arxiv.org/abs/2410.09982v1

原文摘要:
Large language models have driven significant progress in natural language
processing, but their deployment requires substantial compute and memory
resources. As models scale, compression techniques become essential for
balancing model quality with computational efficiency. Structured pruning,
which removes less critical components of the model, is a promising strategy
for reducing complexity. However, one-shot pruning often results in significant
quality degradation, particularly in tasks requiring multi-step reasoning. To
recover lost quality, supervised fine-tuning (SFT) is commonly applied, but it
can lead to catastrophic forgetting by shifting the model's learned data
distribution. Therefore, addressing the degradation from both pruning and SFT
is essential to preserve the original model's quality. In this work, we utilize
self-data distilled fine-tuning to address these challenges. Our approach
leverages the original, unpruned model to generate a distilled dataset that
preserves semantic richness and mitigates catastrophic forgetting by
maintaining alignment with the base model's knowledge. Empirically, we
demonstrate that self-data distillation consistently outperforms standard SFT,
improving average accuracy by up to 8% on the HuggingFace OpenLLM Leaderboard
v1. Specifically, when pruning six decoder blocks on Llama3.1-8B Instruct
(i.e., 32 to 26 layers, reducing the model size from 8.03B to 6.72B
parameters), our method retains 91.2% of the original model's accuracy compared
to 81.7% with SFT, while reducing real-world FLOPs by 16.3%. Furthermore,
combining self-data distilled models through model merging yields enhanced
quality retention. Additionally, leveraging these pruned models in speculative
decoding increases token acceptance rates, thereby improving inference
efficiency in applied settings.

中文翻译:
以下是符合您要求的中文翻译：

大型语言模型推动了自然语言处理的重大进展，但其部署需要消耗大量计算和内存资源。随着模型规模扩大，压缩技术对平衡模型质量与计算效率变得至关重要。结构化剪枝通过移除模型中相对次要的组件，成为降低复杂度的有效策略。然而，一次性剪枝通常会导致显著的性能退化，尤其在需要多步推理的任务中。为恢复损失的质量，监督微调（SFT）虽被广泛采用，但可能因改变模型已学习的数据分布而引发灾难性遗忘。因此，必须同时解决剪枝和SFT导致的质量下降问题，才能保持原始模型的性能。本研究采用自数据蒸馏微调技术应对这些挑战：通过原始未剪枝模型生成蒸馏数据集，既保留语义丰富性，又通过保持与基础模型知识的一致性来缓解灾难性遗忘。实验表明，自数据蒸馏方法在HuggingFace OpenLLM Leaderboard v1上始终优于标准SFT，平均准确率最高提升8%。具体而言，当对Llama3.1-8B Instruct模型剪除六个解码器块（即从32层缩减至26层，参数量从8.03B降至6.72B）时，本方法能保持原始模型91.2%的准确率（SFT仅81.7%），同时实际FLOPs降低16.3%。此外，通过模型融合整合自数据蒸馏模型可进一步提升质量保留率。这些剪枝模型应用于推测解码时还能提高token接受率，从而增强实际推理效率。

（注：根据学术规范，专业术语首次出现时保留英文缩写，如SFT；模型名称Llama3.1-8B Instruct保持原格式；数值单位遵循"B=十亿"的国际通用表示法；长句按中文习惯拆分为符合"话题-说明"结构的短句群）
