# TrafficSafetyGPT: Tuning a Pre-trained Large Language Model to a Domain-Specific Expert in Transportation Safety

链接: http://arxiv.org/abs/2307.15311v1

原文摘要:
Large Language Models (LLMs) have shown remarkable effectiveness in various
general-domain natural language processing (NLP) tasks. However, their
performance in transportation safety domain tasks has been suboptimal,
primarily attributed to the requirement for specialized transportation safety
expertise in generating accurate responses [1]. To address this challenge, we
introduce TrafficSafetyGPT, a novel LLAMA-based model, which has undergone
supervised fine-tuning using TrafficSafety-2K dataset which has human labels
from government produced guiding books and ChatGPT-generated instruction-output
pairs. Our proposed TrafficSafetyGPT model and TrafficSafety-2K train dataset
are accessible at https://github.com/ozheng1993/TrafficSafetyGPT.

中文翻译:
大语言模型（LLMs）在通用领域的自然语言处理（NLP）任务中展现出卓越性能，但在交通安全领域的应用效果欠佳，主要归因于该领域需要专业安全知识才能生成准确响应[1]。为解决这一挑战，我们推出TrafficSafetyGPT——基于LLAMA架构的创新模型，该模型通过TrafficSafety-2K数据集进行监督微调，该数据集包含政府指导手册的人工标注数据及ChatGPT生成的指令-输出对。我们研发的TrafficSafetyGPT模型及TrafficSafety-2K训练数据集已开源，访问地址为：https://github.com/ozheng1993/TrafficSafetyGPT。

（翻译说明：采用技术文献常用句式结构，将被动语态转换为中文主动表达；专业术语如"supervised fine-tuning"译为业界通用表述"监督微调"；长句拆分符合中文表达习惯；保留模型名称及URL等专有名词原貌；补充连接词使逻辑更连贯；"suboptimal"译为"欠佳"准确传达程度差异）
