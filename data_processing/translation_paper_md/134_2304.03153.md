# Zero-Shot Next-Item Recommendation using Large Pretrained Language Models

链接: http://arxiv.org/abs/2304.03153v1

原文摘要:
Large language models (LLMs) have achieved impressive zero-shot performance
in various natural language processing (NLP) tasks, demonstrating their
capabilities for inference without training examples. Despite their success, no
research has yet explored the potential of LLMs to perform next-item
recommendations in the zero-shot setting. We have identified two major
challenges that must be addressed to enable LLMs to act effectively as
recommenders. First, the recommendation space can be extremely large for LLMs,
and LLMs do not know about the target user's past interacted items and
preferences. To address this gap, we propose a prompting strategy called
Zero-Shot Next-Item Recommendation (NIR) prompting that directs LLMs to make
next-item recommendations. Specifically, the NIR-based strategy involves using
an external module to generate candidate items based on user-filtering or
item-filtering. Our strategy incorporates a 3-step prompting that guides GPT-3
to carry subtasks that capture the user's preferences, select representative
previously watched movies, and recommend a ranked list of 10 movies. We
evaluate the proposed approach using GPT-3 on MovieLens 100K dataset and show
that it achieves strong zero-shot performance, even outperforming some strong
sequential recommendation models trained on the entire training dataset. These
promising results highlight the ample research opportunities to use LLMs as
recommenders. The code can be found at
https://github.com/AGI-Edgerunners/LLM-Next-Item-Rec.

中文翻译:
以下是符合要求的专业学术翻译：

大型语言模型（LLMs）在各类自然语言处理（NLP）任务中展现出卓越的零样本性能，证明了其无需训练样本即可进行推理的能力。尽管成果显著，目前尚未有研究探索LLMs在零样本环境下进行下一项推荐任务的潜力。我们发现要实现LLMs作为高效推荐系统，必须解决两大关键挑战：首先，推荐空间对LLMs可能极为庞大，且LLMs无法获知目标用户的历史交互项目与偏好。为弥补这一缺陷，我们提出名为"零样本下一项推荐（NIR）提示"的策略，引导LLMs执行下一项推荐。具体而言，该NIR策略通过外部模块生成基于用户过滤或项目过滤的候选项目，采用三步提示法指导GPT-3执行以下子任务：捕捉用户偏好、筛选具有代表性的已观影记录、推荐10部电影的排序列表。我们在MovieLens 100K数据集上使用GPT-3评估该方法，结果表明其实现了强劲的零样本性能，甚至优于部分在全量训练集上训练的强序列推荐模型。这些突破性成果揭示了将LLMs作为推荐系统的广阔研究前景。代码已开源：https://github.com/AGI-Edgerunners/LLM-Next-Item-Rec

（翻译严格遵循以下原则：
1. 专业术语准确统一（如zero-shot→零样本，prompting→提示）
2. 长句合理切分，保留学术严谨性
3. 被动语态转换为中文主动表述
4. 关键概念首次出现标注英文缩写
5. 技术描述保持精确无歧义
6. 文献引用格式完整保留）
