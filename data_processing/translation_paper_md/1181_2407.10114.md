# TokenSHAP: Interpreting Large Language Models with Monte Carlo Shapley Value Estimation

链接: http://arxiv.org/abs/2407.10114v1

原文摘要:
As large language models (LLMs) become increasingly prevalent in critical
applications, the need for interpretable AI has grown. We introduce TokenSHAP,
a novel method for interpreting LLMs by attributing importance to individual
tokens or substrings within input prompts. This approach adapts Shapley values
from cooperative game theory to natural language processing, offering a
rigorous framework for understanding how different parts of an input contribute
to a model's response. TokenSHAP leverages Monte Carlo sampling for
computational efficiency, providing interpretable, quantitative measures of
token importance. We demonstrate its efficacy across diverse prompts and LLM
architectures, showing consistent improvements over existing baselines in
alignment with human judgments, faithfulness to model behavior, and
consistency.
  Our method's ability to capture nuanced interactions between tokens provides
valuable insights into LLM behavior, enhancing model transparency, improving
prompt engineering, and aiding in the development of more reliable AI systems.
TokenSHAP represents a significant step towards the necessary interpretability
for responsible AI deployment, contributing to the broader goal of creating
more transparent, accountable, and trustworthy AI systems.

中文翻译:
随着大语言模型（LLMs）在关键应用中的日益普及，对可解释人工智能的需求与日俱增。本文提出TokenSHAP——一种通过量化输入提示中单个词元或子字符串重要性来解释LLMs的新方法。该方法将合作博弈论中的沙普利值（Shapley values）适配到自然语言处理领域，为理解输入内容如何影响模型响应提供了严谨的框架。TokenSHAP采用蒙特卡洛采样提升计算效率，可生成可解释的、量化的词元重要性指标。我们通过多样化提示和不同LLM架构验证其有效性，结果显示该方法在人类判断一致性、模型行为忠实度和解释稳定性方面均优于现有基线方法。

该方法捕捉词元间微妙交互作用的能力，为理解LLM行为提供了宝贵洞见，有助于增强模型透明度、优化提示工程，并推动构建更可靠的AI系统。TokenSHAP标志着向负责任AI部署所需可解释性迈出的重要一步，对实现更透明、可问责且可信赖的AI系统这一宏大目标具有积极意义。

（翻译说明：采用技术文档的简洁风格，保留"TokenSHAP"等专业术语首字母大写；将"Shapley values"译为专业术语"沙普利值"并添加括号标注原文；通过"与日俱增""微妙交互作用"等四字结构增强专业性；拆分英文长句为符合中文表达习惯的短句；使用"洞见""可问责"等符合AI领域术语习惯的措辞）
