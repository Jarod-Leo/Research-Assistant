# Pensez: Less Data, Better Reasoning -- Rethinking French LLM

链接: http://arxiv.org/abs/2503.13661v1

原文摘要:
Large language models (LLMs) have demonstrated remarkable capabilities in
various natural language processing tasks. However, achieving strong
performance in specialized domains like mathematical reasoning and non-English
languages often requires extensive training on massive datasets. This paper
investigates a contrasting approach: strategic fine-tuning on a small,
high-quality, bilingual (English-French) dataset to enhance both the reasoning
capabilities and French language proficiency of a large language model. Rather
than relying on scale, we explore the hypothesis that targeted data curation
and optimized training can achieve competitive, or even superior, performance.
We demonstrate, through targeted supervised fine-tuning (SFT) on only 2,000
carefully selected samples, significant improvements in mathematical reasoning.
Specifically, Pensez 7B exhibits an increase in accuracy of the base model up
to 20% on the AIME25 and a 12% increase on a French MATH level 5 benchmark.
These results challenge the prevailing assumption that massive datasets are
aprerequisite for strong reasoning performance in LLMs, highlighting the
potential of strategic data curation and optimized fine-tuning for enhancing
both specialized skills and multilingual capabilities. Our findings have
implications for the efficient development of high-performing, multilingual
LLMs, especially in resource-constrained scenarios.

中文翻译:
以下是符合您要求的中文翻译：

大型语言模型（LLMs）已在各类自然语言处理任务中展现出卓越能力。然而，要在数学推理等专业领域或非英语语言中实现强劲性能，传统方法通常需要海量数据训练。本文研究了一种截然不同的路径：通过对小型高质量英法双语数据集进行策略性微调，同步提升大语言模型的推理能力和法语水平。我们摒弃"规模至上"的思路，验证了通过定向数据筛选和优化训练即可获得竞争力甚至更优表现的假设。实验表明，仅需对2000个精选样本进行有监督微调（SFT），模型在数学推理上就取得显著提升——Pensez 7B基础模型在AIME25测试中的准确率最高提升20%，在法语MATH 5级基准测试中提升12%。这些发现颠覆了"大模型强推理必须依赖大数据"的固有认知，揭示了策略性数据构建与优化微调对增强专业能力和多语言表现的潜力。本研究对资源受限环境下高效开发高性能多语言大模型具有重要启示价值。

翻译说明：
1. 专业术语处理：LLMs保留英文缩写+中文全称，SFT/AIME25等专业测试名称保留英文
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句（如第一段重组为三个递进短句）
3. 概念显化："contrasting approach"译为"截然不同的路径"以突出方法论创新
4. 数据呈现：精确保持"2000个/20%/12%"等数字信息，添加"最高"等限定词确保严谨性
5. 学术风格：使用"揭示/颠覆/启示价值"等符合论文摘要的正式用语
6. 文化适配："resource-constrained scenarios"译为"资源受限环境"更符合中文技术文献表述
