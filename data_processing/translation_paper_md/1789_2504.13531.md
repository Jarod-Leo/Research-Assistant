# Can Local Representation Alignment RNNs Solve Temporal Tasks?

链接: http://arxiv.org/abs/2504.13531v1

原文摘要:
Recurrent Neural Networks (RNNs) are commonly used for real-time processing,
streaming data, and cases where the amount of training samples is limited.
Backpropagation Through Time (BPTT) is the predominant algorithm for training
RNNs; however, it is frequently criticized for being prone to exploding and
vanishing gradients and being biologically implausible. In this paper, we
present and evaluate a target propagation-based method for RNNs, which uses
local updates and seeks to reduce the said instabilities. Having stable RNN
models increases their practical use in a wide range of fields such as natural
language processing, time-series forecasting, anomaly detection, control
systems, and robotics.
  The proposed solution uses local representation alignment (LRA). We
thoroughly analyze the performance of this method, experiment with
normalization and different local error functions, and invalidate certain
assumptions about the behavior of this type of learning. Namely, we demonstrate
that despite the decomposition of the network into sub-graphs, the model still
suffers from vanishing gradients. We also show that gradient clipping as
proposed in LRA has little to no effect on network performance. This results in
an LRA RNN model that is very difficult to train due to vanishing gradients. We
address this by introducing gradient regularization in the direction of the
update and demonstrate that this modification promotes gradient flow and
meaningfully impacts convergence. We compare and discuss the performance of the
algorithm, and we show that the regularized LRA RNN considerably outperforms
the unregularized version on three landmark tasks: temporal order, 3-bit
temporal order, and random permutation.

中文翻译:
循环神经网络（RNN）通常用于实时处理、流数据以及训练样本有限的场景。基于时间的反向传播（BPTT）是训练RNN的主流算法，但该算法常因梯度爆炸/消失问题及缺乏生物合理性而受到质疑。本文提出并评估了一种基于目标传播的RNN训练方法，该方法采用局部更新机制以缓解上述不稳定性。稳定的RNN模型可显著提升其在自然语言处理、时间序列预测、异常检测、控制系统及机器人等广泛领域的实用价值。

本方案采用局部表示对齐（LRA）机制。我们深入分析了该方法的性能，通过标准化处理和不同局部误差函数的实验，验证了此类学习行为中的若干假设缺陷：首先证明即使将网络分解为子图结构，模型仍存在梯度消失问题；其次揭示LRA提出的梯度裁剪方案对网络性能几乎无实质影响。这些发现表明传统LRA-RNN模型因梯度消失问题而难以有效训练。为此，我们提出沿更新方向的梯度正则化方案，证实该改进能促进梯度流动并显著提升收敛性。通过时序排序、3比特时序排序和随机排列三项基准任务的对比实验，证明正则化LRA-RNN模型的性能显著优于未正则化版本。

（注：根据学术翻译规范，对部分专业术语和长句结构进行了优化处理：
1. "exploding and vanishing gradients"译为"梯度爆炸/消失问题"符合中文文献表述习惯
2. "biologically implausible"意译为"缺乏生物合理性"更符合中文科技文本表达
3. 将原文最后两句合并重组，使三项实验任务的表述更紧凑
4. 使用"机制""方案"等词准确传达技术方法的内涵
5. 保持"局部表示对齐（LRA）"等专业术语中英文对照的规范性）
