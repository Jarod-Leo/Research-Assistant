# Partially Recentralization Softmax Loss for Vision-Language Models Robustness

链接: http://arxiv.org/abs/2402.03627v1

原文摘要:
As Large Language Models make a breakthrough in natural language processing
tasks (NLP), multimodal technique becomes extremely popular. However, it has
been shown that multimodal NLP are vulnerable to adversarial attacks, where the
outputs of a model can be dramatically changed by a perturbation to the input.
While several defense techniques have been proposed both in computer vision and
NLP models, the multimodal robustness of models have not been fully explored.
In this paper, we study the adversarial robustness provided by modifying loss
function of pre-trained multimodal models, by restricting top K softmax
outputs. Based on the evaluation and scoring, our experiments show that after a
fine-tuning, adversarial robustness of pre-trained models can be significantly
improved, against popular attacks. Further research should be studying, such as
output diversity, generalization and the robustness-performance trade-off of
this kind of loss functions. Our code will be available after this paper is
accepted

中文翻译:
随着大语言模型在自然语言处理任务（NLP）中取得突破性进展，多模态技术变得极为流行。然而研究表明，多模态NLP系统容易受到对抗攻击的影响——输入的微小扰动就可能导致模型输出发生显著变化。尽管计算机视觉和NLP领域已提出多种防御技术，但模型的多模态鲁棒性尚未得到充分探索。本文通过限制前K个softmax输出的方法，研究修改预训练多模态模型损失函数所提供的对抗鲁棒性。实验评估与评分表明，经过微调后的预训练模型针对常见攻击的对抗鲁棒性可获得显著提升。后续研究应进一步探讨此类损失函数的输出多样性、泛化能力以及鲁棒性与性能的权衡关系。本文录用后代码将予以公开。  

（翻译说明：  
1. 专业术语准确处理："adversarial attacks"译为"对抗攻击"，"softmax outputs"保留技术术语  
2. 长句拆分重构：将原文复合句按中文表达习惯分解为多个短句，如第二句通过破折号衔接因果关系  
3. 被动语态转化："it has been shown"转为主动式"研究表明"  
4. 技术概念显化："fine-tuning"明确译为"微调"而非字面直译  
5. 学术风格保持：使用"本文""研究表明"等符合论文摘要的规范表述  
6. 逻辑衔接处理：通过"后续研究应"自然过渡到未来工作部分  
7. 代码公开声明按中文论文惯例调整语序）
