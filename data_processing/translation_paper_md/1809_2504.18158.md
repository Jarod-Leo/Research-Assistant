# E-InMeMo: Enhanced Prompting for Visual In-Context Learning

链接: http://arxiv.org/abs/2504.18158v1

原文摘要:
Large-scale models trained on extensive datasets have become the standard due
to their strong generalizability across diverse tasks. In-context learning
(ICL), widely used in natural language processing, leverages these models by
providing task-specific prompts without modifying their parameters. This
paradigm is increasingly being adapted for computer vision, where models
receive an input-output image pair, known as an in-context pair, alongside a
query image to illustrate the desired output. However, the success of visual
ICL largely hinges on the quality of these prompts. To address this, we propose
Enhanced Instruct Me More (E-InMeMo), a novel approach that incorporates
learnable perturbations into in-context pairs to optimize prompting. Through
extensive experiments on standard vision tasks, E-InMeMo demonstrates superior
performance over existing state-of-the-art methods. Notably, it improves mIoU
scores by 7.99 for foreground segmentation and by 17.04 for single object
detection when compared to the baseline without learnable prompts. These
results highlight E-InMeMo as a lightweight yet effective strategy for
enhancing visual ICL. Code is publicly available at:
https://github.com/Jackieam/E-InMeMo

中文翻译:
基于海量数据训练的大规模模型因其在多样化任务中展现的强大泛化能力，已成为当前技术标准。上下文学习（ICL）作为自然语言处理领域的常用技术，通过提供任务特定的提示而不修改模型参数来发挥这些模型的潜力。这种范式正逐步应用于计算机视觉领域，模型通过接收输入-输出图像对（即上下文配对样本）和查询图像来理解预期输出。然而视觉ICL的成功很大程度上依赖于提示质量。为此，我们提出"增强型指导优化框架"（E-InMeMo），该创新方法通过将可学习扰动引入上下文配对样本来优化提示效果。在标准视觉任务上的大量实验表明，E-InMeMo显著优于现有最先进方法：与不可学习提示的基线相比，其在前景分割任务中mIoU指标提升7.99，在单目标检测任务中提升17.04。这些成果证实E-InMeMo是一种轻量级但高效的视觉ICL增强策略。代码已开源：https://github.com/Jackieam/E-InMeMo

（注：根据学术翻译规范，对部分术语进行了优化处理：
1. "in-context pair"译为"上下文配对样本"以保持计算机视觉领域术语一致性
2. "learnable perturbations"译为"可学习扰动"符合机器学习领域术语标准
3. 模型名称"E-InMeMo"保留英文缩写形式，同时补充全称译法"增强型指导优化框架"便于中文读者理解
4. 技术指标"mIoU"保留英文缩写并补充"指标"二字确保专业性
5. 长难句采用拆分重组策略，如将原文最后复合句拆分为两个中文短句，符合中文表达习惯）
