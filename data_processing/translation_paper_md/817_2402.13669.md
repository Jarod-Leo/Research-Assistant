# Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning

链接: http://arxiv.org/abs/2402.13669v1

原文摘要:
The surge in Large Language Models (LLMs) has revolutionized natural language
processing, but fine-tuning them for specific tasks often encounters challenges
in balancing performance and preserving general instruction-following
abilities. In this paper, we posit that the distribution gap between task
datasets and the LLMs serves as the primary underlying cause. To address the
problem, we introduce Self-Distillation Fine-Tuning (SDFT), a novel approach
that bridges the distribution gap by guiding fine-tuning with a distilled
dataset generated by the model itself to match its original distribution.
Experimental results on the Llama-2-chat model across various benchmarks
demonstrate that SDFT effectively mitigates catastrophic forgetting while
achieving comparable or superior performance on downstream tasks compared to
the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain
the helpfulness and safety alignment of LLMs. Our code is available at
https://github.com/sail-sg/sdft.

中文翻译:
大型语言模型（LLMs）的迅猛发展彻底改变了自然语言处理领域，但针对特定任务进行微调时，往往难以平衡性能与保持通用指令遵循能力。本文提出，任务数据集与LLMs之间的分布差异是这一问题的根本原因。为此，我们创新性地提出自蒸馏微调方法（SDFT），通过模型自身生成的蒸馏数据集来引导微调过程，从而弥合分布差异，使其与原始分布保持一致。基于Llama-2-chat模型在多个基准测试上的实验表明：相较于传统微调方法，SDFT不仅能有效缓解灾难性遗忘现象，在下游任务中取得相当或更优的性能，还展现出保持LLMs辅助功能和安全对齐特性的潜力。相关代码已开源：https://github.com/sail-sg/sdft。

（注：译文严格遵循学术论文摘要的规范表述，具有以下特点：
1. 专业术语准确统一："catastrophic forgetting"译为"灾难性遗忘"，"distribution gap"译为"分布差异"
2. 被动语态转化："we posit"译为"本文提出"，符合中文摘要惯用表达
3. 长句拆分重组：将原文复合句按中文表达习惯分解为多个短句
4. 逻辑连接显化：通过"为此"、"从而"等连接词明确因果关系
5. 数据呈现规范：完整保留技术名称"SDFT"和开源链接格式）
