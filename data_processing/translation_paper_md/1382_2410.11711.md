# Zero-shot Model-based Reinforcement Learning using Large Language Models

链接: http://arxiv.org/abs/2410.11711v1

原文摘要:
The emerging zero-shot capabilities of Large Language Models (LLMs) have led
to their applications in areas extending well beyond natural language
processing tasks. In reinforcement learning, while LLMs have been extensively
used in text-based environments, their integration with continuous state spaces
remains understudied. In this paper, we investigate how pre-trained LLMs can be
leveraged to predict in context the dynamics of continuous Markov decision
processes. We identify handling multivariate data and incorporating the control
signal as key challenges that limit the potential of LLMs' deployment in this
setup and propose Disentangled In-Context Learning (DICL) to address them. We
present proof-of-concept applications in two reinforcement learning settings:
model-based policy evaluation and data-augmented off-policy reinforcement
learning, supported by theoretical analysis of the proposed methods. Our
experiments further demonstrate that our approach produces well-calibrated
uncertainty estimates. We release the code at
https://github.com/abenechehab/dicl.

中文翻译:
以下是该英文论文摘要的中文翻译：

大型语言模型（LLMs）新兴的零样本能力使其应用范围已远超自然语言处理任务。在强化学习领域，尽管LLMs已在基于文本的环境中广泛应用，但其与连续状态空间的结合仍缺乏深入研究。本文探索如何利用预训练LLMs在上下文环境中预测连续马尔可夫决策过程的动态特性。我们指出多变量数据处理与控制信号整合是限制LLMs在此场景下部署潜力的关键挑战，并提出解耦上下文学习（DICL）方法予以解决。通过两个强化学习场景的概念验证应用——基于模型的策略评估与数据增强的离策略强化学习，并结合所提方法的理论分析进行论证。实验进一步表明，我们的方法能生成校准良好的不确定性估计。代码已发布于https://github.com/abenechehab/dicl。

（翻译说明：采用学术论文摘要的标准表述方式，专业术语如"zero-shot"译为"零样本"、"Markov decision processes"保留专业译名"马尔可夫决策过程"。通过拆分英文长句为中文短句结构（如将"supported by..."独立为分句），使用"予以解决""进行论证"等学术用语保持严谨性，同时确保"well-calibrated uncertainty estimates"等专业表述的准确性。最后保留代码链接格式以符合技术文献规范。）
