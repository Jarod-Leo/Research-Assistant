# Can a Frozen Pretrained Language Model be used for Zero-shot Neural Retrieval on Entity-centric Questions?

链接: http://arxiv.org/abs/2303.05153v1

原文摘要:
Neural document retrievers, including dense passage retrieval (DPR), have
outperformed classical lexical-matching retrievers, such as BM25, when
fine-tuned and tested on specific question-answering datasets. However, it has
been shown that the existing dense retrievers do not generalize well not only
out of domain but even in domain such as Wikipedia, especially when a named
entity in a question is a dominant clue for retrieval. In this paper, we
propose an approach toward in-domain generalization using the embeddings
generated by the frozen language model trained with the entities in the domain.
By not fine-tuning, we explore the possibility that the rich knowledge
contained in a pretrained language model can be used for retrieval tasks. The
proposed method outperforms conventional DPRs on entity-centric questions in
Wikipedia domain and achieves almost comparable performance to BM25 and
state-of-the-art SPAR model. We also show that the contextualized keys lead to
strong improvements compared to BM25 when the entity names consist of common
words. Our results demonstrate the feasibility of the zero-shot retrieval
method for entity-centric questions of Wikipedia domain, where DPR has
struggled to perform.

中文翻译:
神经文档检索模型（包括稠密段落检索DPR）在经过特定问答数据集微调和测试时，其表现优于BM25等传统词匹配检索模型。然而研究表明，现有稠密检索器不仅存在跨领域泛化困难，即使在维基百科等同领域内，当问题中的命名实体成为检索主导线索时，其性能也会显著下降。本文提出一种利用领域内实体训练的冻结语言模型生成嵌入向量的方法，旨在提升同领域泛化能力。通过避免微调操作，我们探索了预训练语言模型中丰富知识直接用于检索任务的可能性。在维基百科领域以实体为核心的问题上，本方法性能超越传统DPR模型，与BM25及当前最先进的SPAR模型表现相当。研究还表明，当实体名称由常见词汇构成时，情境化键值相较BM25能带来显著提升。实验结果证实了零样本检索方法在DPR长期表现不佳的维基百科领域实体类问题上的可行性。
