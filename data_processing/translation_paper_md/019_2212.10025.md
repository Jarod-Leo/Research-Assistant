# When Federated Learning Meets Pre-trained Language Models' Parameter-Efficient Tuning Methods

链接: http://arxiv.org/abs/2212.10025v1

原文摘要:
With increasing privacy concerns on data, recent studies have made
significant progress using federated learning (FL) on privacy-sensitive natural
language processing (NLP) tasks. Much literature suggests fully fine-tuning
pre-trained language models (PLMs) in the FL paradigm can mitigate the data
heterogeneity problem and close the performance gap with centralized training.
However, large PLMs bring the curse of prohibitive communication overhead and
local model adaptation costs for the FL system. To this end, we introduce
various parameter-efficient tuning (PETuning) methods into federated learning.
Specifically, we provide a holistic empirical study of representative PLMs
tuning methods in FL. The experimental results cover the analysis of data
heterogeneity levels, data scales, and different FL scenarios. Overall
communication overhead can be significantly reduced by locally tuning and
globally aggregating lightweight model parameters while maintaining acceptable
performance in various FL settings. To facilitate the research of PETuning in
FL, we also develop a federated tuning framework FedPETuning, which allows
practitioners to exploit different PETuning methods under the FL training
paradigm conveniently. The source code is available at
\url{https://github.com/iezhuozhuo/FedETuning/tree/deltaTuning}.

中文翻译:
随着数据隐私问题日益受到关注，联邦学习（FL）在隐私敏感的自然语言处理（NLP）任务中的应用近期取得显著进展。大量研究表明，在联邦学习范式下对预训练语言模型（PLMs）进行全参数微调，既能缓解数据异构性问题，又能缩小与集中式训练的性能差距。然而，大规模PLMs会带来通信开销过高和本地模型适配成本激增的问题。为此，我们将多种参数高效调优（PETuning）方法引入联邦学习体系，系统性地对FL环境下的代表性PLMs调优方法进行实证研究。实验结果涵盖不同数据异构程度、数据规模及联邦学习场景的分析。通过本地调优与全局聚合轻量级模型参数，能在保持各类FL场景可接受性能的同时显著降低总体通信开销。为促进FL中PETuning的研究，我们还开发了联邦调优框架FedPETuning，使研究者能便捷地在FL训练范式下探索不同参数高效调优方法。