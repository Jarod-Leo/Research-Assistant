# A Survey on Large Language Model Acceleration based on KV Cache Management

链接: http://arxiv.org/abs/2412.19442v1

原文摘要:
Large Language Models (LLMs) have revolutionized a wide range of domains such
as natural language processing, computer vision, and multi-modal tasks due to
their ability to comprehend context and perform logical reasoning. However, the
computational and memory demands of LLMs, particularly during inference, pose
significant challenges when scaling them to real-world, long-context, and
real-time applications. Key-Value (KV) cache management has emerged as a
critical optimization technique for accelerating LLM inference by reducing
redundant computations and improving memory utilization. This survey provides a
comprehensive overview of KV cache management strategies for LLM acceleration,
categorizing them into token-level, model-level, and system-level
optimizations. Token-level strategies include KV cache selection, budget
allocation, merging, quantization, and low-rank decomposition, while
model-level optimizations focus on architectural innovations and attention
mechanisms to enhance KV reuse. System-level approaches address memory
management, scheduling, and hardware-aware designs to improve efficiency across
diverse computing environments. Additionally, the survey provides an overview
of both text and multimodal datasets and benchmarks used to evaluate these
strategies. By presenting detailed taxonomies and comparative analyses, this
work aims to offer useful insights for researchers and practitioners to support
the development of efficient and scalable KV cache management techniques,
contributing to the practical deployment of LLMs in real-world applications.
The curated paper list for KV cache management is in:
\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.

中文翻译:
大语言模型（LLMs）凭借其上下文理解与逻辑推理能力，已在自然语言处理、计算机视觉和多模态任务等领域引发革命性变革。然而，LLMs在计算与内存方面的高需求——尤其是推理过程中的资源消耗——使其在现实世界长上下文、实时应用场景中的规模化面临重大挑战。键值缓存（KV Cache）管理技术通过减少冗余计算并提升内存利用率，已成为加速LLM推理的关键优化手段。本文系统综述了面向LLM加速的KV缓存管理策略，将其划分为令牌级、模型级和系统级三个优化维度：令牌级策略涵盖KV缓存选择、预算分配、合并、量化和低秩分解等技术；模型级优化聚焦架构创新与注意力机制改进以提升KV复用效率；系统级方法则针对内存管理、任务调度和硬件感知设计，优化跨计算环境的整体效能。此外，本文还梳理了用于评估这些策略的文本与多模态数据集及基准测试。通过详尽的分类体系与对比分析，本研究旨在为学术界与工业界提供有价值的洞见，推动高效可扩展的KV缓存管理技术发展，促进LLMs在实际应用中的落地部署。相关论文资源列表详见：\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}。
