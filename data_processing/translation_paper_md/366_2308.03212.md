# Average-Hard Attention Transformers are Constant-Depth Uniform Threshold Circuits

链接: http://arxiv.org/abs/2308.03212v1

原文摘要:
Transformers have emerged as a widely used neural network model for various
natural language processing tasks. Previous research explored their
relationship with constant-depth threshold circuits, making two assumptions:
average-hard attention and logarithmic precision for internal computations
relative to input length. Merrill et al. (2022) prove that average-hard
attention transformers recognize languages that fall within the complexity
class TC0, denoting the set of languages that can be recognized by
constant-depth polynomial-size threshold circuits. Likewise, Merrill and
Sabharwal (2023) show that log-precision transformers recognize languages
within the class of uniform TC0. This shows that both transformer models can be
simulated by constant-depth threshold circuits, with the latter being more
robust due to generating a uniform circuit family. Our paper shows that the
first result can be extended to yield uniform circuits as well.

中文翻译:
Transformer已成为广泛应用于各类自然语言处理任务的神经网络模型。既往研究在探讨其与恒定深度阈值电路关系时，提出了两个关键假设：平均硬注意力机制（average-hard attention）以及相对于输入长度的对数精度内部计算。Merrill等人（2022）证明，采用平均硬注意力的Transformer可识别的语言类别属于复杂度类TC0——即可被恒定深度多项式规模阈值电路识别的语言集合。类似地，Merrill与Sabharwal（2023）研究表明，具备对数精度的Transformer可识别均匀TC0类中的语言。这些成果表明两种Transformer模型均可由恒定深度阈值电路模拟，且后者因能生成均匀电路族而更具鲁棒性。本文证明，第一项研究结果同样可扩展至生成均匀电路。
