# CHOSEN: Compilation to Hardware Optimization Stack for Efficient Vision Transformer Inference

链接: http://arxiv.org/abs/2407.12736v1

原文摘要:
Vision Transformers (ViTs) represent a groundbreaking shift in machine
learning approaches to computer vision. Unlike traditional approaches, ViTs
employ the self-attention mechanism, which has been widely used in natural
language processing, to analyze image patches. Despite their advantages in
modeling visual tasks, deploying ViTs on hardware platforms, notably
Field-Programmable Gate Arrays (FPGAs), introduces considerable challenges.
These challenges stem primarily from the non-linear calculations and high
computational and memory demands of ViTs. This paper introduces CHOSEN, a
software-hardware co-design framework to address these challenges and offer an
automated framework for ViT deployment on the FPGAs in order to maximize
performance. Our framework is built upon three fundamental contributions:
multi-kernel design to maximize the bandwidth, mainly targeting benefits of
multi DDR memory banks, approximate non-linear functions that exhibit minimal
accuracy degradation, and efficient use of available logic blocks on the FPGA,
and efficient compiler to maximize the performance and memory-efficiency of the
computing kernels by presenting a novel algorithm for design space exploration
to find optimal hardware configuration that achieves optimal throughput and
latency. Compared to the state-of-the-art ViT accelerators, CHOSEN achieves a
1.5x and 1.42x improvement in the throughput on the DeiT-S and DeiT-B models.

中文翻译:
视觉Transformer（ViTs）代表了计算机视觉机器学习方法的突破性变革。与传统方法不同，ViTs采用自然语言处理中广泛使用的自注意力机制来分析图像块。尽管在视觉任务建模方面具有优势，但在硬件平台（尤其是现场可编程门阵列FPGA）上部署ViTs仍存在重大挑战，这些挑战主要源于ViT的非线性计算特性及其对算力和内存的高需求。本文提出CHOSEN软硬件协同设计框架，通过三大核心创新应对这些挑战，为FPGA部署ViT提供自动化解决方案以实现最优性能：1）多内核设计充分利用多DDR内存组带宽优势；2）精度损失极小的近似非线性函数实现方案；3）通过新型设计空间探索算法开发的高效编译器，在优化计算内核性能与内存效率的同时，寻找实现最佳吞吐量与延迟的硬件配置方案。实验表明，相较于最先进的ViT加速器，CHOSEN在DeiT-S和DeiT-B模型上分别实现了1.5倍和1.42倍的吞吐量提升。

（注：译文严格遵循以下技术规范：
1. 专业术语统一："self-attention mechanism"译为"自注意力机制"，"Field-Programmable Gate Arrays"保留英文缩写"FPGA"并首次出现时标注全称"现场可编程门阵列"
2. 被动语态转化："are built upon"译为主动式"通过三大核心创新"
3. 长句拆分：将原文复合句拆分为符合中文表达习惯的短句结构
4. 逻辑显化：通过"1）2）3）"序号明确框架的三个贡献点
5. 数据呈现：精确保留"1.5x/1.42x"等量化指标及模型名称"DeiT-S/DeiT-B"）
