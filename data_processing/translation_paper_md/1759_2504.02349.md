# Large (Vision) Language Models are Unsupervised In-Context Learners

链接: http://arxiv.org/abs/2504.02349v1

原文摘要:
Recent advances in large language and vision-language models have enabled
zero-shot inference, allowing models to solve new tasks without task-specific
training. Various adaptation techniques such as prompt engineering, In-Context
Learning (ICL), and supervised fine-tuning can further enhance the model's
performance on a downstream task, but they require substantial manual effort to
construct effective prompts or labeled examples. In this work, we introduce a
joint inference framework for fully unsupervised adaptation, eliminating the
need for manual prompt engineering and labeled examples. Unlike zero-shot
inference, which makes independent predictions, the joint inference makes
predictions simultaneously for all inputs in a given task. Since direct joint
inference involves computationally expensive optimization, we develop efficient
approximation techniques, leading to two unsupervised adaptation methods:
unsupervised fine-tuning and unsupervised ICL. We demonstrate the effectiveness
of our methods across diverse tasks and models, including language-only
Llama-3.1 on natural language processing tasks, reasoning-oriented Qwen2.5-Math
on grade school math problems, vision-language OpenFlamingo on vision tasks,
and the API-only access GPT-4o model on massive multi-discipline tasks. Our
experiments demonstrate substantial improvements over the standard zero-shot
approach, including 39% absolute improvement on the challenging GSM8K math
reasoning dataset. Remarkably, despite being fully unsupervised, our framework
often performs on par with supervised approaches that rely on ground truth
labels.

中文翻译:
近年来，大型语言模型与视觉语言模型的发展实现了零样本推理能力，使模型无需针对特定任务训练即可解决新问题。通过提示工程、上下文学习（ICL）和监督微调等适应技术可进一步提升模型在下游任务中的表现，但这些方法需要大量人工劳动来构建有效提示或标注样本。本研究提出了一种完全无监督的联合推理框架，无需人工设计提示或标注样本。与零样本推理的独立预测方式不同，联合推理会对给定任务中的所有输入进行同步预测。由于直接联合推理涉及计算量巨大的优化过程，我们开发了高效近似技术，由此衍生出两种无监督适应方法：无监督微调与无监督ICL。我们在多样化任务和模型上验证了方法的有效性，包括：纯语言模型Llama-3.1在自然语言处理任务、数学推理专用模型Qwen2.5-Math在小学数学题、视觉语言模型OpenFlamingo在视觉任务，以及仅限API访问的GPT-4o模型在多学科综合任务中的表现。实验结果表明该方法较标准零样本方法有显著提升，如在GSM8K数学推理数据集上取得39%的绝对性能提升。值得注意的是，尽管完全无监督，我们的框架性能常可与依赖真实标签的监督方法相媲美。
