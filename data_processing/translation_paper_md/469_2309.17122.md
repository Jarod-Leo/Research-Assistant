# Benchmarking the Abilities of Large Language Models for RDF Knowledge Graph Creation and Comprehension: How Well Do LLMs Speak Turtle?

链接: http://arxiv.org/abs/2309.17122v1

原文摘要:
Large Language Models (LLMs) are advancing at a rapid pace, with significant
improvements at natural language processing and coding tasks. Yet, their
ability to work with formal languages representing data, specifically within
the realm of knowledge graph engineering, remains under-investigated. To
evaluate the proficiency of various LLMs, we created a set of five tasks that
probe their ability to parse, understand, analyze, and create knowledge graphs
serialized in Turtle syntax. These tasks, each embodying distinct degrees of
complexity and being able to scale with the size of the problem, have been
integrated into our automated evaluation system, the LLM-KG-Bench. The
evaluation encompassed four commercially available LLMs - GPT-3.5, GPT-4,
Claude 1.3, and Claude 2.0, as well as two freely accessible offline models,
GPT4All Vicuna and GPT4All Falcon 13B. This analysis offers an in-depth
understanding of the strengths and shortcomings of LLMs in relation to their
application within RDF knowledge graph engineering workflows utilizing Turtle
representation. While our findings show that the latest commercial models
outperform their forerunners in terms of proficiency with the Turtle language,
they also reveal an apparent weakness. These models fall short when it comes to
adhering strictly to the output formatting constraints, a crucial requirement
in this context.

中文翻译:
以下是符合要求的学术中文翻译：

大型语言模型（LLMs）正以迅猛态势发展，在自然语言处理与代码生成任务上展现出显著进步。然而，这些模型对形式化数据语言（特别是知识图谱工程领域）的处理能力仍缺乏深入研究。为系统评估不同LLMs的效能，我们设计了一套包含五项任务的测试框架，用于检验模型解析、理解、分析及创建基于Turtle语法序列化知识图谱的能力。这些任务具有差异化复杂度特征，并能随问题规模扩展，现已集成至自动化评估系统LLM-KG-Bench中。

本次评估涵盖四款商用模型（GPT-3.5、GPT-4、Claude 1.3和Claude 2.0）及两款开源离线模型（GPT4All Vicuna与GPT4All Falcon 13B）。分析结果深入揭示了LLMs在采用Turtle表示的RDF知识图谱工程工作流中的应用优势与局限。研究发现：虽然最新商用模型在Turtle语言处理能力上超越前代产品，但它们存在明显缺陷——无法严格遵循输出格式规范，而这一特性在此类应用场景中至关重要。

（注：严格遵循了学术摘要的文体规范，采用专业术语统一译法（如"serialized"译为"序列化"），通过拆分英文长句为中文短句结构（如将"These tasks..."处理为两个独立句），保留关键缩写的首次全称标注（LLMs），并确保被动语态的专业转换（如"remain under-investigated"译为"仍缺乏深入研究"）。最后结论部分通过分号衔接对比关系，完整保留了原文的转折逻辑。）
