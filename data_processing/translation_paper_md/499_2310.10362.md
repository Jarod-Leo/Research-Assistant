# Prompt Tuning for Multi-View Graph Contrastive Learning

链接: http://arxiv.org/abs/2310.10362v1

原文摘要:
Graphs have become an important modeling tool for web applications, and Graph
Neural Networks (GNNs) have achieved great success in graph representation
learning. However, the performance of traditional GNNs heavily relies on a
large amount of supervision. Recently, ``pre-train, fine-tune'' has become the
paradigm to address the issues of label dependency and poor generalization.
However, the pre-training strategies vary for graphs with homophily and
heterophily, and the objectives for various downstream tasks also differ. This
leads to a gap between pretexts and downstream tasks, resulting in ``negative
transfer'' and poor performance. Inspired by prompt learning in Natural
Language Processing (NLP), many studies turn to bridge the gap and fully
leverage the pre-trained model. However, existing methods for graph prompting
are tailored to homophily, neglecting inherent heterophily on graphs.
Meanwhile, most of them rely on the randomly initialized prompts, which
negatively impact on the stability. Therefore, we propose Self-Prompt, a
prompting framework for graphs based on the model and data itself. We first
introduce asymmetric graph contrastive learning for pretext to address
heterophily and align the objectives of pretext and downstream tasks. Then we
reuse the component from pre-training phase as the self adapter and introduce
self-prompts based on graph itself for task adaptation. Finally, we conduct
extensive experiments on 11 benchmark datasets to demonstrate its superiority.
We provide our codes at https://github.com/gongchenghua/Self-Pro.

中文翻译:
以下是符合学术规范的中文翻译：

图结构已成为网络应用的重要建模工具，图神经网络（GNNs）在图表示学习领域取得了显著成功。然而，传统GNN的性能高度依赖大量监督数据。近年来，"预训练-微调"范式成为解决标签依赖性强和泛化能力不足的主流方案。但同质图与异质图需要不同的预训练策略，且下游任务目标各异，这导致预训练任务与下游任务之间存在语义鸿沟，引发"负迁移"现象并影响模型性能。

受自然语言处理（NLP）中提示学习的启发，许多研究试图弥合这一鸿沟以充分利用预训练模型。但现有图提示方法均针对同质图设计，忽视了图的固有异质性；同时大多依赖随机初始化的提示向量，影响模型稳定性。为此，我们提出Self-Prompt——基于模型与数据自适应的图提示学习框架：首先采用非对称图对比学习作为预训练任务以兼容异质性，并实现预训练与下游任务的目标对齐；然后复用预训练组件作为自适应器，基于图数据自身生成提示向量完成任务适配。最终在11个基准数据集上的实验验证了本方法的优越性。代码已开源在https://github.com/gongchenghua/Self-Pro。

（翻译严格遵循以下原则：
1. 专业术语准确统一（如homophily/heterophily译为"同质/异质"）
2. 被动语态转换为中文主动句式（如"are tailored to"译为"针对...设计"）
3. 长难句拆分重组（如将原文最后复合句拆分为三个中文短句）
4. 学术表达规范（如"pretext"译为"预训练任务"而非字面义）
5. 重要概念首次出现标注英文原文（如首次出现"Self-Prompt"时保留英文））
