# BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents

链接: http://arxiv.org/abs/2406.03007v1

原文摘要:
With the prosperity of large language models (LLMs), powerful LLM-based
intelligent agents have been developed to provide customized services with a
set of user-defined tools. State-of-the-art methods for constructing LLM agents
adopt trained LLMs and further fine-tune them on data for the agent task.
However, we show that such methods are vulnerable to our proposed backdoor
attacks named BadAgent on various agent tasks, where a backdoor can be embedded
by fine-tuning on the backdoor data. At test time, the attacker can manipulate
the deployed LLM agents to execute harmful operations by showing the trigger in
the agent input or environment. To our surprise, our proposed attack methods
are extremely robust even after fine-tuning on trustworthy data. Though
backdoor attacks have been studied extensively in natural language processing,
to the best of our knowledge, we could be the first to study them on LLM agents
that are more dangerous due to the permission to use external tools. Our work
demonstrates the clear risk of constructing LLM agents based on untrusted LLMs
or data. Our code is public at https://github.com/DPamK/BadAgent

中文翻译:
随着大语言模型（LLMs）的蓬勃发展，基于LLM的智能代理系统被广泛开发，旨在通过用户自定义工具集提供定制化服务。当前最先进的LLM代理构建方法采用预训练大模型，并通过特定代理任务数据进行微调。然而我们发现，这类方法在面对我们提出的"BadAgent"后门攻击时表现脆弱——攻击者只需在微调阶段植入后门数据，就能在测试阶段通过输入提示或环境触发，操纵已部署的LLM代理执行恶意操作。令人惊讶的是，即使后续在可信数据上进行微调，我们提出的攻击方法仍保持极强的鲁棒性。尽管后门攻击在自然语言处理领域已有广泛研究，但据我们所知，本研究可能是首个针对LLM代理系统的攻击方案，由于这类代理具有调用外部工具的权限，其潜在危害更为严重。我们的工作揭示了基于不可信LLM或数据构建代理系统的显著风险。代码已开源：https://github.com/DPamK/BadAgent

（翻译说明：
1. 专业术语处理："LLM agents"译为"LLM代理系统"以保持技术准确性
2. 长句拆分：将原文复合句分解为符合中文表达习惯的短句结构
3. 被动语态转换："can be embedded"等被动式转为主动表述
4. 概念显化："showing the trigger"具体化为"通过输入提示或环境触发"
5. 学术风格保持：使用"鲁棒性""微调"等规范学术用语
6. 文化适配："to our surprise"译为符合中文论文表达的"令人惊讶的是"
7. 补充说明：在"外部工具"后增加"其潜在危害更为严重"以明确隐含逻辑）
