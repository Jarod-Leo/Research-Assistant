# Leveraging Knowledge and Reinforcement Learning for Enhanced Reliability of Language Models

链接: http://arxiv.org/abs/2308.13467v1

原文摘要:
The Natural Language Processing(NLP) community has been using crowd sourcing
techniques to create benchmark datasets such as General Language Understanding
and Evaluation(GLUE) for training modern Language Models such as BERT. GLUE
tasks measure the reliability scores using inter annotator metrics i.e. Cohens
Kappa. However, the reliability aspect of LMs has often been overlooked. To
counter this problem, we explore a knowledge-guided LM ensembling approach that
leverages reinforcement learning to integrate knowledge from ConceptNet and
Wikipedia as knowledge graph embeddings. This approach mimics human annotators
resorting to external knowledge to compensate for information deficits in the
datasets. Across nine GLUE datasets, our research shows that ensembling
strengthens reliability and accuracy scores, outperforming state of the art.

中文翻译:
自然语言处理（NLP）领域一直采用众包技术构建基准数据集（如通用语言理解评估基准GLUE），用于训练BERT等现代语言模型。GLUE任务通过标注者间一致性指标（如科恩卡帕系数）衡量可靠性评分，但语言模型自身的可靠性常被忽视。为此，我们提出一种知识引导的语言模型集成方法：通过强化学习融合ConceptNet和维基百科的知识图谱嵌入表示。该方法模拟人类标注者借助外部知识弥补数据集信息不足的决策过程。在九项GLUE数据集上的实验表明，该集成策略能同步提升模型的可靠性与准确率，性能超越现有最优水平。

（翻译说明：
1. 专业术语规范处理："inter annotator metrics"译为"标注者间一致性指标"，"knowledge graph embeddings"译为"知识图谱嵌入表示"
2. 句式重构：将原文复合长句拆分为符合中文表达习惯的短句，如将"approach that leverages..."处理为独立分句
3. 概念显化："resorting to external knowledge"译为"借助外部知识"并补充"决策过程"使语义完整
4. 学术用语准确："state of the art"采用学界通用译法"现有最优水平"
5. 保持被动语态与主动语态的合理转换，如"has often been overlooked"转为主动句式"常被忽视"）
