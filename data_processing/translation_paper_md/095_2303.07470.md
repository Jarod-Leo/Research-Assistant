# X-Former: In-Memory Acceleration of Transformers

链接: http://arxiv.org/abs/2303.07470v1

原文摘要:
Transformers have achieved great success in a wide variety of natural
language processing (NLP) tasks due to the attention mechanism, which assigns
an importance score for every word relative to other words in a sequence.
However, these models are very large, often reaching hundreds of billions of
parameters, and therefore require a large number of DRAM accesses. Hence,
traditional deep neural network (DNN) accelerators such as GPUs and TPUs face
limitations in processing Transformers efficiently. In-memory accelerators
based on non-volatile memory promise to be an effective solution to this
challenge, since they provide high storage density while performing massively
parallel matrix vector multiplications within memory arrays. However, attention
score computations, which are frequently used in Transformers (unlike CNNs and
RNNs), require matrix vector multiplications (MVM) where both operands change
dynamically for each input. As a result, conventional NVM-based accelerators
incur high write latency and write energy when used for Transformers, and
further suffer from the low endurance of most NVM technologies. To address
these challenges, we present X-Former, a hybrid in-memory hardware accelerator
that consists of both NVM and CMOS processing elements to execute transformer
workloads efficiently. To improve the hardware utilization of X-Former, we also
propose a sequence blocking dataflow, which overlaps the computations of the
two processing elements and reduces execution time. Across several benchmarks,
we show that X-Former achieves upto 85x and 7.5x improvements in latency and
energy over a NVIDIA GeForce GTX 1060 GPU and upto 10.7x and 4.6x improvements
in latency and energy over a state-of-the-art in-memory NVM accelerator.

中文翻译:
基于注意力机制的Transformer模型通过为序列中每个单词分配相对重要性权重，已在各类自然语言处理（NLP）任务中取得显著成功。然而这类模型参数量极为庞大（常达数千亿规模），导致需要频繁访问DRAM。传统深度神经网络（DNN）加速器（如GPU和TPU）在高效处理Transformer时面临局限。基于非易失性存储器（NVM）的内存计算加速器因其高存储密度和存内并行矩阵向量乘法能力，有望成为有效解决方案。但Transformer特有的注意力得分计算（与CNN/RNN不同）需要动态变化的双操作数矩阵向量乘法，导致传统NVM加速器存在高写入延迟、高写入能耗及NVM耐久性不足等问题。为此，我们提出X-Former混合内存计算加速器架构，通过协同整合NVM与CMOS处理单元实现高效Transformer计算。为提升硬件利用率，我们还提出序列分块数据流技术，通过重叠双处理单元计算来降低时延。实验表明：相比NVIDIA GeForce GTX 1060 GPU，X-Former可实现85倍时延优化与7.5倍能效提升；相比先进NVM内存加速器，可获得10.7倍时延降低与4.6倍能效改进。

