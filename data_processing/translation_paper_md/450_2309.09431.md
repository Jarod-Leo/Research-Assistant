# FactoFormer: Factorized Hyperspectral Transformers with Self-Supervised Pre-Training

链接: http://arxiv.org/abs/2309.09431v1

原文摘要:
Hyperspectral images (HSIs) contain rich spectral and spatial information.
Motivated by the success of transformers in the field of natural language
processing and computer vision where they have shown the ability to learn long
range dependencies within input data, recent research has focused on using
transformers for HSIs. However, current state-of-the-art hyperspectral
transformers only tokenize the input HSI sample along the spectral dimension,
resulting in the under-utilization of spatial information. Moreover,
transformers are known to be data-hungry and their performance relies heavily
on large-scale pretraining, which is challenging due to limited annotated
hyperspectral data. Therefore, the full potential of HSI transformers has not
been fully realized. To overcome these limitations, we propose a novel
factorized spectral-spatial transformer that incorporates factorized
self-supervised pretraining procedures, leading to significant improvements in
performance. The factorization of the inputs allows the spectral and spatial
transformers to better capture the interactions within the hyperspectral data
cubes. Inspired by masked image modeling pretraining, we also devise efficient
masking strategies for pretraining each of the spectral and spatial
transformers. We conduct experiments on six publicly available datasets for HSI
classification task and demonstrate that our model achieves state-of-the-art
performance in all the datasets. The code for our model will be made available
at https://github.com/csiro-robotics/factoformer.

中文翻译:
高光谱图像（HSI）蕴含丰富的光谱与空间信息。受Transformer在自然语言处理和计算机视觉领域成功建模输入数据长程依赖关系的启发，近期研究开始探索Transformer在高光谱图像中的应用。然而，当前最先进的高光谱Transformer仅沿光谱维度对输入样本进行标记化处理，导致空间信息利用不足。此外，Transformer模型通常需要海量数据支撑，其性能高度依赖大规模预训练，而高光谱图像标注数据的稀缺性使得这一过程面临挑战。因此，HSI Transformer的潜力尚未得到充分释放。

为突破这些限制，我们提出一种新型因子化光谱-空间Transformer架构，通过融合因子化自监督预训练策略实现性能显著提升。输入数据的因子化处理使光谱与空间Transformer能更有效地捕捉高光谱立方体内部的交互关系。受掩码图像建模预训练的启发，我们还为光谱和空间Transformer分别设计了高效的掩码预训练策略。在六个公开高光谱分类数据集上的实验表明，我们的模型在所有数据集上均达到最先进性能。模型代码将在https://github.com/csiro-robotics/factoformer开源。

（注：根据学术翻译规范，对部分术语进行了标准化处理：
1. "long range dependencies"译为"长程依赖关系"符合信息论术语
2. "tokenize"保留计算机领域惯用译法"标记化"
3. "factorized"统一译为"因子化"以保持数学概念一致性
4. 长句按中文习惯拆分为短句，如将原文最后两句合并重组为符合中文论文摘要的结论句式
5. 技术表述如"masked image modeling"采用领域通用译法"掩码图像建模"）
