# Towards a More Inclusive AI: Progress and Perspectives in Large Language Model Training for the Sámi Language

链接: http://arxiv.org/abs/2405.05777v1

原文摘要:
S\'ami, an indigenous language group comprising multiple languages, faces
digital marginalization due to the limited availability of data and
sophisticated language models designed for its linguistic intricacies. This
work focuses on increasing technological participation for the S\'ami language.
We draw the attention of the ML community towards the language modeling problem
of Ultra Low Resource (ULR) languages. ULR languages are those for which the
amount of available textual resources is very low, and the speaker count for
them is also very low. ULRLs are also not supported by mainstream Large
Language Models (LLMs) like ChatGPT, due to which gathering artificial training
data for them becomes even more challenging. Mainstream AI foundational model
development has given less attention to this category of languages. Generally,
these languages have very few speakers, making it hard to find them. However,
it is important to develop foundational models for these ULR languages to
promote inclusion and the tangible abilities and impact of LLMs. To this end,
we have compiled the available S\'ami language resources from the web to create
a clean dataset for training language models. In order to study the behavior of
modern LLM models with ULR languages (S\'ami), we have experimented with
different kinds of LLMs, mainly at the order of $\sim$ seven billion
parameters. We have also explored the effect of multilingual LLM training for
ULRLs. We found that the decoder-only models under a sequential multilingual
training scenario perform better than joint multilingual training, whereas
multilingual training with high semantic overlap, in general, performs better
than training from scratch.This is the first study on the S\'ami language for
adapting non-statistical language models that use the latest developments in
the field of natural language processing (NLP).

中文翻译:
萨米语作为包含多种语言的土著语系，由于缺乏针对其复杂语言特性设计的数据资源和成熟语言模型，正面临数字边缘化困境。本研究致力于提升萨米语的技术参与度，旨在唤起机器学习社区对超低资源语言（ULR）建模问题的关注。超低资源语言是指可用文本资源极少且使用人口基数极小的语种，主流大语言模型（如ChatGPT）均未提供支持，这导致其人工训练数据的获取更具挑战性。当前人工智能基础模型的开发浪潮中，此类语言长期处于被忽视状态——尽管使用人口稀少导致数据采集困难，但为其开发基础模型对促进技术包容性、实现大语言模型的实际应用价值具有重要意义。

为此，我们系统爬取了网络可及的萨米语资源并构建了洁净的训练数据集。为探究现代大语言模型在超低资源语言（萨米语）上的表现特性，我们以约70亿参数规模的模型为主，对不同架构的大语言模型进行了实验验证，同时考察了多语言训练对超低资源语言的影响。研究发现：在序列化多语言训练策略下，纯解码器模型表现优于联合多语言训练；而具有高语义重叠度的多语言训练整体优于从零开始训练。本研究首次将自然语言处理领域的最新技术应用于萨米语非统计语言模型的适配工作，填补了该领域的空白。
