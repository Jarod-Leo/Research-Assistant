# IgnitionInnovators at "Discharge Me!": Chain-of-Thought Instruction Finetuning Large Language Models for Discharge Summaries

链接: http://arxiv.org/abs/2407.17636v1

原文摘要:
This paper presents our proposed approach to the Discharge Me! shared task,
collocated with the 23th Workshop on Biomedical Natural Language Processing
(BioNLP). In this work, we develop an LLM-based framework for solving the
Discharge Summary Documentation (DSD) task, i.e., generating the two critical
target sections `Brief Hospital Course' and `Discharge Instructions' in the
discharge summary. By streamlining the recent instruction-finetuning process on
LLMs, we explore several prompting strategies for optimally adapting LLMs to
specific generation task of DSD. Experimental results show that providing a
clear output structure, complimented by a set of comprehensive
Chain-of-Thoughts (CoT) questions, effectively improves the model's reasoning
capability, and thereby, enhancing the structural correctness and faithfulness
of clinical information in the generated text. Source code is available at:
https://github.com/antangrocket1312/Discharge_LLM

中文翻译:
本文介绍了我们为第23届生物医学自然语言处理研讨会（BioNLP）附属任务"Discharge Me!"提出的解决方案。本研究开发了一个基于大语言模型（LLM）的框架，用于完成出院摘要文档（DSD）生成任务，即自动生成出院摘要中两个关键目标章节"简要住院病程"和"出院指导"。通过优化近期大语言模型的指令微调流程，我们探索了多种提示策略，以实现LLM在DSD特定生成任务中的最佳适配。实验结果表明，提供清晰的输出结构模板，辅以一组全面的思维链（CoT）问题，能有效提升模型的推理能力，从而增强生成文本的结构准确性和临床信息的忠实度。项目源代码已开源：https://github.com/antangrocket1312/Discharge_LLM

（翻译说明：
1. 专业术语处理："discharge summary"译为"出院摘要"，"Chain-of-Thoughts"采用学界通用译法"思维链"
2. 句式重构：将英语长句拆分为符合中文表达习惯的短句，如将"By streamlining..."处理为独立分句
3. 被动语态转换："Experimental results show that..."译为主动句式"实验结果表明..."
4. 概念显化："LLM"首次出现时补充完整称谓"大语言模型"，后文使用简称
5. 技术表述准确性："instruction-finetuning"译为"指令微调"，符合NLP领域术语规范
6. 链接信息保留：完整保留原始GitHub链接及格式）
