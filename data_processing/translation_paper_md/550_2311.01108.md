# Noise-Robust Fine-Tuning of Pretrained Language Models via External Guidance

链接: http://arxiv.org/abs/2311.01108v1

原文摘要:
Adopting a two-stage paradigm of pretraining followed by fine-tuning,
Pretrained Language Models (PLMs) have achieved substantial advancements in the
field of natural language processing. However, in real-world scenarios, data
labels are often noisy due to the complex annotation process, making it
essential to develop strategies for fine-tuning PLMs with such noisy labels. To
this end, we introduce an innovative approach for fine-tuning PLMs using noisy
labels, which incorporates the guidance of Large Language Models (LLMs) like
ChatGPT. This guidance assists in accurately distinguishing between clean and
noisy samples and provides supplementary information beyond the noisy labels,
thereby boosting the learning process during fine-tuning PLMs. Extensive
experiments on synthetic and real-world noisy datasets further demonstrate the
superior advantages of our framework over the state-of-the-art baselines.

中文翻译:
采用"预训练+微调"的两阶段范式，预训练语言模型（PLMs）在自然语言处理领域取得了显著进展。然而在实际应用场景中，由于标注过程的复杂性，数据标签往往存在噪声，这使得开发针对带噪标签的PLMs微调策略变得至关重要。为此，我们提出了一种创新的带噪标签微调方法，通过引入ChatGPT等大语言模型（LLMs）的指导：其不仅能精准区分干净样本与噪声样本，还能提供超出噪声标签范围的补充信息，从而有效增强PLMs在微调阶段的学习效果。在合成与真实带噪数据集上的大量实验进一步证明，我们的框架相较现有最优基线模型具有显著优势。

（翻译说明：
1. 专业术语处理：PLMs/LMs等专业缩写保留英文并添加中文全称，ChatGPT作为专有名词保留
2. 句式重构：将原文复合长句拆分为符合中文表达习惯的短句，如将"which incorporates..."定语从句转换为冒号引导的说明句式
3. 语态转换：将被动语态"are often noisy"等转化为中文常用的主动表述
4. 术语统一："noisy labels"统一译为"带噪标签"，"clean samples"对应"干净样本"
5. 学术风格保持：使用"微调""基线模型""显著优势"等符合学术论文表达的词汇
6. 逻辑显化：通过"其不仅...还能..."的递进句式清晰呈现LLMs的双重作用）
