# DB-LLM: Accurate Dual-Binarization for Efficient LLMs

链接: http://arxiv.org/abs/2402.11960v1

原文摘要:
Large language models (LLMs) have significantly advanced the field of natural
language processing, while the expensive memory and computation consumption
impede their practical deployment. Quantization emerges as one of the most
effective methods for improving the computational efficiency of LLMs. However,
existing ultra-low-bit quantization always causes severe accuracy drops. In
this paper, we empirically relieve the micro and macro characteristics of
ultra-low bit quantization and present a novel Dual-Binarization method for
LLMs, namely DB-LLM. For the micro-level, we take both the accuracy advantage
of 2-bit-width and the efficiency advantage of binarization into account,
introducing Flexible Dual Binarization (FDB). By splitting 2-bit quantized
weights into two independent sets of binaries, FDB ensures the accuracy of
representations and introduces flexibility, utilizing the efficient bitwise
operations of binarization while retaining the inherent high sparsity of
ultra-low bit quantization. For the macro-level, we find the distortion that
exists in the prediction of LLM after quantization, which is specified as the
deviations related to the ambiguity of samples. We propose the Deviation-Aware
Distillation (DAD) method, enabling the model to focus differently on various
samples. Comprehensive experiments show that our DB-LLM not only significantly
surpasses the current State-of-The-Art (SoTA) in ultra-low bit quantization
(eg, perplexity decreased from 9.64 to 7.23), but also achieves an additional
20\% reduction in computational consumption compared to the SOTA method under
the same bit-width. Our code will be released soon.

中文翻译:
以下是符合要求的学术中文翻译：

大语言模型（LLMs）显著推动了自然语言处理领域的发展，但其高昂的内存与计算消耗阻碍了实际部署。量化技术作为提升LLMs计算效率的最有效方法之一应运而生，然而现有超低位量化往往导致严重的精度损失。本文通过实证研究揭示了超低位量化的微观与宏观特性，并提出一种新型双二值化方法DB-LLM。在微观层面，我们兼顾2比特位宽的精度优势与二值化的效率优势，提出柔性双二值化（FDB）方案：通过将2比特量化权重拆分为两组独立二进制数，FDB在保留超低位量化固有高稀疏性的同时，既保证了表征精度又引入灵活性，可充分利用二值化的高效位运算特性。在宏观层面，我们发现量化后LLM预测中存在特定于样本模糊性相关的偏差，据此提出偏差感知蒸馏（DAD）方法，使模型能差异化处理不同样本。综合实验表明，DB-LLM不仅显著超越当前超低位量化最优成果（如困惑度从9.64降至7.23），在相同位宽下相较SOTA方法还能额外减少20%计算消耗。代码即将开源。

（翻译严格遵循以下原则：
1. 专业术语准确统一（如quantization=量化，perplexity=困惑度）
2. 长句按中文习惯切分重组，保留学术严谨性
3. 被动语态转换为主动表述（如"are split"→"拆分为"）
4. 关键方法名称保留英文缩写并首次标注中文全称
5. 数据指标精确传达（20%/9.64→7.23等）
6. 符合学术摘要简洁性要求，无冗余修饰）
