# Combining Denoising Autoencoders with Contrastive Learning to fine-tune Transformer Models

链接: http://arxiv.org/abs/2405.14437v1

原文摘要:
Recently, using large pretrained Transformer models for transfer learning
tasks has evolved to the point where they have become one of the flagship
trends in the Natural Language Processing (NLP) community, giving rise to
various outlooks such as prompt-based, adapters or combinations with
unsupervised approaches, among many others. This work proposes a 3 Phase
technique to adjust a base model for a classification task. First, we adapt the
model's signal to the data distribution by performing further training with a
Denoising Autoencoder (DAE). Second, we adjust the representation space of the
output to the corresponding classes by clustering through a Contrastive
Learning (CL) method. In addition, we introduce a new data augmentation
approach for Supervised Contrastive Learning to correct the unbalanced
datasets. Third, we apply fine-tuning to delimit the predefined categories.
These different phases provide relevant and complementary knowledge to the
model to learn the final task. We supply extensive experimental results on
several datasets to demonstrate these claims. Moreover, we include an ablation
study and compare the proposed method against other ways of combining these
techniques.

中文翻译:
近年来，基于大规模预训练Transformer模型的迁移学习技术已发展成为自然语言处理（NLP）领域的旗舰趋势之一，由此催生了提示学习、适配器架构、与无监督方法结合等多种技术路线。本研究提出一种三阶段分类任务适配框架：首先通过去噪自编码器（DAE）进行增量训练，使模型信号适应目标数据分布；其次采用对比学习（CL）聚类方法调整输出表征空间以匹配目标类别，并创新性地提出面向监督对比学习的增强数据方法以修正数据集不平衡问题；最后通过微调明确预定义类别边界。这三个阶段为模型习得最终任务提供了互补性知识。我们在多个数据集上进行了广泛实验验证，同时通过消融研究对比了本方案与其他技术组合方式的性能差异。

（注：译文采用学术论文摘要的标准表述方式，具有以下特点：
1. 专业术语准确对应（如Transformer/迁移学习/消融研究等）
2. 长句拆分符合中文表达习惯（如将原文复合从句拆分为多个短句）
3. 被动语态转化（如"is proposed"译为"提出"）
4. 逻辑连接词显化（如"首先/其次/最后"的阶段性表述）
5. 关键概念首次出现时保留英文缩写（DAE/CL）便于学术检索）
