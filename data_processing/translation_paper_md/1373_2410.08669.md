# SmartPretrain: Model-Agnostic and Dataset-Agnostic Representation Learning for Motion Prediction

链接: http://arxiv.org/abs/2410.08669v1

原文摘要:
Predicting the future motion of surrounding agents is essential for
autonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed
environments. However, the scarcity of large-scale driving datasets has
hindered the development of robust and generalizable motion prediction models,
limiting their ability to capture complex interactions and road geometries.
Inspired by recent advances in natural language processing (NLP) and computer
vision (CV), self-supervised learning (SSL) has gained significant attention in
the motion prediction community for learning rich and transferable scene
representations. Nonetheless, existing pre-training methods for motion
prediction have largely focused on specific model architectures and single
dataset, limiting their scalability and generalizability. To address these
challenges, we propose SmartPretrain, a general and scalable SSL framework for
motion prediction that is both model-agnostic and dataset-agnostic. Our
approach integrates contrastive and reconstructive SSL, leveraging the
strengths of both generative and discriminative paradigms to effectively
represent spatiotemporal evolution and interactions without imposing
architectural constraints. Additionally, SmartPretrain employs a
dataset-agnostic scenario sampling strategy that integrates multiple datasets,
enhancing data volume, diversity, and robustness. Extensive experiments on
multiple datasets demonstrate that SmartPretrain consistently improves the
performance of state-of-the-art prediction models across datasets, data splits
and main metrics. For instance, SmartPretrain significantly reduces the
MissRate of Forecast-MAE by 10.6%. These results highlight SmartPretrain's
effectiveness as a unified, scalable solution for motion prediction, breaking
free from the limitations of the small-data regime. Codes are available at
https://github.com/youngzhou1999/SmartPretrain

中文翻译:
以下是符合学术规范的中文翻译：

预测周围智能体的未来运动轨迹对于自动驾驶车辆（AVs）在动态人机混合环境中安全运行至关重要。然而，大规模驾驶数据集的稀缺性阻碍了鲁棒且可泛化运动预测模型的发展，限制了模型捕捉复杂交互与道路几何特征的能力。受自然语言处理（NLP）和计算机视觉（CV）领域最新进展的启发，自监督学习（SSL）在运动预测领域获得了广泛关注，因其能学习丰富且可迁移的场景表征。然而，现有运动预测预训练方法多局限于特定模型架构和单一数据集，制约了方法的可扩展性与泛化能力。为应对这些挑战，我们提出SmartPretrain——一个模型无关且数据集无关的通用可扩展SSL框架。该方法融合对比式与重构式自监督学习，结合生成式与判别式范式的优势，在不强加架构限制的前提下有效表征时空演化与交互特征。此外，SmartPretrain采用数据集无关的场景采样策略，通过整合多源数据集提升数据规模、多样性与鲁棒性。在多数据集上的大量实验表明，SmartPretrain能持续提升前沿预测模型在不同数据集、数据划分和核心指标上的性能。例如，该方法使Forecast-MAE的漏检率显著降低10.6%。这些结果证明SmartPretrain可作为突破小数据局限的统一解决方案。代码已开源于https://github.com/youngzhou1999/SmartPretrain

（翻译严格遵循以下原则：
1. 专业术语准确统一（如"self-supervised learning"译为"自监督学习"）
2. 被动语态转换为主动句式（如"has been hindered"译为"阻碍了"）
3. 长难句合理切分（如将原文复合从句拆分为多个短句）
4. 保留技术概念精确性（如"contrastive and reconstructive SSL"译为"对比式与重构式自监督学习"）
5. 学术用语规范化（如"state-of-the-art"译为"前沿"而非"最先进"）
6. 重要数据完整保留（如10.6%的精度提升））
