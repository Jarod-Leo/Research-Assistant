# LoRA-GGPO: Mitigating Double Descent in LoRA Fine-Tuning via Gradient-Guided Perturbation Optimization

链接: http://arxiv.org/abs/2502.14538v1

原文摘要:
Large Language Models (LLMs) have achieved remarkable success in natural
language processing, but their full fine-tuning remains resource-intensive.
Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation
(LoRA), have emerged as a practical solution by approximating parameter updates
with low-rank matrices. However, LoRA often exhibits a "double descent"
phenomenon during fine-tuning, where model performance degrades due to
overfitting and limited expressiveness caused by low-rank constraints. To
address this issue, we propose LoRA-GGPO (Gradient-Guided Perturbation
Optimization), a novel method that leverages gradient and weight norms to
generate targeted perturbations. By optimizing the sharpness of the loss
landscape, LoRA-GGPO guides the model toward flatter minima, mitigating the
double descent problem and improving generalization. Extensive experiments on
natural language understanding (NLU) and generation (NLG) tasks demonstrate
that LoRA-GGPO outperforms LoRA and its state-of-the-art variants. Furthermore,
extended experiments specifically designed to analyze the double descent
phenomenon confirm that LoRA-GGPO effectively alleviates this issue, producing
more robust and generalizable models. Our work provides a robust and efficient
solution for fine-tuning LLMs, with broad applicability in real-world
scenarios. The code is available at https://github.com/llm172/LoRA-GGPO.

中文翻译:
以下是符合您要求的中文翻译：

大语言模型（LLMs）在自然语言处理领域取得了显著成就，但完整的全参数微调仍存在资源消耗过高的问题。参数高效微调方法（PEFT）如低秩自适应（LoRA）通过低秩矩阵近似参数更新，已成为实用解决方案。然而，LoRA在微调过程中常出现"双下降"现象——由于低秩约束导致的过拟合和表达能力受限，模型性能会阶段性下降。针对这一问题，我们提出LoRA-GGPO（梯度引导扰动优化），该方法创新性地利用梯度和权重范数生成定向扰动，通过优化损失函数的平坦性引导模型收敛至更平坦的极小值，从而缓解双下降问题并提升泛化能力。在自然语言理解（NLU）和生成（NLG）任务上的大量实验表明，LoRA-GGPO性能优于原始LoRA及其先进变体。特别设计的双下降现象专项实验进一步证实，该方法能有效缓解该问题，生成更具鲁棒性和泛化能力的模型。本研究为LLMs微调提供了高效可靠的解决方案，具有广泛的实际应用价值。代码已开源：https://github.com/llm172/LoRA-GGPO。

（翻译严格遵循以下要求：
1. 专业术语准确统一："double descent"译为"双下降"，"low-rank matrices"译为"低秩矩阵"
2. 被动语态转化："have emerged"译为"已成为"
3. 长句拆分：将原文复合句拆分为符合中文表达习惯的短句
4. 逻辑显化：通过破折号和括号明确技术问题的因果关系
5. 学术风格：使用"针对""证实""泛化能力"等规范学术用语
6. 文化适配："real-world scenarios"译为"实际应用"而非字面直译）
