# LMExplainer: a Knowledge-Enhanced Explainer for Language Models

链接: http://arxiv.org/abs/2303.16537v1

原文摘要:
Language models (LMs) like GPT-4 are important in AI applications, but their
opaque decision-making process reduces user trust, especially in
safety-critical areas. We introduce LMExplainer, a novel knowledge-grounded
explainer that clarifies the reasoning process of LMs through intuitive,
human-understandable explanations. By leveraging a graph attention network
(GAT) with a large-scale knowledge graph (KG), LMExplainer not only precisely
narrows the reasoning space to focus on the most relevant knowledge but also
grounds its reasoning in structured, verifiable knowledge to reduce
hallucinations and enhance interpretability. LMExplainer effectively generates
human-understandable explanations to enhance transparency and streamline the
decision-making process. Additionally, by incorporating debugging into the
explanation, it offers expertise suggestions that improve LMs from a
developmental perspective. Thus, LMExplainer stands as an enhancement in making
LMs more accessible and understandable to users. We evaluate LMExplainer on
benchmark datasets such as CommonsenseQA and OpenBookQA, demonstrating that it
outperforms most existing methods. By comparing the explanations generated by
LMExplainer with those of other models, we show that our approach offers more
comprehensive and clearer explanations of the reasoning process. LMExplainer
provides a deeper understanding of the inner workings of LMs, advancing towards
more reliable, transparent, and equitable AI.

中文翻译:
以下是符合您要求的中文翻译：

【译文】
以GPT-4为代表的大语言模型（LMs）在人工智能应用中发挥着重要作用，但其不透明的决策过程降低了用户信任度，在安全关键领域尤为突出。我们提出LMExplainer——一种新型的知识驱动解释器，通过直观易懂的人类可理解解释，阐明语言模型的推理过程。该框架通过图注意力网络（GAT）与大规模知识图谱（KG）的协同，不仅能精确收缩推理空间以聚焦最相关知识，还将推理过程锚定在结构化、可验证的知识体系中，从而减少幻觉生成并提升可解释性。LMExplainer能有效生成人类可理解的解释，既增强了模型透明度，又优化了决策流程。此外，通过将调试机制融入解释过程，它还能从开发视角提供改进语言模型的专家级建议，使LMs更易被用户理解和使用。我们在CommonsenseQA和OpenBookQA等基准数据集上的评估表明，LMExplainer性能优于现有大多数方法。通过与其他模型生成的解释对比，本方法能提供更全面、清晰的推理过程阐释。LMExplainer深化了人们对语言模型内部机制的理解，推动人工智能向更可靠、透明、公平的方向发展。

【翻译要点说明】
1. 专业术语处理：
- "knowledge-grounded explainer"译为"知识驱动解释器"，保留学术性同时确保易懂
- "graph attention network"规范译为"图注意力网络"并标注缩写GAT
- "hallucinations"译为专业术语"幻觉生成"

2. 句式重构：
- 将英语长句"By leveraging..."拆分为两个中文短句，符合汉语表达习惯
- "grounds its reasoning in..."意译为"将推理过程锚定在..."，增强可读性

3. 概念显化：
- "debugging"具体化为"调试机制"
- "developmental perspective"译为"开发视角"更符合中文技术文档表述

4. 学术风格保持：
- 使用"阐释""锚定""协同"等书面词汇
- 保留"可解释性""透明度"等专业表述

5. 文化适配：
- "human-understandable"统一处理为"人类可理解的"，避免直译生硬
- "equitable AI"译为"公平的"而非字面"平等的"，更符合中文技术伦理讨论语境
