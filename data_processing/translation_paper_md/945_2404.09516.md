# State Space Model for New-Generation Network Alternative to Transformers: A Survey

链接: http://arxiv.org/abs/2404.09516v1

原文摘要:
In the post-deep learning era, the Transformer architecture has demonstrated
its powerful performance across pre-trained big models and various downstream
tasks. However, the enormous computational demands of this architecture have
deterred many researchers. To further reduce the complexity of attention
models, numerous efforts have been made to design more efficient methods. Among
them, the State Space Model (SSM), as a possible replacement for the
self-attention based Transformer model, has drawn more and more attention in
recent years. In this paper, we give the first comprehensive review of these
works and also provide experimental comparisons and analysis to better
demonstrate the features and advantages of SSM. Specifically, we first give a
detailed description of principles to help the readers quickly capture the key
ideas of SSM. After that, we dive into the reviews of existing SSMs and their
various applications, including natural language processing, computer vision,
graph, multi-modal and multi-media, point cloud/event stream, time series data,
and other domains. In addition, we give statistical comparisons and analysis of
these models and hope it helps the readers to understand the effectiveness of
different structures on various tasks. Then, we propose possible research
points in this direction to better promote the development of the theoretical
model and application of SSM. More related works will be continuously updated
on the following GitHub:
https://github.com/Event-AHU/Mamba_State_Space_Model_Paper_List.

中文翻译:
在后深度学习时代，Transformer架构已在预训练大模型与各类下游任务中展现出强大性能。然而该架构巨大的计算需求令众多研究者望而却步。为进一步降低注意力模型的复杂度，学界已付出诸多努力来设计更高效的方法。其中，状态空间模型（State Space Model, SSM）作为基于自注意力机制的Transformer模型的潜在替代方案，近年来受到越来越多关注。本文首次系统综述了相关研究成果，并通过实验对比与分析来更清晰地展现SSM的特性与优势。

具体而言，我们首先详细阐释基本原理，帮助读者快速把握SSM的核心思想；继而深入评述现有SSM模型及其多样化应用场景，涵盖自然语言处理、计算机视觉、图数据、多模态与多媒体、点云/事件流、时间序列数据及其他领域。此外，我们对这些模型进行了统计性对比与分析，以期帮助读者理解不同结构在各任务中的有效性。最后，我们提出该方向潜在的研究要点，旨在推动SSM理论模型与应用发展的进一步突破。更多相关成果将持续更新于以下GitHub项目：https://github.com/Event-AHU/Mamba_State_Space_Model_Paper_List

（注：译文严格遵循学术论文摘要的规范表述，采用专业术语统一原则：
1. "post-deep learning era"译为"后深度学习时代"符合中文技术文献表述习惯
2. "State Space Model"首次出现时保留英文缩写并标注全称，后续统一使用"SSM"
3. "self-attention"译为专业术语"自注意力机制"
4. 长难句处理采用拆分策略，如将原文最后复合句拆分为两个中文句群，确保符合中文表达逻辑）
