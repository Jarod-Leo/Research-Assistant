# Continuous Sign Language Recognition with Adapted Conformer via Unsupervised Pretraining

链接: http://arxiv.org/abs/2405.12018v1

原文摘要:
Conventional Deep Learning frameworks for continuous sign language
recognition (CSLR) are comprised of a single or multi-modal feature extractor,
a sequence-learning module, and a decoder for outputting the glosses. The
sequence learning module is a crucial part wherein transformers have
demonstrated their efficacy in the sequence-to-sequence tasks. Analyzing the
research progress in the field of Natural Language Processing and Speech
Recognition, a rapid introduction of various transformer variants is observed.
However, in the realm of sign language, experimentation in the sequence
learning component is limited. In this work, the state-of-the-art Conformer
model for Speech Recognition is adapted for CSLR and the proposed model is
termed ConSignformer. This marks the first instance of employing Conformer for
a vision-based task. ConSignformer has bimodal pipeline of CNN as feature
extractor and Conformer for sequence learning. For improved context learning we
also introduce Cross-Modal Relative Attention (CMRA). By incorporating CMRA
into the model, it becomes more adept at learning and utilizing complex
relationships within the data. To further enhance the Conformer model,
unsupervised pretraining called Regressional Feature Extraction is conducted on
a curated sign language dataset. The pretrained Conformer is then fine-tuned
for the downstream recognition task. The experimental results confirm the
effectiveness of the adopted pretraining strategy and demonstrate how CMRA
contributes to the recognition process. Remarkably, leveraging a
Conformer-based backbone, our model achieves state-of-the-art performance on
the benchmark datasets: PHOENIX-2014 and PHOENIX-2014T.

中文翻译:
传统连续手语识别（CSLR）的深度学习框架通常由单模态或多模态特征提取器、序列学习模块以及用于输出词汇标记的解码器构成。其中序列学习模块是关键组成部分，而Transformer架构已在序列到序列任务中展现出卓越性能。通过分析自然语言处理和语音识别领域的研究进展，可观察到各类Transformer变体正被快速引入。然而在手语研究领域，针对序列学习组件的创新探索仍较为有限。

本研究将语音识别领域最先进的Conformer模型适配于CSLR任务，提出名为ConSignformer的新型模型。这标志着Conformer首次被应用于基于视觉的任务。ConSignformer采用CNN作为特征提取器与Conformer序列学习的双模态架构。为提升上下文学习能力，我们创新性地提出跨模态相对注意力机制（CMRA）。该机制的引入使模型能更有效地学习并利用数据中的复杂关联关系。

为进一步增强Conformer性能，我们在精选手语数据集上进行了名为"回归特征提取"的无监督预训练，随后对预训练模型进行下游识别任务的微调。实验结果验证了所采用预训练策略的有效性，并证实CMRA机制对识别过程的显著贡献。值得注意的是，基于Conformer架构的模型在PHOENIX-2014和PHOENIX-2014T基准测试中取得了当前最优性能。
