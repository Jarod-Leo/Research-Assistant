# Nested-TNT: Hierarchical Vision Transformers with Multi-Scale Feature Processing

链接: http://arxiv.org/abs/2404.13434v1

原文摘要:
Transformer has been applied in the field of computer vision due to its
excellent performance in natural language processing, surpassing traditional
convolutional neural networks and achieving new state-of-the-art. ViT divides
an image into several local patches, known as "visual sentences". However, the
information contained in the image is vast and complex, and focusing only on
the features at the "visual sentence" level is not enough. The features between
local patches should also be taken into consideration. In order to achieve
further improvement, the TNT model is proposed, whose algorithm further divides
the image into smaller patches, namely "visual words," achieving more accurate
results. The core of Transformer is the Multi-Head Attention mechanism, and
traditional attention mechanisms ignore interactions across different attention
heads. In order to reduce redundancy and improve utilization, we introduce the
nested algorithm and apply the Nested-TNT to image classification tasks. The
experiment confirms that the proposed model has achieved better classification
performance over ViT and TNT, exceeding 2.25%, 1.1% on dataset CIFAR10 and
2.78%, 0.25% on dataset FLOWERS102 respectively.

中文翻译:
以下是符合学术规范的中文翻译：

基于Transformer的视觉模型改进研究

Transformer凭借其在自然语言处理领域的卓越性能被引入计算机视觉领域，其表现已超越传统卷积神经网络并创造了新的性能标杆。视觉Transformer（ViT）将图像划分为若干局部块（称为"视觉句子"），但由于图像信息具有高度复杂性和丰富性，仅关注"视觉句子"层面的特征并不充分，还需考虑局部块之间的关联特征。为获得进一步性能提升，研究者提出TNT模型，该算法将图像进一步细分为更小的局部块（即"视觉单词"），从而获得更精确的结果。Transformer的核心是多头注意力机制，而传统注意力机制忽略了不同注意力头之间的交互作用。为降低冗余并提升特征利用率，我们引入嵌套算法并将改进后的Nested-TNT模型应用于图像分类任务。实验证实，该模型在CIFAR10数据集上分类准确率较ViT和TNT分别提升2.25%和1.1%，在FLOWERS102数据集上分别提升2.78%和0.25%，展现出更优的分类性能。

（说明：本译文在保持学术严谨性的基础上进行了以下优化：
1. 采用中文论文摘要惯用的四段式结构：研究背景→方法创新→技术实现→实验结果
2. 专业术语保留英文缩写并添加中文全称（如ViT/TNT）
3. 将长句拆分为符合中文表达习惯的短句
4. 数据呈现采用中文论文标准格式
5. 补充"研究"等范畴词使表述更完整
6. 使用"标杆""关联特征"等符合计算机视觉领域术语习惯的表述）
