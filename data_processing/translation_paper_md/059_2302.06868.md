# SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains

链接: http://arxiv.org/abs/2302.06868v1

原文摘要:
Prompting pre-trained language models leads to promising results across
natural language processing tasks but is less effective when applied in
low-resource domains, due to the domain gap between the pre-training data and
the downstream task. In this work, we bridge this gap with a novel and
lightweight prompting methodology called SwitchPrompt for the adaptation of
language models trained on datasets from the general domain to diverse
low-resource domains. Using domain-specific keywords with a trainable gated
prompt, SwitchPrompt offers domain-oriented prompting, that is, effective
guidance on the target domains for general-domain language models. Our few-shot
experiments on three text classification benchmarks demonstrate the efficacy of
the general-domain pre-trained language models when used with SwitchPrompt.
They often even outperform their domain-specific counterparts trained with
baseline state-of-the-art prompting methods by up to 10.7% performance increase
in accuracy. This result indicates that SwitchPrompt effectively reduces the
need for domain-specific language model pre-training.

中文翻译:
针对预训练语言模型的提示学习方法在自然语言处理任务中展现出良好效果，但当应用于低资源领域时，由于预训练数据与下游任务间的领域差异，其效能会显著降低。本研究提出了一种新颖的轻量级提示方法SwitchPrompt，用于将通用领域训练的语言模型适配到多样化的低资源领域。该方法通过结合领域关键词与可训练门控提示机制，实现了面向领域的提示策略，即为通用领域语言模型提供针对目标领域的有效引导。我们在三个文本分类基准上进行的少样本实验表明：当配合SwitchPrompt使用时，通用领域预训练语言模型展现出卓越性能，其准确率最高可超越采用基线前沿提示方法的领域专用模型10.7%。这一结果表明，SwitchPrompt能有效减少对领域专用语言模型预训练的依赖。
