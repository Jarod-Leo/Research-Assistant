# Better Explain Transformers by Illuminating Important Information

链接: http://arxiv.org/abs/2401.09972v2

原文摘要:
Transformer-based models excel in various natural language processing (NLP)
tasks, attracting countless efforts to explain their inner workings. Prior
methods explain Transformers by focusing on the raw gradient and attention as
token attribution scores, where non-relevant information is often considered
during explanation computation, resulting in confusing results. In this work,
we propose highlighting the important information and eliminating irrelevant
information by a refined information flow on top of the layer-wise relevance
propagation (LRP) method. Specifically, we consider identifying syntactic and
positional heads as important attention heads and focus on the relevance
obtained from these important heads. Experimental results demonstrate that
irrelevant information does distort output attribution scores and then should
be masked during explanation computation. Compared to eight baselines on both
classification and question-answering datasets, our method consistently
outperforms with over 3\% to 33\% improvement on explanation metrics, providing
superior explanation performance. Our anonymous code repository is available
at: https://github.com/LinxinS97/Mask-LRP

中文翻译:
基于Transformer的模型在各类自然语言处理（NLP）任务中表现卓越，这促使研究者们不断探索其内部工作机制。现有解释方法主要通过原始梯度和注意力机制作为词元归因分数，但在解释计算过程中常包含不相关信息，导致结果难以理解。本研究提出在分层相关性传播（LRP）方法基础上，通过优化信息流来突出重要信息并消除无关信息。具体而言，我们将识别句法和位置信息的注意力头判定为重要注意力头，并聚焦于这些重要头产生的相关性。实验结果表明，无关信息确实会扭曲输出归因分数，因此在解释计算过程中应当被屏蔽。在分类和问答数据集上与八种基线方法对比中，本方法在解释指标上始终优于基线模型，提升幅度达3%至33%，展现出更优越的解释性能。匿名代码仓库地址：https://github.com/LinxinS97/Mask-LRP  

（注：根据学术翻译规范，对原文进行了以下处理：
1. 将"Transformer"保留英文形式作为专有名词
2. "token attribution scores"译为"词元归因分数"符合NLP领域术语
3. "layer-wise relevance propagation"采用学界通用译名"分层相关性传播"
4. 长难句拆分为符合中文表达习惯的短句结构
5. 百分比范围"3\% to 33\%"规范译为"3%至33%"
6. 技术术语如"syntactic and positional heads"译为"句法和位置信息的注意力头"确保准确性）
