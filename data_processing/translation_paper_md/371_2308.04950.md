# Performance Analysis of Transformer Based Models (BERT, ALBERT and RoBERTa) in Fake News Detection

链接: http://arxiv.org/abs/2308.04950v1

原文摘要:
Fake news is fake material in a news media format but is not processed
properly by news agencies. The fake material can provoke or defame significant
entities or individuals or potentially even for the personal interests of the
creators, causing problems for society. Distinguishing fake news and real news
is challenging due to limited of domain knowledge and time constraints.
According to the survey, the top three areas most exposed to hoaxes and
misinformation by residents are in Banten, DKI Jakarta and West Java. The model
of transformers is referring to an approach in the field of artificial
intelligence (AI) in natural language processing utilizing the deep learning
architectures. Transformers exercise a powerful attention mechanism to process
text in parallel and produce rich and contextual word representations. A
previous study indicates a superior performance of a transformer model known as
BERT over and above non transformer approach. However, some studies suggest the
performance can be improved with the use of improved BERT models known as
ALBERT and RoBERTa. However, the modified BERT models are not well explored for
detecting fake news in Bahasa Indonesia. In this research, we explore those
transformer models and found that ALBERT outperformed other models with 87.6%
accuracy, 86.9% precision, 86.9% F1-score, and 174.5 run-time (s/epoch)
respectively. Source code available at:
https://github.com/Shafna81/fakenewsdetection.git

中文翻译:
【中文译文】  
假新闻是以新闻媒体形式呈现但未经正规新闻机构处理的虚假信息。这类虚假内容可能通过诽谤重要实体或个人来煽动舆论，甚至为创作者谋取私利，从而引发社会问题。由于领域知识的局限性和时间压力，区分真假新闻具有挑战性。调查显示，印度尼西亚居民接触虚假信息最多的前三个地区分别是万丹省、雅加达首都特区和西爪哇省。  

Transformer模型是指人工智能（AI）领域在自然语言处理中采用的一种深度学习架构。该模型通过强大的注意力机制并行处理文本，生成丰富且具有上下文关联的词表征。先前研究表明，名为BERT的Transformer模型性能显著优于非Transformer方法。但亦有研究指出，采用改进版BERT模型（如ALBERT和RoBERTa）可进一步提升效果。然而，这些改进模型在印尼语假新闻检测领域的应用尚未得到充分探索。  

本研究对上述Transformer模型进行了实验，发现ALBERT模型以87.6%准确率、86.9%精确率、86.9% F1值及174.5秒/周期的运行时间优于其他模型。项目源代码已开源：https://github.com/Shafna81/fakenewsdetection.git  

【翻译要点说明】  
1. 术语处理：  
   - "attention mechanism"译为"注意力机制"（NLP领域标准译法）  
   - "run-time (s/epoch)"译为"运行时间（秒/周期）"（保留技术参数单位）  

2. 长句拆分：  
   - 将原文复合句"Fake news is...causing problems for society"拆分为两个中文短句，符合汉语表达习惯  
   - 技术描述部分采用"先结论后细节"的语序调整（如ALBERT性能数据集中呈现）  

3. 文化适配：  
   - "Banten, DKI Jakarta and West Java"保留原名称并补充"印度尼西亚"作为地域说明  
   - "Bahasa Indonesia"译为"印尼语"而非直译"印度尼西亚语"（采用语言学界通用简称）  

4. 被动语态转换：  
   - "is not processed properly"译为主动态"未经...处理"  
   - "are not well explored"转化为"尚未得到充分探索"  

5. 技术概念显化：  
   - "rich and contextual word representations"增译为"具有上下文关联的词表征"（明确NLP特性）  
   - 模型性能指标保留专业术语（F1值、精确率等）
