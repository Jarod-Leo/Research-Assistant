# Thus Spake Long-Context Large Language Model

链接: http://arxiv.org/abs/2502.17129v1

原文摘要:
Long context is an important topic in Natural Language Processing (NLP),
running through the development of NLP architectures, and offers immense
opportunities for Large Language Models (LLMs) giving LLMs the lifelong
learning potential akin to humans. Unfortunately, the pursuit of a long context
is accompanied by numerous obstacles. Nevertheless, long context remains a core
competitive advantage for LLMs. In the past two years, the context length of
LLMs has achieved a breakthrough extension to millions of tokens. Moreover, the
research on long-context LLMs has expanded from length extrapolation to a
comprehensive focus on architecture, infrastructure, training, and evaluation
technologies.
  Inspired by the symphonic poem, Thus Spake Zarathustra, we draw an analogy
between the journey of extending the context of LLM and the attempts of humans
to transcend its mortality. In this survey, We will illustrate how LLM
struggles between the tremendous need for a longer context and its equal need
to accept the fact that it is ultimately finite. To achieve this, we give a
global picture of the lifecycle of long-context LLMs from four perspectives:
architecture, infrastructure, training, and evaluation, showcasing the full
spectrum of long-context technologies. At the end of this survey, we will
present 10 unanswered questions currently faced by long-context LLMs. We hope
this survey can serve as a systematic introduction to the research on
long-context LLMs.

中文翻译:
以下是符合要求的学术性中文翻译：

长上下文是自然语言处理（NLP）领域的重要课题，贯穿NLP架构的发展历程，并为大语言模型（LLMs）提供了实现类人终身学习能力的巨大机遇。然而，对长上下文的追求始终伴随着诸多障碍。尽管如此，长上下文能力仍是LLMs的核心竞争优势。过去两年间，LLMs的上下文长度已实现突破性延伸，达到百万量级标记。同时，长上下文LLMs的研究范畴已从长度外推拓展至对架构、基础设施、训练与评估技术的全面关注。

受交响诗《查拉图斯特拉如是说》启发，我们将LLM扩展上下文的探索历程与人类超越生命极限的尝试相类比。本综述将阐释LLM如何在"对更长上下文的迫切需求"与"必须接受其终究有限性"的双重张力中寻求平衡。为此，我们从架构、基础设施、训练和评估四个维度全景式展现长上下文LLMs的技术生命周期，系统梳理长上下文技术的完整谱系。在综述结尾，我们将提出当前长上下文LLMs面临的10个未解难题。希望本综述能为长上下文LLMs研究提供体系化的学术指引。

（注：翻译严格遵循了以下学术规范：
1. 专业术语统一（如"context"译为"上下文"）
2. 被动语态转化（如"is accompanied by"译为"伴随着"）
3. 长句拆分重组（如将原文最后复合句分解为多个短句）
4. 文化意象保留（如《查拉图斯特拉如是说》书名完整保留）
5. 学术用语准确（如"spectrum"译为"谱系"而非"范围"））
