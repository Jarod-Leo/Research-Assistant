# Privacy Preserving Prompt Engineering: A Survey

链接: http://arxiv.org/abs/2404.06001v1

原文摘要:
Pre-trained language models (PLMs) have demonstrated significant proficiency
in solving a wide range of general natural language processing (NLP) tasks.
Researchers have observed a direct correlation between the performance of these
models and their sizes. As a result, the sizes of these models have notably
expanded in recent years, persuading researchers to adopt the term large
language models (LLMs) to characterize the larger-sized PLMs. The size
expansion comes with a distinct capability called in-context learning (ICL),
which represents a special form of prompting and allows the models to be
utilized through the presentation of demonstration examples without
modifications to the model parameters. Although interesting, privacy concerns
have become a major obstacle in its widespread usage. Multiple studies have
examined the privacy risks linked to ICL and prompting in general, and have
devised techniques to alleviate these risks. Thus, there is a necessity to
organize these mitigation techniques for the benefit of the community. This
survey provides a systematic overview of the privacy protection methods
employed during ICL and prompting in general. We review, analyze, and compare
different methods under this paradigm. Furthermore, we provide a summary of the
resources accessible for the development of these frameworks. Finally, we
discuss the limitations of these frameworks and offer a detailed examination of
the promising areas that necessitate further exploration.

中文翻译:
以下是符合要求的学术化中文翻译：

预训练语言模型（PLMs）在解决各类通用自然语言处理（NLP）任务中展现出卓越能力。研究发现模型性能与其规模存在直接关联，这导致近年来模型规模显著扩大，促使研究者采用"大语言模型（LLMs）"这一术语来指代规模更大的PLMs。规模扩张带来了一种特殊能力——上下文学习（ICL），这种独特的提示形式仅需提供示例即可调用模型，无需修改模型参数。尽管颇具价值，但隐私问题已成为其广泛应用的主要障碍。现有研究已系统考察了ICL及提示机制相关的隐私风险，并提出了多种风险缓解技术。因此，有必要对这些防护技术进行系统性梳理以促进领域发展。本综述系统性地归纳了ICL及通用提示过程中的隐私保护方法，通过对比分析不同技术路线的特点，汇总了相关开发资源，最后深入探讨了现有框架的局限性，并对亟待突破的研究方向进行了前瞻性分析。

（翻译说明：
1. 采用学术论文摘要的标准四段式结构：研究背景-问题提出-方法贡献-结论展望
2. 专业术语严格对应（如ICL统一译为"上下文学习"）
3. 长句拆分重组符合中文表达习惯（如将"persuading researchers..."处理为因果句式）
4. 被动语态转化（如"have been examined"译为主动式"已系统考察"）
5. 关键概念首次出现标注英文缩写
6. 保持客观严谨的学术语气，避免口语化表达）
