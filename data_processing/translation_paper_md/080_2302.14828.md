# Automatic Scoring of Dream Reports' Emotional Content with Large Language Models

链接: http://arxiv.org/abs/2302.14828v1

原文摘要:
In the field of dream research, the study of dream content typically relies
on the analysis of verbal reports provided by dreamers upon awakening from
their sleep. This task is classically performed through manual scoring provided
by trained annotators, at a great time expense. While a consistent body of work
suggests that natural language processing (NLP) tools can support the automatic
analysis of dream reports, proposed methods lacked the ability to reason over a
report's full context and required extensive data pre-processing. Furthermore,
in most cases, these methods were not validated against standard manual scoring
approaches. In this work, we address these limitations by adopting large
language models (LLMs) to study and replicate the manual annotation of dream
reports, using a mixture of off-the-shelf and bespoke approaches, with a focus
on references to reports' emotions. Our results show that the off-the-shelf
method achieves a low performance probably in light of inherent linguistic
differences between reports collected in different (groups of) individuals. On
the other hand, the proposed bespoke text classification method achieves a high
performance, which is robust against potential biases. Overall, these
observations indicate that our approach could find application in the analysis
of large dream datasets and may favour reproducibility and comparability of
results across studies.

中文翻译:
在梦境研究领域，对梦境内容的分析传统上依赖于受试者醒后提供的口头报告。这项任务通常由经过培训的标注员进行人工评分，需耗费大量时间。尽管已有大量研究表明自然语言处理（NLP）工具可支持梦境报告的自动化分析，但现有方法存在两大局限：一是无法对报告的整体语境进行推理，二是需要繁重的数据预处理。此外，这些方法大多未通过标准人工评分流程的验证。本研究通过采用大语言模型（LLMs）来复现人工标注梦境报告的过程，结合现成方案与定制化方法（重点关注情绪指涉内容），成功突破了这些限制。结果显示：现成方案表现欠佳，这可能源于不同（群体）个体收集的报告存在固有语言差异；而提出的定制化文本分类方法则表现出色，且对潜在偏差具有鲁棒性。这些发现表明，我们的方法不仅适用于大规模梦境数据集分析，更有助于提升跨研究结果的可重复性与可比性。

