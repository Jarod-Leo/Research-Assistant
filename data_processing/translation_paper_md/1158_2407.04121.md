# Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models

链接: http://arxiv.org/abs/2407.04121v1

原文摘要:
Large Language Models (LLMs) have gained widespread adoption in various
natural language processing tasks, including question answering and dialogue
systems. However, a major drawback of LLMs is the issue of hallucination, where
they generate unfaithful or inconsistent content that deviates from the input
source, leading to severe consequences. In this paper, we propose a robust
discriminator named RelD to effectively detect hallucination in LLMs' generated
answers. RelD is trained on the constructed RelQA, a bilingual
question-answering dialogue dataset along with answers generated by LLMs and a
comprehensive set of metrics. Our experimental results demonstrate that the
proposed RelD successfully detects hallucination in the answers generated by
diverse LLMs. Moreover, it performs well in distinguishing hallucination in
LLMs' generated answers from both in-distribution and out-of-distribution
datasets. Additionally, we also conduct a thorough analysis of the types of
hallucinations that occur and present valuable insights. This research
significantly contributes to the detection of reliable answers generated by
LLMs and holds noteworthy implications for mitigating hallucination in the
future work.

中文翻译:
大型语言模型（LLMs）已在问答系统和对话系统等多种自然语言处理任务中得到广泛应用。然而，这类模型存在一个显著缺陷——幻觉问题，即生成与输入源不符的虚假或矛盾内容，可能引发严重后果。本文提出一种名为RelD的鲁棒判别器，可有效检测LLM生成答案中的幻觉现象。该判别器基于我们构建的RelQA数据集进行训练，该双语问答对话数据集包含LLM生成的答案及一套综合性评估指标。实验结果表明，RelD能成功检测多种LLM生成答案中的幻觉内容，且在分布内和分布外数据集上均表现出优异的幻觉识别能力。此外，我们还对幻觉类型进行了系统分析并提出了重要见解。本研究对提升LLM生成答案的可靠性检测具有重要贡献，并为未来工作中缓解幻觉问题提供了有价值的参考。  

（注：根据学术摘要的翻译规范，在保持专业性的同时进行了以下优化：  
1. 将"hallucination"统一译为"幻觉"这一学界通用术语  
2. "in-distribution/out-of-distribution"采用"分布内/分布外"的标准译法  
3. 通过拆分英文长句（如第二句）、转换被动语态（如"are trained"译为"基于...进行训练"）提升中文可读性  
4. 关键术语如"RelD"、"RelQA"保留原名确保准确性  
5. 结尾句使用"具有重要贡献"、"提供有价值的参考"等符合中文论文表述习惯的措辞）
