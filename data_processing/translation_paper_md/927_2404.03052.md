# GPT-DETOX: An In-Context Learning-Based Paraphraser for Text Detoxification

链接: http://arxiv.org/abs/2404.03052v1

原文摘要:
Harmful and offensive communication or content is detrimental to social
bonding and the mental state of users on social media platforms. Text
detoxification is a crucial task in natural language processing (NLP), where
the goal is removing profanity and toxicity from text while preserving its
content. Supervised and unsupervised learning are common approaches for
designing text detoxification solutions. However, these methods necessitate
fine-tuning, leading to computational overhead. In this paper, we propose
GPT-DETOX as a framework for prompt-based in-context learning for text
detoxification using GPT-3.5 Turbo. We utilize zero-shot and few-shot prompting
techniques for detoxifying input sentences. To generate few-shot prompts, we
propose two methods: word-matching example selection (WMES) and
context-matching example selection (CMES). We additionally take into account
ensemble in-context learning (EICL) where the ensemble is shaped by base
prompts from zero-shot and all few-shot settings. We use ParaDetox and APPDIA
as benchmark detoxification datasets. Our experimental results show that the
zero-shot solution achieves promising performance, while our best few-shot
setting outperforms the state-of-the-art models on ParaDetox and shows
comparable results on APPDIA. Our EICL solutions obtain the greatest
performance, adding at least 10% improvement, against both datasets.

中文翻译:
【中文译文】  
社交媒体平台上的有害及冒犯性内容会损害用户间的社交联结与心理健康。文本净化是自然语言处理（NLP）中的关键任务，其目标是在保留文本原意的前提下消除污言秽语与毒性内容。现有解决方案通常采用监督学习或无监督学习方法，但这些方法需要微调模型，导致计算开销较大。本文提出GPT-DETOX框架，基于GPT-3.5 Turbo通过提示上下文学习实现文本净化。我们采用零样本提示与小样本提示技术对输入语句进行净化处理，并提出两种小样本提示生成方法：词语匹配示例选择法（WMES）与上下文匹配示例选择法（CMES）。此外，我们引入集成式上下文学习（EICL），通过整合零样本与所有小样本设置的基础提示构建集成模型。实验采用ParaDetox和APPDIA作为基准净化数据集，结果表明：零样本方案已具备良好性能，而最优小样本设置在ParaDetox上超越现有最优模型，在APPDIA上亦达到可比效果。集成式上下文学习方案表现最佳，在两个数据集上均实现至少10%的性能提升。  

【翻译要点说明】  
1. 术语处理：  
   - "text detoxification"译为"文本净化"（学界通用译法）  
   - "prompt-based in-context learning"译为"提示上下文学习"（保留prompt技术特性）  
   - "zero-shot/few-shot"统一译为"零样本/小样本"（符合NLP领域规范）  

2. 长句拆分：  
   原文第二段复合句拆分为三个中文短句，通过"其目标"、"现有"等衔接词保持逻辑连贯  

3. 被动语态转化：  
   "are common approaches"译为"通常采用"（符合中文主动表达习惯）  

4. 技术概念显化：  
   "ensemble in-context learning"增译为"集成式上下文学习"，通过"整合...构建集成模型"具体说明实现方式  

5. 数据呈现优化：  
   实验结果部分采用"而"、"亦"等转折词替代原文连接词，更符合中文科技论文表述风格
