# Landmark Attention: Random-Access Infinite Context Length for Transformers

链接: http://arxiv.org/abs/2305.16300v1

原文摘要:
While Transformers have shown remarkable success in natural language
processing, their attention mechanism's large memory requirements have limited
their ability to handle longer contexts. Prior approaches, such as recurrent
memory or retrieval-based augmentation, have either compromised the
random-access flexibility of attention (i.e., the capability to select any
token in the entire context) or relied on separate mechanisms for relevant
context retrieval, which may not be compatible with the model's attention. In
this paper, we present a novel approach that allows access to the complete
context while retaining random-access flexibility, closely resembling running
attention on the entire context. Our method uses a landmark token to represent
each block of the input and trains the attention to use it for selecting
relevant blocks, enabling retrieval of blocks directly through the attention
mechanism instead of by relying on a separate mechanism. Our approach
seamlessly integrates with specialized data structures and the system's memory
hierarchy, enabling processing of arbitrarily long context lengths. We
demonstrate that our method can obtain comparable performance with
Transformer-XL while significantly reducing the number of retrieved tokens in
each step. Finally, we show that fine-tuning LLaMA 7B with our method
successfully extends its context length capacity to over 32k tokens, allowing
for inference at the context lengths of GPT-4. We release the implementation of
landmark attention and the code to reproduce our experiments at
https://github.com/epfml/landmark-attention/.

中文翻译:
虽然Transformer模型在自然语言处理领域展现出卓越性能，但其注意力机制的巨大内存需求限制了处理长上下文的能力。现有解决方案（如循环记忆或基于检索的增强）要么牺牲了注意力的随机访问灵活性（即在整个上下文中选择任意标记的能力），要么依赖与模型注意力机制不兼容的独立检索机制。本文提出一种创新方法，既能访问完整上下文，又保持随机访问灵活性，其效果近似于在整个上下文上运行注意力机制。我们采用地标标记（landmark token）表示每个输入块，通过训练注意力机制利用这些标记选择相关块，从而直接通过注意力机制（而非独立机制）实现块检索。该方法能无缝集成专用数据结构和系统内存层次结构，支持任意长度上下文的处理。实验表明，本方法在保持与Transformer-XL相当性能的同时，显著减少了每步检索的标记数量。最终，我们通过对LLaMA 7B模型进行微调，成功将其上下文长度扩展至超过32k标记，达到与GPT-4相当的推理上下文长度。地标注意力实现代码及实验复现资源已发布于https://github.com/epfml/landmark-attention/。

（翻译说明：
1. 专业术语处理："landmark token"译为"地标标记"符合计算机领域命名惯例，首次出现标注英文原词
2. 长句拆分：将原文复合长句分解为符合中文表达习惯的短句，如方法原理部分
3. 被动语态转换："are trained"等被动结构转换为中文主动句式
4. 概念显化："random-access flexibility"译为"随机访问灵活性"并补充括号说明
5. 技术表述准确性：严格保持"attention mechanism/注意力机制"等专业表述的一致性
6. 逻辑连接词优化：使用"从而/通过/支持"等词替代英文连接词，使技术逻辑更清晰
7. 数据规范：保留"32k/7B"等技术参数格式，符合中文技术文献惯例）
