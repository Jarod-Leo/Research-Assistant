# Knowledge Distillation in Vision Transformers: A Critical Review

链接: http://arxiv.org/abs/2302.02108v1

原文摘要:
In Natural Language Processing (NLP), Transformers have already
revolutionized the field by utilizing an attention-based encoder-decoder model.
Recently, some pioneering works have employed Transformer-like architectures in
Computer Vision (CV) and they have reported outstanding performance of these
architectures in tasks such as image classification, object detection, and
semantic segmentation. Vision Transformers (ViTs) have demonstrated impressive
performance improvements over Convolutional Neural Networks (CNNs) due to their
competitive modelling capabilities. However, these architectures demand massive
computational resources which makes these models difficult to be deployed in
the resource-constrained applications. Many solutions have been developed to
combat this issue, such as compressive transformers and compression functions
such as dilated convolution, min-max pooling, 1D convolution, etc. Model
compression has recently attracted considerable research attention as a
potential remedy. A number of model compression methods have been proposed in
the literature such as weight quantization, weight multiplexing, pruning and
Knowledge Distillation (KD). However, techniques like weight quantization,
pruning and weight multiplexing typically involve complex pipelines for
performing the compression. KD has been found to be a simple and much effective
model compression technique that allows a relatively simple model to perform
tasks almost as accurately as a complex model. This paper discusses various
approaches based upon KD for effective compression of ViT models. The paper
elucidates the role played by KD in reducing the computational and memory
requirements of these models. The paper also presents the various challenges
faced by ViTs that are yet to be resolved.

中文翻译:
在自然语言处理（NLP）领域，基于注意力机制的编码器-解码器架构Transformer已引发革命性变革。近期，一些开创性研究将类Transformer架构引入计算机视觉（CV）领域，这些架构在图像分类、目标检测和语义分割等任务中展现出卓越性能。视觉Transformer（ViT）凭借其强大的建模能力，相较传统卷积神经网络（CNN实现了显著性能提升。然而，这类架构需要消耗大量计算资源，导致其在资源受限场景中的部署面临挑战。为应对该问题，研究者已开发出多种解决方案，包括压缩型Transformer架构，以及扩张卷积、最小-最大池化、一维卷积等压缩函数。模型压缩技术作为潜在解决途径，近期受到广泛研究关注。现有文献提出了权重量化、权重复用、剪枝和知识蒸馏（KD）等多种压缩方法。其中权重量化、剪枝和权重复用等方法通常涉及复杂的处理流程，而知识蒸馏因其简洁高效的特性脱颖而出——该技术能使轻量模型以接近复杂模型的精度完成任务。本文系统论述了基于知识蒸馏的ViT模型高效压缩方法，阐释了知识蒸馏在降低模型计算与内存需求方面的核心作用，同时指出了视觉Transformer尚未解决的技术挑战。
