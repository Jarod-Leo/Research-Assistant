# Transformers are Expressive, But Are They Expressive Enough for Regression?

链接: http://arxiv.org/abs/2402.15478v1

原文摘要:
Transformers have become pivotal in Natural Language Processing,
demonstrating remarkable success in applications like Machine Translation and
Summarization. Given their widespread adoption, several works have attempted to
analyze the expressivity of Transformers. Expressivity of a neural network is
the class of functions it can approximate. A neural network is fully expressive
if it can act as a universal function approximator. We attempt to analyze the
same for Transformers. Contrary to existing claims, our findings reveal that
Transformers struggle to reliably approximate smooth functions, relying on
piecewise constant approximations with sizable intervals. The central question
emerges as: ''Are Transformers truly Universal Function Approximators?'' To
address this, we conduct a thorough investigation, providing theoretical
insights and supporting evidence through experiments. Theoretically, we prove
that Transformer Encoders cannot approximate smooth functions. Experimentally,
we complement our theory and show that the full Transformer architecture cannot
approximate smooth functions. By shedding light on these challenges, we
advocate a refined understanding of Transformers' capabilities. Code Link:
https://github.com/swaroop-nath/transformer-expressivity.

中文翻译:
以下是符合要求的学术中文翻译：

Transformer模型已成为自然语言处理领域的核心架构，在机器翻译与文本摘要等任务中展现出卓越性能。鉴于其广泛应用，已有诸多研究尝试分析Transformer的表达能力——即神经网络能够逼近的函数类别。若某神经网络可作为通用函数逼近器，则称其具备完全表达能力。本文针对Transformer模型开展同类分析。与现有结论相反，我们的研究发现：Transformer难以可靠地逼近光滑函数，其逼近方式依赖于具有较大间隔的分段常数近似。这引出了核心问题："Transformer是否真的具备通用函数逼近能力？"为此，我们通过理论论证与实验验证展开系统研究。理论上，我们证明Transformer编码器无法逼近光滑函数；实验方面，我们进一步验证完整Transformer架构同样存在此局限。通过揭示这些挑战，本文主张学界需要更精确地理解Transformer的能力边界。代码地址：https://github.com/swaroop-nath/transformer-expressivity

（说明：译文严格遵循学术规范，采用术语统一原则，如"expressivity"译为"表达能力"、"universal function approximator"译为"通用函数逼近器"等。通过拆分英文长句为中文短句结构（如将"Contrary to..."处理为独立分句），并保留原文严谨的学术风格。技术概念如"smooth functions"译为"光滑函数"、"piecewise constant approximations"译为"分段常数近似"符合数学领域惯例。最后补充的代码链接采用国际期刊通用格式。）
