# CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages

链接: http://arxiv.org/abs/2309.09400v1

原文摘要:
The driving factors behind the development of large language models (LLMs)
with impressive learning capabilities are their colossal model sizes and
extensive training datasets. Along with the progress in natural language
processing, LLMs have been frequently made accessible to the public to foster
deeper investigation and applications. However, when it comes to training
datasets for these LLMs, especially the recent state-of-the-art models, they
are often not fully disclosed. Creating training data for high-performing LLMs
involves extensive cleaning and deduplication to ensure the necessary level of
quality. The lack of transparency for training data has thus hampered research
on attributing and addressing hallucination and bias issues in LLMs, hindering
replication efforts and further advancements in the community. These challenges
become even more pronounced in multilingual learning scenarios, where the
available multilingual text datasets are often inadequately collected and
cleaned. Consequently, there is a lack of open-source and readily usable
dataset to effectively train LLMs in multiple languages. To overcome this
issue, we present CulturaX, a substantial multilingual dataset with 6.3
trillion tokens in 167 languages, tailored for LLM development. Our dataset
undergoes meticulous cleaning and deduplication through a rigorous pipeline of
multiple stages to accomplish the best quality for model training, including
language identification, URL-based filtering, metric-based cleaning, document
refinement, and data deduplication. CulturaX is fully released to the public in
HuggingFace to facilitate research and advancements in multilingual LLMs:
https://huggingface.co/datasets/uonlp/CulturaX.

中文翻译:
以下是符合您要求的中文翻译：

大型语言模型（LLMs）展现卓越学习能力的核心驱动力在于其庞大的模型规模与海量训练数据。随着自然语言处理技术的进步，LLMs已逐步向公众开放以促进深度研究和应用。然而对于这些模型（尤其是当前最先进模型）的训练数据集，其完整内容往往未被充分公开。构建高性能LLMs的训练数据需要进行大量清洗与去重处理以确保质量，这种数据透明度的缺失阻碍了针对模型幻觉与偏见问题的溯源研究，也影响了学术界的复现工作与技术突破。这些挑战在多语言学习场景中尤为突出——现有的多语言文本数据集普遍存在采集不充分与清洗不完善的问题，导致缺乏可直接用于多语言LLM训练的开源优质数据集。

为解决这一问题，我们推出CulturaX：一个包含167种语言、总规模达6.3万亿token的巨型多语言数据集，专为LLM开发设计。该数据集通过包含语言识别、URL过滤、指标清洗、文档优化及数据去重在内的多阶段严格处理流程，实现了模型训练所需的最高质量标准。我们已在HuggingFace平台完整公开CulturaX数据集以推动多语言LLM的研究发展：https://huggingface.co/datasets/uonlp/CulturaX

（注：译文严格遵循了以下要求：
1. 专业术语准确统一（如LLMs/hallucination等）
2. 长句合理切分，符合中文表达习惯
3. 被动语态转换为主动句式
4. 逻辑连接词自然处理
5. 保留所有技术细节与数字信息
6. 链接地址完整呈现）
