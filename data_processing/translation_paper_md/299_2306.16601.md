# An Efficient Sparse Inference Software Accelerator for Transformer-based Language Models on CPUs

链接: http://arxiv.org/abs/2306.16601v1

原文摘要:
In recent years, Transformer-based language models have become the standard
approach for natural language processing tasks. However, stringent throughput
and latency requirements in industrial applications are limiting their
adoption. To mitigate the gap, model compression techniques such as structured
pruning are being used to improve inference efficiency. However, most existing
neural network inference runtimes lack adequate support for structured
sparsity. In this paper, we propose an efficient sparse deep learning inference
software stack for Transformer-based language models where the weights are
pruned with constant block size. Our sparse software accelerator leverages
Intel Deep Learning Boost to maximize the performance of sparse matrix - dense
matrix multiplication (commonly abbreviated as SpMM) on CPUs. Our SpMM kernel
outperforms the existing sparse libraries (oneMKL, TVM, and LIBXSMM) by an
order of magnitude on a wide range of GEMM shapes under 5 representative
sparsity ratios (70%, 75%, 80%, 85%, 90%). Moreover, our SpMM kernel shows up
to 5x speedup over dense GEMM kernel of oneDNN, a well-optimized dense library
widely used in industry. We apply our sparse accelerator on widely-used
Transformer-based language models including Bert-Mini, DistilBERT, Bert-Base,
and BERT-Large. Our sparse inference software shows up to 1.5x speedup over
Neural Magic's Deepsparse under same configurations on Xeon on Amazon Web
Services under proxy production latency constraints. We also compare our
solution with two framework-based inference solutions, ONNX Runtime and
PyTorch, and demonstrate up to 37x speedup over ONNX Runtime and 345x over
PyTorch on Xeon under the latency constraints. All the source code is publicly
available on Github: https://github.com/intel/intel-extension-for-transformers.

中文翻译:
近年来，基于Transformer的语言模型已成为自然语言处理任务的主流方法。然而工业应用中严格的吞吐量和延迟要求限制了其大规模部署。为缓解这一矛盾，业界开始采用结构化剪枝等模型压缩技术来提升推理效率。但现有神经网络推理运行时大多对结构化稀疏缺乏充分支持。本文提出了一种高效的稀疏深度学习推理软件栈，专为采用恒定块大小权重剪枝的Transformer语言模型设计。我们的稀疏软件加速器通过英特尔深度学习加速技术，在CPU上实现了稀疏矩阵-稠密矩阵乘法（SpMM）的极致性能优化。实验表明，在5种典型稀疏率（70%、75%、80%、85%、90%）下，针对多种GEMM矩阵形状，我们的SpMM内核性能较现有稀疏库（oneMKL、TVM和LIBXSMM）提升达一个数量级；相较于工业级优化稠密库oneDNN的GEMM内核，更可实现最高5倍的加速比。我们将该稀疏加速器应用于Bert-Mini、DistilBERT、Bert-Base和BERT-Large等主流Transformer模型，在亚马逊云服务Xeon平台模拟生产环境延迟约束下，推理速度较Neural Magic的Deepsparse提升达1.5倍。与ONNX Runtime和PyTorch两种框架方案对比，在相同延迟约束下分别实现37倍和345倍的显著加速。全部源代码已在GitHub开源：https://github.com/intel/intel-extension-for-transformers。

（注：译文严格遵循技术文献的学术规范，采用专业术语统一原则，如"structured pruning"译为"结构化剪枝"、"sparsity ratios"译为"稀疏率"等。通过拆分英语长句为中文短句结构，如将三个对比实验的复合句转换为排比句式，既保持信息密度又符合中文表达习惯。关键性能数据保留原文数字表述方式确保准确性，GitHub链接按技术文档惯例保留原始格式。）
