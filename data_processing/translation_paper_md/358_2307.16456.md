# Camoscio: an Italian Instruction-tuned LLaMA

链接: http://arxiv.org/abs/2307.16456v1

原文摘要:
In recent years Large Language Models (LLMs) have increased the state of the
art on several natural language processing tasks. However, their accessibility
is often limited to paid API services, posing challenges for researchers in
conducting extensive investigations. On the other hand, while some open-source
models have been proposed by the community, they are typically English-centric
or multilingual without a specific adaptation for the Italian language. In an
effort to democratize the available and open resources for the Italian
language, in this paper we introduce Camoscio: a language model specifically
tuned to follow users' prompts in Italian. Specifically, we finetuned the
smallest variant of LLaMA (7b) with LoRA on a corpus of instruction prompts
translated to Italian via ChatGPT. Results indicate that the model's zero-shot
performance on various downstream tasks in Italian competes favorably with
existing models specifically finetuned for those tasks. All the artifacts
(code, dataset, model) are released to the community at the following url:
https://github.com/teelinsan/camoscio

中文翻译:
近年来，大型语言模型（LLMs）在多项自然语言处理任务中实现了技术突破。然而，这类模型通常仅通过付费API服务提供访问权限，这为研究人员开展深入探索设置了障碍。另一方面，尽管学术界已提出若干开源模型，但这些模型大多以英语为核心，或是未针对意大利语进行专门优化的多语言模型。为促进意大利语资源的开放共享，本文推出Camoscio——一个专门针对意大利语用户指令进行优化的语言模型。具体而言，我们采用LoRA技术对LLaMA最小版本（70亿参数）进行微调，训练数据为通过ChatGPT翻译成意大利语的指令提示语料库。实验结果表明，该模型在意大利语各类下游任务中的零样本性能表现优异，甚至可与专门针对这些任务微调的现有模型相媲美。所有相关资源（代码、数据集、模型）已通过以下链接向社区开源：https://github.com/teelinsan/camoscio

（译文特点说明：
1. 专业术语准确处理："zero-shot performance"译为"零样本性能"，"finetuned"译为"微调"
2. 句式重构：将英语长句拆分为符合中文表达习惯的短句，如原文第一句拆分后逻辑更清晰
3. 被动语态转化："are typically English-centric"处理为主动式"大多以英语为核心"
4. 文化适配："democratize"译为"促进开放共享"更符合中文学术语境
5. 技术概念保留：LLaMA、LoRA等专有名词保持原貌
6. 衔接自然：使用破折号、括号等标点保持学术文本的严谨性）
