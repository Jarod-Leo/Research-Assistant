# SALAD: Improving Robustness and Generalization through Contrastive Learning with Structure-Aware and LLM-Driven Augmented Data

链接: http://arxiv.org/abs/2504.12185v1

原文摘要:
In various natural language processing (NLP) tasks, fine-tuning Pre-trained
Language Models (PLMs) often leads to the issue of spurious correlations, which
negatively impacts performance, particularly when dealing with
out-of-distribution data. To address this problem, we propose SALAD}(Structure
Aware and LLM-driven Augmented Data), a novel approach designed to enhance
model robustness and generalization by generating structure-aware and
counterfactually augmented data for contrastive learning. Our method leverages
a tagging-based approach to generate structure-aware positive samples and
utilizes large language models (LLMs) to generate counterfactual negative
samples with diverse sentence patterns. By applying contrastive learning, SALAD
enables the model to focus on learning the structural relationships between key
sentence components while minimizing reliance on spurious correlations. We
validate our approach through experiments on three tasks: Sentiment
Classification, Sexism Detection, and Natural Language Inference. The results
demonstrate that SALAD not only improves model robustness and performance
across different environments but also enhances generalization to
out-of-distribution datasets and cross-domain scenarios.

中文翻译:
在各类自然语言处理（NLP）任务中，对预训练语言模型（PLMs）进行微调时常会引发伪相关性问题，这种问题会损害模型性能，尤其在处理分布外数据时表现尤为突出。为解决这一难题，我们提出SALAD（结构感知与大语言模型驱动的增强数据）——一种通过生成结构感知样本和反事实增强数据以支持对比学习的新方法。该方法采用基于标注的策略生成结构感知正样本，并利用大语言模型（LLMs）生成具有多样化句式结构的反事实负样本。通过对比学习，SALAD使模型能够聚焦于学习关键句子成分间的结构关系，同时降低对伪相关特征的依赖。我们在情感分类、性别歧视检测和自然语言推理三项任务上验证了该方法的有效性。实验结果表明，SALAD不仅能提升模型在不同环境下的鲁棒性和性能，还能显著增强模型对分布外数据集及跨域场景的泛化能力。

（注：翻译过程中对技术术语进行了如下处理：
1. "spurious correlations"译为"伪相关性"而非"虚假关联"以符合机器学习领域术语习惯
2. "out-of-distribution"统一译为"分布外"保持术语一致性
3. "Structure Aware"译为"结构感知"以准确传达原意
4. 长难句进行合理切分，如将原文最后复合句拆分为两个中文短句
5. 被动语态转换为主动表述，如"is validated"译为"验证了"
6. 专业缩写首次出现时保留英文并添加中文注释）
