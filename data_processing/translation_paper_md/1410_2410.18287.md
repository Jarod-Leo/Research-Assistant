# LEGO: Language Model Building Blocks

链接: http://arxiv.org/abs/2410.18287v1

原文摘要:
Large language models (LLMs) are essential in natural language processing
(NLP) but are costly in data collection, pre-training, fine-tuning, and
inference. Task-specific small language models (SLMs) offer a cheaper
alternative but lack robustness and generalization. This paper proposes LEGO, a
novel technique to extract SLMs from an LLM and recombine them. Using
state-of-the-art LLM pruning strategies, we can create task- and user-specific
SLM building blocks that are efficient for fine-tuning and inference while also
preserving user data privacy. LEGO utilizes Federated Learning and a novel
aggregation scheme for the LLM reconstruction, maintaining robustness without
high costs and preserving user data privacy. We experimentally demonstrate the
versatility of LEGO, showing its ability to enable model heterogeneity and
mitigate the effects of data heterogeneity while maintaining LLM robustness.

中文翻译:
以下是符合要求的学术中文翻译：

大型语言模型（LLMs）在自然语言处理（NLP）领域具有重要作用，但其数据收集、预训练、微调和推理过程成本高昂。针对特定任务的小型语言模型（SLMs）虽成本较低，但缺乏鲁棒性和泛化能力。本文提出LEGO技术，通过从LLM中提取并重组SLMs来解决这一问题。采用最先进的LLM剪枝策略，我们能够创建适用于特定任务和用户的SLM构建模块，这些模块在保持用户数据隐私的同时，能高效进行微调和推理。LEGO结合联邦学习与创新的聚合方案进行LLM重构，既维持了模型鲁棒性，又避免了高昂成本并保护了用户数据隐私。实验证明，LEGO在保持LLM鲁棒性的同时，能够支持模型异构性并缓解数据异构性影响，展现出卓越的通用性。

注：翻译严格遵循了以下原则：
1. 专业术语准确统一（如LLMs/SLMs不翻译，pruning译为"剪枝"）
2. 被动语态转换为中文主动句式（如"are costly"译为"成本高昂"）
3. 长句拆分符合中文表达习惯（如原文最后长句拆分为三个分句）
4. 关键概念首次出现标注英文缩写（LLM/SLM）
5. 学术用语规范（"robustness"译为"鲁棒性"，"generalization"译为"泛化能力"）
6. 创新方法名称LEGO保留不译
