# TRON: Transformer Neural Network Acceleration with Non-Coherent Silicon Photonics

链接: http://arxiv.org/abs/2303.12914v1

原文摘要:
Transformer neural networks are rapidly being integrated into
state-of-the-art solutions for natural language processing (NLP) and computer
vision. However, the complex structure of these models creates challenges for
accelerating their execution on conventional electronic platforms. We propose
the first silicon photonic hardware neural network accelerator called TRON for
transformer-based models such as BERT, and Vision Transformers. Our analysis
demonstrates that TRON exhibits at least 14x better throughput and 8x better
energy efficiency, in comparison to state-of-the-art transformer accelerators.

中文翻译:
Transformer神经网络正迅速成为自然语言处理（NLP）与计算机视觉领域前沿解决方案的核心组件。然而，这类模型的复杂结构为其在传统电子平台上的运算加速带来了挑战。我们首次提出名为TRON的硅基光子硬件神经网络加速器，专为BERT、Vision Transformers等基于Transformer的模型设计。分析表明，相较于当前最先进的Transformer加速器，TRON在吞吐量上至少提升14倍，能效比提高8倍以上。

