# Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model

链接: http://arxiv.org/abs/2305.11176v1

原文摘要:
Foundation models have made significant strides in various applications,
including text-to-image generation, panoptic segmentation, and natural language
processing. This paper presents Instruct2Act, a framework that utilizes Large
Language Models to map multi-modal instructions to sequential actions for
robotic manipulation tasks. Specifically, Instruct2Act employs the LLM model to
generate Python programs that constitute a comprehensive perception, planning,
and action loop for robotic tasks. In the perception section, pre-defined APIs
are used to access multiple foundation models where the Segment Anything Model
(SAM) accurately locates candidate objects, and CLIP classifies them. In this
way, the framework leverages the expertise of foundation models and robotic
abilities to convert complex high-level instructions into precise policy codes.
Our approach is adjustable and flexible in accommodating various instruction
modalities and input types and catering to specific task demands. We validated
the practicality and efficiency of our approach by assessing it on robotic
tasks in different scenarios within tabletop manipulation domains. Furthermore,
our zero-shot method outperformed many state-of-the-art learning-based policies
in several tasks. The code for our proposed approach is available at
https://github.com/OpenGVLab/Instruct2Act, serving as a robust benchmark for
high-level robotic instruction tasks with assorted modality inputs.

中文翻译:
基础模型在文本到图像生成、全景分割和自然语言处理等多个应用领域取得了显著进展。本文提出Instruct2Act框架，该框架利用大语言模型将多模态指令映射为机器人操作任务的序列化动作。具体而言，Instruct2Act通过大语言模型生成构成机器人任务完整感知-规划-行动循环的Python程序。在感知模块中，框架通过预定义API调用多个基础模型：Segment Anything Model（SAM）精确定位候选对象，CLIP模型进行分类。这种设计充分发挥了基础模型的专长与机器人操作能力，将复杂的高级指令转化为精确的策略代码。我们的方法具有高度可调性和灵活性，既能适应不同模态的指令输入，又能满足特定任务需求。通过在桌面操作领域的不同场景中进行机器人任务评估，我们验证了该方法的实用性和高效性。实验表明，我们的零样本方法在多项任务中超越了众多基于学习的最优策略模型。项目代码已开源在https://github.com/OpenGVLab/Instruct2Act，为多模态输入的高级机器人指令任务提供了可靠的基准方案。

（翻译说明：
1. 专业术语处理："foundation models"统一译为"基础模型"，"Large Language Models"采用通用译名"大语言模型"
2. 技术概念转化："panoptic segmentation"译为计算机视觉领域标准术语"全景分割"
3. 句式重构：将英文长句拆解为符合中文表达习惯的短句，如将"generate Python programs that constitute..."处理为"生成构成...的Python程序"
4. 被动语态转换："are used to"译为主动式"通过...调用"
5. 文化适配："zero-shot method"保留技术概念原意译为"零样本方法"，而非字面直译
6. 术语一致性：全篇保持"modality"统一译为"模态"，"benchmark"译为"基准"
7. 补充说明：项目链接保留原始URL格式，符合学术规范）
