# Self-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models

链接: http://arxiv.org/abs/2406.09206v1

原文摘要:
Active learning is an iterative labeling process that is used to obtain a
small labeled subset, despite the absence of labeled data, thereby enabling to
train a model for supervised tasks such as text classification. While active
learning has made considerable progress in recent years due to improvements
provided by pre-trained language models, there is untapped potential in the
often neglected unlabeled portion of the data, although it is available in
considerably larger quantities than the usually small set of labeled data. In
this work, we investigate how self-training, a semi-supervised approach that
uses a model to obtain pseudo-labels for unlabeled data, can be used to improve
the efficiency of active learning for text classification. Building on a
comprehensive reproduction of four previous self-training approaches, some of
which are evaluated for the first time in the context of active learning or
natural language processing, we introduce HAST, a new and effective
self-training strategy, which is evaluated on four text classification
benchmarks. Our results show that it outperforms the reproduced self-training
approaches and reaches classification results comparable to previous
experiments for three out of four datasets, using as little as 25% of the data.
The code is publicly available at
https://github.com/chschroeder/self-training-for-sample-efficient-active-learning .

中文翻译:
以下是符合要求的学术摘要中文翻译：

主动学习是一种在缺乏标注数据情况下，通过迭代标注流程获取小规模标注子集的方法，可用于训练文本分类等监督任务的模型。尽管预训练语言模型的进步推动了主动学习近年来的显著发展，但数据中未标注部分（其数量通常远超小型标注集）的潜力仍未得到充分挖掘。本研究探讨了如何利用自训练（一种通过模型为未标注数据生成伪标签的半监督方法）来提升文本分类中主动学习的效率。在系统复现四种现有自训练方法（其中部分方法系首次在主动学习或自然语言处理领域进行评估）的基础上，我们提出了HAST——一种新型高效的自训练策略，并在四个文本分类基准上进行了评估。实验结果表明：1）HAST优于所有复现的自训练方法；2）仅需25%的数据量即可在四分之三的数据集上达到与既往实验相当的分类效果。相关代码已开源：https://github.com/chschroeder/self-training-for-sample-efficient-active-learning。

（翻译严格遵循以下原则：
1. 专业术语准确统一（如active learning→主动学习，self-training→自训练）
2. 被动语态转换为中文主动句式（如"is used to"→"通过...方法"）
3. 长难句合理切分（将原文复合句拆解为多个短句）
4. 逻辑关系显化（添加"尽管"、"但"等连接词）
5. 学术规范保持（保留技术术语及文献引用格式）
6. 文化适应性调整（"as little as 25%"→"仅需25%"增强表现力））
