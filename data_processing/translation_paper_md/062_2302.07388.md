# Adding Instructions during Pretraining: Effective Way of Controlling Toxicity in Language Models

链接: http://arxiv.org/abs/2302.07388v1

原文摘要:
Pretrained large language models have become indispensable for solving
various natural language processing (NLP) tasks. However, safely deploying them
in real world applications is challenging because they generate toxic content.
To address this challenge, we propose two novel pretraining data augmentation
strategies that significantly reduce model toxicity without compromising its
utility. Our two strategies are: (1) MEDA: adds raw toxicity score as meta-data
to the pretraining samples, and (2) INST: adds instructions to those samples
indicating their toxicity. Our results indicate that our best performing
strategy (INST) substantially reduces the toxicity probability up to 61% while
preserving the accuracy on five benchmark NLP tasks as well as improving AUC
scores on four bias detection tasks by 1.3%. We also demonstrate the
generalizability of our techniques by scaling the number of training samples
and the number of model parameters.

中文翻译:
预训练大语言模型已成为解决各类自然语言处理（NLP）任务的核心工具。然而，由于模型可能生成有害内容，其在实际应用中的安全部署仍面临挑战。为应对这一挑战，我们提出两种创新的预训练数据增强策略，能在保持模型实用性的同时显著降低其毒性。具体策略包括：（1）MEDA：将原始毒性分数作为元数据添加到预训练样本中；（2）INST：为样本添加标注其毒性的指令。实验结果表明，最佳策略（INST）可使毒性生成概率最高降低61%，同时在五项NLP基准任务中保持准确率，并将四项偏见检测任务的AUC分数提升1.3%。通过扩展训练样本量和模型参数量，我们进一步验证了该技术的泛化能力。
