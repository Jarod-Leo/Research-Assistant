# IndicSentEval: How Effectively do Multilingual Transformer Models encode Linguistic Properties for Indic Languages?

链接: http://arxiv.org/abs/2410.02611v1

原文摘要:
Transformer-based models have revolutionized the field of natural language
processing. To understand why they perform so well and to assess their
reliability, several studies have focused on questions such as: Which
linguistic properties are encoded by these models, and to what extent? How
robust are these models in encoding linguistic properties when faced with
perturbations in the input text? However, these studies have mainly focused on
BERT and the English language. In this paper, we investigate similar questions
regarding encoding capability and robustness for 8 linguistic properties across
13 different perturbations in 6 Indic languages, using 9 multilingual
Transformer models (7 universal and 2 Indic-specific). To conduct this study,
we introduce a novel multilingual benchmark dataset, IndicSentEval, containing
approximately $\sim$47K sentences. Surprisingly, our probing analysis of
surface, syntactic, and semantic properties reveals that while almost all
multilingual models demonstrate consistent encoding performance for English,
they show mixed results for Indic languages. As expected, Indic-specific
multilingual models capture linguistic properties in Indic languages better
than universal models. Intriguingly, universal models broadly exhibit better
robustness compared to Indic-specific models, particularly under perturbations
such as dropping both nouns and verbs, dropping only verbs, or keeping only
nouns. Overall, this study provides valuable insights into probing and
perturbation-specific strengths and weaknesses of popular multilingual
Transformer-based models for different Indic languages. We make our code and
dataset publicly available [https://tinyurl.com/IndicSentEval}].

中文翻译:
基于Transformer的模型已经彻底改变了自然语言处理领域。为理解其卓越性能并评估可靠性，多项研究聚焦于以下问题：这些模型编码了哪些语言特性？编码程度如何？当输入文本出现扰动时，这些模型编码语言特性的稳健性如何？然而现有研究主要集中于BERT模型和英语。本文通过9个多语言Transformer模型（7个通用模型和2个印度语言专用模型），针对6种印度语言在13种不同扰动下对8种语言特性的编码能力与稳健性展开研究。为此，我们构建了一个新颖的多语言基准数据集IndicSentEval，包含约47K句子。令人惊讶的是，通过对表层、句法和语义特性的探测分析发现：虽然几乎所有多语言模型对英语都表现出稳定的编码性能，但对印度语言却呈现参差不齐的结果。符合预期的是，印度语言专用模型比通用模型能更好地捕捉印度语言特性。但有趣的是，通用模型总体上展现出比专用模型更好的稳健性，尤其在同时删除名词动词、仅删除动词或仅保留名词等扰动场景下。本研究为不同印度语言环境下主流多语言Transformer模型的探测特性和扰动特异性优劣提供了宝贵洞见。代码和数据集已公开[https://tinyurl.com/IndicSentEval}]。

（注：译文严格遵循以下原则：
1. 专业术语准确统一（如Transformer/稳健性/扰动等）
2. 长句合理切分，符合中文表达习惯
3. 被动语态转化（"it is revealed that"→"发现"）
4. 数据呈现方式本地化（保留K表示千位）
5. 补充逻辑连接词（"但有趣的是"）
6. 技术表述完整（"表层、句法和语义特性"对应surface/syntactic/semantic）
7. 链接信息完整保留）
