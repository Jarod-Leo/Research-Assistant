# The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models

链接: http://arxiv.org/abs/2410.06554v1

原文摘要:
Reinforcement Learning from Human Feedback significantly enhances Natural
Language Processing by aligning language models with human expectations. A
critical factor in this alignment is the strength of reward models used during
training. This study explores whether stronger reward models invariably lead to
better language models. In this paper, through experiments on relevance,
factuality, and completeness tasks using the QA-FEEDBACK dataset and reward
models based on Longformer, we uncover a surprising paradox: language models
trained with moderately accurate reward models outperform those guided by
highly accurate ones. This challenges the widely held belief that stronger
reward models always lead to better language models, and opens up new avenues
for future research into the key factors driving model performance and how to
choose the most suitable reward models. Code and additional details are
available at https://github.com/EIT-NLP/AccuracyParadox-RLHF.

中文翻译:
基于人类反馈的强化学习通过使语言模型与人类期望对齐，显著提升了自然语言处理性能。其中，训练过程中奖励模型的强度是影响对齐效果的关键因素。本研究探讨了更强的奖励模型是否必然会产生更优的语言模型。本文通过在QA-FEEDBACK数据集上开展相关性、事实性和完整性任务的实验，并采用基于Longformer架构的奖励模型，揭示了一个令人惊奇的悖论：由中等准确度奖励模型训练的语言模型，其表现反而优于由高准确度奖励模型指导的模型。这一发现挑战了"奖励模型越强则语言模型越优"的普遍认知，为未来研究模型性能的核心驱动因素及如何选择最适配的奖励模型开辟了新方向。相关代码及补充材料详见https://github.com/EIT-NLP/AccuracyParadox-RLHF。

（翻译说明：
1. 专业术语准确处理："Reinforcement Learning from Human Feedback"译为"基于人类反馈的强化学习"，"reward models"统一译为"奖励模型"
2. 句式结构优化：将英文长句拆分为符合中文表达习惯的短句，如将"A critical factor..."处理为独立分句
3. 被动语态转换：将"are used"等被动式转为"采用"等主动表达
4. 概念显化处理："paradox"译为"悖论"并添加"令人惊奇的"进行语义强化
5. 学术规范：保留专业数据集名称"QA-FEEDBACK"和技术框架"Longformer"的原文形式
6. 衔接自然化：使用"其中""反而""这一发现"等连接词保持逻辑连贯
7. 补充说明整合：将代码链接信息融入正文，符合中文论文常见表述方式）
