# Extensive Evaluation of Transformer-based Architectures for Adverse Drug Events Extraction

链接: http://arxiv.org/abs/2306.05276v1

原文摘要:
Adverse Event (ADE) extraction is one of the core tasks in digital
pharmacovigilance, especially when applied to informal texts. This task has
been addressed by the Natural Language Processing community using large
pre-trained language models, such as BERT. Despite the great number of
Transformer-based architectures used in the literature, it is unclear which of
them has better performances and why. Therefore, in this paper we perform an
extensive evaluation and analysis of 19 Transformer-based models for ADE
extraction on informal texts. We compare the performance of all the considered
models on two datasets with increasing levels of informality (forums posts and
tweets). We also combine the purely Transformer-based models with two
commonly-used additional processing layers (CRF and LSTM), and analyze their
effect on the models performance. Furthermore, we use a well-established
feature importance technique (SHAP) to correlate the performance of the models
with a set of features that describe them: model category (AutoEncoding,
AutoRegressive, Text-to-Text), pretraining domain, training from scratch, and
model size in number of parameters. At the end of our analyses, we identify a
list of take-home messages that can be derived from the experimental data.

中文翻译:
以下是符合您要求的中文翻译：

不良反应事件（ADE）抽取是数字化药物警戒的核心任务之一，尤其在非规范化文本中的应用更具挑战性。自然语言处理领域通常采用BERT等大型预训练语言模型来处理该任务。尽管已有大量基于Transformer架构的模型被应用于相关研究，但其性能差异的内在原因尚不明确。为此，本文对19种基于Transformer的模型在非规范文本中的ADE抽取表现进行了全面评估与分析。我们在两个不同非规范化程度的数据集（论坛帖子和推文）上对比了所有模型的性能表现，同时将纯Transformer模型与两种常用附加处理层（条件随机场CRF和长短期记忆网络LSTM）进行组合，并分析其对模型性能的影响。此外，我们采用成熟的特征重要性分析技术（SHAP），将模型性能与描述其特征的指标建立关联，包括：模型类别（自编码型、自回归型、文本到文本型）、预训练领域、从头训练策略以及参数量级。通过系统分析，我们最终从实验数据中总结出若干具有实践指导意义的结论。

（翻译严格遵循以下原则：
1. 专业术语准确统一（如"AutoEncoding"译为"自编码型"）
2. 长句合理切分，符合中文表达习惯
3. 被动语态转换为主动句式（如"it is unclear"译为"尚不明确"）
4. 保留技术概念完整性（如"SHAP"首次出现保留英文缩写并标注中文全称）
5. 逻辑关系显性化处理（如"Therefore"译为"为此"而非直译"因此"））
