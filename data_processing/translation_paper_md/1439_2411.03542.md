# Exploring the Benefits of Domain-Pretraining of Generative Large Language Models for Chemistry

链接: http://arxiv.org/abs/2411.03542v1

原文摘要:
A proliferation of Large Language Models (the GPT series, BLOOM, LLaMA, and
more) are driving forward novel development of multipurpose AI for a variety of
tasks, particularly natural language processing (NLP) tasks. These models
demonstrate strong performance on a range of tasks; however, there has been
evidence of brittleness when applied to more niche or narrow domains where
hallucinations or fluent but incorrect responses reduce performance. Given the
complex nature of scientific domains, it is prudent to investigate the
trade-offs of leveraging off-the-shelf versus more targeted foundation models
for scientific domains. In this work, we examine the benefits of in-domain
pre-training for a given scientific domain, chemistry, and compare these to
open-source, off-the-shelf models with zero-shot and few-shot prompting. Our
results show that not only do in-domain base models perform reasonably well on
in-domain tasks in a zero-shot setting but that further adaptation using
instruction fine-tuning yields impressive performance on chemistry-specific
tasks such as named entity recognition and molecular formula generation.

中文翻译:
大型语言模型（如GPT系列、BLOOM、LLaMA等）的激增正推动着多功能人工智能在各类任务中的创新发展，尤其在自然语言处理（NLP）领域表现突出。这些模型虽在广泛任务中展现出强大性能，但已有证据表明，当应用于更专业或狭窄的领域时，其脆弱性会显现——幻觉现象或流利但错误的响应会导致性能下降。鉴于科学领域的复杂性，有必要权衡现成通用模型与针对性基础模型在科学领域的应用差异。本研究以化学领域为例，探究领域内预训练模型的优势，并与开源现成模型在零样本和小样本提示下的表现进行对比。实验结果表明：领域内基础模型不仅在零样本设置下对化学任务表现良好，而且经过指令微调后，在化学命名实体识别、分子式生成等特定任务中展现出卓越性能。
