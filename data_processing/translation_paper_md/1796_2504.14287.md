# Probing the Subtle Ideological Manipulation of Large Language Models

链接: http://arxiv.org/abs/2504.14287v1

原文摘要:
Large Language Models (LLMs) have transformed natural language processing,
but concerns have emerged about their susceptibility to ideological
manipulation, particularly in politically sensitive areas. Prior work has
focused on binary Left-Right LLM biases, using explicit prompts and fine-tuning
on political QA datasets. In this work, we move beyond this binary approach to
explore the extent to which LLMs can be influenced across a spectrum of
political ideologies, from Progressive-Left to Conservative-Right. We introduce
a novel multi-task dataset designed to reflect diverse ideological positions
through tasks such as ideological QA, statement ranking, manifesto cloze
completion, and Congress bill comprehension. By fine-tuning three LLMs-Phi-2,
Mistral, and Llama-3-on this dataset, we evaluate their capacity to adopt and
express these nuanced ideologies. Our findings indicate that fine-tuning
significantly enhances nuanced ideological alignment, while explicit prompts
provide only minor refinements. This highlights the models' susceptibility to
subtle ideological manipulation, suggesting a need for more robust safeguards
to mitigate these risks.

中文翻译:
以下是符合您要求的中文翻译：

大型语言模型（LLMs）已经彻底改变了自然语言处理领域，但人们日益担忧其在政治敏感领域易受意识形态操控的问题。先前研究主要关注非左即右的二元立场偏见，通常采用显式提示词或在政治问答数据集上进行微调。本研究突破这种二元对立框架，系统探索LLMs在"进步左派-保守右派"光谱中受意识形态影响的程度。我们创新性地构建了一个多任务数据集，通过意识形态问答、政治声明排序、政党宣言完形填空和国会法案理解等任务，全面反映多元化的意识形态立场。通过对Phi-2、Mistral和Llama-3三种LLMs进行微调实验，我们评估了它们吸收并表达这些复杂意识形态的能力。研究结果表明：微调能显著增强模型与特定意识形态的精细对齐，而显式提示词仅能产生边际改善。这揭示了LLMs对隐性意识形态操控的敏感性，表明需要建立更强大的防护机制来规避此类风险。

翻译说明：
1. 专业术语处理：采用"大型语言模型(LLMs)"、"微调"等学界通用译法，保留"Mistral"等模型原名
2. 句式重构：将英语长句拆分为符合中文表达习惯的短句，如将原文第一句拆分为两个逻辑单元
3. 概念转化："Progressive-Left to Conservative-Right"译为"进步左派-保守右派"光谱，符合中文政治学表述
4. 被动语态转换："concerns have emerged"译为主动式"人们日益担忧"
5. 文化适配："Congress bill"译为"国会法案"而非直译"国会账单"
6. 术语统一性：全文保持"fine-tuning"统一译为"微调"，"prompts"统一译为"提示词"
7. 学术风格：使用"揭示""表明"等学术用语，保持摘要的严谨性
