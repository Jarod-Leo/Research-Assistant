# Revisiting k-NN for Pre-trained Language Models

链接: http://arxiv.org/abs/2304.09058v1

原文摘要:
Pre-trained Language Models (PLMs), as parametric-based eager learners, have
become the de-facto choice for current paradigms of Natural Language Processing
(NLP). In contrast, k-Nearest-Neighbor (kNN) classifiers, as the lazy learning
paradigm, tend to mitigate over-fitting and isolated noise. In this paper, we
revisit kNN classifiers for augmenting the PLMs-based classifiers. From the
methodological level, we propose to adopt kNN with textual representations of
PLMs in two steps: (1) Utilize kNN as prior knowledge to calibrate the training
process. (2) Linearly interpolate the probability distribution predicted by kNN
with that of the PLMs' classifier. At the heart of our approach is the
implementation of kNN-calibrated training, which treats predicted results as
indicators for easy versus hard examples during the training process. From the
perspective of the diversity of application scenarios, we conduct extensive
experiments on fine-tuning, prompt-tuning paradigms and zero-shot, few-shot and
fully-supervised settings, respectively, across eight diverse end-tasks. We
hope our exploration will encourage the community to revisit the power of
classical methods for efficient NLP. Code and datasets are available in
https://github.com/zjunlp/Revisit-KNN.

中文翻译:
预训练语言模型（PLMs）作为基于参数的主动学习范式，已成为当前自然语言处理（NLP）领域的事实标准。相比之下，k近邻（kNN）分类器作为惰性学习范式，往往能有效缓解过拟合和孤立噪声问题。本文重新探索了kNN分类器对PLMs分类器的增强作用：在方法论层面，我们提出分两步采用PLMs文本表征的kNN算法——首先利用kNN作为先验知识校准训练过程，其次将kNN预测的概率分布与PLMs分类器的输出进行线性插值。该方案的核心在于实施kNN校准训练机制，将预测结果作为训练过程中区分简单样本与困难样本的指示器。从应用场景多样性角度，我们在八种不同的终端任务上，分别针对微调范式、提示调优范式以及零样本、少样本和全监督设置开展了广泛实验。希望本研究能推动学界重新审视经典方法在高效NLP中的潜力。代码与数据集已开源：https://github.com/zjunlp/Revisit-KNN。

（翻译说明：
1. 专业术语采用学界通用译法，如"parametric-based eager learners"译为"基于参数的主动学习范式"
2. 长句拆分重组，如原文方法论部分的两步操作转换为中文更习惯的破折号列举式表达
3. 被动语态转换，如"are conducted"译为主动式的"开展"
4. 概念性表述意译，如"de-facto choice"译为"事实标准"而非字面直译
5. 技术表述精确性保持，如"linear interpolate"严格译为"线性插值"
6. 学术用语规范化，如"paradigms"统一译为"范式"而非"模式"
7. 保持原文的学术严谨性，同时符合中文科技论文摘要的简洁特征）
