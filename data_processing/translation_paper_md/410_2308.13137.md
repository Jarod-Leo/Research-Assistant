# OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models

链接: http://arxiv.org/abs/2308.13137v1

原文摘要:
Large language models (LLMs) have revolutionized natural language processing
tasks. However, their practical deployment is hindered by their immense memory
and computation requirements. Although recent post-training quantization (PTQ)
methods are effective in reducing memory footprint and improving the
computational efficiency of LLM, they hand-craft quantization parameters,
leading to low performance, especially in extremely low-bit quantization. To
tackle this issue, we introduce an Omnidirectionally calibrated Quantization
(\textbf{OmniQuant}) technique for LLMs, which achieves good performance in
diverse quantization settings while maintaining the computational efficiency of
PTQ by efficiently optimizing various quantization parameters. OmniQuant
comprises two innovative components including Learnable Weight Clipping (LWC)
and Learnable Equivalent Transformation (LET). LWC modulates the extreme values
of weights by optimizing the clipping threshold. Meanwhile, LET tackles
activation outliers by shifting the challenge of quantization from activations
to weights. Operating within a differentiable framework using block-wise error
minimization, OmniQuant can optimize the quantization process efficiently for
both weight-only and weight-activation quantization. For instance, the LLaMA-2
model family size 7-70B can be processed with OmniQuant on a single A100-40G
GPU within 1-16 hours using 128 samples. Extensive experiments validate
OmniQuant's superior performance across diverse quantization configurations
such as W4A4 (4-bit weight, 4-bit activation), W6A6, W4A16, W3A16, and W2A16.
Additionally, OmniQuant demonstrates effectiveness in instruction-tuned models
and delivers notable improvements in inference speed and memory reduction on
real devices. Codes are available at
\url{https://github.com/OpenGVLab/OmniQuant}.

中文翻译:
大语言模型（LLMs）彻底改变了自然语言处理任务，但其庞大的内存与计算需求阻碍了实际部署。尽管近期训练后量化（PTQ）方法能有效减少内存占用并提升LLM计算效率，但其手工设计的量化参数导致性能低下，尤其在极低位量化场景中表现不佳。为解决这一问题，我们提出一种全方位校准量化技术（OmniQuant），通过高效优化各类量化参数，在多样化量化配置下保持PTQ计算效率的同时实现优异性能。

OmniQuant包含两大创新组件：可学习权重截断（LWC）和可学习等效变换（LET）。LWC通过优化截断阈值调节权重的极端值分布，而LET通过将量化挑战从激活值转移至权重来处理异常激活值。该技术基于分块误差最小化的可微分框架运行，能高效优化纯权重量化及权重-激活值联合量化流程。例如，LLaMA-2系列7B-70B模型可在单张A100-40G GPU上使用128个样本，在1-16小时内完成OmniQuant处理。大量实验验证了OmniQuant在W4A4（4位权重-4位激活）、W6A6、W4A16、W3A16及W2A16等多样化量化配置中的卓越性能。此外，该技术对指令微调模型同样有效，并在真实设备上显著提升推理速度、降低内存占用。代码已开源于\url{https://github.com/OpenGVLab/OmniQuant}。

（注：根据学术规范，技术术语如"post-training quantization"采用"训练后量化"标准译法，"block-wise"译为"分块"符合计算机领域习惯。对于模型规模描述"7-70B"保留数字+单位形式，保持技术文档精确性。长句按中文表达习惯拆分为短句，如将原文复合状语结构转换为分句处理。）
