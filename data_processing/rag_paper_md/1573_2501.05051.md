# On the Generalizability of Transformer Models to Code Completions of Different Lengths

链接: http://arxiv.org/abs/2501.05051v1

原文摘要:
The programming landscape is nowadays being reshaped by the advent of Large
Language Models (LLMs) able to automate code-related tasks related to code
implementation (e.g., code completion) and comprehension (e.g., code
summarization). Such a paradigm shift comes with a number of implications
related to how software will be written, maintained, and evolved. Also, these
LLMs are extremely expensive to train, posing questions on their sustainability
over time. Given their training cost, their ability to generalize, namely their
ability to work on task instances different from those on which they have been
trained, is an aspect worth being investigated. Previous work already showed
that transformer models can successfully support code completion in a
cross-project setting. However, it is unclear whether LLM are able to
generalize to inputs having lengths not seen during training. For example, it
is known that training a model on short instances allows to substantially
reduce the training cost. However, the extent to which such a model would
provide good performance on sequences having lengths not seen during training
is not known. Many recent works in Natural Language Processing (NLP) tackled
this problem in the context of decoder-only LLMs, i.e., xPOS and ALiBi. To
assess if these solutions extend to encoder-decoder LLMs usually adopted in the
code-related tasks, we present a large empirical study evaluating this
generalization property of these and other encoding schemes proposed in the
literature, namely Sinusoidal, xPOS, ALiBi, and T5. We found that none of these
solutions successfully generalize to unseen lengths and that the only safe
solution is to ensure the representativeness in the training set of all lengths
likely to be encountered at inference time.

中文翻译:
当前，大型语言模型（LLMs）的出现正在重塑编程领域，这些模型能够自动化与代码实现（如代码补全）和理解（如代码摘要）相关的任务。这一范式转变对软件的编写、维护和演进方式产生了深远影响。同时，这些LLMs的训练成本极其高昂，引发了对其长期可持续性的质疑。鉴于其训练成本，其泛化能力——即处理与训练数据不同的任务实例的能力——成为一个值得深入研究的方面。  

先前的研究已表明，Transformer模型能够在跨项目环境中成功支持代码补全。然而，LLMs是否能够泛化到训练中未见的输入长度尚不明确。例如，已知在短实例上训练模型可大幅降低训练成本，但此类模型在未见长度序列上的性能表现仍属未知。近期自然语言处理（NLP）领域的多项研究（如xPOS和ALiBi）针对仅解码器架构的LLMs探讨了这一问题。  

为评估这些解决方案是否适用于代码任务中常用的编码器-解码器架构LLMs，我们开展了一项大规模实证研究，测试了文献中提出的多种编码方案（包括Sinusoidal、xPOS、ALiBi和T5）的泛化性能。研究发现，这些方案均无法有效泛化到未见长度，唯一可靠的解决方案是确保训练集中包含推理时可能遇到的所有长度样本，以保证其代表性。
