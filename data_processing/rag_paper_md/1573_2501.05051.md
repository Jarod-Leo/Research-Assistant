# On the Generalizability of Transformer Models to Code Completions of Different Lengths

链接: http://arxiv.org/abs/2501.05051v1

原文摘要:
The programming landscape is nowadays being reshaped by the advent of Large
Language Models (LLMs) able to automate code-related tasks related to code
implementation (e.g., code completion) and comprehension (e.g., code
summarization). Such a paradigm shift comes with a number of implications
related to how software will be written, maintained, and evolved. Also, these
LLMs are extremely expensive to train, posing questions on their sustainability
over time. Given their training cost, their ability to generalize, namely their
ability to work on task instances different from those on which they have been
trained, is an aspect worth being investigated. Previous work already showed
that transformer models can successfully support code completion in a
cross-project setting. However, it is unclear whether LLM are able to
generalize to inputs having lengths not seen during training. For example, it
is known that training a model on short instances allows to substantially
reduce the training cost. However, the extent to which such a model would
provide good performance on sequences having lengths not seen during training
is not known. Many recent works in Natural Language Processing (NLP) tackled
this problem in the context of decoder-only LLMs, i.e., xPOS and ALiBi. To
assess if these solutions extend to encoder-decoder LLMs usually adopted in the
code-related tasks, we present a large empirical study evaluating this
generalization property of these and other encoding schemes proposed in the
literature, namely Sinusoidal, xPOS, ALiBi, and T5. We found that none of these
solutions successfully generalize to unseen lengths and that the only safe
solution is to ensure the representativeness in the training set of all lengths
likely to be encountered at inference time.

中文翻译:
当前，大型语言模型（LLMs）的出现正在重塑编程领域的格局。这类模型能够自动化执行与代码实现（如代码补全）和代码理解（如代码摘要）相关的任务。这种范式转变对软件的编写、维护和演进方式产生了深远影响。然而，这些LLMs的训练成本极其高昂，其长期可持续性也引发诸多疑问。鉴于其训练成本，模型的泛化能力——即处理与训练数据不同的任务实例的能力——成为一个值得深入研究的关键问题。

已有研究表明，Transformer模型能够在跨项目场景中有效支持代码补全。但LLMs是否能泛化至训练时未见的输入长度仍不明确。例如，已知在短实例上训练模型可大幅降低训练成本，但此类模型对训练时未接触过的序列长度的表现尚属未知。近期自然语言处理（NLP）领域的多项研究（如xPOS和ALiBi）已在仅解码器架构的LLMs中探讨了这一问题。为验证这些方案是否适用于代码任务中常用的编码器-解码器架构LLMs，我们开展了一项大规模实证研究，评估了包括正弦编码（Sinusoidal）、xPOS、ALiBi和T5在内的多种文献提出的编码方案。研究发现：现有方案均无法有效泛化至未见长度，唯一可靠的解决方案是确保训练集能覆盖推理时可能遇到的所有长度范围。
