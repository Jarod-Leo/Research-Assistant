# Mobile V-MoEs: Scaling Down Vision Transformers via Sparse Mixture-of-Experts

链接: http://arxiv.org/abs/2309.04354v1

原文摘要:
Sparse Mixture-of-Experts models (MoEs) have recently gained popularity due
to their ability to decouple model size from inference efficiency by only
activating a small subset of the model parameters for any given input token. As
such, sparse MoEs have enabled unprecedented scalability, resulting in
tremendous successes across domains such as natural language processing and
computer vision. In this work, we instead explore the use of sparse MoEs to
scale-down Vision Transformers (ViTs) to make them more attractive for
resource-constrained vision applications. To this end, we propose a simplified
and mobile-friendly MoE design where entire images rather than individual
patches are routed to the experts. We also propose a stable MoE training
procedure that uses super-class information to guide the router. We empirically
show that our sparse Mobile Vision MoEs (V-MoEs) can achieve a better trade-off
between performance and efficiency than the corresponding dense ViTs. For
example, for the ViT-Tiny model, our Mobile V-MoE outperforms its dense
counterpart by 3.39% on ImageNet-1k. For an even smaller ViT variant with only
54M FLOPs inference cost, our MoE achieves an improvement of 4.66%.

中文翻译:
稀疏专家混合模型（MoEs）近期因其独特优势而备受关注：通过仅针对每个输入标记激活少量模型参数，实现了模型规模与推理效率的解耦。这种特性使得稀疏MoEs在自然语言处理和计算机视觉等领域展现出前所未有的可扩展性。本研究另辟蹊径，探索利用稀疏MoEs对视觉Transformer（ViTs）进行规模缩减，使其更适用于资源受限的视觉任务。为此，我们提出了一种简化且适配移动端的MoE架构——将整幅图像而非单个图像块路由至专家模块，同时创新性地引入基于超类信息引导路由器的稳定训练方法。实验数据表明，相较于传统稠密ViTs，我们提出的稀疏移动视觉MoE（V-MoEs）能实现更优的性能-效率平衡。以ViT-Tiny模型为例，在ImageNet-1k数据集上，移动V-MoE较稠密版本提升3.39%；对于计算量仅54M FLOPs的微型ViT变体，MoE架构更带来4.66%的性能提升。
