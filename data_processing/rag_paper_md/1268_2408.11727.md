# Efficient Detection of Toxic Prompts in Large Language Models

链接: http://arxiv.org/abs/2408.11727v1

原文摘要:
Large language models (LLMs) like ChatGPT and Gemini have significantly
advanced natural language processing, enabling various applications such as
chatbots and automated content generation. However, these models can be
exploited by malicious individuals who craft toxic prompts to elicit harmful or
unethical responses. These individuals often employ jailbreaking techniques to
bypass safety mechanisms, highlighting the need for robust toxic prompt
detection methods. Existing detection techniques, both blackbox and whitebox,
face challenges related to the diversity of toxic prompts, scalability, and
computational efficiency. In response, we propose ToxicDetector, a lightweight
greybox method designed to efficiently detect toxic prompts in LLMs.
ToxicDetector leverages LLMs to create toxic concept prompts, uses embedding
vectors to form feature vectors, and employs a Multi-Layer Perceptron (MLP)
classifier for prompt classification. Our evaluation on various versions of the
LLama models, Gemma-2, and multiple datasets demonstrates that ToxicDetector
achieves a high accuracy of 96.39\% and a low false positive rate of 2.00\%,
outperforming state-of-the-art methods. Additionally, ToxicDetector's
processing time of 0.0780 seconds per prompt makes it highly suitable for
real-time applications. ToxicDetector achieves high accuracy, efficiency, and
scalability, making it a practical method for toxic prompt detection in LLMs.

中文翻译:
诸如ChatGPT和Gemini等大型语言模型（LLMs）显著推动了自然语言处理的发展，催生了聊天机器人、自动化内容生成等多种应用。然而，恶意使用者可能通过精心设计的有害提示诱导模型产生违背伦理的回复，甚至利用越狱技术绕过安全防护机制，这凸显了构建强健的有害提示检测体系的必要性。现有黑盒与白盒检测技术在应对有害提示多样性、系统可扩展性及计算效率方面面临挑战。为此，我们提出轻量级灰盒检测方法ToxicDetector，其创新性地利用LLMs生成毒性概念提示，通过嵌入向量构建特征空间，并采用多层感知机（MLP）分类器实现高效识别。在LLama各版本、Gemma-2模型及多数据集上的测试表明，ToxicDetector以96.39%的准确率和2.00%的低误报率超越现有最优方法，单条提示处理时间仅0.0780秒，兼具实时性与可扩展性，为LLMs有害提示检测提供了实用化解决方案。
