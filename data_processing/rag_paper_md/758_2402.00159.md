# Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research

链接: http://arxiv.org/abs/2402.00159v1

原文摘要:
Information about pretraining corpora used to train the current
best-performing language models is seldom discussed: commercial models rarely
detail their data, and even open models are often released without accompanying
training data or recipes to reproduce them. As a result, it is challenging to
conduct and advance scientific research on language modeling, such as
understanding how training data impacts model capabilities and limitations. To
facilitate scientific research on language model pretraining, we curate and
release Dolma, a three-trillion-token English corpus, built from a diverse
mixture of web content, scientific papers, code, public-domain books, social
media, and encyclopedic materials. We extensively document Dolma, including its
design principles, details about its construction, and a summary of its
contents. We present analyses and experimental results on intermediate states
of Dolma to share what we have learned about important data curation practices.
Finally, we open-source our data curation toolkit to enable reproduction of our
work as well as support further research in large-scale data curation.

中文翻译:
当前性能最优异的语言模型所采用的预训练语料库信息鲜少被公开讨论：商业模型极少详述其数据构成，即便是开源模型也常在不提供训练数据或复现方案的情况下发布。这种状况导致语言建模领域的科学研究难以深入开展，例如探究训练数据如何影响模型能力与局限等重要课题。为促进语言模型预训练领域的学术探索，我们精心构建并开源了Dolma——一个包含三万亿token的英语语料库，其数据来源涵盖网页内容、学术论文、编程代码、公有领域书籍、社交媒体及百科全书等多元素材。我们对Dolma进行了全面记录，包括设计原则、构建细节及内容概要，并通过分析中间构建阶段的实验数据，分享了关于关键数据筛选实践的重要发现。最后，我们开源了数据整理工具包，既支持本工作的完整复现，也为大规模数据治理的后续研究提供基础设施。
