# An Exploration of In-Context Learning for Speech Language Model

链接: http://arxiv.org/abs/2310.12477v1

原文摘要:
Ever since the development of GPT-3 in the natural language processing (NLP)
field, in-context learning (ICL) has played an essential role in utilizing
large language models (LLMs). By presenting the LM utterance-label
demonstrations at the input, the LM can accomplish few-shot learning without
relying on gradient descent or requiring explicit modification of its
parameters. This enables the LM to perform various downstream tasks in a
black-box manner. Despite the success of ICL in NLP, little work is exploring
the possibility of ICL in speech processing. This study is the first work
exploring ICL for speech classification tasks with textless speech LM. We first
show that the current speech LM lacks the ICL capability. We then perform
warmup training on the speech LM, equipping the LM with demonstration learning
capability. This paper explores and proposes the first speech LM capable of
performing unseen classification tasks in an ICL manner.

中文翻译:
自自然语言处理（NLP）领域开发出GPT-3以来，上下文学习（ICL）在利用大语言模型（LLMs）中发挥了关键作用。通过在输入中提供语言模型的语句-标签示例，语言模型能够在不依赖梯度下降或显式修改其参数的情况下完成小样本学习。这使得语言模型能够以黑盒方式执行各种下游任务。尽管ICL在NLP中取得了成功，但探索ICL在语音处理中可能性的研究却寥寥无几。本研究是首次探索无文本语音语言模型在语音分类任务中应用ICL的工作。我们首先展示了当前语音语言模型缺乏ICL能力，随后通过对语音语言模型进行预热训练，赋予其示范学习能力。本文探索并提出了首个能够以ICL方式执行未见分类任务的语音语言模型。
