# Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems

链接: http://arxiv.org/abs/2302.03735v1

原文摘要:
The emergence of Pre-trained Language Models (PLMs) has achieved tremendous
success in the field of Natural Language Processing (NLP) by learning universal
representations on large corpora in a self-supervised manner. The pre-trained
models and the learned representations can be beneficial to a series of
downstream NLP tasks. This training paradigm has recently been adapted to the
recommendation domain and is considered a promising approach by both academia
and industry. In this paper, we systematically investigate how to extract and
transfer knowledge from pre-trained models learned by different PLM-related
training paradigms to improve recommendation performance from various
perspectives, such as generality, sparsity, efficiency and effectiveness.
Specifically, we propose a comprehensive taxonomy to divide existing PLM-based
recommender systems w.r.t. their training strategies and objectives. Then, we
analyze and summarize the connection between PLM-based training paradigms and
different input data types for recommender systems. Finally, we elaborate on
open issues and future research directions in this vibrant field.

中文翻译:
预训练语言模型（PLMs）的出现，通过自监督方式在海量语料上学习通用表征，为自然语言处理（NLP）领域带来了革命性突破。这类预训练模型及其习得的表征能力可有效赋能下游NLP任务群。该训练范式近期被成功迁移至推荐系统领域，成为学界与工业界共同关注的前沿方向。本文系统探究了如何从不同PLM相关训练范式中提取并迁移知识，从通用性、稀疏性、效率及有效性等多维度提升推荐性能。具体而言，我们构建了全新分类体系，依据训练策略与目标对现有基于PLM的推荐系统进行系统划分；进而深入剖析PLM训练范式与推荐系统多模态输入数据间的关联机制；最后针对这一快速发展的领域，详细阐述了当前面临的开放性问题与未来研究方向。
