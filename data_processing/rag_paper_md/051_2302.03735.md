# Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems

链接: http://arxiv.org/abs/2302.03735v1

原文摘要:
The emergence of Pre-trained Language Models (PLMs) has achieved tremendous
success in the field of Natural Language Processing (NLP) by learning universal
representations on large corpora in a self-supervised manner. The pre-trained
models and the learned representations can be beneficial to a series of
downstream NLP tasks. This training paradigm has recently been adapted to the
recommendation domain and is considered a promising approach by both academia
and industry. In this paper, we systematically investigate how to extract and
transfer knowledge from pre-trained models learned by different PLM-related
training paradigms to improve recommendation performance from various
perspectives, such as generality, sparsity, efficiency and effectiveness.
Specifically, we propose a comprehensive taxonomy to divide existing PLM-based
recommender systems w.r.t. their training strategies and objectives. Then, we
analyze and summarize the connection between PLM-based training paradigms and
different input data types for recommender systems. Finally, we elaborate on
open issues and future research directions in this vibrant field.

中文翻译:
预训练语言模型（PLMs）的出现通过自监督方式在海量语料库上学习通用表征，为自然语言处理（NLP）领域带来了革命性突破。这类预训练模型及其习得的表征能力可有效赋能下游系列NLP任务。当前，这种训练范式已成功迁移至推荐系统领域，被学界与工业界共同视为极具前景的研究方向。本文系统性地探究了如何从不同PLM相关训练范式中提取并迁移知识，从通用性、稀疏性、效率及有效性等多维度提升推荐性能。具体而言，我们首先提出一个层次化分类体系，依据训练策略与目标对现有基于PLM的推荐系统进行归类；继而解析并总结了PLM训练范式与推荐系统多源输入数据类型的关联机制；最后对这一活跃领域中的开放性问题与未来研究方向进行了深入探讨。

（翻译说明：
1. 专业术语处理："self-supervised"译为"自监督"，"downstream tasks"译为"下游任务"，保持学术规范性
2. 句式重构：将原文复合长句拆分为符合中文表达习惯的短句，如第一句拆分为因果逻辑的递进句式
3. 概念显化："w.r.t."扩展为"依据"，"vibrant field"意译为"活跃领域"
4. 学术风格保留：使用"表征能力""赋能""范式"等符合计算机领域论文的表述
5. 逻辑连接强化：通过"继而""最后"等衔接词明确论文结构脉络
6. 被动语态转化：将英文被动式转换为中文主动式，如"is considered"译为"被视为"）
