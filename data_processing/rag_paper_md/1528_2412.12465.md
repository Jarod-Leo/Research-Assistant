# Core Context Aware Attention for Long Context Language Modeling

链接: http://arxiv.org/abs/2412.12465v1

原文摘要:
Transformer-based Large Language Models (LLMs) have exhibited remarkable
success in various natural language processing tasks primarily attributed to
self-attention mechanism, which requires a token to consider all preceding
tokens as its context to compute the attention score. However, when the context
length L becomes very large (e.g., 32K), more redundant context information
will be included w.r.t. any tokens, making the self-attention suffer from two
main limitations: 1) The computational and memory complexity scales
quadratically w.r.t. L; 2) The presence of redundant context information may
hamper the model to capture dependencies among crucial tokens, which may
degrade the representation performance. In this paper, we propose a
plug-and-play Core Context Aware (CCA) Attention for efficient long-range
context modeling, which consists of two components: 1) Globality-pooling
attention that divides input tokens into groups and then dynamically merges
tokens within each group into one core token based on their significance; 2)
Locality-preserved attention that incorporates neighboring tokens into the
attention calculation. The two complementary attentions will then be fused to
the final attention, maintaining comprehensive modeling ability as the full
self-attention. In this way, the core context information w.r.t. a given token
will be automatically focused and strengthened, while the context information
in redundant groups will be diminished during the learning process. As a
result, the computational and memory complexity will be significantly reduced.
More importantly, the CCA-Attention can improve the long-context modeling
ability by diminishing the redundant context information. Extensive
experimental results demonstrate that our CCA-Attention significantly
outperforms state-of-the-art models in terms of computational efficiency and
long-context modeling ability.

中文翻译:
基于Transformer架构的大规模语言模型（LLMs）凭借其自注意力机制在各类自然语言处理任务中取得了显著成功，该机制要求每个标记（token）将所有前置标记作为上下文来计算注意力分数。然而当上下文长度L极大时（例如32K），任何标记所关联的上下文信息都会包含更多冗余内容，这使得自注意力机制面临两大核心局限：1）计算与内存复杂度随L呈平方级增长；2）冗余上下文信息可能阻碍模型捕捉关键标记间的依赖关系，进而削弱表征性能。本文提出一种即插即用的核心上下文感知（CCA）注意力机制，其包含两个组件：1）全局池化注意力——将输入标记分组后，根据重要性动态合并每组标记为核心标记；2）局部保持注意力——在计算中融入相邻标记信息。这两种互补注意力最终融合为完整注意力，在保持与全自注意力相当建模能力的同时，能自动聚焦强化关键上下文信息，并在学习过程中弱化冗余分组的上下文信息。由此不仅显著降低计算与内存复杂度，更重要的是通过抑制冗余信息提升了长上下文建模能力。大量实验表明，CCA-Attention在计算效率和长上下文建模能力上均显著优于当前最先进模型。
