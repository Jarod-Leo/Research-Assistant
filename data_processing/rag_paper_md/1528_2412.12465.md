# Core Context Aware Attention for Long Context Language Modeling

链接: http://arxiv.org/abs/2412.12465v1

原文摘要:
Transformer-based Large Language Models (LLMs) have exhibited remarkable
success in various natural language processing tasks primarily attributed to
self-attention mechanism, which requires a token to consider all preceding
tokens as its context to compute the attention score. However, when the context
length L becomes very large (e.g., 32K), more redundant context information
will be included w.r.t. any tokens, making the self-attention suffer from two
main limitations: 1) The computational and memory complexity scales
quadratically w.r.t. L; 2) The presence of redundant context information may
hamper the model to capture dependencies among crucial tokens, which may
degrade the representation performance. In this paper, we propose a
plug-and-play Core Context Aware (CCA) Attention for efficient long-range
context modeling, which consists of two components: 1) Globality-pooling
attention that divides input tokens into groups and then dynamically merges
tokens within each group into one core token based on their significance; 2)
Locality-preserved attention that incorporates neighboring tokens into the
attention calculation. The two complementary attentions will then be fused to
the final attention, maintaining comprehensive modeling ability as the full
self-attention. In this way, the core context information w.r.t. a given token
will be automatically focused and strengthened, while the context information
in redundant groups will be diminished during the learning process. As a
result, the computational and memory complexity will be significantly reduced.
More importantly, the CCA-Attention can improve the long-context modeling
ability by diminishing the redundant context information. Extensive
experimental results demonstrate that our CCA-Attention significantly
outperforms state-of-the-art models in terms of computational efficiency and
long-context modeling ability.

中文翻译:
基于Transformer的大型语言模型（LLMs）在各类自然语言处理任务中展现出卓越性能，这主要归功于自注意力机制——该机制要求每个标记（token）将所有前序标记作为上下文来计算注意力分数。然而，当上下文长度L极大时（例如32K），任何标记都会包含更多冗余上下文信息，导致自注意力面临两大局限：1）计算与内存复杂度随L呈平方级增长；2）冗余上下文信息可能阻碍模型捕捉关键标记间的依赖关系，从而降低表征性能。本文提出一种即插即用的核心上下文感知（CCA）注意力机制，用于高效长程上下文建模，其包含两个组件：1）全局池化注意力——将输入标记分组后，根据重要性动态合并每组标记为一个核心标记；2）局部保留注意力——将邻近标记纳入注意力计算。这两种互补注意力最终融合为完整注意力，保持与全自注意力同等的综合建模能力。该方法能自动聚焦并强化关键上下文信息，同时在学习过程中弱化冗余分组的上下文信息，从而显著降低计算与内存复杂度。更重要的是，CCA注意力通过削弱冗余信息提升了长上下文建模能力。大量实验结果表明，该机制在计算效率和长上下文建模能力上显著优于当前最先进模型。
