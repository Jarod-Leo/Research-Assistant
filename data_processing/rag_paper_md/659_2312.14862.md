# YAYI 2: Multilingual Open-Source Large Language Models

链接: http://arxiv.org/abs/2312.14862v1

原文摘要:
As the latest advancements in natural language processing, large language
models (LLMs) have achieved human-level language understanding and generation
abilities in many real-world tasks, and even have been regarded as a potential
path to the artificial general intelligence. To better facilitate research on
LLMs, many open-source LLMs, such as Llama 2 and Falcon, have recently been
proposed and gained comparable performances to proprietary models. However,
these models are primarily designed for English scenarios and exhibit poor
performances in Chinese contexts. In this technical report, we propose YAYI 2,
including both base and chat models, with 30 billion parameters. YAYI 2 is
pre-trained from scratch on a multilingual corpus which contains 2.65 trillion
tokens filtered by our pre-training data processing pipeline. The base model is
aligned with human values through supervised fine-tuning with millions of
instructions and reinforcement learning from human feedback. Extensive
experiments on multiple benchmarks, such as MMLU and CMMLU, consistently
demonstrate that the proposed YAYI 2 outperforms other similar sized
open-source models.

中文翻译:
作为自然语言处理领域的最新进展，大型语言模型（LLMs）已在众多实际任务中展现出类人的语言理解与生成能力，甚至被视为通往通用人工智能的潜在路径。为促进LLM研究的开放发展，近期涌现出Llama 2、Falcon等开源模型，其性能已可比肩商业闭源模型。然而这些模型主要面向英语场景，在中文语境下表现欠佳。本技术报告提出包含300亿参数的YAYI 2系列模型（含基础模型与对话模型），其基于我们自主研发的预训练数据处理流程，从经过筛选的2.65万亿token多语言语料库中从头开始训练。通过数百万条指令的监督微调与人类反馈强化学习，基础模型实现了与人类价值观的对齐。在MMLU、CMMLU等多个基准测试上的大量实验表明，YAYI 2模型持续优于同规模开源模型。
