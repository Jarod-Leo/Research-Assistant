# LAMM: Label Alignment for Multi-Modal Prompt Learning

链接: http://arxiv.org/abs/2312.08212v1

原文摘要:
With the success of pre-trained visual-language (VL) models such as CLIP in
visual representation tasks, transferring pre-trained models to downstream
tasks has become a crucial paradigm. Recently, the prompt tuning paradigm,
which draws inspiration from natural language processing (NLP), has made
significant progress in VL field. However, preceding methods mainly focus on
constructing prompt templates for text and visual inputs, neglecting the gap in
class label representations between the VL models and downstream tasks. To
address this challenge, we introduce an innovative label alignment method named
\textbf{LAMM}, which can dynamically adjust the category embeddings of
downstream datasets through end-to-end training. Moreover, to achieve a more
appropriate label distribution, we propose a hierarchical loss, encompassing
the alignment of the parameter space, feature space, and logits space. We
conduct experiments on 11 downstream vision datasets and demonstrate that our
method significantly improves the performance of existing multi-modal prompt
learning models in few-shot scenarios, exhibiting an average accuracy
improvement of 2.31(\%) compared to the state-of-the-art methods on 16 shots.
Moreover, our methodology exhibits the preeminence in continual learning
compared to other prompt tuning methods. Importantly, our method is synergistic
with existing prompt tuning methods and can boost the performance on top of
them. Our code and dataset will be publicly available at
https://github.com/gaojingsheng/LAMM.

中文翻译:
随着CLIP等预训练视觉语言（VL）模型在视觉表征任务中的成功应用，将预训练模型迁移至下游任务已成为重要范式。近期，受自然语言处理（NLP）启发的提示调优范式在VL领域取得显著进展。然而现有方法主要集中于构建文本和视觉输入的提示模板，忽视了VL模型与下游任务在类别标签表征上的差异。针对这一挑战，我们提出了一种创新的标签对齐方法**LAMM**，该方法能通过端到端训练动态调整下游数据集的类别嵌入。此外，为实现更合理的标签分布，我们提出包含参数空间、特征空间和逻辑空间对齐的分层损失函数。我们在11个下游视觉数据集上进行实验，证明该方法在少样本场景下显著提升了现有多模态提示学习模型的性能，在16样本设置下相比最先进方法平均准确率提升2.31%。值得注意的是，相较于其他提示调优方法，我们的方案在持续学习场景中展现出卓越优势。该方法与现有提示调优技术具有协同效应，可在此基础上进一步提升性能。代码与数据集已公开于https://github.com/gaojingsheng/LAMM。
