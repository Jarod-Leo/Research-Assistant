# Training-free Lexical Backdoor Attacks on Language Models

链接: http://arxiv.org/abs/2302.04116v1

原文摘要:
Large-scale language models have achieved tremendous success across various
natural language processing (NLP) applications. Nevertheless, language models
are vulnerable to backdoor attacks, which inject stealthy triggers into models
for steering them to undesirable behaviors. Most existing backdoor attacks,
such as data poisoning, require further (re)training or fine-tuning language
models to learn the intended backdoor patterns. The additional training process
however diminishes the stealthiness of the attacks, as training a language
model usually requires long optimization time, a massive amount of data, and
considerable modifications to the model parameters. In this work, we propose
Training-Free Lexical Backdoor Attack (TFLexAttack) as the first training-free
backdoor attack on language models. Our attack is achieved by injecting lexical
triggers into the tokenizer of a language model via manipulating its embedding
dictionary using carefully designed rules. These rules are explainable to human
developers which inspires attacks from a wider range of hackers. The sparse
manipulation of the dictionary also habilitates the stealthiness of our attack.
We conduct extensive experiments on three dominant NLP tasks based on nine
language models to demonstrate the effectiveness and universality of our
attack. The code of this work is available at
https://github.com/Jinxhy/TFLexAttack.

中文翻译:
大规模语言模型在各类自然语言处理（NLP）应用中取得了巨大成功。然而，语言模型易受后门攻击影响——攻击者通过植入隐蔽触发器来操控模型产生异常行为。现有后门攻击（如数据投毒）大多需要重新训练或微调模型以学习预设的后门模式，但附加的训练过程会削弱攻击隐蔽性，因为语言模型训练通常耗时漫长、数据需求庞大且需大幅修改模型参数。本研究提出首个免训练的词法后门攻击方法TFLexAttack，通过精心设计的规则操纵语言模型分词器的嵌入词典来植入词法触发器。这些规则对人类开发者具有可解释性，可能激发更广泛的黑客攻击行为，而对词典的稀疏操作也保障了攻击的隐蔽性。我们在三大主流NLP任务和九种语言模型上进行了广泛实验，验证了该攻击方法的有效性和普适性。项目代码已开源：https://github.com/Jinxhy/TFLexAttack。
