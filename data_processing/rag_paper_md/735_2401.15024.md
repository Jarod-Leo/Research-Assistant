# SliceGPT: Compress Large Language Models by Deleting Rows and Columns

链接: http://arxiv.org/abs/2401.15024v1

原文摘要:
Large language models have become the cornerstone of natural language
processing, but their use comes with substantial costs in terms of compute and
memory resources. Sparsification provides a solution to alleviate these
resource constraints, and recent works have shown that trained models can be
sparsified post-hoc. Existing sparsification techniques face challenges as they
need additional data structures and offer constrained speedup with current
hardware. In this paper we present SliceGPT, a new post-training sparsification
scheme which replaces each weight matrix with a smaller (dense) matrix,
reducing the embedding dimension of the network. Through extensive
experimentation, we show that SliceGPT can remove up to 25% of the model
parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models
while maintaining 99%, 99% and 90% zero-shot task performance of the dense
model respectively. Our sliced models run on fewer GPUs and run faster without
any additional code optimization: on 24GB consumer GPUs we reduce the total
compute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB
A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance
in transformer networks, which enables SliceGPT and we hope it will inspire and
enable future avenues to reduce memory and computation demands for pre-trained
models. Code is available at:
https://github.com/microsoft/TransformerCompression

中文翻译:
以下是符合要求的学术化中文翻译：

大型语言模型已成为自然语言处理的基石，但其使用伴随着巨大的计算与内存资源消耗。稀疏化技术为缓解这些资源限制提供了解决方案，近期研究表明已训练模型可通过事后稀疏化进行处理。现有稀疏化方法面临双重挑战：既需要额外数据结构，在当前硬件上又只能提供有限的加速效果。本文提出SliceGPT——一种新型训练后稀疏化方案，通过用更小的（密集）矩阵替代每个权重矩阵，从而降低网络的嵌入维度。大量实验表明，SliceGPT能为LLAMA2-70B、OPT 66B和Phi-2模型移除高达25%的参数（包括嵌入层），同时分别保持稠密模型99%、99%和90%的零样本任务性能。经切片的模型可在更少GPU上运行，且无需额外代码优化即可获得加速：在24GB消费级GPU上，我们将LLAMA2-70B推理的总计算量降至稠密模型的64%；在40GB A100 GPU上降至66%。我们提出了"Transformer网络计算不变性"的新见解，该原理不仅使SliceGPT成为可能，更有望启发未来更多降低预训练模型内存与计算需求的新方法。代码已开源：https://github.com/microsoft/TransformerCompression

（翻译严格遵循以下原则：
1. 专业术语准确统一："sparsification"译为"稀疏化"，"post-hoc"译为"事后"，"zero-shot"译为"零样本"
2. 被动语态转化："it is shown"转为主动式"研究表明"
3. 长句拆分重组：将原文复合句按中文表达习惯分解为多个短句
4. 数字规范处理：保持原数值精度，百分数统一用"%"符号
5. 学术风格保持：使用"本文""提出""表明"等学术用语
6. 技术概念显化："computational invariance"译为"计算不变性"并加引号强调
7. 链接信息完整保留：GitHub地址完整呈现）
