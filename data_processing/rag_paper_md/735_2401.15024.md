# SliceGPT: Compress Large Language Models by Deleting Rows and Columns

链接: http://arxiv.org/abs/2401.15024v1

原文摘要:
Large language models have become the cornerstone of natural language
processing, but their use comes with substantial costs in terms of compute and
memory resources. Sparsification provides a solution to alleviate these
resource constraints, and recent works have shown that trained models can be
sparsified post-hoc. Existing sparsification techniques face challenges as they
need additional data structures and offer constrained speedup with current
hardware. In this paper we present SliceGPT, a new post-training sparsification
scheme which replaces each weight matrix with a smaller (dense) matrix,
reducing the embedding dimension of the network. Through extensive
experimentation, we show that SliceGPT can remove up to 25% of the model
parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models
while maintaining 99%, 99% and 90% zero-shot task performance of the dense
model respectively. Our sliced models run on fewer GPUs and run faster without
any additional code optimization: on 24GB consumer GPUs we reduce the total
compute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB
A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance
in transformer networks, which enables SliceGPT and we hope it will inspire and
enable future avenues to reduce memory and computation demands for pre-trained
models. Code is available at:
https://github.com/microsoft/TransformerCompression

中文翻译:
大型语言模型已成为自然语言处理的基石，但其使用伴随着巨大的计算和内存资源消耗。稀疏化技术为缓解这些资源限制提供了解决方案，近期研究表明已训练模型可通过事后稀疏化处理。现有稀疏化方法面临双重挑战：既需引入额外数据结构，又受限于当前硬件条件下的加速效果。本文提出SliceGPT这一新型训练后稀疏化方案，通过将每个权重矩阵替换为更小的（密集）矩阵来降低网络嵌入维度。经大量实验验证，SliceGPT能在LLAMA2-70B、OPT 66B和Phi-2模型上分别去除高达25%的模型参数（含嵌入层），同时保持密集模型99%、99%和90%的零样本任务性能。经切割的模型可在更少GPU上运行，且无需额外代码优化即可提速：在24GB消费级GPU上，LLAMA2-70B推理总计算量降至密集模型的64%；在40GB A100 GPU上降至66%。我们揭示了Transformer网络的计算不变性这一新见解，该特性不仅使SliceGPT成为可能，更有望启发未来更多降低预训练模型内存与计算需求的新途径。代码已开源于：https://github.com/microsoft/TransformerCompression
