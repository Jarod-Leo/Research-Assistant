# LIFBench: Evaluating the Instruction Following Performance and Stability of Large Language Models in Long-Context Scenarios

链接: http://arxiv.org/abs/2411.07037v1

原文摘要:
As Large Language Models (LLMs) evolve in natural language processing (NLP),
their ability to stably follow instructions in long-context inputs has become
critical for real-world applications. However, existing benchmarks seldom focus
on instruction-following in long-context scenarios or stability on different
inputs. To bridge this gap, we introduce LIFBench, a scalable dataset designed
to evaluate LLMs' instruction-following capabilities and stability across long
contexts. LIFBench comprises three long-context scenarios and eleven diverse
tasks, featuring 2,766 instructions generated through an automated expansion
method across three dimensions: length, expression, and variables. For
evaluation, we propose LIFEval, a rubric-based assessment method that enables
precise, automated scoring of complex LLM responses without reliance on
LLM-assisted assessments or human judgment. This method allows for a
comprehensive analysis of model performance and stability from multiple
perspectives. We conduct detailed experiments on 20 prominent LLMs across six
length intervals. Our work contributes LIFBench and LIFEval as robust tools for
assessing LLM performance in complex and long-context settings, offering
valuable insights to guide future advancements in LLM development.

中文翻译:
随着大语言模型（LLMs）在自然语言处理（NLP）领域的不断发展，其在长上下文输入中稳定遵循指令的能力已成为实际应用的关键。然而，现有基准测试很少关注长上下文场景下的指令遵循能力或对不同输入的稳定性。为填补这一空白，我们推出了LIFBench——一个可扩展的数据集，旨在评估LLMs在长上下文中的指令遵循能力和稳定性。该数据集包含三种长上下文场景和十一项多样化任务，通过自动化扩展方法在长度、表达方式和变量三个维度上生成了2,766条指令。

针对评估需求，我们提出了LIFEval这一基于量规的评估方法，无需依赖LLM辅助评估或人工判断即可对复杂LLM响应进行精准自动化评分。该方法支持从多角度全面分析模型性能和稳定性。我们对20个主流LLM在六个长度区间上进行了详细实验。本研究贡献的LIFBench和LIFEval作为评估LLM在复杂长上下文环境中性能的强效工具，为引导LLM未来发展提供了宝贵洞见。
