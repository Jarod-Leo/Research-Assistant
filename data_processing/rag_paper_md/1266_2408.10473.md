# Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism

链接: http://arxiv.org/abs/2408.10473v1

原文摘要:
Pre-trained language models (PLMs) are engineered to be robust in contextual
understanding and exhibit outstanding performance in various natural language
processing tasks. However, their considerable size incurs significant
computational and storage costs. Modern pruning strategies employ one-shot
techniques to compress PLMs without the need for retraining on task-specific or
otherwise general data; however, these approaches often lead to an
indispensable reduction in performance. In this paper, we propose SDS, a
Sparse-Dense-Sparse pruning framework to enhance the performance of the pruned
PLMs from a weight distribution optimization perspective. We outline the
pruning process in three steps. Initially, we prune less critical connections
in the model using conventional one-shot pruning methods. Next, we reconstruct
a dense model featuring a pruning-friendly weight distribution by reactivating
pruned connections with sparse regularization. Finally, we perform a second
pruning round, yielding a superior pruned model compared to the initial
pruning. Experimental results demonstrate that SDS outperforms the
state-of-the-art pruning techniques SparseGPT and Wanda under an identical
sparsity configuration. For instance, SDS reduces perplexity by 9.13 on
Raw-Wikitext2 and improves accuracy by an average of 2.05% across multiple
zero-shot benchmarks for OPT-125M with 2:4 sparsity.

中文翻译:
预训练语言模型（PLMs）在设计上具备强大的上下文理解能力，并在各类自然语言处理任务中展现出卓越性能。然而，其庞大的规模带来了高昂的计算与存储成本。现代剪枝策略采用一次性技术压缩PLMs，无需针对特定任务或通用数据进行再训练，但这些方法往往不可避免地导致性能下降。本文提出SDS（稀疏-密集-稀疏）剪枝框架，从权重分布优化的角度提升剪枝后PLMs的性能。我们将剪枝过程分为三步：首先使用传统一次性方法剪除模型中重要性较低的连接；随后通过稀疏正则化重新激活已剪枝连接，重建具有剪枝友好型权重分布的密集模型；最后进行第二轮剪枝，获得优于初始剪枝结果的模型。实验表明，在相同稀疏度配置下，SDS优于当前最先进的SparseGPT和Wanda剪枝技术。例如对于2:4稀疏度的OPT-125M模型，SDS在Raw-Wikitext2上将困惑度降低9.13，在多个零样本基准测试中平均准确率提升2.05%。
