# OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models

链接: http://arxiv.org/abs/2406.01775v1

原文摘要:
The advent of large language models (LLMs) has revolutionized natural
language processing, enabling unprecedented capabilities in understanding and
generating human-like text. However, the computational cost and convergence
times associated with fine-tuning these models remain significant challenges.
Low-Rank Adaptation (LoRA) has emerged as a promising method to mitigate these
issues by introducing efficient fine-tuning techniques with a reduced number of
trainable parameters. In this paper, we present OLoRA, an enhancement to the
LoRA method that leverages orthonormal matrix initialization through QR
decomposition. OLoRA significantly accelerates the convergence of LLM training
while preserving the efficiency benefits of LoRA, such as the number of
trainable parameters and GPU memory footprint. Our empirical evaluations
demonstrate that OLoRA not only converges faster but also exhibits improved
performance compared to standard LoRA across a variety of language modeling
tasks. This advancement opens new avenues for more efficient and accessible
fine-tuning of LLMs, potentially enabling broader adoption and innovation in
natural language applications.

中文翻译:
大型语言模型（LLM）的出现彻底改变了自然语言处理领域，使其在理解和生成类人文本方面展现出前所未有的能力。然而，这些模型的微调过程仍面临计算成本高、收敛时间长等重大挑战。低秩自适应（LoRA）技术通过引入可训练参数更少的高效微调方法，为解决这些问题提供了可行方案。本文提出OLoRA——一种基于QR分解正交矩阵初始化的LoRA增强方法，在保留LoRA参数效率（可训练参数数量和GPU内存占用）优势的同时，显著加速了LLM训练的收敛速度。实证研究表明，在多种语言建模任务中，OLoRA不仅收敛更快，其性能也优于标准LoRA。这一进展为LLM的高效微调开辟了新途径，有望推动自然语言应用更广泛的普及与创新。
