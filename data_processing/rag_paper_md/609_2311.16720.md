# RankingGPT: Empowering Large Language Models in Text Ranking with Progressive Enhancement

链接: http://arxiv.org/abs/2311.16720v1

原文摘要:
Text ranking is a critical task in information retrieval. Recent advances in
pre-trained language models (PLMs), especially large language models (LLMs),
present new opportunities for applying them to text ranking. While supervised
fine-tuning (SFT) with ranking data has been widely explored to better align
PLMs with text ranking goals, previous studies have focused primarily on
encoder-only and encoder-decoder PLMs. Research on leveraging decoder-only LLMs
for text ranking remains scarce. An exception to this is RankLLaMA, which uses
direct SFT to explore LLaMA's potential for text ranking. In this work, we
propose a two-stage progressive paradigm to better adapt LLMs to text ranking.
First, we conduct continual pre-training (CPT) of LLMs on a large
weakly-supervised corpus. Second, we perform SFT, and propose an improved
optimization strategy building upon RankLLaMA. Our experimental results on
multiple benchmarks show that our approach outperforms previous methods in both
in-domain and out-domain scenarios.

中文翻译:
文本排序是信息检索中的关键任务。预训练语言模型（PLMs），尤其是大语言模型（LLMs）的最新进展，为将其应用于文本排序提供了新的机遇。虽然通过排序数据进行监督微调（SFT）已被广泛探索以更好地使PLMs与文本排序目标对齐，但先前的研究主要集中在仅编码器和编码器-解码器PLMs上。关于利用仅解码器LLMs进行文本排序的研究仍然很少。RankLLaMA是一个例外，它使用直接SFT来探索LLaMA在文本排序中的潜力。在这项工作中，我们提出了一个两阶段的渐进范式，以更好地使LLMs适应文本排序。首先，我们在一个大型弱监督语料库上对LLMs进行持续预训练（CPT）。其次，我们进行SFT，并提出了基于RankLLaMA的改进优化策略。我们在多个基准测试上的实验结果表明，我们的方法在领域内和领域外场景中均优于先前的方法。
