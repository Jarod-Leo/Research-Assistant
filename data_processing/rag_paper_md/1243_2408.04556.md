# Bias-Aware Low-Rank Adaptation: Mitigating Catastrophic Inheritance of Large Language Models

链接: http://arxiv.org/abs/2408.04556v1

原文摘要:
Large language models (LLMs) have demonstrated remarkable proficiency across
various natural language processing (NLP) tasks. However, adapting LLMs to
downstream applications requires computationally intensive and memory-demanding
fine-tuning procedures. To alleviate these burdens, parameter-efficient
fine-tuning (PEFT) techniques have emerged as a promising approach to tailor
LLMs with minimal computational overhead. While PEFT methods offer substantial
advantages, they do not fully address the pervasive issue of bias propagation
from pre-training data. This work introduces Bias-Alleviating Low-Rank
Adaptation (BA-LoRA), a novel PEFT method designed to counteract bias
inheritance. BA-LoRA incorporates three distinct regularization terms: (1) a
consistency regularizer, (2) a diversity regularizer, and (3) a singular value
decomposition regularizer. These regularizers aim to enhance the models'
consistency, diversity, and generalization capabilities during fine-tuning. We
conduct extensive experiments on natural language understanding (NLU) and
natural language generation (NLG) tasks using prominent LLMs such as LLaMA,
Mistral, and Gemma. The results demonstrate that BA-LoRA outperforms LoRA and
its state-of-the-art variants. Moreover, our method effectively mitigates the
adverse effects of pre-training bias, leading to more reliable and robust model
outputs. The code is available at https://github.com/cyp-jlu-ai/BA-LoRA.

中文翻译:
大型语言模型（LLMs）在各种自然语言处理（NLP）任务中展现出卓越的能力。然而，将LLMs适配至下游应用需要耗费大量计算资源和内存的微调过程。为缓解这一负担，参数高效微调（PEFT）技术应运而生，成为以最小计算开销定制LLMs的有效方案。尽管PEFT方法具有显著优势，但尚未完全解决预训练数据中偏差传播的普遍问题。本文提出偏差缓解低秩适配（BA-LoRA），这是一种创新的PEFT方法，旨在对抗偏差继承问题。BA-LoRA整合了三种独特的正则化项：（1）一致性正则器，（2）多样性正则器，以及（3）奇异值分解正则器。这些正则器通过增强模型在微调过程中的一致性、多样性和泛化能力来实现目标。我们基于LLaMA、Mistral和Gemma等主流LLMs，在自然语言理解（NLU）和自然语言生成（NLG）任务上进行了广泛实验。结果表明BA-LoRA在性能上超越LoRA及其现有最优变体。此外，该方法有效减轻了预训练偏差的负面影响，使模型输出更加可靠和鲁棒。代码已开源：https://github.com/cyp-jlu-ai/BA-LoRA。
