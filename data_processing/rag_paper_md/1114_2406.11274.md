# Skip-Layer Attention: Bridging Abstract and Detailed Dependencies in Transformers

链接: http://arxiv.org/abs/2406.11274v1

原文摘要:
The Transformer architecture has significantly advanced deep learning,
particularly in natural language processing, by effectively managing long-range
dependencies. However, as the demand for understanding complex relationships
grows, refining the Transformer's architecture becomes critical. This paper
introduces Skip-Layer Attention (SLA) to enhance Transformer models by enabling
direct attention between non-adjacent layers. This method improves the model's
ability to capture dependencies between high-level abstract features and
low-level details. By facilitating direct attention between these diverse
feature levels, our approach overcomes the limitations of current Transformers,
which often rely on suboptimal intra-layer attention. Our implementation
extends the Transformer's functionality by enabling queries in a given layer to
interact with keys and values from both the current layer and one preceding
layer, thus enhancing the diversity of multi-head attention without additional
computational burden. Extensive experiments demonstrate that our enhanced
Transformer model achieves superior performance in language modeling tasks,
highlighting the effectiveness of our skip-layer attention mechanism.

中文翻译:
Transformer架构通过有效处理长程依赖关系，显著推动了深度学习尤其是自然语言处理领域的发展。然而随着对复杂关系理解需求的增长，优化Transformer架构变得至关重要。本文提出跳跃层注意力机制（SLA），通过实现非相邻层间的直接注意力交互来增强Transformer模型。该方法提升了模型捕捉高层抽象特征与底层细节间依赖关系的能力，克服了现有Transformer仅依赖次优层内注意力的局限。我们的实现方案允许给定层的查询向量同时与当前层及前一层的键值对交互，在不增加计算负担的前提下增强了多头注意力的多样性。大量实验表明，改进后的Transformer模型在语言建模任务中表现出更优性能，验证了跳跃层注意力机制的有效性。
