# Nonparametric Variational Regularisation of Pretrained Transformers

链接: http://arxiv.org/abs/2312.00662v1

原文摘要:
The current paradigm of large-scale pre-training and fine-tuning Transformer
large language models has lead to significant improvements across the board in
natural language processing. However, such large models are susceptible to
overfitting to their training data, and as a result the models perform poorly
when the domain changes. Also, due to the model's scale, the cost of
fine-tuning the model to the new domain is large. Nonparametric Variational
Information Bottleneck (NVIB) has been proposed as a regulariser for training
cross-attention in Transformers, potentially addressing the overfitting
problem. We extend the NVIB framework to replace all types of attention
functions in Transformers, and show that existing pretrained Transformers can
be reinterpreted as Nonparametric Variational (NV) models using a proposed
identity initialisation. We then show that changing the initialisation
introduces a novel, information-theoretic post-training regularisation in the
attention mechanism, which improves out-of-domain generalisation without any
training. This success supports the hypothesis that pretrained Transformers are
implicitly NV Bayesian models.

中文翻译:
当前大规模预训练与微调Transformer大语言模型的范式，已在自然语言处理领域带来全面显著提升。然而这类大模型易对训练数据产生过拟合，导致领域变化时性能急剧下降。同时受模型规模限制，针对新领域进行微调的成本极高。非参数化变分信息瓶颈（NVIB）被提出作为Transformer交叉注意力机制的调节器，有望解决过拟合问题。本研究将NVIB框架扩展至Transformer所有类型的注意力函数，通过提出的恒等初始化方法，证明现有预训练Transformer可被重新诠释为非参数化变分（NV）模型。进一步研究表明，改变初始化方式能在注意力机制中引入一种基于信息论的后训练正则化方法，无需任何训练即可提升跨领域泛化能力。这一发现支持了"预训练Transformer本质上是隐式NV贝叶斯模型"的理论假设。
