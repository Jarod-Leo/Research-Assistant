# Empirical evaluation of Uncertainty Quantification in Retrieval-Augmented Language Models for Science

链接: http://arxiv.org/abs/2311.09358v1

原文摘要:
Large language models (LLMs) have shown remarkable achievements in natural
language processing tasks, producing high-quality outputs. However, LLMs still
exhibit limitations, including the generation of factually incorrect
information. In safety-critical applications, it is important to assess the
confidence of LLM-generated content to make informed decisions. Retrieval
Augmented Language Models (RALMs) is relatively a new area of research in NLP.
RALMs offer potential benefits for scientific NLP tasks, as retrieved
documents, can serve as evidence to support model-generated content. This
inclusion of evidence enhances trustworthiness, as users can verify and explore
the retrieved documents to validate model outputs. Quantifying uncertainty in
RALM generations further improves trustworthiness, with retrieved text and
confidence scores contributing to a comprehensive and reliable model for
scientific applications. However, there is limited to no research on UQ for
RALMs, particularly in scientific contexts. This study aims to address this gap
by conducting a comprehensive evaluation of UQ in RALMs, focusing on scientific
tasks. This research investigates how uncertainty scores vary when scientific
knowledge is incorporated as pretraining and retrieval data and explores the
relationship between uncertainty scores and the accuracy of model-generated
outputs. We observe that an existing RALM finetuned with scientific knowledge
as the retrieval data tends to be more confident in generating predictions
compared to the model pretrained only with scientific knowledge. We also found
that RALMs are overconfident in their predictions, making inaccurate
predictions more confidently than accurate ones. Scientific knowledge provided
either as pretraining or retrieval corpus does not help alleviate this issue.
We released our code, data and dashboards at https://github.com/pnnl/EXPERT2.

中文翻译:
大型语言模型（LLMs）在自然语言处理任务中展现出卓越成就，能够生成高质量的输出内容。然而，LLMs仍存在局限性，包括产生事实性错误信息。在安全关键型应用中，评估LLM生成内容的置信度对于做出明智决策至关重要。检索增强语言模型（RALMs）是自然语言处理领域相对新兴的研究方向。RALMs为科学NLP任务提供了潜在优势，因为检索到的文档可作为支撑模型生成内容的证据。这种证据的引入增强了可信度，用户可通过核查检索文档来验证模型输出。量化RALM生成内容的不确定性进一步提升了可靠性，检索文本与置信度评分共同构成了科学应用中全面可靠的计算模型。但目前针对RALMs（特别是科学场景下）的不确定性量化研究极为匮乏。本研究旨在通过系统评估科学任务中RALMs的不确定性来填补这一空白，重点探究科学知识作为预训练和检索数据时不确定性评分的变化规律，并分析不确定性评分与模型输出准确性之间的关联。我们发现：相较于仅用科学知识预训练的模型，采用科学知识作为检索数据进行微调的现有RALM在生成预测时往往表现出更高置信度。研究还表明RALMs存在预测过度自信现象，对错误预测的置信度反而高于正确预测。无论是作为预训练语料还是检索库，科学知识的引入均未能缓解这一问题。相关代码、数据及可视化面板已发布于https://github.com/pnnl/EXPERT2。
