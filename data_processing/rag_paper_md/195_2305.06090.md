# XTab: Cross-table Pretraining for Tabular Transformers

链接: http://arxiv.org/abs/2305.06090v1

原文摘要:
The success of self-supervised learning in computer vision and natural
language processing has motivated pretraining methods on tabular data. However,
most existing tabular self-supervised learning models fail to leverage
information across multiple data tables and cannot generalize to new tables. In
this work, we introduce XTab, a framework for cross-table pretraining of
tabular transformers on datasets from various domains. We address the challenge
of inconsistent column types and quantities among tables by utilizing
independent featurizers and using federated learning to pretrain the shared
component. Tested on 84 tabular prediction tasks from the OpenML-AutoML
Benchmark (AMLB), we show that (1) XTab consistently boosts the
generalizability, learning speed, and performance of multiple tabular
transformers, (2) by pretraining FT-Transformer via XTab, we achieve superior
performance than other state-of-the-art tabular deep learning models on various
tasks such as regression, binary, and multiclass classification.

中文翻译:
计算机视觉与自然语言处理领域自监督学习的成功，激发了针对表格数据的预训练方法研究。然而，现有大多数表格自监督学习模型无法有效利用跨表格信息，且难以泛化至新表。本文提出XTab框架，通过跨域数据集对表格Transformer进行联合预训练。针对不同表格间列类型与数量不一致的挑战，我们采用独立特征化器处理各表，并运用联邦学习策略预训练共享组件。基于OpenML-AutoML基准测试(AMLB)的84项表格预测任务实验表明：(1) XTab能持续提升多种表格Transformer的泛化能力、学习速度及预测性能；(2) 经XTab预训练的FT-Transformer在回归、二分类及多分类任务中均超越当前最先进的表格深度学习模型。
