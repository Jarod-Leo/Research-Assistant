# Probing the Subtle Ideological Manipulation of Large Language Models

链接: http://arxiv.org/abs/2504.14287v1

原文摘要:
Large Language Models (LLMs) have transformed natural language processing,
but concerns have emerged about their susceptibility to ideological
manipulation, particularly in politically sensitive areas. Prior work has
focused on binary Left-Right LLM biases, using explicit prompts and fine-tuning
on political QA datasets. In this work, we move beyond this binary approach to
explore the extent to which LLMs can be influenced across a spectrum of
political ideologies, from Progressive-Left to Conservative-Right. We introduce
a novel multi-task dataset designed to reflect diverse ideological positions
through tasks such as ideological QA, statement ranking, manifesto cloze
completion, and Congress bill comprehension. By fine-tuning three LLMs-Phi-2,
Mistral, and Llama-3-on this dataset, we evaluate their capacity to adopt and
express these nuanced ideologies. Our findings indicate that fine-tuning
significantly enhances nuanced ideological alignment, while explicit prompts
provide only minor refinements. This highlights the models' susceptibility to
subtle ideological manipulation, suggesting a need for more robust safeguards
to mitigate these risks.

中文翻译:
大型语言模型（LLM）已彻底改变了自然语言处理领域，但人们日益担忧其在政治敏感议题上易受意识形态操控的问题。现有研究多聚焦于左右二元对立的模型偏见，主要通过显式提示词或在政治问答数据集上进行微调。本研究突破这种二元框架，系统探究LLM在"进步左翼-保守右翼"光谱中受意识形态影响的程度。我们创新性地构建了一个多任务数据集，通过意识形态问答、政治声明排序、政党宣言完形填空和国会法案理解等任务，全面反映多样化政治立场。通过对Phi-2、Mistral和Llama-3三种LLM进行微调实验，评估其采纳与表达复杂意识形态的能力。研究结果表明：微调能显著增强模型对精细意识形态的契合度，而显式提示仅能产生边际改善。这一发现揭示了模型易受隐性意识形态操控的脆弱性，亟需建立更强大的防护机制来规避此类风险。
