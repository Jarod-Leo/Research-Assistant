# DrBERT: Unveiling the Potential of Masked Language Modeling Decoder in BERT pretraining

链接: http://arxiv.org/abs/2401.15861v1

原文摘要:
BERT (Bidirectional Encoder Representations from Transformers) has
revolutionized the field of natural language processing through its exceptional
performance on numerous tasks. Yet, the majority of researchers have mainly
concentrated on enhancements related to the model structure, such as relative
position embedding and more efficient attention mechanisms. Others have delved
into pretraining tricks associated with Masked Language Modeling, including
whole word masking. DeBERTa introduced an enhanced decoder adapted for BERT's
encoder model for pretraining, proving to be highly effective. We argue that
the design and research around enhanced masked language modeling decoders have
been underappreciated. In this paper, we propose several designs of enhanced
decoders and introduce BPDec (BERT Pretraining Decoder), a novel method for
modeling training. Typically, a pretrained BERT model is fine-tuned for
specific Natural Language Understanding (NLU) tasks. In our approach, we
utilize the original BERT model as the encoder, making only changes to the
decoder without altering the encoder. This approach does not necessitate
extensive modifications to the encoder architecture and can be seamlessly
integrated into existing fine-tuning pipelines and services, offering an
efficient and effective enhancement strategy. Compared to other methods, while
we also incur a moderate training cost for the decoder during the pretraining
process, our approach does not introduce additional training costs during the
fine-tuning phase. We test multiple enhanced decoder structures after
pretraining and evaluate their performance on the GLUE tasks and SQuAD tasks.
Our results demonstrate that BPDec, having only undergone subtle refinements to
the model structure during pretraining, significantly enhances model
performance without escalating the finetuning cost, inference time and serving
budget.

中文翻译:
BERT（基于Transformer的双向编码器表示）凭借其在多项任务中的卓越表现，彻底革新了自然语言处理领域。然而，大多数研究者的关注点主要集中在模型结构优化方面，如相对位置嵌入和更高效的注意力机制。另一些学者则深入探索了与掩码语言建模相关的预训练技巧，包括全词掩码策略。DeBERTa通过为BERT编码器模型适配增强型解码器进行预训练，被证明具有显著效果。我们认为针对增强型掩码语言建模解码器的设计与研究长期被低估。本文提出多种增强解码器设计方案，并创新性地引入BPDec（BERT预训练解码器）这一新型建模训练方法。传统上，预训练BERT模型需针对具体自然语言理解（NLU）任务进行微调。我们的方法保持原始BERT编码器架构不变，仅对解码器进行改造。这种策略既无需对编码器结构进行大规模调整，又能无缝对接现有微调流程和服务体系，提供了一种高效且实用的性能增强方案。相较于其他方法，虽然在预训练阶段我们同样需要承担解码器的适度训练成本，但在微调阶段不会引入额外训练开销。我们在预训练后测试了多种增强解码器结构，并在GLUE基准和SQuAD任务上评估其表现。实验结果表明，BPDec仅在预训练阶段对模型结构进行精妙改进，就能在不增加微调成本、推理时间及服务预算的前提下，显著提升模型性能。
