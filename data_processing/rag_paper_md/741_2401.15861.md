# DrBERT: Unveiling the Potential of Masked Language Modeling Decoder in BERT pretraining

链接: http://arxiv.org/abs/2401.15861v1

原文摘要:
BERT (Bidirectional Encoder Representations from Transformers) has
revolutionized the field of natural language processing through its exceptional
performance on numerous tasks. Yet, the majority of researchers have mainly
concentrated on enhancements related to the model structure, such as relative
position embedding and more efficient attention mechanisms. Others have delved
into pretraining tricks associated with Masked Language Modeling, including
whole word masking. DeBERTa introduced an enhanced decoder adapted for BERT's
encoder model for pretraining, proving to be highly effective. We argue that
the design and research around enhanced masked language modeling decoders have
been underappreciated. In this paper, we propose several designs of enhanced
decoders and introduce BPDec (BERT Pretraining Decoder), a novel method for
modeling training. Typically, a pretrained BERT model is fine-tuned for
specific Natural Language Understanding (NLU) tasks. In our approach, we
utilize the original BERT model as the encoder, making only changes to the
decoder without altering the encoder. This approach does not necessitate
extensive modifications to the encoder architecture and can be seamlessly
integrated into existing fine-tuning pipelines and services, offering an
efficient and effective enhancement strategy. Compared to other methods, while
we also incur a moderate training cost for the decoder during the pretraining
process, our approach does not introduce additional training costs during the
fine-tuning phase. We test multiple enhanced decoder structures after
pretraining and evaluate their performance on the GLUE tasks and SQuAD tasks.
Our results demonstrate that BPDec, having only undergone subtle refinements to
the model structure during pretraining, significantly enhances model
performance without escalating the finetuning cost, inference time and serving
budget.

中文翻译:
BERT（基于Transformer的双向编码器表征）凭借其在多项任务中的卓越表现，彻底革新了自然语言处理领域。然而，大多数研究者主要聚焦于模型结构相关的改进，如相对位置嵌入和更高效的注意力机制；另一些学者则深入探索与掩码语言建模相关的预训练技巧，包括全词掩码策略。DeBERTa提出了一种适配BERT编码器模型的增强型解码器用于预训练，被证明极具成效。我们认为，针对增强型掩码语言建模解码器的设计与研究长期未得到足够重视。本文提出多种增强解码器设计方案，并创新性地引入BPDec（BERT预训练解码器）这一建模训练新方法。传统方案通常对预训练后的BERT模型进行下游自然语言理解（NLU）任务的微调，而我们的方法保持原始BERT编码器不变，仅对解码器进行改造。这种策略既无需对编码器架构进行大规模修改，又能无缝对接现有微调流程与服务系统，提供了一种高效便捷的性能增强方案。相较于其他方法，虽然我们在预训练阶段同样需要承担适度的解码器训练成本，但在微调阶段不会引入额外训练开销。我们在预训练后测试了多种增强解码器结构，并在GLUE基准和SQuAD任务上进行性能评估。实验结果表明，BPDec仅在预训练阶段对模型结构进行精微调整，就能在不增加微调成本、推理耗时及服务预算的前提下，显著提升模型性能。
