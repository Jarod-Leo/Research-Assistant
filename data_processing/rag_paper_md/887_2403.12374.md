# Improving Generalizability of Extracting Social Determinants of Health Using Large Language Models through Prompt-tuning

链接: http://arxiv.org/abs/2403.12374v1

原文摘要:
The progress in natural language processing (NLP) using large language models
(LLMs) has greatly improved patient information extraction from clinical
narratives. However, most methods based on the fine-tuning strategy have
limited transfer learning ability for cross-domain applications. This study
proposed a novel approach that employs a soft prompt-based learning
architecture, which introduces trainable prompts to guide LLMs toward desired
outputs. We examined two types of LLM architectures, including encoder-only
GatorTron and decoder-only GatorTronGPT, and evaluated their performance for
the extraction of social determinants of health (SDoH) using a
cross-institution dataset from the 2022 n2c2 challenge and a cross-disease
dataset from the University of Florida (UF) Health. The results show that
decoder-only LLMs with prompt tuning achieved better performance in
cross-domain applications. GatorTronGPT achieved the best F1 scores for both
datasets, outperforming traditional fine-tuned GatorTron by 8.9% and 21.8% in a
cross-institution setting, and 5.5% and 14.5% in a cross-disease setting.

中文翻译:
以下是符合要求的学术中文翻译：

基于大语言模型的自然语言处理技术进展显著提升了从临床叙述文本中提取患者信息的能力。然而，大多数基于微调策略的方法在跨领域应用时存在迁移学习能力受限的问题。本研究提出了一种创新的软提示学习架构，通过引入可训练提示词来引导大语言模型生成目标输出。我们评估了两种大语言模型架构（仅编码器的GatorTron和仅解码器的GatorTronGPT）在健康社会决定因素（SDoH）提取任务中的表现，使用的测试数据集包括：2022年n2c2挑战赛的跨机构数据集和佛罗里达大学健康中心的跨疾病数据集。实验结果表明，采用提示调优的仅解码器大语言模型在跨领域应用中表现更优。GatorTronGPT在两个数据集上均取得最佳F1值：在跨机构场景中分别以8.9%和21.8%的优势超越传统微调版GatorTron，在跨疾病场景中分别领先5.5%和14.5%。

（翻译严格遵循以下原则：
1. 专业术语准确对应（如soft prompt→软提示，cross-domain→跨领域）
2. 长句按中文习惯切分为短句群
3. 被动语态转换为主动表述（如"were evaluated"→"评估了"）
4. 数据呈现方式符合中文科技论文规范
5. 保留所有关键技术细节和量化结果
6. 使用"本研究""实验结果表明"等学术用语）
