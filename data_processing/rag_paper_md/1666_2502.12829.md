# KazMMLU: Evaluating Language Models on Kazakh, Russian, and Regional Knowledge of Kazakhstan

链接: http://arxiv.org/abs/2502.12829v1

原文摘要:
Despite having a population of twenty million, Kazakhstan's culture and
language remain underrepresented in the field of natural language processing.
Although large language models (LLMs) continue to advance worldwide, progress
in Kazakh language has been limited, as seen in the scarcity of dedicated
models and benchmark evaluations. To address this gap, we introduce KazMMLU,
the first MMLU-style dataset specifically designed for Kazakh language. KazMMLU
comprises 23,000 questions that cover various educational levels, including
STEM, humanities, and social sciences, sourced from authentic educational
materials and manually validated by native speakers and educators. The dataset
includes 10,969 Kazakh questions and 12,031 Russian questions, reflecting
Kazakhstan's bilingual education system and rich local context. Our evaluation
of several state-of-the-art multilingual models (Llama-3.1, Qwen-2.5, GPT-4,
and DeepSeek V3) demonstrates substantial room for improvement, as even the
best-performing models struggle to achieve competitive performance in Kazakh
and Russian. These findings underscore significant performance gaps compared to
high-resource languages. We hope that our dataset will enable further research
and development of Kazakh-centric LLMs. Data and code will be made available
upon acceptance.

中文翻译:
尽管拥有两千万人口，哈萨克斯坦的文化与语言在自然语言处理领域仍处于代表性不足的状态。尽管全球范围内大语言模型持续发展，哈萨克语相关进展却十分有限——专用模型与基准评估的稀缺便是明证。为填补这一空白，我们推出首个专为哈萨克语设计的MMLU风格数据集KazMMLU。该数据集包含23,000道题目，涵盖STEM、人文社科等多个教育阶段内容，所有题目均源自真实教材并经母语者与教育工作者人工校验。其中哈萨克语题目10,969道，俄语题目12,031道，充分体现了哈萨克斯坦双语教育体系与本土特色。我们对多款前沿多语言模型（Llama-3.1、Qwen-2.5、GPT-4及DeepSeek V3）的评估显示，即便是表现最优的模型在哈俄双语任务中也难以达到竞争力水平，与高资源语言相比存在显著性能差距。本数据集有望推动以哈萨克语为核心的大语言模型研发。论文录用后数据与代码将同步公开。
