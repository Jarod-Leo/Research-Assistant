# Training and Evaluating Language Models with Template-based Data Generation

链接: http://arxiv.org/abs/2411.18104v1

原文摘要:
The rapid advancement of large language models (LLMs) such as GPT-3, PaLM,
and Llama has significantly transformed natural language processing, showcasing
remarkable capabilities in understanding and generating language. However,
these models often struggle with tasks requiring complex reasoning,
particularly in mathematical problem-solving, due in part to the scarcity of
large-scale, high-quality, domain-specific datasets necessary for training
sophisticated reasoning abilities. To address this limitation, we introduce
Template-based Data Generation (TDG), a novel approach that leverages LLMs
(GPT-4) to automatically generate parameterized meta-templates, which are then
used to synthesize a vast array of high-quality problems and solutions.
Leveraging TDG, we create TemplateMath Part I: TemplateGSM, a dataset
comprising over 7 million synthetically generated grade school math
problems--each accompanied by code-based and natural language solutions--with
the potential to generate an effectively unlimited number more. This dataset
alleviates the scarcity of large-scale mathematical datasets and serves as a
valuable resource for pre-training, fine-tuning, and evaluating LLMs in
mathematical reasoning. Our method not only enables the generation of virtually
infinite data but also elevates data augmentation to a new level by using GPT-4
for meta-template generation, ensuring diverse and high-quality problem
structures. The TemplateMath Part I: TemplateGSM dataset is publicly available
at https://huggingface.co/datasets/math-ai/TemplateGSM. The code is available
at https://github.com/iiis-ai/TemplateMath.

中文翻译:
以GPT-3、PaLM和Llama为代表的大语言模型（LLMs）快速发展，深刻改变了自然语言处理领域，展现出卓越的语言理解和生成能力。然而，这些模型在需要复杂推理的任务（尤其是数学问题求解）中往往表现欠佳，部分原因在于缺乏训练高级推理能力所需的大规模、高质量领域专用数据集。为突破这一局限，我们提出基于模板的数据生成（TDG）方法——通过利用LLMs（GPT-4）自动生成参数化元模板，进而合成海量高质量问题及对应解法。基于TDG技术，我们构建了TemplateMath Part I: TemplateGSM数据集，包含超过700万道合成生成的小学数学题（每道题均配有代码与自然语言双解法），且具备近乎无限的扩展潜力。该数据集有效缓解了大规模数学数据稀缺问题，为数学推理领域的模型预训练、微调和评估提供了宝贵资源。我们的方法不仅实现了理论上无限的数据生成，更通过GPT-4驱动的元模板生成技术将数据增强提升至新高度，确保问题结构的多样性与高质量。TemplateMath Part I: TemplateGSM数据集已公开于https://huggingface.co/datasets/math-ai/TemplateGSM，相关代码发布于https://github.com/iiis-ai/TemplateMath。
