# Training and Evaluating Language Models with Template-based Data Generation

链接: http://arxiv.org/abs/2411.18104v1

原文摘要:
The rapid advancement of large language models (LLMs) such as GPT-3, PaLM,
and Llama has significantly transformed natural language processing, showcasing
remarkable capabilities in understanding and generating language. However,
these models often struggle with tasks requiring complex reasoning,
particularly in mathematical problem-solving, due in part to the scarcity of
large-scale, high-quality, domain-specific datasets necessary for training
sophisticated reasoning abilities. To address this limitation, we introduce
Template-based Data Generation (TDG), a novel approach that leverages LLMs
(GPT-4) to automatically generate parameterized meta-templates, which are then
used to synthesize a vast array of high-quality problems and solutions.
Leveraging TDG, we create TemplateMath Part I: TemplateGSM, a dataset
comprising over 7 million synthetically generated grade school math
problems--each accompanied by code-based and natural language solutions--with
the potential to generate an effectively unlimited number more. This dataset
alleviates the scarcity of large-scale mathematical datasets and serves as a
valuable resource for pre-training, fine-tuning, and evaluating LLMs in
mathematical reasoning. Our method not only enables the generation of virtually
infinite data but also elevates data augmentation to a new level by using GPT-4
for meta-template generation, ensuring diverse and high-quality problem
structures. The TemplateMath Part I: TemplateGSM dataset is publicly available
at https://huggingface.co/datasets/math-ai/TemplateGSM. The code is available
at https://github.com/iiis-ai/TemplateMath.

中文翻译:
以下是符合要求的学术化中文翻译：

大型语言模型（如GPT-3、PaLM和Llama）的快速发展显著改变了自然语言处理领域，展现出卓越的语言理解与生成能力。然而，这些模型在需要复杂推理的任务（尤其是数学问题求解）中往往表现欠佳，部分原因在于缺乏训练高级推理能力所需的大规模、高质量领域特定数据集。为突破这一局限，我们提出基于模板的数据生成方法（TDG）：该创新方法利用大型语言模型（GPT-4）自动生成参数化元模板，进而合成海量高质量问题及解决方案。基于TDG框架，我们构建了TemplateMath Part I: TemplateGSM数据集——包含超过700万条合成生成的小学数学问题（每个问题均配有代码与自然语言双解），且具备无限扩展潜力。该数据集有效缓解了大规模数学数据稀缺问题，为数学推理领域的模型预训练、微调和评估提供了宝贵资源。我们的方法不仅实现了理论上无限的数据生成，更通过GPT-4驱动的元模板生成技术将数据增强提升至新高度，确保问题结构的多样性与高质量。TemplateMath Part I: TemplateGSM数据集已开源发布于https://huggingface.co/datasets/math-ai/TemplateGSM，相关代码可在https://github.com/iiis-ai/TemplateMath获取。

（说明：翻译严格遵循了以下学术规范：
1. 专业术语统一（如"parameterized meta-templates"译为"参数化元模板"）
2. 被动语态转化（英文被动句转换为中文主动表述）
3. 长句拆分重组（如将原文复合长句分解为符合中文表达习惯的短句）
4. 概念准确传达（如"synthetically generated"译为"合成生成"而非直译"人工合成"）
5. 学术用语规范（如"fine-tuning"统一译为"微调"而非"调优"））
