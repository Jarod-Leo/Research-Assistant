# TrafficSafetyGPT: Tuning a Pre-trained Large Language Model to a Domain-Specific Expert in Transportation Safety

链接: http://arxiv.org/abs/2307.15311v1

原文摘要:
Large Language Models (LLMs) have shown remarkable effectiveness in various
general-domain natural language processing (NLP) tasks. However, their
performance in transportation safety domain tasks has been suboptimal,
primarily attributed to the requirement for specialized transportation safety
expertise in generating accurate responses [1]. To address this challenge, we
introduce TrafficSafetyGPT, a novel LLAMA-based model, which has undergone
supervised fine-tuning using TrafficSafety-2K dataset which has human labels
from government produced guiding books and ChatGPT-generated instruction-output
pairs. Our proposed TrafficSafetyGPT model and TrafficSafety-2K train dataset
are accessible at https://github.com/ozheng1993/TrafficSafetyGPT.

中文翻译:
大型语言模型（LLMs）在各类通用领域自然语言处理（NLP）任务中展现出卓越效能，但在交通安全领域的应用表现却不尽如人意，主要归因于该领域需要专业安全知识才能生成准确响应[1]。为解决这一挑战，我们推出了TrafficSafetyGPT——基于LLAMA架构的创新模型，该模型通过使用TrafficSafety-2K数据集进行监督微调，该数据集包含政府指导手册的人工标注数据及ChatGPT生成的指令-输出对。我们研发的TrafficSafetyGPT模型及TrafficSafety-2K训练数据集已开源，访问地址为：https://github.com/ozheng1993/TrafficSafetyGPT。
