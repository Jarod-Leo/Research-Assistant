# Big Little Transformer Decoder

链接: http://arxiv.org/abs/2302.07863v1

原文摘要:
The recent emergence of Large Language Models based on the Transformer
architecture has enabled dramatic advancements in the field of Natural Language
Processing. However, these models have long inference latency, which limits
their deployment and makes them prohibitively expensive for various real-time
applications. The inference latency is further exacerbated by autoregressive
generative tasks, as models need to run iteratively to generate tokens
sequentially without leveraging token-level parallelization. To address this,
we propose Big Little Decoder (BiLD), a framework that can improve inference
efficiency and latency for a wide range of text generation applications. The
BiLD framework contains two models with different sizes that collaboratively
generate text. The small model runs autoregressively to generate text with a
low inference cost, and the large model is only invoked occasionally to refine
the small model's inaccurate predictions in a non-autoregressive manner. To
coordinate the small and large models, BiLD introduces two simple yet effective
policies: (1) the fallback policy that determines when to hand control over to
the large model; and (2) the rollback policy that determines when the large
model needs to correct the small model's inaccurate predictions. To evaluate
our framework across different tasks and models, we apply BiLD to various text
generation scenarios encompassing machine translation on IWSLT 2017 De-En and
WMT 2014 De-En, and summarization on XSUM and CNN/DailyMail. On an NVIDIA T4
GPU, our framework achieves a speedup of up to 2.12x speedup with minimal
generation quality degradation. Furthermore, our framework is fully
plug-and-play and can be applied without any modifications in the training
process or model architecture. Our code is open-sourced

中文翻译:
近期，基于Transformer架构的大语言模型的出现，显著推动了自然语言处理领域的进步。然而，这些模型存在推理延迟高的问题，不仅限制了其部署范围，更使得各类实时应用的成本变得难以承受。自回归生成任务进一步加剧了推理延迟，因为模型需要迭代运行以逐个生成词元，无法利用词元级并行化。针对这一挑战，我们提出了大-小解码器框架（BiLD），该框架能够提升多种文本生成应用的推理效率并降低延迟。

BiLD框架包含两个协同工作的不同规模模型：小型模型以自回归方式运行，通过低推理成本生成文本；大型模型仅被偶尔调用，以非自回归方式修正小型模型的不准确预测。为协调两个模型的工作，BiLD引入了两项简单而有效的策略：（1）回退策略——决定何时将控制权移交大型模型；（2）回滚策略——判定大型模型何时需要纠正小型模型的错误预测。

为验证框架的通用性，我们将BiLD应用于IWSLT 2017德英翻译、WMT 2014德英翻译、XSUM摘要生成和CNN/DailyMail摘要生成等多个文本生成场景。在NVIDIA T4 GPU上的实验表明，该框架在生成质量几乎无损的情况下，最高可实现2.12倍的加速效果。此外，BiLD框架具备完全即插即用的特性，无需对训练过程或模型架构进行任何修改。相关代码已开源。
