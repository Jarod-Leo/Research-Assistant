# Mitigating Social Bias in Large Language Models: A Multi-Objective Approach within a Multi-Agent Framework

链接: http://arxiv.org/abs/2412.15504v1

原文摘要:
Natural language processing (NLP) has seen remarkable advancements with the
development of large language models (LLMs). Despite these advancements, LLMs
often produce socially biased outputs. Recent studies have mainly addressed
this problem by prompting LLMs to behave ethically, but this approach results
in unacceptable performance degradation. In this paper, we propose a
multi-objective approach within a multi-agent framework (MOMA) to mitigate
social bias in LLMs without significantly compromising their performance. The
key idea of MOMA involves deploying multiple agents to perform causal
interventions on bias-related contents of the input questions, breaking the
shortcut connection between these contents and the corresponding answers.
Unlike traditional debiasing techniques leading to performance degradation,
MOMA substantially reduces bias while maintaining accuracy in downstream tasks.
Our experiments conducted on two datasets and two models demonstrate that MOMA
reduces bias scores by up to 87.7%, with only a marginal performance
degradation of up to 6.8% in the BBQ dataset. Additionally, it significantly
enhances the multi-objective metric icat in the StereoSet dataset by up to
58.1%. Code will be made available at https://github.com/Cortantse/MOMA.

中文翻译:
随着大语言模型（LLMs）的发展，自然语言处理（NLP）领域取得了显著进展。然而，这些模型常会生成带有社会偏见的输出。近期研究主要通过提示LLMs遵循伦理准则来解决该问题，但这种方法会导致模型性能出现难以接受的下降。本文提出一种多智能体框架下的多目标优化方法（MOMA），可在不明显影响模型性能的前提下减轻LLMs的社会偏见。MOMA的核心思想是部署多个智能体对输入问题中涉及偏见的内容进行因果干预，切断这些内容与对应答案之间的捷径关联。与传统去偏技术导致性能下降不同，MOMA在保持下游任务准确率的同时显著降低了偏见水平。我们在两个数据集和两种模型上的实验表明：MOMA在BBQ数据集上将偏见分数最高降低87.7%，性能损失仅6.8%；在StereoSet数据集上使多目标评价指标icat最高提升58.1%。代码已发布于https://github.com/Cortantse/MOMA。
