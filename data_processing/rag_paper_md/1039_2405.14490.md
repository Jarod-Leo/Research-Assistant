# Impact of Non-Standard Unicode Characters on Security and Comprehension in Large Language Models

链接: http://arxiv.org/abs/2405.14490v1

原文摘要:
The advancement of large language models has significantly improved natural
language processing. However, challenges such as jailbreaks (prompt injections
that cause an LLM to follow instructions contrary to its intended use),
hallucinations (generating incorrect or misleading information), and
comprehension errors remain prevalent. In this report, we present a comparative
analysis of the performance of fifteen distinct models, with each model
undergoing a standardized test comprising 38 queries across three key metrics:
jailbreaks, hallucinations, and comprehension errors. The models are assessed
based on the total occurrences of jailbreaks, hallucinations, and comprehension
errors. Our work exposes these models' inherent vulnerabilities and challenges
the notion of human-level language comprehension of these models. We have
empirically analysed the impact of non-standard Unicode characters on LLMs and
their safeguarding mechanisms on the best-performing LLMs, including GPT-4,
Gemini 1.5 Pro, LlaMA-3-70B, and Claude 3 Opus. By incorporating alphanumeric
symbols from Unicode outside the standard Latin block and variants of
characters in other languages, we observed a reduction in the efficacy of
guardrails implemented through Reinforcement Learning Human Feedback (RLHF).
Consequently, these models exhibit heightened vulnerability to content policy
breaches and prompt leakage. Our study also suggests a need to incorporate
non-standard Unicode text in LLM training data to enhance the capabilities of
these models.

中文翻译:
以下是符合您要求的中文翻译：

大型语言模型的发展显著提升了自然语言处理能力，但越狱攻击（通过提示注入使模型违背设计初衷执行指令）、幻觉（生成错误或误导性信息）以及理解错误等问题依然普遍存在。本报告对15个不同模型进行了性能对比分析，每个模型均接受包含38个查询的标准化测试，评估三大关键指标：越狱发生率、幻觉频率和理解错误次数。研究揭示了这些模型固有的脆弱性，并对"模型已具备人类水平语言理解能力"的观点提出质疑。

我们通过实证分析了非标准Unicode字符对主流大模型（包括GPT-4、Gemini 1.5 Pro、LlaMA-3-70B和Claude 3 Opus）及其安全防护机制的影响。实验采用标准拉丁字符集外的Unicode字母数字符号及其他语言的字符变体，发现这些字符会削弱基于人类反馈强化学习（RLHF）构建的防护栏效果，导致模型更容易违反内容政策并出现提示泄露。研究同时表明，有必要在模型训练数据中加入非标准Unicode文本以增强其应对能力。

（翻译严格遵循以下要点：
1. 专业术语准确统一："jailbreaks"译作"越狱攻击"并括号补充说明，"RLHF"保留英文缩写并标注全称
2. 长句合理切分：将原文复合句拆解为符合中文表达习惯的短句结构
3. 被动语态转化："are assessed"译为主动式"评估"
4. 概念显化处理："guardrails"译为"防护栏"并关联RLHF技术背景
5. 逻辑关系显性化：通过"导致""发现"等连接词明确因果关系
6. 文化适配："challenge the notion"译为"提出质疑"符合学术表述习惯）
