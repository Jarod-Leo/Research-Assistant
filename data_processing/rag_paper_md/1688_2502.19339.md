# Evaluating LLMs and Pre-trained Models for Text Summarization Across Diverse Datasets

链接: http://arxiv.org/abs/2502.19339v1

原文摘要:
Text summarization plays a crucial role in natural language processing by
condensing large volumes of text into concise and coherent summaries. As
digital content continues to grow rapidly and the demand for effective
information retrieval increases, text summarization has become a focal point of
research in recent years. This study offers a thorough evaluation of four
leading pre-trained and open-source large language models: BART, FLAN-T5,
LLaMA-3-8B, and Gemma-7B, across five diverse datasets CNN/DM, Gigaword, News
Summary, XSum, and BBC News. The evaluation employs widely recognized automatic
metrics, including ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, and METEOR, to assess
the models' capabilities in generating coherent and informative summaries. The
results reveal the comparative strengths and limitations of these models in
processing various text types.

中文翻译:
文本摘要技术在自然语言处理领域具有关键作用，其通过将海量文本压缩为简洁连贯的摘要来满足信息高效获取的需求。随着数字内容持续快速增长以及有效信息检索需求的日益提升，文本摘要已成为近年来的研究热点。本研究对BART、FLAN-T5、LLaMA-3-8B和Gemma-7B四种主流预训练开源大语言模型进行了全面评估，测试范围涵盖CNN/DM、Gigaword、News Summary、XSum和BBC News五个多样化数据集。评估采用ROUGE-1、ROUGE-2、ROUGE-L、BERTScore和METEOR等广泛认可的自动指标，以衡量模型生成连贯且信息丰富摘要的能力。实验结果揭示了这些模型在处理不同类型文本时的相对优势与局限性。
