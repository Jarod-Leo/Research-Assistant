# Can Local Representation Alignment RNNs Solve Temporal Tasks?

链接: http://arxiv.org/abs/2504.13531v1

原文摘要:
Recurrent Neural Networks (RNNs) are commonly used for real-time processing,
streaming data, and cases where the amount of training samples is limited.
Backpropagation Through Time (BPTT) is the predominant algorithm for training
RNNs; however, it is frequently criticized for being prone to exploding and
vanishing gradients and being biologically implausible. In this paper, we
present and evaluate a target propagation-based method for RNNs, which uses
local updates and seeks to reduce the said instabilities. Having stable RNN
models increases their practical use in a wide range of fields such as natural
language processing, time-series forecasting, anomaly detection, control
systems, and robotics.
  The proposed solution uses local representation alignment (LRA). We
thoroughly analyze the performance of this method, experiment with
normalization and different local error functions, and invalidate certain
assumptions about the behavior of this type of learning. Namely, we demonstrate
that despite the decomposition of the network into sub-graphs, the model still
suffers from vanishing gradients. We also show that gradient clipping as
proposed in LRA has little to no effect on network performance. This results in
an LRA RNN model that is very difficult to train due to vanishing gradients. We
address this by introducing gradient regularization in the direction of the
update and demonstrate that this modification promotes gradient flow and
meaningfully impacts convergence. We compare and discuss the performance of the
algorithm, and we show that the regularized LRA RNN considerably outperforms
the unregularized version on three landmark tasks: temporal order, 3-bit
temporal order, and random permutation.

中文翻译:
循环神经网络（RNN）因其适用于实时处理、流数据及训练样本有限场景而广泛应用。时间反向传播（BPTT）是RNN训练的主流算法，但常因梯度爆炸/消失问题及生物学不合理性受到质疑。本文提出并评估了一种基于目标传播的RNN训练方法，该方法采用局部更新机制以缓解上述不稳定性。稳定的RNN模型可显著提升其在自然语言处理、时间序列预测、异常检测、控制系统及机器人等领域的实用价值。

该方案采用局部表示对齐（LRA）机制。我们深入分析了该方法的性能，测试了归一化处理与不同局部误差函数的效果，并验证了此类学习行为中的某些假设不成立。具体而言，研究发现：尽管网络被分解为子图结构，模型仍存在梯度消失现象；LRA中提出的梯度裁剪策略对网络性能影响甚微。这些发现表明标准LRA-RNN模型因梯度消失问题导致训练极其困难。为此，我们引入更新方向上的梯度正则化，证实该改进能有效促进梯度流动并显著改善收敛性。通过时序排序、3位时序排序和随机排列三项基准任务的对比实验，证明正则化LRA-RNN模型性能显著优于未正则化版本。
