# Towards Cross-Lingual LLM Evaluation for European Languages

链接: http://arxiv.org/abs/2410.08928v1

原文摘要:
The rise of Large Language Models (LLMs) has revolutionized natural language
processing across numerous languages and tasks. However, evaluating LLM
performance in a consistent and meaningful way across multiple European
languages remains challenging, especially due to the scarcity of
language-parallel multilingual benchmarks. We introduce a multilingual
evaluation approach tailored for European languages. We employ translated
versions of five widely-used benchmarks to assess the capabilities of 40 LLMs
across 21 European languages. Our contributions include examining the
effectiveness of translated benchmarks, assessing the impact of different
translation services, and offering a multilingual evaluation framework for LLMs
that includes newly created datasets: EU20-MMLU, EU20-HellaSwag, EU20-ARC,
EU20-TruthfulQA, and EU20-GSM8K. The benchmarks and results are made publicly
available to encourage further research in multilingual LLM evaluation.

中文翻译:
大型语言模型（LLMs）的崛起彻底改变了多种语言及任务的自然语言处理格局。然而，如何在欧洲多语言环境下以一致且有效的方式评估LLM性能仍面临挑战，尤其是缺乏语言平行的多语言基准测试集。为此，我们提出了一种专为欧洲语言定制的多语言评估方法：通过五个广泛使用基准测试的翻译版本，系统评估了40个LLM在21种欧洲语言中的表现。研究贡献包括：验证翻译版基准测试的有效性、分析不同翻译服务的影响，并构建包含新创建数据集（EU20-MMLU、EU20-HellaSwag、EU20-ARC、EU20-TruthfulQA和EU20-GSM8K）的多语言评估框架。所有基准数据及评估结果均已公开，以推动多语言LLM评估领域的深入研究。
