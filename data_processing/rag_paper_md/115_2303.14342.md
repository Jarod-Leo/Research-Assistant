# An Analysis of GPT-3's Performance in Grammatical Error Correction

链接: http://arxiv.org/abs/2303.14342v1

原文摘要:
GPT-3 and GPT-4 models are powerful, achieving high performance on a variety
of Natural Language Processing tasks. However, there is a relative lack of
detailed published analysis of their performance on the task of grammatical
error correction (GEC). To address this, we perform experiments testing the
capabilities of a GPT-3.5 model (text-davinci-003) and a GPT-4 model
(gpt-4-0314) on major GEC benchmarks. We compare the performance of different
prompts in both zero-shot and few-shot settings, analyzing intriguing or
problematic outputs encountered with different prompt formats. We report the
performance of our best prompt on the BEA-2019 and JFLEG datasets, finding that
the GPT models can perform well in a sentence-level revision setting, with
GPT-4 achieving a new high score on the JFLEG benchmark. Through human
evaluation experiments, we compare the GPT models' corrections to source, human
reference, and baseline GEC system sentences and observe differences in editing
strategies and how they are scored by human raters.

中文翻译:
GPT-3与GPT-4模型展现出强大的性能，在多种自然语言处理任务中均取得优异表现。然而针对语法错误修正（GEC）任务，目前缺乏对其性能的详细公开分析。为此，我们通过在主流GEC基准测试上对GPT-3.5模型（text-davinci-003）和GPT-4模型（gpt-4-0314）进行实验评估。研究对比了零样本和小样本设置下不同提示模板的效果，分析了各类提示格式产生的有趣或问题性输出。我们在BEA-2019和JFLEG数据集上报告了最优提示模板的性能表现，发现GPT模型在句子级修正场景中表现突出，其中GPT-4在JFLEG基准测试中创造了新的最高分。通过人工评估实验，我们将GPT模型的修正结果与原始句、人工参考修正及基线GEC系统输出进行对比，观察到不同模型在编辑策略上的差异及其对应的人工评分变化。
