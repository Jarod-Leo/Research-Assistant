# Learn to Disguise: Avoid Refusal Responses in LLM's Defense via a Multi-agent Attacker-Disguiser Game

链接: http://arxiv.org/abs/2404.02532v1

原文摘要:
With the enhanced performance of large models on natural language processing
tasks, potential moral and ethical issues of large models arise. There exist
malicious attackers who induce large models to jailbreak and generate
information containing illegal, privacy-invasive information through techniques
such as prompt engineering. As a result, large models counter malicious
attackers' attacks using techniques such as safety alignment. However, the
strong defense mechanism of the large model through rejection replies is easily
identified by attackers and used to strengthen attackers' capabilities. In this
paper, we propose a multi-agent attacker-disguiser game approach to achieve a
weak defense mechanism that allows the large model to both safely reply to the
attacker and hide the defense intent. First, we construct a multi-agent
framework to simulate attack and defense scenarios, playing different roles to
be responsible for attack, disguise, safety evaluation, and disguise evaluation
tasks. After that, we design attack and disguise game algorithms to optimize
the game strategies of the attacker and the disguiser and use the curriculum
learning process to strengthen the capabilities of the agents. The experiments
verify that the method in this paper is more effective in strengthening the
model's ability to disguise the defense intent compared with other methods.
Moreover, our approach can adapt any black-box large model to assist the model
in defense and does not suffer from model version iterations.

中文翻译:
随着大模型在自然语言处理任务上的性能提升，其潜在的道德伦理问题也随之显现。存在恶意攻击者通过提示工程等技术诱导大模型越狱，生成包含违法、侵犯隐私等信息。为此，大模型采用安全对齐等技术对抗恶意攻击。然而大模型通过拒绝回答形成的强防御机制易被攻击者识别，并用于增强攻击能力。本文提出一种多智能体攻击者-伪装者博弈方法，实现既能安全回复攻击者又能隐藏防御意图的弱防御机制。首先构建多智能体框架模拟攻防场景，通过扮演不同角色分别负责攻击、伪装、安全性评估和伪装性评估任务；随后设计攻击与伪装博弈算法优化攻击者与伪装者的博弈策略，并采用课程学习过程强化智能体能力。实验验证本文方法相比其他方法能更有效增强模型隐藏防御意图的能力。此外，我们的方案可适配任意黑盒大模型辅助防御，且不受模型版本迭代影响。
