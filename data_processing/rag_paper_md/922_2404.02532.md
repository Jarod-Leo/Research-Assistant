# Learn to Disguise: Avoid Refusal Responses in LLM's Defense via a Multi-agent Attacker-Disguiser Game

链接: http://arxiv.org/abs/2404.02532v1

原文摘要:
With the enhanced performance of large models on natural language processing
tasks, potential moral and ethical issues of large models arise. There exist
malicious attackers who induce large models to jailbreak and generate
information containing illegal, privacy-invasive information through techniques
such as prompt engineering. As a result, large models counter malicious
attackers' attacks using techniques such as safety alignment. However, the
strong defense mechanism of the large model through rejection replies is easily
identified by attackers and used to strengthen attackers' capabilities. In this
paper, we propose a multi-agent attacker-disguiser game approach to achieve a
weak defense mechanism that allows the large model to both safely reply to the
attacker and hide the defense intent. First, we construct a multi-agent
framework to simulate attack and defense scenarios, playing different roles to
be responsible for attack, disguise, safety evaluation, and disguise evaluation
tasks. After that, we design attack and disguise game algorithms to optimize
the game strategies of the attacker and the disguiser and use the curriculum
learning process to strengthen the capabilities of the agents. The experiments
verify that the method in this paper is more effective in strengthening the
model's ability to disguise the defense intent compared with other methods.
Moreover, our approach can adapt any black-box large model to assist the model
in defense and does not suffer from model version iterations.

中文翻译:
随着大模型在自然语言处理任务上的性能增强，大模型潜在的道德伦理问题随之而来。存在恶意攻击者通过提示工程等技术诱导大模型越狱，生成包含违法、侵犯隐私等信息。对此，大模型使用安全对齐等技术对抗恶意攻击者的攻击。然而，大模型通过拒绝回复的强防御机制容易被攻击者识别，并用于增强攻击者的能力。本文提出一种多智能体攻击者-伪装者博弈方法，实现一种弱防御机制，使得大模型既能够安全回复攻击者，又能够隐藏防御意图。首先，构建多智能体框架模拟攻防场景，扮演不同角色负责攻击、伪装、安全评估和伪装评估任务。之后，设计攻击和伪装博弈算法优化攻击者和伪装者的博弈策略，并使用课程学习过程增强智能体的能力。实验验证本文方法与其他方法相比，在增强模型伪装防御意图能力上更加有效。此外，本文方法能够适配任意黑盒大模型辅助模型防御，且不受模型版本迭代影响。
