# MiMu: Mitigating Multiple Shortcut Learning Behavior of Transformers

链接: http://arxiv.org/abs/2504.10551v1

原文摘要:
Empirical Risk Minimization (ERM) models often rely on spurious correlations
between features and labels during the learning process, leading to shortcut
learning behavior that undermines robustness generalization performance.
Current research mainly targets identifying or mitigating a single shortcut;
however, in real-world scenarios, cues within the data are diverse and unknown.
In empirical studies, we reveal that the models rely to varying extents on
different shortcuts. Compared to weak shortcuts, models depend more heavily on
strong shortcuts, resulting in their poor generalization ability. To address
these challenges, we propose MiMu, a novel method integrated with
Transformer-based ERMs designed to Mitigate Multiple shortcut learning
behavior, which incorporates self-calibration strategy and self-improvement
strategy. In the source model, we preliminarily propose the self-calibration
strategy to prevent the model from relying on shortcuts and make overconfident
predictions. Then, we further design self-improvement strategy in target model
to reduce the reliance on multiple shortcuts. The random mask strategy involves
randomly masking partial attention positions to diversify the focus of target
model other than concentrating on a fixed region. Meanwhile, the adaptive
attention alignment module facilitates the alignment of attention weights to
the calibrated source model, without the need for post-hoc attention maps or
supervision. Finally, extensive experiments conducted on Natural Language
Processing (NLP) and Computer Vision (CV) demonstrate the effectiveness of MiMu
in improving robustness generalization abilities.

中文翻译:
经验风险最小化（ERM）模型在学习过程中常依赖特征与标签间的伪相关性，导致捷径学习行为，削弱了模型的鲁棒泛化性能。现有研究多聚焦于识别或消除单一捷径，然而现实场景中数据内部的关联线索具有多样性与未知性。实证研究表明，模型对不同捷径的依赖程度存在差异——相较于弱捷径，模型更易受强捷径主导，从而导致泛化能力显著下降。

针对上述挑战，本文提出MiMu方法，这是一种与基于Transformer的ERM模型协同工作的创新框架，旨在通过双重策略缓解多重捷径学习问题。首先，在源模型中引入自校准策略，通过抑制模型对捷径的依赖并修正其过度自信的预测倾向；其次，在目标模型中设计自改进策略：采用随机掩码机制动态遮蔽部分注意力区域，促使模型关注多样化特征而非固定模式；同时开发自适应注意力对齐模块，无需依赖后验注意力图或监督信号，即可实现目标模型与校准后源模型的注意力权重对齐。在自然语言处理（NLP）和计算机视觉（CV）领域的广泛实验验证了MiMu在提升模型鲁棒泛化能力方面的有效性。
