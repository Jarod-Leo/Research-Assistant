# Empirical Analysis of Efficient Fine-Tuning Methods for Large Pre-Trained Language Models

链接: http://arxiv.org/abs/2401.04051v1

原文摘要:
Fine-tuning large pre-trained language models for downstream tasks remains a
critical challenge in natural language processing. This paper presents an
empirical analysis comparing two efficient fine-tuning methods - BitFit and
adapter modules - to standard full model fine-tuning. Experiments conducted on
GLUE benchmark datasets (MRPC, COLA, STS-B) reveal several key insights. The
BitFit approach, which trains only bias terms and task heads, matches full
fine-tuning performance across varying amounts of training data and time
constraints. It demonstrates remarkable stability even with only 30\% of data,
outperforming full fine-tuning at intermediate data levels. Adapter modules
exhibit high variability, with inconsistent gains over default models. The
findings indicate BitFit offers an attractive balance between performance and
parameter efficiency. Our work provides valuable perspectives on model tuning,
emphasizing robustness and highlighting BitFit as a promising alternative for
resource-constrained or streaming task settings. The analysis offers actionable
guidelines for efficient adaptation of large pre-trained models, while
illustrating open challenges in stabilizing techniques like adapter modules.

中文翻译:
以下是符合您要求的学术中文翻译：

针对下游任务微调大型预训练语言模型仍是自然语言处理领域的关键挑战。本文通过实证分析对比了两种高效微调方法（BitFit与适配器模块）与标准全模型微调的性能差异。基于GLUE基准数据集（MRPC、COLA、STS-B）的实验揭示了若干重要发现：仅训练偏置项和任务头的BitFit方法，在不同训练数据量和时间约束下均能达到与全微调相当的性能；该方法展现出显著稳定性——即使仅使用30%数据时仍优于中等数据规模下的全微调表现。适配器模块则呈现较大波动性，其相对于默认模型的性能提升并不稳定。研究结果表明，BitFit在性能与参数效率之间实现了理想平衡。本工作为模型调优提供了重要视角：在强调方法鲁棒性的同时，凸显BitFit作为资源受限或流式任务场景的理想替代方案。该分析不仅为大型预训练模型的高效适配提供了可行指南，同时揭示了适配器模块等技术的稳定性优化这一开放挑战。

（翻译严格遵循以下原则：
1. 专业术语准确统一（如fine-tuning=微调，adapter modules=适配器模块）
2. 被动语态转换为中文主动表述（"experiments reveal"译为"实验揭示"）
3. 长难句合理切分（将原文复合句拆解为符合中文表达习惯的短句）
4. 学术风格保持（使用"呈现""凸显""揭示"等正式措辞）
5. 数据表述精确（30%严格保留数字符号）
6. 机构名称保留英文缩写（GLUE基准数据集））
