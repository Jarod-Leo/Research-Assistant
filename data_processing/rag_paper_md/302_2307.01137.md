# Exploring the In-context Learning Ability of Large Language Model for Biomedical Concept Linking

链接: http://arxiv.org/abs/2307.01137v1

原文摘要:
The biomedical field relies heavily on concept linking in various areas such
as literature mining, graph alignment, information retrieval,
question-answering, data, and knowledge integration. Although large language
models (LLMs) have made significant strides in many natural language processing
tasks, their effectiveness in biomedical concept mapping is yet to be fully
explored. This research investigates a method that exploits the in-context
learning (ICL) capabilities of large models for biomedical concept linking. The
proposed approach adopts a two-stage retrieve-and-rank framework. Initially,
biomedical concepts are embedded using language models, and then embedding
similarity is utilized to retrieve the top candidates. These candidates'
contextual information is subsequently incorporated into the prompt and
processed by a large language model to re-rank the concepts. This approach
achieved an accuracy of 90.% in BC5CDR disease entity normalization and 94.7%
in chemical entity normalization, exhibiting a competitive performance relative
to supervised learning methods. Further, it showed a significant improvement,
with an over 20-point absolute increase in F1 score on an oncology matching
dataset. Extensive qualitative assessments were conducted, and the benefits and
potential shortcomings of using large language models within the biomedical
domain were discussed. were discussed.

中文翻译:
生物医学领域在文献挖掘、图结构对齐、信息检索、问答系统、数据与知识整合等诸多方面高度依赖概念链接技术。尽管大语言模型（LLMs）在多项自然语言处理任务中取得重大突破，但其在生物医学概念映射中的应用效能仍有待深入探索。本研究提出一种利用大模型上下文学习（ICL）能力实现生物医学概念链接的方法。该方案采用"检索-排序"两阶段框架：首先通过语言模型嵌入生物医学概念，利用向量相似度检索候选概念；随后将候选概念的上下文信息整合至提示模板，由大语言模型进行概念重排序。实验表明，该方法在BC5CDR疾病实体标准化任务中达到90.0%准确率，在化学实体标准化中取得94.7%准确率，性能媲美监督学习方法。在肿瘤学概念匹配数据集上更实现F1值超过20个百分点的显著提升。研究通过大量定性评估，系统探讨了大语言模型在生物医学领域的应用优势与潜在局限。
