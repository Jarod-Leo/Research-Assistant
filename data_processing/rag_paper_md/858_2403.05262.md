# Debiasing Large Visual Language Models

链接: http://arxiv.org/abs/2403.05262v1

原文摘要:
In the realms of computer vision and natural language processing, Large
Vision-Language Models (LVLMs) have become indispensable tools, proficient in
generating textual descriptions based on visual inputs. Despite their
advancements, our investigation reveals a noteworthy bias in the generated
content, where the output is primarily influenced by the underlying Large
Language Models (LLMs) prior rather than the input image. Our empirical
experiments underscore the persistence of this bias, as LVLMs often provide
confident answers even in the absence of relevant images or given incongruent
visual input. To rectify these biases and redirect the model's focus toward
vision information, we introduce two simple, training-free strategies. Firstly,
for tasks such as classification or multi-choice question-answering (QA), we
propose a ``calibration'' step through affine transformation to adjust the
output distribution. This ``Post-Hoc debias'' approach ensures uniform scores
for each answer when the image is absent, serving as an effective
regularization technique to alleviate the influence of LLM priors. For more
intricate open-ended generation tasks, we extend this method to ``Debias
sampling'', drawing inspirations from contrastive decoding methods.
Furthermore, our investigation sheds light on the instability of LVLMs across
various decoding configurations. Through systematic exploration of different
settings, we significantly enhance performance, surpassing reported results and
raising concerns about the fairness of existing evaluations. Comprehensive
experiments substantiate the effectiveness of our proposed strategies in
mitigating biases. These strategies not only prove beneficial in minimizing
hallucinations but also contribute to the generation of more helpful and
precise illustrations.

中文翻译:
在计算机视觉与自然语言处理领域，大型视觉语言模型（LVLMs）已成为不可或缺的工具，擅长根据视觉输入生成文本描述。尽管技术不断进步，我们的研究发现生成内容存在显著偏差：输出结果主要受底层大型语言模型（LLMs）先验知识主导，而非输入图像本身。实证实验表明，即使在没有相关图像或视觉输入不匹配的情况下，LVLMs仍会给出高置信度答案，这种偏差具有持续性。

为纠正这些偏差并将模型注意力重新导向视觉信息，我们提出两种无需训练的简易策略。首先针对分类或多选题（QA）任务，我们通过仿射变换引入"校准"步骤来调整输出分布。这种"事后去偏"方法确保当图像缺失时每个答案获得均等分数，成为有效缓解LLM先验影响的正则化技术。对于更复杂的开放式生成任务，我们受对比解码方法启发，将该方法扩展为"去偏采样"。

此外，研究揭示了LVLMs在不同解码配置下的不稳定性。通过对各种设置的系统性探索，我们显著提升了模型性能，不仅超越已报道结果，更引发对现有评估公平性的质疑。综合实验证实了所提策略在减轻偏差方面的有效性：这些方法不仅能有效减少幻觉现象，还可辅助生成更具实用性和精确性的图文内容。
