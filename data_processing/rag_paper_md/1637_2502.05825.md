# Delta - Contrastive Decoding Mitigates Text Hallucinations in Large Language Models

链接: http://arxiv.org/abs/2502.05825v1

原文摘要:
Large language models (LLMs) demonstrate strong capabilities in natural
language processing but remain prone to hallucinations, generating factually
incorrect or fabricated content. This issue undermines their reliability,
particularly in high-stakes domains such as healthcare and legal advisory. To
address this challenge, we propose Delta, an inference-time method that reduces
hallucinations without requiring model retraining or additional data. Delta
works by randomly masking parts of the input prompt and contrasting the output
distributions for the original and masked inputs, effectively suppressing
hallucinations through inference-only computations. We evaluate Delta on
context-rich question-answering benchmarks, achieving absolute improvements of
approximately 3 and 6 percentage points on SQuAD v1.1 and v2, respectively, and
7 and 2 percentage points on TriviaQA and Natural Questions under-sampling
decoding. Delta also improves the no-answer exact match score on SQuAD v2 by
over ten percentage points, demonstrating its effectiveness in mitigating
hallucinations arising from contextual ambiguity. These results highlight Delta
as a computationally efficient and scalable approach for improving the
reliability of LLMs in real-world applications.

中文翻译:
大语言模型在自然语言处理方面展现出强大能力，但仍存在幻觉问题，容易生成事实错误或虚构内容。这一缺陷严重影响了模型在医疗、法律咨询等关键领域的可靠性。为解决该问题，我们提出Delta方法——一种无需重新训练模型或额外数据的推理阶段干预技术。该方法通过随机掩码输入提示的部分内容，对比原始输入与掩码输入的输出分布差异，仅通过推理计算即可有效抑制幻觉。我们在上下文密集型问答基准测试中验证Delta的效果：在SQuAD v1.1和v2上分别实现约3%和6%的绝对性能提升，在TriviaQA和Natural Questions欠采样解码条件下取得7%和2%的改进。特别值得注意的是，Delta使SQuAD v2的"无答案精确匹配"指标提升超过十个百分点，证明其能有效缓解语境模糊引发的幻觉。这些结果表明Delta是一种计算高效、可扩展的方案，能显著提升大语言模型在实际应用中的可靠性。
