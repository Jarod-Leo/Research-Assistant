# ConcEPT: Concept-Enhanced Pre-Training for Language Models

链接: http://arxiv.org/abs/2401.05669v1

原文摘要:
Pre-trained language models (PLMs) have been prevailing in state-of-the-art
methods for natural language processing, and knowledge-enhanced PLMs are
further proposed to promote model performance in knowledge-intensive tasks.
However, conceptual knowledge, one essential kind of knowledge for human
cognition, still remains understudied in this line of research. This limits
PLMs' performance in scenarios requiring human-like cognition, such as
understanding long-tail entities with concepts. In this paper, we propose
ConcEPT, which stands for Concept-Enhanced Pre-Training for language models, to
infuse conceptual knowledge into PLMs. ConcEPT exploits external taxonomies
with entity concept prediction, a novel pre-training objective to predict the
concepts of entities mentioned in the pre-training contexts. Unlike previous
concept-enhanced methods, ConcEPT can be readily adapted to various downstream
applications without entity linking or concept mapping. Results of extensive
experiments show the effectiveness of ConcEPT in four tasks such as entity
typing, which validates that our model gains improved conceptual knowledge with
concept-enhanced pre-training.

中文翻译:
预训练语言模型（PLMs）已成为自然语言处理领域前沿方法的主流，而知识增强型PLMs的提出进一步提升了模型在知识密集型任务中的表现。然而，作为人类认知关键要素之一的概念性知识，在此类研究中仍未得到充分探索。这限制了PLMs在需要类人认知的场景（如通过概念理解长尾实体）中的表现。本文提出概念增强预训练模型ConcEPT，通过将概念知识注入PLMs来突破这一局限。该方法利用外部分类体系，通过实体概念预测这一新型预训练目标——即预测预训练上下文中提及实体的所属概念——来实现知识融合。与以往概念增强方法不同，ConcEPT无需实体链接或概念映射即可直接适配各类下游应用。大量实验结果表明，ConcEPT在实体类型标注等四项任务中表现优异，验证了通过概念增强预训练能有效提升模型的概念知识获取能力。
