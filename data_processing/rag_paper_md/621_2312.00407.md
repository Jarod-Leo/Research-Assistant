# CoLLiE: Collaborative Training of Large Language Models in an Efficient Way

链接: http://arxiv.org/abs/2312.00407v1

原文摘要:
Large language models (LLMs) are increasingly pivotal in a wide range of
natural language processing tasks. Access to pre-trained models, courtesy of
the open-source community, has made it possible to adapt these models to
specific applications for enhanced performance. However, the substantial
resources required for training these models necessitate efficient solutions.
This paper introduces CoLLiE, an efficient library that facilitates
collaborative training of large language models using 3D parallelism,
parameter-efficient fine-tuning (PEFT) methods, and optimizers such as Lion,
Adan, Sophia, LOMO and AdaLomo. With its modular design and comprehensive
functionality, CoLLiE offers a balanced blend of efficiency, ease of use, and
customization. CoLLiE has proven superior training efficiency in comparison
with prevalent solutions in pre-training and fine-tuning scenarios.
Furthermore, we provide an empirical evaluation of the correlation between
model size and GPU memory consumption under different optimization methods, as
well as an analysis of the throughput. Lastly, we carry out a comprehensive
comparison of various optimizers and PEFT methods within the instruction-tuning
context. CoLLiE is available at https://github.com/OpenLMLab/collie.

中文翻译:
大型语言模型（LLMs）在各类自然语言处理任务中日益占据核心地位。得益于开源社区的贡献，预训练模型的获取使得针对特定应用优化模型性能成为可能。然而，训练这些模型所需的庞大资源要求高效的解决方案。本文介绍CoLLiE——一个高效库，通过三维并行、参数高效微调（PEFT）方法及Lion、Adan、Sophia、LOMO和AdaLomo等优化器，支持大型语言模型的协同训练。该库采用模块化设计和全面功能，在效率、易用性和定制化之间实现了出色平衡。实验表明，在预训练和微调场景中，CoLLiE相比主流解决方案具有更优的训练效率。此外，我们通过实证评估揭示了不同优化方法下模型规模与GPU内存占用的关联性，并分析了训练吞吐量。最后，我们在指令微调场景中对多种优化器和PEFT方法进行了全面比较。CoLLiE已开源于https://github.com/OpenLMLab/collie。
