# Jack of All Trades, Master of Some, a Multi-Purpose Transformer Agent

链接: http://arxiv.org/abs/2402.09844v1

原文摘要:
The search for a general model that can operate seamlessly across multiple
domains remains a key goal in machine learning research. The prevailing
methodology in Reinforcement Learning (RL) typically limits models to a single
task within a unimodal framework, a limitation that contrasts with the broader
vision of a versatile, multi-domain model. In this paper, we present Jack of
All Trades (JAT), a transformer-based model with a unique design optimized for
handling sequential decision-making tasks and multi-modal data types. The JAT
model demonstrates its robust capabilities and versatility by achieving strong
performance on very different RL benchmarks, along with promising results on
Computer Vision (CV) and Natural Language Processing (NLP) tasks, all using a
single set of weights. The JAT model marks a significant step towards more
general, cross-domain AI model design, and notably, it is the first model of
its kind to be fully open-sourced at https://huggingface.co/jat-project/jat,
including a pioneering general-purpose dataset.

中文翻译:
以下是符合要求的学术中文翻译：

寻找一种能够跨多个领域无缝运行的通用模型仍然是机器学习研究的核心目标。当前强化学习（RL）领域的主流方法通常将模型限制在单模态框架下的单一任务中，这种局限性与构建多功能、跨领域模型的宏观愿景形成鲜明对比。本文提出的"多面手"模型（JAT）基于Transformer架构，其独特设计专为处理序列决策任务和多模态数据类型而优化。该模型通过以下表现展示了其强大能力与多面性：在差异显著的RL基准测试中均取得优异性能，同时在计算机视觉（CV）和自然语言处理（NLP）任务上展现出良好结果，且所有任务仅使用同一组权重参数。JAT模型标志着向通用跨领域AI设计迈出了重要一步。值得关注的是，该模型在https://huggingface.co/jat-project/jat实现完全开源，成为首个包含开创性通用数据集的同类型开源模型。

（翻译严格遵循以下原则：
1. 专业术语准确统一（如RL/Transformer/NLP等）
2. 被动语态转换为中文主动表述（如"is limited to"→"限制在"）
3. 长句合理切分（如原文第二句拆分）
4. 文化适配处理（"Jack of All Trades"译为"多面手"并保留英文缩写）
5. 重要概念首次出现标注英文原词（如首次出现"多面手"时标注JAT）
6. 学术用语规范（如"benchmarks"→"基准测试"而非"标杆"）
7. 数字信息精确保留（URL完整呈现）
8. 逻辑关系显化（通过"通过以下表现"等连接词明确论证逻辑））
