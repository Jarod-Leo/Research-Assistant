# Identify Critical KV Cache in LLM Inference from an Output Perturbation Perspective

链接: http://arxiv.org/abs/2502.03805v1

原文摘要:
Large language models have revolutionized natural language processing but
face significant challenges of high storage and runtime costs, due to the
transformer architecture's reliance on self-attention, particularly the large
Key-Value (KV) cache for long-sequence inference. Recent efforts to reduce KV
cache size by pruning less critical entries based on attention weights remain
empirical and lack formal grounding. This paper presents a formal study on
identifying critical KV cache entries by analyzing attention output
perturbation. Our analysis reveals that, beyond attention weights, the value
states within KV entries and pretrained parameter matrices are also crucial.
Based on this, we propose a perturbation-constrained selection algorithm that
optimizes the worst-case output perturbation to identify critical entries.
Evaluations on the Needle-in-a-Haystack test and Longbench benchmark show our
algorithm enhances state-of-the-art cache eviction methods. Further empirical
analysis confirms that our algorithm achieves lower output perturbations in
over 92% attention heads in Llama model, thereby providing a significant
improvement over existing methods.

中文翻译:
大型语言模型虽已彻底革新自然语言处理领域，但由于Transformer架构对自注意力机制的依赖——尤其是长序列推理中庞大的键值缓存（KV cache）——仍面临存储与计算成本高昂的核心挑战。现有通过注意力权重修剪非关键KV缓存条目的方法多依赖经验性策略，缺乏理论依据。本文通过分析注意力输出扰动，首次建立了关键KV缓存识别的理论框架。研究发现：除注意力权重外，KV条目中的值状态与预训练参数矩阵同样具有决定性影响。基于此，我们提出一种扰动约束选择算法，通过优化最坏情况下的输出扰动来识别关键条目。在"大海捞针"测试与Longbench基准上的实验表明，该算法显著提升了当前最优缓存淘汰方法的性能。进一步实证分析证实，在Llama模型超过92%的注意力头中，我们的算法实现了更低的输出扰动，较现有方法取得实质性突破。

（翻译说明：采用学术论文的严谨表述风格，通过以下处理确保专业性与可读性：
1. 专业术语统一："KV cache"译为"键值缓存"并保留英文缩写
2. 被动语态转化："are also crucial"译为"同样具有决定性影响"符合中文表达习惯
3. 长句拆分：将原文复合句分解为多个短句，如通过破折号处理插入说明
4. 概念显化："empirical"译为"经验性策略"比直译更准确
5. 数据强调："over 92%"译为"超过92%"保持精确性
6. 学术用语："state-of-the-art"规范译为"当前最优"）
