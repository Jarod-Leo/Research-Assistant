# M-Ped: Multi-Prompt Ensemble Decoding for Large Language Models

链接: http://arxiv.org/abs/2412.18299v1

原文摘要:
With the widespread application of Large Language Models (LLMs) in the field
of Natural Language Processing (NLP), enhancing their performance has become a
research hotspot. This paper presents a novel multi-prompt ensemble decoding
approach designed to bolster the generation quality of LLMs by leveraging the
aggregation of outcomes from multiple prompts. Given a unique input $X$, we
submit $n$ variations of prompts with $X$ to LLMs in batch mode to decode and
derive probability distributions. For each token prediction, we calculate the
ensemble probability by averaging the $n$ probability distributions within the
batch, utilizing this aggregated probability to generate the token. This
technique is dubbed Inner-Batch Ensemble. To facilitate efficient batch
inference, we implement a Left-Padding strategy to maintain uniform input
lengths across the n prompts. Through extensive experimentation on diverse NLP
tasks, including machine translation, code generation, and text simplification,
we demonstrate the efficacy of our method in enhancing LLM performance. The
results show substantial improvements in BLEU scores, pass@$k$ rates, and LENS
metrics over conventional methods.

中文翻译:
随着大语言模型（LLM）在自然语言处理（NLP）领域的广泛应用，提升其性能已成为研究热点。本文提出了一种新颖的多提示集成解码方法，旨在通过聚合多个提示的生成结果来增强LLM的输出质量。给定输入$X$时，我们将$X$对应的$n$种提示变体以批处理模式提交给LLM进行解码，获得各提示对应的概率分布。针对每个待预测词元，通过计算批次内$n$个概率分布的平均值得到集成概率，并基于该聚合概率生成最终词元。该方法被命名为"批内集成"。为实现高效批处理推理，我们采用左填充策略确保$n$个提示的输入长度一致。通过在机器翻译、代码生成和文本简化等多样化NLP任务上的大量实验，验证了本方法对LLM性能的提升效果。实验结果表明，相较于传统方法，该方法在BLEU分数、pass@$k$通过率和LENS评估指标上均取得显著改进。
