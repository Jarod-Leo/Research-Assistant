# Systematic Biases in LLM Simulations of Debates

链接: http://arxiv.org/abs/2402.04049v1

原文摘要:
The emergence of Large Language Models (LLMs), has opened exciting
possibilities for constructing computational simulations designed to replicate
human behavior accurately. Current research suggests that LLM-based agents
become increasingly human-like in their performance, sparking interest in using
these AI agents as substitutes for human participants in behavioral studies.
However, LLMs are complex statistical learners without straightforward
deductive rules, making them prone to unexpected behaviors. Hence, it is
crucial to study and pinpoint the key behavioral distinctions between humans
and LLM-based agents. In this study, we highlight the limitations of LLMs in
simulating human interactions, particularly focusing on LLMs' ability to
simulate political debates on topics that are important aspects of people's
day-to-day lives and decision-making processes. Our findings indicate a
tendency for LLM agents to conform to the model's inherent social biases
despite being directed to debate from certain political perspectives. This
tendency results in behavioral patterns that seem to deviate from
well-established social dynamics among humans. We reinforce these observations
using an automatic self-fine-tuning method, which enables us to manipulate the
biases within the LLM and demonstrate that agents subsequently align with the
altered biases. These results underscore the need for further research to
develop methods that help agents overcome these biases, a critical step toward
creating more realistic simulations.

中文翻译:
大型语言模型（LLMs）的出现为构建精准模拟人类行为的计算仿真开辟了令人振奋的可能性。现有研究表明，基于LLM的智能体在行为表现上愈发接近人类特征，这引发了将其作为人类受试者替代品应用于行为学研究的兴趣。然而，LLMs作为复杂的统计学习系统缺乏直观的演绎规则，容易产生预期外的行为模式。因此，系统研究并明确人类与LLM智能体之间的关键行为差异至关重要。本研究聚焦LLMs在模拟人类互动方面的局限性，特别考察其对涉及日常生活与决策核心议题的政治辩论的模拟能力。实验发现，即便被要求从特定政治立场展开辩论，LLM智能体仍倾向于遵循模型固有的社会偏见，这种行为模式与人类社会中明确存在的社会动态存在显著偏差。我们通过自动化自微调方法强化了这一发现——该方法能有效操纵LLM内部的偏见参数，并证实智能体会随之与调整后的偏见保持一致。这些结果凸显了未来研究需开发新方法以帮助智能体克服此类偏见，这是构建更真实模拟系统的关键步骤。
