# Setting the Trap: Capturing and Defeating Backdoors in Pretrained Language Models through Honeypots

链接: http://arxiv.org/abs/2310.18633v1

原文摘要:
In the field of natural language processing, the prevalent approach involves
fine-tuning pretrained language models (PLMs) using local samples. Recent
research has exposed the susceptibility of PLMs to backdoor attacks, wherein
the adversaries can embed malicious prediction behaviors by manipulating a few
training samples. In this study, our objective is to develop a
backdoor-resistant tuning procedure that yields a backdoor-free model, no
matter whether the fine-tuning dataset contains poisoned samples. To this end,
we propose and integrate a honeypot module into the original PLM, specifically
designed to absorb backdoor information exclusively. Our design is motivated by
the observation that lower-layer representations in PLMs carry sufficient
backdoor features while carrying minimal information about the original tasks.
Consequently, we can impose penalties on the information acquired by the
honeypot module to inhibit backdoor creation during the fine-tuning process of
the stem network. Comprehensive experiments conducted on benchmark datasets
substantiate the effectiveness and robustness of our defensive strategy.
Notably, these results indicate a substantial reduction in the attack success
rate ranging from 10\% to 40\% when compared to prior state-of-the-art methods.

中文翻译:
在自然语言处理领域，当前主流方法是通过本地样本微调预训练语言模型（PLMs）。最新研究揭示了PLMs存在后门攻击的脆弱性——攻击者仅需操纵少量训练样本即可植入恶意预测行为。本研究旨在开发一种抗后门干扰的微调机制，确保无论微调数据集是否包含污染样本，最终模型均能免受后门影响。为此，我们在原始PLM中创新性地集成"蜜罐模块"，该模块专用于吸附后门信息。这一设计基于关键发现：PLMs底层表征既充分携带后门特征，又几乎不包含原始任务信息。通过惩罚蜜罐模块获取的信息，我们能有效抑制主干网络在微调过程中形成后门。基准数据集上的系统实验验证了防御策略的有效性与鲁棒性，结果显示相较于现有最优方法，攻击成功率显著降低10%至40%。
