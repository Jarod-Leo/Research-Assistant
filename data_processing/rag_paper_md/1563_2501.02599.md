# Empowering Bengali Education with AI: Solving Bengali Math Word Problems through Transformer Models

链接: http://arxiv.org/abs/2501.02599v1

原文摘要:
Mathematical word problems (MWPs) involve the task of converting textual
descriptions into mathematical equations. This poses a significant challenge in
natural language processing, particularly for low-resource languages such as
Bengali. This paper addresses this challenge by developing an innovative
approach to solving Bengali MWPs using transformer-based models, including
Basic Transformer, mT5, BanglaT5, and mBART50. To support this effort, the
"PatiGonit" dataset was introduced, containing 10,000 Bengali math problems,
and these models were fine-tuned to translate the word problems into equations
accurately. The evaluation revealed that the mT5 model achieved the highest
accuracy of 97.30%, demonstrating the effectiveness of transformer models in
this domain. This research marks a significant step forward in Bengali natural
language processing, offering valuable methodologies and resources for
educational AI tools. By improving math education, it also supports the
development of advanced problem-solving skills for Bengali-speaking students.

中文翻译:
数学应用题（MWPs）的核心任务是将文本描述转化为数学方程，这对自然语言处理领域提出了重大挑战，尤其在孟加拉语等低资源语言中更为突出。本研究通过开发基于Transformer架构的创新方法应对这一挑战，采用了基础Transformer、mT5、BanglaT5和mBART50等模型。为支持研究，团队构建了包含10,000道孟加拉数学题的"PatiGonit"数据集，并对模型进行微调以实现文字问题到方程的精准转换。评估显示mT5模型以97.30%的准确率表现最优，验证了Transformer模型在此领域的有效性。该研究标志着孟加拉语自然语言处理的重要进展，为教育类AI工具提供了方法论和资源支持，通过提升数学教育水平助力孟加拉语学生发展高阶解题能力。
