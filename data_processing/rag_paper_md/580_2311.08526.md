# GLiNER: Generalist Model for Named Entity Recognition using Bidirectional Transformer

链接: http://arxiv.org/abs/2311.08526v1

原文摘要:
Named Entity Recognition (NER) is essential in various Natural Language
Processing (NLP) applications. Traditional NER models are effective but limited
to a set of predefined entity types. In contrast, Large Language Models (LLMs)
can extract arbitrary entities through natural language instructions, offering
greater flexibility. However, their size and cost, particularly for those
accessed via APIs like ChatGPT, make them impractical in resource-limited
scenarios. In this paper, we introduce a compact NER model trained to identify
any type of entity. Leveraging a bidirectional transformer encoder, our model,
GLiNER, facilitates parallel entity extraction, an advantage over the slow
sequential token generation of LLMs. Through comprehensive testing, GLiNER
demonstrate strong performance, outperforming both ChatGPT and fine-tuned LLMs
in zero-shot evaluations on various NER benchmarks.

中文翻译:
命名实体识别（NER）在多种自然语言处理（NLP）应用中具有关键作用。传统NER模型虽有效，但仅能识别预定义的实体类型。相比之下，大语言模型（LLMs）通过自然语言指令即可提取任意类型的实体，展现出更强的灵活性。然而，其庞大的参数量与高昂成本（尤其是ChatGPT等基于API调用的模型），使其在资源受限场景中难以实际应用。本文提出一种经过训练可识别任意实体类型的轻量化NER模型。通过采用双向Transformer编码器架构，我们的GLiNER模型实现了并行实体抽取，相比LLMs缓慢的序列化token生成具有显著速度优势。经全面测试表明，GLiNER在多个NER基准的零样本评估中表现优异，性能超越ChatGPT与微调后的LLMs。
