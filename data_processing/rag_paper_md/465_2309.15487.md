# Tackling VQA with Pretrained Foundation Models without Further Training

链接: http://arxiv.org/abs/2309.15487v1

原文摘要:
Large language models (LLMs) have achieved state-of-the-art results in many
natural language processing tasks. They have also demonstrated ability to adapt
well to different tasks through zero-shot or few-shot settings. With the
capability of these LLMs, researchers have looked into how to adopt them for
use with Visual Question Answering (VQA). Many methods require further training
to align the image and text embeddings. However, these methods are
computationally expensive and requires large scale image-text dataset for
training. In this paper, we explore a method of combining pretrained LLMs and
other foundation models without further training to solve the VQA problem. The
general idea is to use natural language to represent the images such that the
LLM can understand the images. We explore different decoding strategies for
generating textual representation of the image and evaluate their performance
on the VQAv2 dataset.

中文翻译:
大型语言模型（LLMs）已在众多自然语言处理任务中取得最先进的成果，其通过零样本或少样本设置展现出了出色的任务适应能力。基于LLMs的这一特性，研究者们开始探索如何将其应用于视觉问答（VQA）领域。现有方法大多需要对图像与文本嵌入进行对齐训练，但这类方法计算成本高昂且依赖大规模图文数据集。本文研究了一种无需额外训练、结合预训练LLMs与其他基础模型来解决VQA问题的方法，核心思路是通过自然语言表征图像信息以使LLMs能够理解视觉内容。我们比较了不同图像文本化解码策略在VQAv2数据集上的性能表现。
