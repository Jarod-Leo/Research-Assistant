# Feed-Forward Blocks Control Contextualization in Masked Language Models

链接: http://arxiv.org/abs/2302.00456v1

原文摘要:
Transformers are ubiquitous in wide tasks. Interpreting their internals is a
pivotal goal. Nevertheless, their particular components, feed-forward (FF)
blocks, have typically been less analyzed despite their substantial parameter
amounts. We analyze the input contextualization effects of FF blocks by
rendering them in the attention maps as a human-friendly visualization scheme.
Our experiments with both masked- and causal-language models reveal that FF
networks modify the input contextualization to emphasize specific types of
linguistic compositions. In addition, FF and its surrounding components tend to
cancel out each other's effects, suggesting potential redundancy in the
processing of the Transformer layer.

中文翻译:
Transformer模型在广泛任务中无处不在，理解其内部机制成为关键目标。然而，尽管其前馈网络（FF）模块参数量庞大，却鲜有深入分析。本研究通过将FF模块在注意力图中以人类友好的可视化方式呈现，揭示了其对输入语境化的影响机制。针对掩码语言模型和因果语言模型的实验表明：前馈网络通过调整输入语境化来强化特定类型的语言组合模式。此外，FF模块与其相邻组件往往相互抵消彼此作用，暗示Transformer层处理过程中可能存在冗余性。
