# Feed-Forward Blocks Control Contextualization in Masked Language Models

链接: http://arxiv.org/abs/2302.00456v1

原文摘要:
Transformers are ubiquitous in wide tasks. Interpreting their internals is a
pivotal goal. Nevertheless, their particular components, feed-forward (FF)
blocks, have typically been less analyzed despite their substantial parameter
amounts. We analyze the input contextualization effects of FF blocks by
rendering them in the attention maps as a human-friendly visualization scheme.
Our experiments with both masked- and causal-language models reveal that FF
networks modify the input contextualization to emphasize specific types of
linguistic compositions. In addition, FF and its surrounding components tend to
cancel out each other's effects, suggesting potential redundancy in the
processing of the Transformer layer.

中文翻译:
Transformer模型已广泛应用于各类任务中，理解其内部机制成为关键目标。然而，尽管前馈网络（FF）模块参数量庞大，却鲜有深入研究。本研究通过将FF模块在注意力图谱中进行可视化呈现，以人类可读的方式解析其对输入语境化的影响。基于掩码语言模型和因果语言模型的实验表明：前馈网络通过调整输入语境化来强化特定类型的语言组合模式。此外，FF模块与其相邻组件往往产生相互抵消的效果，这暗示了Transformer层在处理过程中可能存在冗余结构。

（翻译说明：
1. 专业术语处理："feed-forward blocks"统一译为"前馈网络模块"，"attention maps"译为"注意力图谱"
2. 句式重构：将英文长句拆解为符合中文表达习惯的短句，如将"by rendering..."处理为独立分句
3. 概念显化："linguistic compositions"译为"语言组合模式"以增强专业性
4. 逻辑显化：通过"研究表明"等措辞明确研究结论的呈现方式
5. 被动语态转换：将英文被动式"have been less analyzed"主动化为"鲜有深入研究"
6. 术语一致性：全篇保持"Transformer/FF"等核心术语译名统一）
