# NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning

链接: http://arxiv.org/abs/2307.08941v1

原文摘要:
Fine-tuning a pre-trained language model (PLM) emerges as the predominant
strategy in many natural language processing applications. However, this
process is known to be expensive, especially on edge devices with low computing
power. While general approaches (e.g. quantization and distillation) have been
widely studied to reduce the compute/memory of PLM fine-tuning, one-shot
compression techniques specifically designed for fine-tuning remain largely
unexplored. In this paper, we investigate the neural tangent kernel
(NTK)--which reveals the gradient descent dynamics of neural networks--of the
multilayer perceptrons (MLP) modules in a PLM and propose to coin a lightweight
PLM through NTK-approximating MLP fusion. By incorporating NTK into the
compression process, MLP Fusion not only preserves the original model's output
but also maintains its training dynamics. To achieve this, we reconsider the
MLP as a bundle of sub-MLPs and cluster them into a given number of centroids,
which can then be restored as a compressed MLP and surprisingly well
approximate the NTK of the original PLM. Our approach is applicable to both
standard MLP modules and Mixture-of-Experts (MoE) modules in PLMs,
demonstrating its scalability and versatility. Additionally, we provide
theoretical derivations to demonstrate how the proposed compression preserves
the NTK. Extensive experiments of PLM fine-tuning on both natural language
understanding and generation tasks are provided to verify the effectiveness of
MLP fusion. Our code is available at https://github.com/weitianxin/MLP_Fusion.

中文翻译:
微调预训练语言模型（PLM）已成为众多自然语言处理任务中的核心策略，但其计算成本高昂的问题在算力有限的边缘设备上尤为突出。尽管通用压缩方法（如量化和蒸馏）已广泛用于降低PLM微调的计算/内存需求，但针对微调阶段设计的单次压缩技术仍属空白。本文通过分析PLM中多层感知机（MLP）模块的神经正切核（NTK）——该理论揭示了神经网络梯度下降的动态特性——提出基于NTK近似的MLP融合轻量化方法。MLP融合技术将NTK理论融入压缩过程，不仅能保持原始模型输出，还可有效保留其训练动态特性。具体实现中，我们将MLP重构为多个子MLP的集合，通过聚类生成指定数量的质心，进而重构为压缩后的MLP模块。实验证明，该方法能惊人地逼近原始PLM的NTK特性。该技术同时适用于标准MLP模块和专家混合（MoE）模块，展现出卓越的扩展性与通用性。我们通过理论推导论证了压缩过程对NTK的保持机制，并在自然语言理解与生成任务的PLM微调实验中验证了MLP融合的有效性。代码已开源：https://github.com/weitianxin/MLP_Fusion。
