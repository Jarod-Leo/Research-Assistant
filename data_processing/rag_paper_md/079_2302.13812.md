# Adapting Pre-trained Language Models for Quantum Natural Language Processing

链接: http://arxiv.org/abs/2302.13812v1

原文摘要:
The emerging classical-quantum transfer learning paradigm has brought a
decent performance to quantum computational models in many tasks, such as
computer vision, by enabling a combination of quantum models and classical
pre-trained neural networks. However, using quantum computing with pre-trained
models has yet to be explored in natural language processing (NLP). Due to the
high linearity constraints of the underlying quantum computing infrastructures,
existing Quantum NLP models are limited in performance on real tasks. We fill
this gap by pre-training a sentence state with complex-valued BERT-like
architecture, and adapting it to the classical-quantum transfer learning scheme
for sentence classification. On quantum simulation experiments, the pre-trained
representation can bring 50\% to 60\% increases to the capacity of end-to-end
quantum models.

中文翻译:
新兴的经典-量子迁移学习范式通过结合量子模型与经典预训练神经网络，为量子计算模型在计算机视觉等多项任务中带来了显著性能提升。然而，将量子计算与预训练模型应用于自然语言处理（NLP）领域的研究尚未深入展开。受限于量子计算基础设施的高度线性约束，现有量子NLP模型在实际任务中的表现存在明显局限。本研究通过预训练具有复数形态的类BERT架构句子状态，并将其适配至经典-量子迁移学习框架以完成句子分类任务，填补了这一空白。量子模拟实验表明，预训练表征能使端到端量子模型的能力提升50%至60%。
