# Making Them a Malicious Database: Exploiting Query Code to Jailbreak Aligned Large Language Models

链接: http://arxiv.org/abs/2502.09723v1

原文摘要:
Recent advances in large language models (LLMs) have demonstrated remarkable
potential in the field of natural language processing. Unfortunately, LLMs face
significant security and ethical risks. Although techniques such as safety
alignment are developed for defense, prior researches reveal the possibility of
bypassing such defenses through well-designed jailbreak attacks. In this paper,
we propose QueryAttack, a novel framework to examine the generalizability of
safety alignment. By treating LLMs as knowledge databases, we translate
malicious queries in natural language into structured non-natural query
language to bypass the safety alignment mechanisms of LLMs. We conduct
extensive experiments on mainstream LLMs, and the results show that QueryAttack
not only can achieve high attack success rates (ASRs), but also can jailbreak
various defense methods. Furthermore, we tailor a defense method against
QueryAttack, which can reduce ASR by up to 64% on GPT-4-1106. Our code is
available at https://github.com/horizonsinzqs/QueryAttack.

中文翻译:
近期，大型语言模型（LLM）在自然语言处理领域展现出非凡潜力，但其面临显著的安全与伦理风险。尽管研究者开发了安全对齐等技术进行防御，已有研究表明精心设计的越狱攻击可能绕过此类防护机制。本文提出QueryAttack框架，用于检验安全对齐的泛化能力：通过将LLM视为知识库，将自然语言中的恶意查询转换为结构化非自然查询语言，从而规避LLM的安全对齐机制。我们在主流LLM上进行了大量实验，结果表明QueryAttack不仅能实现高攻击成功率（ASR），还能突破多种防御方法。此外，我们专门设计了一种防御方案，可使GPT-4-1106的ASR最高降低64%。代码已开源于https://github.com/horizonsinzqs/QueryAttack。
