# Predicting Anti-microbial Resistance using Large Language Models

链接: http://arxiv.org/abs/2401.00642v1

原文摘要:
During times of increasing antibiotic resistance and the spread of infectious
diseases like COVID-19, it is important to classify genes related to antibiotic
resistance. As natural language processing has advanced with transformer-based
language models, many language models that learn characteristics of nucleotide
sequences have also emerged. These models show good performance in classifying
various features of nucleotide sequences. When classifying nucleotide
sequences, not only the sequence itself, but also various background knowledge
is utilized. In this study, we use not only a nucleotide sequence-based
language model but also a text language model based on PubMed articles to
reflect more biological background knowledge in the model. We propose a method
to fine-tune the nucleotide sequence language model and the text language model
based on various databases of antibiotic resistance genes. We also propose an
LLM-based augmentation technique to supplement the data and an ensemble method
to effectively combine the two models. We also propose a benchmark for
evaluating the model. Our method achieved better performance than the
nucleotide sequence language model in the drug resistance class prediction.

中文翻译:
在抗生素耐药性日益加剧及COVID-19等传染病蔓延的背景下，准确分类抗生素耐药基因至关重要。随着基于Transformer架构的自然语言处理技术发展，涌现出众多能学习核苷酸序列特征的语言模型，这些模型在核苷酸序列多特征分类任务中表现优异。值得注意的是，核苷酸序列分类不仅依赖序列本身，还需结合多种背景知识。本研究创新性地联合使用核苷酸序列语言模型与基于PubMed文献的文本语言模型，将更丰富的生物学背景知识融入模型。我们提出一种微调方法：基于多种抗生素耐药基因数据库，分别优化核苷酸序列语言模型和文本语言模型；并开发基于大语言模型的数据增强技术以扩充训练数据，以及有效融合双模型结果的集成方法。同时建立了专门的评估基准测试体系。实验表明，本方法在耐药类别预测任务中的性能显著优于单一核苷酸序列语言模型。
