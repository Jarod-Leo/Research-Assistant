# Probabilistic Topic Modelling with Transformer Representations

链接: http://arxiv.org/abs/2403.03737v1

原文摘要:
Topic modelling was mostly dominated by Bayesian graphical models during the
last decade. With the rise of transformers in Natural Language Processing,
however, several successful models that rely on straightforward clustering
approaches in transformer-based embedding spaces have emerged and consolidated
the notion of topics as clusters of embedding vectors. We propose the
Transformer-Representation Neural Topic Model (TNTM), which combines the
benefits of topic representations in transformer-based embedding spaces and
probabilistic modelling. Therefore, this approach unifies the powerful and
versatile notion of topics based on transformer embeddings with fully
probabilistic modelling, as in models such as Latent Dirichlet Allocation
(LDA). We utilize the variational autoencoder (VAE) framework for improved
inference speed and modelling flexibility. Experimental results show that our
proposed model achieves results on par with various state-of-the-art approaches
in terms of embedding coherence while maintaining almost perfect topic
diversity. The corresponding source code is available at
https://github.com/ArikReuter/TNTM.

中文翻译:
在过去十年中，主题建模领域主要被贝叶斯图模型主导。然而随着自然语言处理中Transformer模型的崛起，一系列基于嵌入空间直接聚类的新型模型应运而生，这些成功案例确立了"主题即嵌入向量簇"的新范式。我们提出Transformer表征神经主题模型（TNTM），该模型融合了基于Transformer的嵌入空间主题表征优势与概率建模方法。这一创新方案既保留了Transformer嵌入空间强大而灵活的主题表征能力，又实现了类似潜在狄利克雷分配（LDA）模型的完全概率化建模。我们采用变分自编码器（VAE）框架以提升推理速度并增强建模灵活性。实验结果表明，所提模型在保持近乎完美主题多样性的同时，其嵌入连贯性指标与各类前沿方法相当。相关源代码已发布于https://github.com/ArikReuter/TNTM。

（翻译说明：
1. 专业术语处理："Latent Dirichlet Allocation"保留专业缩写LDA并补充全称，"variational autoencoder"译为行业通用译名"变分自编码器"
2. 长句拆分：将原文复合长句分解为符合中文表达习惯的短句结构，如将"unifies..."从句独立处理
3. 概念显化："notion of topics as clusters"译为"主题即嵌入向量簇"的新范式，通过"新范式"补全逻辑关系
4. 动态对等："on par with"译为"与...相当"，"state-of-the-art"译为"前沿方法"符合学术论文表述规范
5. 被动语态转换：将英文被动式"was mostly dominated by"转化为中文主动态"主要被...主导"
6. 逻辑连接词补充：通过"既...又..."等关联词明确原文隐含的并列关系）
