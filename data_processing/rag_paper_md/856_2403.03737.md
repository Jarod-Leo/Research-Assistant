# Probabilistic Topic Modelling with Transformer Representations

链接: http://arxiv.org/abs/2403.03737v1

原文摘要:
Topic modelling was mostly dominated by Bayesian graphical models during the
last decade. With the rise of transformers in Natural Language Processing,
however, several successful models that rely on straightforward clustering
approaches in transformer-based embedding spaces have emerged and consolidated
the notion of topics as clusters of embedding vectors. We propose the
Transformer-Representation Neural Topic Model (TNTM), which combines the
benefits of topic representations in transformer-based embedding spaces and
probabilistic modelling. Therefore, this approach unifies the powerful and
versatile notion of topics based on transformer embeddings with fully
probabilistic modelling, as in models such as Latent Dirichlet Allocation
(LDA). We utilize the variational autoencoder (VAE) framework for improved
inference speed and modelling flexibility. Experimental results show that our
proposed model achieves results on par with various state-of-the-art approaches
in terms of embedding coherence while maintaining almost perfect topic
diversity. The corresponding source code is available at
https://github.com/ArikReuter/TNTM.

中文翻译:
在过去十年中，主题建模领域主要由贝叶斯图模型主导。然而随着自然语言处理中Transformer模型的崛起，一系列基于Transformer嵌入空间直接聚类方法的成功模型相继涌现，确立了"主题即嵌入向量簇"的新范式。本文提出Transformer表征神经主题模型（TNTM），该模型融合了基于Transformer嵌入空间的主题表征优势与概率建模特性。这一方法既保留了Transformer嵌入所赋予主题的强大表征能力，又实现了如潜在狄利克雷分配（LDA）模型般的完全概率化建模。我们采用变分自编码器（VAE）框架以提升推理速度并增强建模灵活性。实验结果表明，所提模型在保持近乎完美主题多样性的同时，其嵌入连贯性指标与各类前沿方法相当。相关源代码已发布于https://github.com/ArikReuter/TNTM。
