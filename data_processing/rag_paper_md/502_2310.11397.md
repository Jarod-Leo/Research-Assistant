# Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning

链接: http://arxiv.org/abs/2310.11397v1

原文摘要:
Large Language Models (LLMs) are powerful tools for natural language
processing, enabling novel applications and user experiences. However, to
achieve optimal performance, LLMs often require adaptation with private data,
which poses privacy and security challenges. Several techniques have been
proposed to adapt LLMs with private data, such as Low-Rank Adaptation (LoRA),
Soft Prompt Tuning (SPT), and In-Context Learning (ICL), but their comparative
privacy and security properties have not been systematically investigated. In
this work, we fill this gap by evaluating the robustness of LoRA, SPT, and ICL
against three types of well-established attacks: membership inference, which
exposes data leakage (privacy); backdoor, which injects malicious behavior
(security); and model stealing, which can violate intellectual property
(privacy and security). Our results show that there is no silver bullet for
privacy and security in LLM adaptation and each technique has different
strengths and weaknesses.

中文翻译:
大型语言模型（LLM）作为自然语言处理的强大工具，能够实现创新应用与用户体验。然而，为达到最佳性能，LLM通常需要利用私有数据进行适配，这带来了隐私与安全挑战。目前已有多种技术被提出用于LLM的私有数据适配，例如低秩适配（LoRA）、软提示调优（SPT）和上下文学习（ICL），但这些技术的隐私与安全特性尚未得到系统研究。本文通过评估LoRA、SPT和ICL对三类典型攻击的鲁棒性填补了这一空白：揭示数据泄露的成员推理攻击（隐私问题）、注入恶意行为的后门攻击（安全问题）以及可能侵犯知识产权的模型窃取攻击（隐私与安全问题）。研究结果表明，LLM适配领域不存在兼顾隐私与安全的万能方案，每种技术皆存在不同的优势与缺陷。
