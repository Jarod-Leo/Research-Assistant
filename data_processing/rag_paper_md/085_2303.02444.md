# Calibrating Transformers via Sparse Gaussian Processes

链接: http://arxiv.org/abs/2303.02444v1

原文摘要:
Transformer models have achieved profound success in prediction tasks in a
wide range of applications in natural language processing, speech recognition
and computer vision. Extending Transformer's success to safety-critical domains
requires calibrated uncertainty estimation which remains under-explored. To
address this, we propose Sparse Gaussian Process attention (SGPA), which
performs Bayesian inference directly in the output space of multi-head
attention blocks (MHAs) in transformer to calibrate its uncertainty. It
replaces the scaled dot-product operation with a valid symmetric kernel and
uses sparse Gaussian processes (SGP) techniques to approximate the posterior
processes of MHA outputs. Empirically, on a suite of prediction tasks on text,
images and graphs, SGPA-based Transformers achieve competitive predictive
accuracy, while noticeably improving both in-distribution calibration and
out-of-distribution robustness and detection.

中文翻译:
Transformer模型在自然语言处理、语音识别和计算机视觉等广泛领域的预测任务中取得了显著成功。将其优势延伸至安全关键领域时，需进行可靠的 uncertainty 估计，而这一方向尚未得到充分探索。为此，我们提出稀疏高斯过程注意力机制（SGPA），通过在多头注意力模块（MHA）的输出空间直接进行贝叶斯推断来校准其不确定性。该方法将缩放点积运算替换为有效的对称核函数，并利用稀疏高斯过程（SGP）技术近似 MHA 输出的后验过程。实验表明，在文本、图像和图结构的一系列预测任务中，基于 SGPA 的 Transformer 模型不仅保持了具有竞争力的预测精度，同时显著提升了分布内校准能力，以及分布外鲁棒性和异常检测性能。
