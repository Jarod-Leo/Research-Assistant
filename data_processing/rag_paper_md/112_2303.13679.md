# Primer: Fast Private Transformer Inference on Encrypted Data

链接: http://arxiv.org/abs/2303.13679v1

原文摘要:
It is increasingly important to enable privacy-preserving inference for cloud
services based on Transformers. Post-quantum cryptographic techniques, e.g.,
fully homomorphic encryption (FHE), and multi-party computation (MPC), are
popular methods to support private Transformer inference. However, existing
works still suffer from prohibitively computational and communicational
overhead. In this work, we present, Primer, to enable a fast and accurate
Transformer over encrypted data for natural language processing tasks. In
particular, Primer is constructed by a hybrid cryptographic protocol optimized
for attention-based Transformer models, as well as techniques including
computation merge and tokens-first ciphertext packing. Comprehensive
experiments on encrypted language modeling show that Primer achieves
state-of-the-art accuracy and reduces the inference latency by 90.6% ~ 97.5%
over previous methods.

中文翻译:
为基于Transformer架构的云服务实现隐私保护推理正变得日益重要。后量子密码学技术（如全同态加密FHE和安全多方计算MPC）是目前支持私有化Transformer推理的主流方法，但现有方案仍存在难以承受的计算与通信开销。本研究提出Primer系统，通过为注意力机制优化的混合密码协议，结合计算融合与令牌优先密文打包等技术，实现了加密数据上高效精准的自然语言处理Transformer模型。在加密语言建模任务的全面实验中，Primer不仅保持了最优精度，相较现有方法更将推理延迟降低了90.6%~97.5%。

（翻译说明：
1. 专业术语采用学术界通用译法，如"fully homomorphic encryption"译为"全同态加密"
2. 被动语态转换为中文主动句式，如"are popular methods"处理为"是...主流方法"
3. 长难句拆分重组，如将包含三个技术要点的原文整合为流畅的递进表述
4. 保留技术概念准确性，如"attention-based Transformer models"译为"注意力机制优化的Transformer模型"
5. 数据呈现方式符合中文习惯，使用中文标点与百分号格式）
