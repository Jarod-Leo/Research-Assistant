# Primer: Fast Private Transformer Inference on Encrypted Data

链接: http://arxiv.org/abs/2303.13679v1

原文摘要:
It is increasingly important to enable privacy-preserving inference for cloud
services based on Transformers. Post-quantum cryptographic techniques, e.g.,
fully homomorphic encryption (FHE), and multi-party computation (MPC), are
popular methods to support private Transformer inference. However, existing
works still suffer from prohibitively computational and communicational
overhead. In this work, we present, Primer, to enable a fast and accurate
Transformer over encrypted data for natural language processing tasks. In
particular, Primer is constructed by a hybrid cryptographic protocol optimized
for attention-based Transformer models, as well as techniques including
computation merge and tokens-first ciphertext packing. Comprehensive
experiments on encrypted language modeling show that Primer achieves
state-of-the-art accuracy and reduces the inference latency by 90.6% ~ 97.5%
over previous methods.

中文翻译:
实现基于Transformer的云服务隐私保护推理日益重要。后量子密码技术（如同态加密FHE和安全多方计算MPC）是支持私有Transformer推理的主流方法，但现有方案仍面临计算与通信开销过大的问题。本研究提出Primer系统，通过专为注意力机制优化的混合密码协议，结合计算融合与令牌优先密文打包等技术，实现了加密数据上高效精准的自然语言处理Transformer模型。加密语言建模实验表明，Primer在保持最优精度的同时，将推理延迟较现有方法降低了90.6%~97.5%。
