# Rethinking Sparse Lexical Representations for Image Retrieval in the Age of Rising Multi-Modal Large Language Models

链接: http://arxiv.org/abs/2408.16296v1

原文摘要:
In this paper, we rethink sparse lexical representations for image retrieval.
By utilizing multi-modal large language models (M-LLMs) that support visual
prompting, we can extract image features and convert them into textual data,
enabling us to utilize efficient sparse retrieval algorithms employed in
natural language processing for image retrieval tasks. To assist the LLM in
extracting image features, we apply data augmentation techniques for key
expansion and analyze the impact with a metric for relevance between images and
textual data. We empirically show the superior precision and recall performance
of our image retrieval method compared to conventional vision-language
model-based methods on the MS-COCO, PASCAL VOC, and NUS-WIDE datasets in a
keyword-based image retrieval scenario, where keywords serve as search queries.
We also demonstrate that the retrieval performance can be improved by
iteratively incorporating keywords into search queries.

中文翻译:
本文重新思考了图像检索中的稀疏词汇表示方法。通过利用支持视觉提示的多模态大语言模型(M-LLMs)，我们能够提取图像特征并将其转化为文本数据，从而将自然语言处理领域高效的稀疏检索算法应用于图像检索任务。为辅助大语言模型提取图像特征，我们采用数据增强技术进行关键词扩展，并通过图像与文本数据相关性指标分析其影响。在基于关键词（作为搜索查询）的图像检索场景下，我们在MS-COCO、PASCAL VOC和NUS-WIDE数据集上的实验表明，与传统视觉语言模型方法相比，本方法具有更优的精确率和召回率。研究还证实，通过迭代地将关键词融入搜索查询可以进一步提升检索性能。
