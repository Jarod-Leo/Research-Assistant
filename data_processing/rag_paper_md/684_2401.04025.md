# IDoFew: Intermediate Training Using Dual-Clustering in Language Models for Few Labels Text Classification

链接: http://arxiv.org/abs/2401.04025v1

原文摘要:
Language models such as Bidirectional Encoder Representations from
Transformers (BERT) have been very effective in various Natural Language
Processing (NLP) and text mining tasks including text classification. However,
some tasks still pose challenges for these models, including text
classification with limited labels. This can result in a cold-start problem.
Although some approaches have attempted to address this problem through
single-stage clustering as an intermediate training step coupled with a
pre-trained language model, which generates pseudo-labels to improve
classification, these methods are often error-prone due to the limitations of
the clustering algorithms. To overcome this, we have developed a novel
two-stage intermediate clustering with subsequent fine-tuning that models the
pseudo-labels reliably, resulting in reduced prediction errors. The key novelty
in our model, IDoFew, is that the two-stage clustering coupled with two
different clustering algorithms helps exploit the advantages of the
complementary algorithms that reduce the errors in generating reliable
pseudo-labels for fine-tuning. Our approach has shown significant improvements
compared to strong comparative models.

中文翻译:
基于Transformer的双向编码器表示（BERT）等语言模型在各类自然语言处理（NLP）及文本挖掘任务（包括文本分类）中表现卓越。然而，某些任务仍对这些模型构成挑战，尤其是标签数据有限的文本分类场景，这可能导致冷启动问题。现有方法尝试通过单阶段聚类作为中间训练步骤，结合预训练语言模型生成伪标签以提升分类性能，但由于聚类算法的固有局限，此类方法往往存在较高错误率。为此，我们提出了一种创新的两阶段中间聚类结合微调策略，通过可靠建模伪标签有效降低预测误差。IDoFew模型的核心创新在于：采用两种互补聚类算法的双阶段架构，充分发挥算法优势，在生成高质量伪标签过程中显著减少误差。实验表明，相较于现有强基线模型，本方法取得了显著性能提升。
