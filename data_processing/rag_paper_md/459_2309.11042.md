# Making Small Language Models Better Multi-task Learners with Mixture-of-Task-Adapters

链接: http://arxiv.org/abs/2309.11042v1

原文摘要:
Recently, Large Language Models (LLMs) have achieved amazing zero-shot
learning performance over a variety of Natural Language Processing (NLP) tasks,
especially for text generative tasks. Yet, the large size of LLMs often leads
to the high computational cost of model training and online deployment. In our
work, we present ALTER, a system that effectively builds the multi-tAsk
Learners with mixTure-of-task-adaptERs upon small language models (with <1B
parameters) to address multiple NLP tasks simultaneously, capturing the
commonalities and differences between tasks, in order to support
domain-specific applications. Specifically, in ALTER, we propose the
Mixture-of-Task-Adapters (MTA) module as an extension to the transformer
architecture for the underlying model to capture the intra-task and inter-task
knowledge. A two-stage training method is further proposed to optimize the
collaboration between adapters at a small computational cost. Experimental
results over a mixture of NLP tasks show that our proposed MTA architecture and
the two-stage training method achieve good performance. Based on ALTER, we have
also produced MTA-equipped language models for various domains.

中文翻译:
近年来，大型语言模型（LLMs）在各类自然语言处理（NLP）任务中展现出卓越的零样本学习能力，尤其在文本生成任务上表现突出。然而，LLMs庞大的参数量往往导致模型训练与在线部署的高计算成本。本研究提出ALTER系统，通过在小规模语言模型（参数<1B）上构建混合任务适配器的多任务学习框架，有效实现多任务协同处理，捕捉任务间的共性与特性，以支持领域专用应用。具体而言，ALTER创新性地设计了混合任务适配器（MTA）模块作为底层Transformer架构的扩展，用于捕获任务内与任务间知识。我们进一步提出两阶段训练方法，以较小计算成本优化适配器间的协作机制。跨领域NLP任务的实验结果表明，所提出的MTA架构与两阶段训练方法均能取得优异性能。基于ALTER系统，我们还为不同领域开发了配备MTA模块的专用语言模型。
