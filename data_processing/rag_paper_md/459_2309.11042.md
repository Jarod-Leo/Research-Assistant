# Making Small Language Models Better Multi-task Learners with Mixture-of-Task-Adapters

链接: http://arxiv.org/abs/2309.11042v1

原文摘要:
Recently, Large Language Models (LLMs) have achieved amazing zero-shot
learning performance over a variety of Natural Language Processing (NLP) tasks,
especially for text generative tasks. Yet, the large size of LLMs often leads
to the high computational cost of model training and online deployment. In our
work, we present ALTER, a system that effectively builds the multi-tAsk
Learners with mixTure-of-task-adaptERs upon small language models (with <1B
parameters) to address multiple NLP tasks simultaneously, capturing the
commonalities and differences between tasks, in order to support
domain-specific applications. Specifically, in ALTER, we propose the
Mixture-of-Task-Adapters (MTA) module as an extension to the transformer
architecture for the underlying model to capture the intra-task and inter-task
knowledge. A two-stage training method is further proposed to optimize the
collaboration between adapters at a small computational cost. Experimental
results over a mixture of NLP tasks show that our proposed MTA architecture and
the two-stage training method achieve good performance. Based on ALTER, we have
also produced MTA-equipped language models for various domains.

中文翻译:
近年来，大型语言模型（LLMs）在各类自然语言处理（NLP）任务中展现出惊人的零样本学习能力，尤其在文本生成任务上表现突出。然而，LLMs庞大的参数量往往导致模型训练和线上部署的高计算成本。本研究提出ALTER系统，通过在小规模语言模型（参数<10亿）上构建混合任务适配器的多任务学习框架，有效实现多NLP任务并行处理，同时捕捉任务间的共性与特性，以支持领域专用应用。具体而言，ALTER系统创新性地提出混合任务适配器（MTA）模块，作为底层Transformer架构的扩展组件，用于捕获任务内与任务间知识。我们进一步提出两阶段训练方法，以较小计算成本优化适配器间的协作机制。在多类型NLP任务上的实验表明，所提出的MTA架构与两阶段训练方法均能取得优异性能。基于ALTER系统，我们还为不同领域开发了配备MTA模块的专用语言模型。

（翻译说明：
1. 专业术语处理："zero-shot learning"译为"零样本学习"，"transformer architecture"保留专业术语"Transformer架构"
2. 句式重构：将英文长句拆解为符合中文表达习惯的短句，如原文第二句拆分为"然而..."和"本研究..."两个逻辑递进句
3. 被动语态转换："we present ALTER"译为主动式"本研究提出ALTER系统"
4. 概念显化："with <1B parameters"译为"参数<10亿"更符合中文计量习惯
5. 技术表述准确性："Mixture-of-Task-Adapters"严格对应"混合任务适配器（MTA）"
6. 学术风格保持：使用"共性与特性""协作机制"等符合计算机学术论文表述的词汇）
