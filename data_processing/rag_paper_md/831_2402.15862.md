# SportQA: A Benchmark for Sports Understanding in Large Language Models

链接: http://arxiv.org/abs/2402.15862v1

原文摘要:
A deep understanding of sports, a field rich in strategic and dynamic
content, is crucial for advancing Natural Language Processing (NLP). This holds
particular significance in the context of evaluating and advancing Large
Language Models (LLMs), given the existing gap in specialized benchmarks. To
bridge this gap, we introduce SportQA, a novel benchmark specifically designed
for evaluating LLMs in the context of sports understanding. SportQA encompasses
over 70,000 multiple-choice questions across three distinct difficulty levels,
each targeting different aspects of sports knowledge from basic historical
facts to intricate, scenario-based reasoning tasks. We conducted a thorough
evaluation of prevalent LLMs, mainly utilizing few-shot learning paradigms
supplemented by chain-of-thought (CoT) prompting. Our results reveal that while
LLMs exhibit competent performance in basic sports knowledge, they struggle
with more complex, scenario-based sports reasoning, lagging behind human
expertise. The introduction of SportQA marks a significant step forward in NLP,
offering a tool for assessing and enhancing sports understanding in LLMs.

中文翻译:
深入理解体育这一富含策略与动态内容的领域，对推动自然语言处理（NLP）发展至关重要。鉴于当前专业评估基准的缺失，这在评估和提升大语言模型（LLMs）能力方面具有特殊意义。为此，我们推出SportQA——一个专为评估LLMs体育理解能力设计的新型基准测试。该数据集包含超过7万道多选题，涵盖三个难度层级，从基础历史事实到复杂场景推理任务，全面考察不同维度的体育知识。我们采用小样本学习范式并结合思维链（CoT）提示策略，对主流LLMs进行了系统评估。结果表明：虽然模型在基础体育知识层面表现合格，但在需要复杂场景推理的任务中明显落后于人类专家水平。SportQA的推出为NLP领域提供了评估和增强LLMs体育理解能力的重要工具，标志着该研究方向的实质性进展。
