# Comparative study of the ansätze in quantum language models

链接: http://arxiv.org/abs/2502.20744v1

原文摘要:
Quantum language models are the alternative to classical language models,
which borrow concepts and methods from quantum machine learning and
computational linguistics. While several quantum natural language processing
(QNLP) methods and frameworks exist for text classification and generation,
there is a lack of systematic study to compare the performance across various
ans\"atze, in terms of their hyperparameters and classical and quantum methods
to implement them. Here, we evaluate the performance of quantum natural
language processing models based on these ans\"atze at different levels in text
classification tasks. We perform a comparative study and optimize the QNLP
models by fine-tuning several critical hyperparameters. Our results demonstrate
how the balance between simplification and expressivity affects model
performance. This study provides extensive data to improve our understanding of
QNLP models and opens the possibility of developing better QNLP algorithms.

中文翻译:
量子语言模型作为经典语言模型的替代方案，借鉴了量子机器学习和计算语言学的概念与方法。尽管目前已存在多种用于文本分类与生成的量子自然语言处理（QNLP）方法和框架，但针对不同理论方案在超参数设置及经典与量子实现方法方面的系统性性能比较研究仍属空白。本研究通过文本分类任务，在不同层级上评估了基于这些理论方案的QNLP模型性能。我们开展对比研究并通过精细调节若干关键超参数对QNLP模型进行优化。实验结果表明简化程度与表达力之间的平衡如何影响模型性能。这项研究提供了大量数据以深化对QNLP模型的理解，并为开发更优QNLP算法开辟了可能性。
