# Scaling Vision-Language Models with Sparse Mixture of Experts

链接: http://arxiv.org/abs/2303.07226v1

原文摘要:
The field of natural language processing (NLP) has made significant strides
in recent years, particularly in the development of large-scale vision-language
models (VLMs). These models aim to bridge the gap between text and visual
information, enabling a more comprehensive understanding of multimedia data.
However, as these models become larger and more complex, they also become more
challenging to train and deploy. One approach to addressing this challenge is
the use of sparsely-gated mixture-of-experts (MoE) techniques, which divide the
model into smaller, specialized sub-models that can jointly solve a task. In
this paper, we explore the effectiveness of MoE in scaling vision-language
models, demonstrating its potential to achieve state-of-the-art performance on
a range of benchmarks over dense models of equivalent computational cost. Our
research offers valuable insights into stabilizing the training of MoE models,
understanding the impact of MoE on model interpretability, and balancing the
trade-offs between compute performance when scaling VLMs. We hope our work will
inspire further research into the use of MoE for scaling large-scale
vision-language models and other multimodal machine learning applications.

中文翻译:
近年来，自然语言处理领域取得了显著进展，尤其是在大规模视觉语言模型（VLM）的开发方面。这类模型致力于弥合文本与视觉信息之间的鸿沟，从而实现对多媒体数据更全面的理解。然而随着模型规模扩大和复杂度提升，其训练与部署也面临更大挑战。稀疏门控专家混合（MoE）技术通过将模型分解为多个协同解决问题的专业化子模型，为解决这一难题提供了新思路。本文探究了MoE在扩展视觉语言模型中的有效性，证明其在同等计算成本下，相较于密集模型能在多项基准测试中实现最先进的性能表现。我们的研究为稳定MoE模型训练、理解MoE对模型可解释性的影响，以及权衡视觉语言模型扩展过程中的计算性能提供了重要见解。期待这项工作能激发更多关于采用MoE技术扩展大规模视觉语言模型及其他多模态机器学习应用的研究。
