# Knowledge Distillation in Automated Annotation: Supervised Text Classification with LLM-Generated Training Labels

链接: http://arxiv.org/abs/2406.17633v1

原文摘要:
Computational social science (CSS) practitioners often rely on human-labeled
data to fine-tune supervised text classifiers. We assess the potential for
researchers to augment or replace human-generated training data with surrogate
training labels from generative large language models (LLMs). We introduce a
recommended workflow and test this LLM application by replicating 14
classification tasks and measuring performance. We employ a novel corpus of
English-language text classification data sets from recent CSS articles in
high-impact journals. Because these data sets are stored in password-protected
archives, our analyses are less prone to issues of contamination. For each
task, we compare supervised classifiers fine-tuned using GPT-4 labels against
classifiers fine-tuned with human annotations and against labels from GPT-4 and
Mistral-7B with few-shot in-context learning. Our findings indicate that
supervised classification models fine-tuned on LLM-generated labels perform
comparably to models fine-tuned with labels from human annotators. Fine-tuning
models using LLM-generated labels can be a fast, efficient and cost-effective
method of building supervised text classifiers.

中文翻译:
计算社会科学（CSS）研究者常依赖人工标注数据来优化监督式文本分类器。本文探讨了利用生成式大语言模型（LLM）产生的替代训练标签来增强或取代人工标注数据的可行性。我们提出一套推荐工作流程，通过复现14项分类任务并评估性能来验证这一LLM应用。研究采用来自高影响力期刊CSS论文的新型英语文本分类数据集，由于这些数据存放于加密数据库，有效规避了数据污染问题。针对每项任务，我们比较了三种方案：基于GPT-4生成标签微调的监督分类器、基于人工标注微调的模型，以及采用GPT-4和Mistral-7B少样本上下文学习生成的标签。结果表明，基于LLM生成标签微调的监督分类模型性能与人工标注微调模型相当。利用LLM生成标签进行模型微调，可成为构建监督文本分类器的高效、快速且经济的方法。
