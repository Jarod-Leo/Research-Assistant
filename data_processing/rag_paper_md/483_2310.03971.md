# Quantized Transformer Language Model Implementations on Edge Devices

链接: http://arxiv.org/abs/2310.03971v1

原文摘要:
Large-scale transformer-based models like the Bidirectional Encoder
Representations from Transformers (BERT) are widely used for Natural Language
Processing (NLP) applications, wherein these models are initially pre-trained
with a large corpus with millions of parameters and then fine-tuned for a
downstream NLP task. One of the major limitations of these large-scale models
is that they cannot be deployed on resource-constrained devices due to their
large model size and increased inference latency. In order to overcome these
limitations, such large-scale models can be converted to an optimized
FlatBuffer format, tailored for deployment on resource-constrained edge
devices. Herein, we evaluate the performance of such FlatBuffer transformed
MobileBERT models on three different edge devices, fine-tuned for Reputation
analysis of English language tweets in the RepLab 2013 dataset. In addition,
this study encompassed an evaluation of the deployed models, wherein their
latency, performance, and resource efficiency were meticulously assessed. Our
experiment results show that, compared to the original BERT large model, the
converted and quantized MobileBERT models have 160$\times$ smaller footprints
for a 4.1% drop in accuracy while analyzing at least one tweet per second on
edge devices. Furthermore, our study highlights the privacy-preserving aspect
of TinyML systems as all data is processed locally within a serverless
environment.

中文翻译:
基于Transformer的大规模模型，如双向编码器表示（BERT），被广泛应用于自然语言处理（NLP）任务中。这类模型通常先通过包含数百万参数的海量语料库进行预训练，再针对下游NLP任务进行微调。然而，这些大型模型的主要局限在于其庞大的体积和较高的推理延迟，导致无法部署在资源受限的设备上。为突破这一限制，可将此类大规模模型转换为专为资源受限边缘设备优化的FlatBuffer格式。

本研究评估了经过FlatBuffer转换的MobileBERT模型在三种不同边缘设备上的性能表现，这些模型针对RepLab 2013数据集中英文推文的声誉分析任务进行了微调。研究还系统评估了部署模型的延迟、性能和资源效率等关键指标。实验结果表明：与原始BERT大型模型相比，经过转换和量化的MobileBERT模型体积缩小了160倍，在边缘设备上每秒至少能处理一条推文的情况下，准确率仅下降4.1%。此外，研究凸显了TinyML系统的隐私保护优势——所有数据均在无服务器环境下实现本地化处理。
