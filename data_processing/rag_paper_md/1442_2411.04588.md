# Tibyan Corpus: Balanced and Comprehensive Error Coverage Corpus Using ChatGPT for Arabic Grammatical Error Correction

链接: http://arxiv.org/abs/2411.04588v1

原文摘要:
Natural language processing (NLP) utilizes text data augmentation to overcome
sample size constraints. Increasing the sample size is a natural and widely
used strategy for alleviating these challenges. In this study, we chose Arabic
to increase the sample size and correct grammatical errors. Arabic is
considered one of the languages with limited resources for grammatical error
correction (GEC). Furthermore, QALB-14 and QALB-15 are the only datasets used
in most Arabic grammatical error correction research, with approximately 20,500
parallel examples, which is considered low compared with other languages.
Therefore, this study aims to develop an Arabic corpus called "Tibyan" for
grammatical error correction using ChatGPT. ChatGPT is used as a data augmenter
tool based on a pair of Arabic sentences containing grammatical errors matched
with a sentence free of errors extracted from Arabic books, called guide
sentences. Multiple steps were involved in establishing our corpus, including
the collection and pre-processing of a pair of Arabic texts from various
sources, such as books and open-access corpora. We then used ChatGPT to
generate a parallel corpus based on the text collected previously, as a guide
for generating sentences with multiple types of errors. By engaging linguistic
experts to review and validate the automatically generated sentences, we
ensured that they were correct and error-free. The corpus was validated and
refined iteratively based on feedback provided by linguistic experts to improve
its accuracy. Finally, we used the Arabic Error Type Annotation tool (ARETA) to
analyze the types of errors in the Tibyan corpus. Our corpus contained 49 of
errors, including seven types: orthography, morphology, syntax, semantics,
punctuation, merge, and split. The Tibyan corpus contains approximately 600 K
tokens.

中文翻译:
自然语言处理（NLP）领域通过文本数据增强技术来突破样本量的限制。增加样本规模是缓解这类挑战最自然且广泛采用的策略。本研究选择阿拉伯语作为研究对象，旨在扩充样本规模并修正语法错误。作为语法纠错（GEC）资源相对匮乏的语言之一，阿拉伯语目前仅有的QALB-14和QALB-15数据集被大多数相关研究采用，其约20,500组平行例句的规模相较于其他语言明显不足。

为此，本研究利用ChatGPT构建了名为"Tibyan"的阿拉伯语语法纠错语料库。该方法以阿拉伯书籍中提取的正确例句（引导句）为基础，通过ChatGPT生成包含语法错误的对应句子作为数据增强工具。语料库建设过程包含多阶段工作：首先从书籍和开放语料库等渠道收集阿拉伯语文本对并进行预处理；随后以这些文本为引导，使用ChatGPT生成包含多种错误类型的平行语料；继而邀请语言学专家对自动生成的句子进行审核验证，确保其正确性；最后基于专家反馈进行迭代优化以提高语料准确性。

经阿拉伯语错误类型标注工具（ARETA）分析，Tibyan语料库涵盖49种错误类型，具体分为七大类：正字法、词法、句法、语义、标点、合并及拆分错误。该语料库最终规模达到约60万词符。
