# Evaluating Diverse Large Language Models for Automatic and General Bug Reproduction

链接: http://arxiv.org/abs/2311.04532v2

原文摘要:
Bug reproduction is a critical developer activity that is also challenging to
automate, as bug reports are often in natural language and thus can be
difficult to transform to test cases consistently. As a result, existing
techniques mostly focused on crash bugs, which are easier to automatically
detect and verify. In this work, we overcome this limitation by using large
language models (LLMs), which have been demonstrated to be adept at natural
language processing and code generation. By prompting LLMs to generate
bug-reproducing tests, and via a post-processing pipeline to automatically
identify promising generated tests, our proposed technique LIBRO could
successfully reproduce about one-third of all bugs in the widely used Defects4J
benchmark. Furthermore, our extensive evaluation on 15 LLMs, including 11
open-source LLMs, suggests that open-source LLMs also demonstrate substantial
potential, with the StarCoder LLM achieving 70% of the reproduction performance
of the closed-source OpenAI LLM code-davinci-002 on the large Defects4J
benchmark, and 90% of performance on a held-out bug dataset likely not part of
any LLM's training data. In addition, our experiments on LLMs of different
sizes show that bug reproduction using LIBRO improves as LLM size increases,
providing information as to which LLMs can be used with the LIBRO pipeline.

中文翻译:
缺陷复现是开发者的一项关键活动，但由于缺陷报告通常采用自然语言描述，难以稳定转化为测试用例，其自动化实现颇具挑战性。因此现有技术主要集中于更易自动检测与验证的崩溃型缺陷。本研究通过运用大型语言模型（LLMs）突破这一局限——这类模型已被证实擅长自然语言处理与代码生成。通过提示LLMs生成缺陷复现测试，并借助后处理流程自动筛选有潜力的生成测试，我们提出的LIBRO技术成功复现了广泛使用的Defects4J基准库中约三分之一的缺陷。

进一步对15个LLMs（含11个开源模型）的评估表明，开源LLMs同样展现出显著潜力：在大型Defects4J基准上，StarCoder模型的复现性能达到闭源OpenAI模型code-davinci-002的70%；而在未参与任何LLM训练数据的保留缺陷数据集上，其性能可达90%。此外，不同规模LLMs的实验显示，随着模型规模增大，LIBRO的缺陷复现效果持续提升，这为选择适用LIBRO流程的LLMs提供了参考依据。
