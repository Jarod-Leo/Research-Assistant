# From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient LLMs

链接: http://arxiv.org/abs/2504.13471v1

原文摘要:
In recent years, Large Language Models (LLMs) have significantly advanced
artificial intelligence by optimizing traditional Natural Language Processing
(NLP) pipelines, improving performance and generalization. This has spurred
their integration into various systems. Many NLP systems, including ours,
employ a "one-stage" pipeline directly incorporating LLMs. While effective,
this approach incurs substantial costs and latency due to the need for large
model parameters to achieve satisfactory outcomes. This paper introduces a
three-stage cost-efficient end-to-end LLM deployment pipeline-including
prototyping, knowledge transfer, and model compression-to tackle the
cost-performance dilemma in LLM-based frameworks. Our approach yields a super
tiny model optimized for cost and performance in online systems, simplifying
the system architecture. Initially, by transforming complex tasks into a
function call-based LLM-driven pipeline, an optimal performance prototype
system is constructed to produce high-quality data as a teacher model. The
second stage combines techniques like rejection fine-tuning, reinforcement
learning, and knowledge distillation to transfer knowledge to a smaller 0.5B
student model, delivering effective performance at minimal cost. The final
stage applies quantization and pruning to extremely compress models to 0.4B,
achieving ultra-low latency and cost. The framework's modular design and
cross-domain capabilities suggest potential applicability in other NLP areas.

中文翻译:
近年来，大型语言模型（LLMs）通过优化传统自然语言处理（NLP）流程，显著提升了人工智能的性能与泛化能力，推动了其在各类系统中的集成应用。当前包括本团队在内的许多NLP系统采用"单阶段"流程直接部署LLMs，虽然效果显著，但为达到理想性能所需的大规模模型参数导致成本与延迟居高不下。本文提出一种三阶段高性价比端到端LLM部署方案——涵盖原型构建、知识迁移与模型压缩——以解决基于LLM框架的成本效益平衡难题。该方案最终产出专为在线系统优化的超小型模型，在简化架构的同时兼顾性能表现。首先通过将复杂任务转化为基于函数调用的LLM驱动流程，构建性能最优的原型系统作为教师模型生成高质量数据；第二阶段融合拒绝微调、强化学习与知识蒸馏等技术，将能力迁移至0.5B参量的学生模型，实现低成本高效能；最终采用量化与剪枝技术将模型极致压缩至0.4B，达成超低延迟与成本控制。该框架的模块化设计与跨领域适应性表明其具备拓展至其他NLP领域的潜力。
