# Mansformer: Efficient Transformer of Mixed Attention for Image Deblurring and Beyond

链接: http://arxiv.org/abs/2404.06135v1

原文摘要:
The Transformer architecture has achieved remarkable success in natural
language processing and high-level vision tasks over the past few years.
However, the inherent complexity of self-attention is quadratic to the size of
the image, leading to unaffordable computational costs for high-resolution
vision tasks. In this paper, we introduce Concertormer, featuring a novel
Concerto Self-Attention (CSA) mechanism designed for image deblurring. The
proposed CSA divides self-attention into two distinct components: one
emphasizes generally global and another concentrates on specifically local
correspondence. By retaining partial information in additional dimensions
independent from the self-attention calculations, our method effectively
captures global contextual representations with complexity linear to the image
size. To effectively leverage the additional dimensions, we present a
Cross-Dimensional Communication module, which linearly combines attention maps
and thus enhances expressiveness. Moreover, we amalgamate the two-staged
Transformer design into a single stage using the proposed gated-dconv MLP
architecture. While our primary objective is single-image motion deblurring,
extensive quantitative and qualitative evaluations demonstrate that our
approach performs favorably against the state-of-the-art methods in other
tasks, such as deraining and deblurring with JPEG artifacts. The source codes
and trained models will be made available to the public.

中文翻译:
Transformer架构在过去几年中已在自然语言处理及高层视觉任务上取得显著成功。然而，自注意力机制固有的计算复杂度与图像尺寸呈平方关系，导致高分辨率视觉任务面临难以承受的计算成本。本文提出Concertormer模型，其核心创新在于专为图像去模糊设计的协奏曲自注意力机制（CSA）。该机制将自注意力分解为两个独立组件：一个侧重全局普遍性关联，另一个聚焦局部特异性对应。通过在自注意力计算之外保留部分信息至额外维度，我们的方法以线性于图像尺寸的复杂度有效捕获全局上下文表征。为充分利用额外维度，我们设计了跨维度通信模块，通过线性组合注意力图来增强表达能力。此外，我们采用提出的门控深度卷积MLP架构，将传统两阶段Transformer设计融合为单阶段结构。虽然主要针对单图像运动去模糊任务，但大量定量与定性实验表明，本方法在去雨、JPEG压缩伪影去模糊等其他任务上也优于当前最先进技术。源代码与训练模型将向公众开放。
