# Deconstructing Recurrence, Attention, and Gating: Investigating the transferability of Transformers and Gated Recurrent Neural Networks in forecasting of dynamical systems

链接: http://arxiv.org/abs/2410.02654v1

原文摘要:
Machine learning architectures, including transformers and recurrent neural
networks (RNNs) have revolutionized forecasting in applications ranging from
text processing to extreme weather. Notably, advanced network architectures,
tuned for applications such as natural language processing, are transferable to
other tasks such as spatiotemporal forecasting tasks. However, there is a
scarcity of ablation studies to illustrate the key components that enable this
forecasting accuracy. The absence of such studies, although explainable due to
the associated computational cost, intensifies the belief that these models
ought to be considered as black boxes. In this work, we decompose the key
architectural components of the most powerful neural architectures, namely
gating and recurrence in RNNs, and attention mechanisms in transformers. Then,
we synthesize and build novel hybrid architectures from the standard blocks,
performing ablation studies to identify which mechanisms are effective for each
task. The importance of considering these components as hyper-parameters that
can augment the standard architectures is exhibited on various forecasting
datasets, from the spatiotemporal chaotic dynamics of the multiscale Lorenz 96
system, the Kuramoto-Sivashinsky equation, as well as standard real world
time-series benchmarks. A key finding is that neural gating and attention
improves the performance of all standard RNNs in most tasks, while the addition
of a notion of recurrence in transformers is detrimental. Furthermore, our
study reveals that a novel, sparsely used, architecture which integrates
Recurrent Highway Networks with neural gating and attention mechanisms, emerges
as the best performing architecture in high-dimensional spatiotemporal
forecasting of dynamical systems.

中文翻译:
机器学习架构，包括Transformer和循环神经网络(RNN)，已在从文本处理到极端天气预测等广泛领域中引发了预测技术的革命。值得注意的是，针对自然语言处理等任务优化的先进网络架构，可迁移至时空预测等其他任务。然而，目前缺乏系统性的消融研究来阐明实现高预测精度的关键组件。尽管高计算成本可以解释这类研究的缺失，但这种情况加剧了将这些模型视为"黑箱"的认知。本研究解构了最强大神经架构的核心组件——RNN中的门控与循环机制，以及Transformer中的注意力机制，通过标准模块的合成与创新性组合构建混合架构，开展消融实验以识别各任务的有效机制。通过在多尺度Lorenz 96系统的时空混沌动力学、Kuramoto-Sivashinsky方程以及标准现实世界时间序列基准测试等数据集上的实验，证明了将这些组件视为可增强标准架构的超参数的重要性。关键发现表明：神经门控和注意力机制能提升所有标准RNN在多数任务中的表现，而在Transformer中引入循环概念则会产生负面影响。此外，研究揭示了一种新型稀疏架构——将循环高速公路网络与神经门控及注意力机制相融合，在动态系统高维时空预测任务中展现出最优性能。
