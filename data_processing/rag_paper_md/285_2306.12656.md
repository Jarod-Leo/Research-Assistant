# Identifying and Extracting Rare Disease Phenotypes with Large Language Models

链接: http://arxiv.org/abs/2306.12656v1

原文摘要:
Rare diseases (RDs) are collectively common and affect 300 million people
worldwide. Accurate phenotyping is critical for informing diagnosis and
treatment, but RD phenotypes are often embedded in unstructured text and
time-consuming to extract manually. While natural language processing (NLP)
models can perform named entity recognition (NER) to automate extraction, a
major bottleneck is the development of a large, annotated corpus for model
training. Recently, prompt learning emerged as an NLP paradigm that can lead to
more generalizable results without any (zero-shot) or few labeled samples
(few-shot). Despite growing interest in ChatGPT, a revolutionary large language
model capable of following complex human prompts and generating high-quality
responses, none have studied its NER performance for RDs in the zero- and
few-shot settings. To this end, we engineered novel prompts aimed at extracting
RD phenotypes and, to the best of our knowledge, are the first the establish a
benchmark for evaluating ChatGPT's performance in these settings. We compared
its performance to the traditional fine-tuning approach and conducted an
in-depth error analysis. Overall, fine-tuning BioClinicalBERT resulted in
higher performance (F1 of 0.689) than ChatGPT (F1 of 0.472 and 0.591 in the
zero- and few-shot settings, respectively). Despite this, ChatGPT achieved
similar or higher accuracy for certain entities (i.e., rare diseases and signs)
in the one-shot setting (F1 of 0.776 and 0.725). This suggests that with
appropriate prompt engineering, ChatGPT has the potential to match or
outperform fine-tuned language models for certain entity types with just one
labeled sample. While the proliferation of large language models may provide
opportunities for supporting RD diagnosis and treatment, researchers and
clinicians should critically evaluate model outputs and be well-informed of
their limitations.

中文翻译:
以下是符合学术规范的中文翻译：

罕见病（Rare Diseases, RDs）总体发病率较高，全球患者数达3亿。精准的表型分析对临床诊疗至关重要，但罕见病表型数据常埋没于非结构化文本中，人工提取耗时费力。虽然自然语言处理（NLP）模型可通过命名实体识别（NER）实现自动化提取，但构建大规模标注语料库用于模型训练仍是主要瓶颈。近期兴起的提示学习（prompt learning）作为一种NLP范式，能够在零样本（zero-shot）或少样本（few-shot）条件下获得更具泛化性的结果。尽管业界对ChatGPT这种能理解复杂人类指令并生成高质量响应的革命性大语言模型兴趣日增，但尚未有研究评估其在零样本和少样本条件下对罕见病的NER性能。为此，我们设计了一套创新性提示方案用于提取罕见病表型特征，并首次建立了评估ChatGPT在此类场景性能的基准体系。通过与传统微调方法的对比研究和深度错误分析发现：BioClinicalBERT经微调后总体性能更优（F1值0.689），优于ChatGPT在零样本（F1值0.472）和少样本（F1值0.591）条件下的表现。但值得注意的是，在单样本场景中，ChatGPT对特定实体（如罕见疾病名称和体征）的识别准确率达到甚至超越微调模型（F1值分别为0.776和0.725）。这表明通过恰当的提示工程，ChatGPT仅需单个标注样本即有可能在特定实体类型上媲美或超越微调语言模型。尽管大语言模型的蓬勃发展可能为罕见病诊疗提供新机遇，但研究人员与临床医师仍需审慎评估模型输出，并充分认知其局限性。

（翻译说明：
1. 专业术语采用医学/NLP领域通用译法，如"phenotyping"译为"表型分析"、"prompt learning"译为"提示学习"
2. 保留关键模型名称原文（ChatGPT/BioClinicalBERT）符合学术惯例
3. 复杂长句按中文表达习惯拆分重组，如将"capable of..."定语从句转为独立分句
4. 计量单位"300 million"转换为中文习惯表述"3亿"
5. 技术指标"F1值"保留专业表述并添加具体数值
6. 最后补充的"翻译说明"仅为说明翻译策略，实际交付时可删除）
