# Benchmarking Large Language Models for Molecule Prediction Tasks

链接: http://arxiv.org/abs/2403.05075v1

原文摘要:
Large Language Models (LLMs) stand at the forefront of a number of Natural
Language Processing (NLP) tasks. Despite the widespread adoption of LLMs in
NLP, much of their potential in broader fields remains largely unexplored, and
significant limitations persist in their design and implementation. Notably,
LLMs struggle with structured data, such as graphs, and often falter when
tasked with answering domain-specific questions requiring deep expertise, such
as those in biology and chemistry. In this paper, we explore a fundamental
question: Can LLMs effectively handle molecule prediction tasks? Rather than
pursuing top-tier performance, our goal is to assess how LLMs can contribute to
diverse molecule tasks. We identify several classification and regression
prediction tasks across six standard molecule datasets. Subsequently, we
carefully design a set of prompts to query LLMs on these tasks and compare
their performance with existing Machine Learning (ML) models, which include
text-based models and those specifically designed for analysing the geometric
structure of molecules. Our investigation reveals several key insights:
Firstly, LLMs generally lag behind ML models in achieving competitive
performance on molecule tasks, particularly when compared to models adept at
capturing the geometric structure of molecules, highlighting the constrained
ability of LLMs to comprehend graph data. Secondly, LLMs show promise in
enhancing the performance of ML models when used collaboratively. Lastly, we
engage in a discourse regarding the challenges and promising avenues to harness
LLMs for molecule prediction tasks. The code and models are available at
https://github.com/zhiqiangzhongddu/LLMaMol.

中文翻译:
大型语言模型（LLMs）在众多自然语言处理（NLP）任务中处于领先地位。尽管LLMs在NLP领域已被广泛采用，但其在更广泛领域的潜力大多尚未开发，且其设计与实现仍存在显著局限。尤为突出的是，LLMs在处理结构化数据（如图形）时表现欠佳，且在回答需要深度专业知识的领域特定问题（如生物学和化学）时往往力不从心。本文探讨一个核心问题：LLMs能否有效处理分子预测任务？我们并不追求顶尖性能，而是旨在评估LLMs如何助力多样化的分子任务。我们在六个标准分子数据集中识别出若干分类与回归预测任务，随后精心设计一组提示词对LLMs进行任务查询，并将其表现与现有机器学习（ML）模型（包括基于文本的模型及专为分析分子几何结构设计的模型）进行对比。研究揭示了以下关键发现：首先，在分子任务上，LLMs普遍落后于ML模型，尤其与擅长捕捉分子几何结构的模型相比，突显了LLMs理解图数据的局限性；其次，当LLMs与ML模型协同使用时，展现出提升后者性能的潜力；最后，我们探讨了利用LLMs进行分子预测任务所面临的挑战与可行路径。代码与模型已发布于https://github.com/zhiqiangzhongddu/LLMaMol。
