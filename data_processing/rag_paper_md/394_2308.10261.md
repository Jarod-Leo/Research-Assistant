# How Good Are Large Language Models at Out-of-Distribution Detection?

链接: http://arxiv.org/abs/2308.10261v1

原文摘要:
Out-of-distribution (OOD) detection plays a vital role in enhancing the
reliability of machine learning (ML) models. The emergence of large language
models (LLMs) has catalyzed a paradigm shift within the ML community,
showcasing their exceptional capabilities across diverse natural language
processing tasks. While existing research has probed OOD detection with
relative small-scale Transformers like BERT, RoBERTa and GPT-2, the stark
differences in scales, pre-training objectives, and inference paradigms call
into question the applicability of these findings to LLMs. This paper embarks
on a pioneering empirical investigation of OOD detection in the domain of LLMs,
focusing on LLaMA series ranging from 7B to 65B in size. We thoroughly evaluate
commonly-used OOD detectors, scrutinizing their performance in both zero-grad
and fine-tuning scenarios. Notably, we alter previous discriminative
in-distribution fine-tuning into generative fine-tuning, aligning the
pre-training objective of LLMs with downstream tasks. Our findings unveil that
a simple cosine distance OOD detector demonstrates superior efficacy,
outperforming other OOD detectors. We provide an intriguing explanation for
this phenomenon by highlighting the isotropic nature of the embedding spaces of
LLMs, which distinctly contrasts with the anisotropic property observed in
smaller BERT family models. The new insight enhances our understanding of how
LLMs detect OOD data, thereby enhancing their adaptability and reliability in
dynamic environments. We have released the source code at
\url{https://github.com/Awenbocc/LLM-OOD} for other researchers to reproduce
our results.

中文翻译:
分布外（OOD）检测在提升机器学习模型可靠性方面发挥着关键作用。随着大语言模型（LLM）的出现，机器学习领域正经历范式转变，这些模型在多样化自然语言处理任务中展现出卓越能力。尽管现有研究已基于BERT、RoBERTa和GPT-2等中小规模Transformer模型探索过OOD检测，但模型规模、预训练目标和推理范式的显著差异使得这些结论在LLM上的适用性存疑。本文首次对LLM领域的OOD检测展开开创性实证研究，聚焦7B至65B参数的LLaMA系列模型。我们系统评估了常用OOD检测器，深入分析其在零梯度与微调场景下的表现。值得注意的是，我们将以往判别式分布内微调改为生成式微调，使LLM的预训练目标与下游任务保持一致。研究发现，简单的余弦距离OOD检测器展现出卓越效能，优于其他检测方法。通过揭示LLM嵌入空间各向同性的本质特征——这与较小规模BERT家族模型表现出的各向异性形成鲜明对比——我们为这一现象提供了新颖解释。该发现深化了我们对LLM识别OOD数据机制的理解，从而增强其在动态环境中的适应性与可靠性。我们已开源代码（见文末链接）以供复现研究结果。
