# How Good Are Large Language Models at Out-of-Distribution Detection?

链接: http://arxiv.org/abs/2308.10261v1

原文摘要:
Out-of-distribution (OOD) detection plays a vital role in enhancing the
reliability of machine learning (ML) models. The emergence of large language
models (LLMs) has catalyzed a paradigm shift within the ML community,
showcasing their exceptional capabilities across diverse natural language
processing tasks. While existing research has probed OOD detection with
relative small-scale Transformers like BERT, RoBERTa and GPT-2, the stark
differences in scales, pre-training objectives, and inference paradigms call
into question the applicability of these findings to LLMs. This paper embarks
on a pioneering empirical investigation of OOD detection in the domain of LLMs,
focusing on LLaMA series ranging from 7B to 65B in size. We thoroughly evaluate
commonly-used OOD detectors, scrutinizing their performance in both zero-grad
and fine-tuning scenarios. Notably, we alter previous discriminative
in-distribution fine-tuning into generative fine-tuning, aligning the
pre-training objective of LLMs with downstream tasks. Our findings unveil that
a simple cosine distance OOD detector demonstrates superior efficacy,
outperforming other OOD detectors. We provide an intriguing explanation for
this phenomenon by highlighting the isotropic nature of the embedding spaces of
LLMs, which distinctly contrasts with the anisotropic property observed in
smaller BERT family models. The new insight enhances our understanding of how
LLMs detect OOD data, thereby enhancing their adaptability and reliability in
dynamic environments. We have released the source code at
\url{https://github.com/Awenbocc/LLM-OOD} for other researchers to reproduce
our results.

中文翻译:
分布外检测（OOD检测）对于提升机器学习模型的可靠性具有关键作用。随着大语言模型（LLM）的崛起，机器学习领域正经历范式转变，这些模型在各类自然语言处理任务中展现出卓越性能。尽管现有研究已基于BERT、RoBERTa和GPT-2等中小规模Transformer模型探索过OOD检测，但LLM在模型规模、预训练目标和推理范式上的显著差异，使得这些结论的适用性存疑。本文首次对LLM领域的OOD检测展开开创性实证研究，重点分析了7B至65B参数的LLaMA系列模型。我们系统评估了常用OOD检测器，在零梯度与微调两种场景下检验其性能。值得注意的是，我们将传统的判别式分布内微调改进为生成式微调，使LLM的预训练目标与下游任务保持一致。研究发现，简单的余弦距离OOD检测器展现出卓越效能，优于其他检测方法。我们通过揭示LLM嵌入空间的各向同性特性（这与小型BERT系列模型表现出的各向异性形成鲜明对比），对这一现象给出了新颖解释。这一发现深化了我们对LLM识别分布外数据机制的理解，从而提升其在动态环境中的适应性与可靠性。相关源代码已发布于\url{https://github.com/Awenbocc/LLM-OOD}以供复现研究。

（翻译说明：
1. 专业术语处理："isotropic/anisotropic"采用"各向同性/各向异性"标准译法，"zero-grad"译为"零梯度"
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句，如将"Notably, we alter..."独立成句并添加"值得注意的是"作为衔接
3. 概念显化："paradigm shift"译为"范式转变"并补充"机器学习领域"明确主体
4. 被动语态转换："are released"主动化为"已发布"
5. 学术风格保持：使用"实证研究""效能""微调"等学术用语
6. 数字规范：统一使用"7B至65B"格式保持技术文档一致性）
