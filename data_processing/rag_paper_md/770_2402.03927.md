# Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs

链接: http://arxiv.org/abs/2402.03927v1

原文摘要:
Natural Language Processing (NLP) research is increasingly focusing on the
use of Large Language Models (LLMs), with some of the most popular ones being
either fully or partially closed-source. The lack of access to model details,
especially regarding training data, has repeatedly raised concerns about data
contamination among researchers. Several attempts have been made to address
this issue, but they are limited to anecdotal evidence and trial and error.
Additionally, they overlook the problem of \emph{indirect} data leaking, where
models are iteratively improved by using data coming from users. In this work,
we conduct the first systematic analysis of work using OpenAI's GPT-3.5 and
GPT-4, the most prominently used LLMs today, in the context of data
contamination. By analysing 255 papers and considering OpenAI's data usage
policy, we extensively document the amount of data leaked to these models
during the first year after the model's release. We report that these models
have been globally exposed to $\sim$4.7M samples from 263 benchmarks. At the
same time, we document a number of evaluation malpractices emerging in the
reviewed papers, such as unfair or missing baseline comparisons and
reproducibility issues. We release our results as a collaborative project on
https://leak-llm.github.io/, where other researchers can contribute to our
efforts.

中文翻译:
自然语言处理（NLP）研究日益聚焦于大型语言模型（LLMs）的应用，其中一些最流行的模型要么完全闭源，要么部分闭源。由于无法获取模型细节（尤其是训练数据相关信息），数据污染问题屡屡引发研究者担忧。尽管已有若干尝试解决该问题，但这些努力仅停留在个案经验与试错层面，且忽视了用户数据迭代优化模型导致的间接数据泄露现象。本研究首次针对当前最主流的OpenAI GPT-3.5和GPT-4模型，在数据污染背景下进行了系统性分析。通过审查255篇论文并结合OpenAI的数据使用政策，我们全面记录了模型发布首年内泄露至这些模型的数据规模：全球范围内这两个模型已接触过来自263个基准测试的约470万样本数据。同时，我们发现在审阅论文中存在若干评估失范行为，包括不公平或缺失的基线对比、可复现性缺陷等问题。相关成果已发布于https://leak-llm.github.io/协作项目，欢迎学界同仁共同完善。
