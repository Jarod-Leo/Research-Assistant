# GIELLM: Japanese General Information Extraction Large Language Model Utilizing Mutual Reinforcement Effect

链接: http://arxiv.org/abs/2311.06838v1

原文摘要:
Information Extraction (IE) stands as a cornerstone in natural language
processing, traditionally segmented into distinct sub-tasks. The advent of
Large Language Models (LLMs) heralds a paradigm shift, suggesting the
feasibility of a singular model addressing multiple IE subtasks. In this vein,
we introduce the General Information Extraction Large Language Model (GIELLM),
which integrates text Classification, Sentiment Analysis, Named Entity
Recognition, Relation Extraction, and Event Extraction using a uniform
input-output schema. This innovation marks the first instance of a model
simultaneously handling such a diverse array of IE subtasks. Notably, the
GIELLM leverages the Mutual Reinforcement Effect (MRE), enhancing performance
in integrated tasks compared to their isolated counterparts. Our experiments
demonstrate State-of-the-Art (SOTA) results in five out of six Japanese mixed
datasets, significantly surpassing GPT-3.5-Turbo. Further, an independent
evaluation using the novel Text Classification Relation and Event
Extraction(TCREE) dataset corroborates the synergistic advantages of MRE in
text and word classification. This breakthrough paves the way for most IE
subtasks to be subsumed under a singular LLM framework. Specialized fine-tune
task-specific models are no longer needed.

中文翻译:
信息抽取（IE）作为自然语言处理领域的基石，传统上被划分为多个独立子任务。随着大语言模型（LLM）的出现，这一范式正经历革命性转变——单一模型处理多重IE子任务成为可能。在此背景下，我们提出通用信息抽取大语言模型（GIELLM），通过统一输入输出框架整合文本分类、情感分析、命名实体识别、关系抽取和事件抽取五大功能。这一创新首次实现了单一模型同步处理如此多元的IE子任务组合。值得注意的是，GIELLM利用互增强效应（MRE），使集成任务的性能显著超越孤立任务场景。实验数据显示，该模型在六分之五的日语混合数据集上达到最先进水平（SOTA），性能大幅超越GPT-3.5-Turbo。此外，基于新型文本分类-关系-事件联合抽取（TCREE）数据集的独立评估，进一步验证了MRE在文本与词汇分类中的协同优势。这一突破性进展昭示着多数IE子任务可被纳入统一的大语言模型框架，特定任务的专用微调模型将不再必要。
