# CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models

链接: http://arxiv.org/abs/2407.02408v1

原文摘要:
As Large Language Models (LLMs) are increasingly deployed to handle various
natural language processing (NLP) tasks, concerns regarding the potential
negative societal impacts of LLM-generated content have also arisen. To
evaluate the biases exhibited by LLMs, researchers have recently proposed a
variety of datasets. However, existing bias evaluation efforts often focus on
only a particular type of bias and employ inconsistent evaluation metrics,
leading to difficulties in comparison across different datasets and LLMs. To
address these limitations, we collect a variety of datasets designed for the
bias evaluation of LLMs, and further propose CEB, a Compositional Evaluation
Benchmark that covers different types of bias across different social groups
and tasks. The curation of CEB is based on our newly proposed compositional
taxonomy, which characterizes each dataset from three dimensions: bias types,
social groups, and tasks. By combining the three dimensions, we develop a
comprehensive evaluation strategy for the bias in LLMs. Our experiments
demonstrate that the levels of bias vary across these dimensions, thereby
providing guidance for the development of specific bias mitigation methods.

中文翻译:
随着大语言模型（LLMs）被日益广泛地部署以处理各类自然语言处理（NLP）任务，关于其生成内容可能带来的负面社会影响的担忧也随之浮现。为评估LLMs表现出的偏见，研究者近期提出了多种测评数据集。然而现有偏见评估工作往往仅关注特定类型的偏见，且采用不一致的评估指标，导致不同数据集与模型间的比较存在困难。针对这些局限性，我们收集了专为LLM偏见评估设计的多样化数据集，并进一步提出CEB——一个涵盖不同社会群体与任务场景下多种偏见类型的组合式评估基准。该基准的构建基于我们新提出的组合分类法，该方法从偏见类型、社会群体和任务三个维度对每个数据集进行特征刻画。通过三维度的有机组合，我们为LLM偏见开发了一套综合性评估策略。实验表明，偏见程度在这些维度上呈现显著差异，从而为开发针对性的偏见缓解方法提供了方向指引。
