# CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models

链接: http://arxiv.org/abs/2407.02408v1

原文摘要:
As Large Language Models (LLMs) are increasingly deployed to handle various
natural language processing (NLP) tasks, concerns regarding the potential
negative societal impacts of LLM-generated content have also arisen. To
evaluate the biases exhibited by LLMs, researchers have recently proposed a
variety of datasets. However, existing bias evaluation efforts often focus on
only a particular type of bias and employ inconsistent evaluation metrics,
leading to difficulties in comparison across different datasets and LLMs. To
address these limitations, we collect a variety of datasets designed for the
bias evaluation of LLMs, and further propose CEB, a Compositional Evaluation
Benchmark that covers different types of bias across different social groups
and tasks. The curation of CEB is based on our newly proposed compositional
taxonomy, which characterizes each dataset from three dimensions: bias types,
social groups, and tasks. By combining the three dimensions, we develop a
comprehensive evaluation strategy for the bias in LLMs. Our experiments
demonstrate that the levels of bias vary across these dimensions, thereby
providing guidance for the development of specific bias mitigation methods.

中文翻译:
随着大语言模型（LLMs）被日益广泛地应用于各类自然语言处理（NLP）任务，关于其生成内容可能带来负面社会影响的担忧也随之显现。为评估LLMs表现出的偏见，研究者近期提出了多种测评数据集。然而现有偏见评估工作往往仅关注特定偏见类型，且采用不一致的评估指标，导致不同数据集与LLMs间的横向比较存在困难。为突破这些局限，我们汇集了专为LLM偏见评估设计的多样化数据集，并进一步提出CEB——一个覆盖不同社会群体与任务场景下多种偏见类型的组合式评估基准。CEB的构建基于我们新提出的组合分类法，该方法从三个维度对数据集进行特征刻画：偏见类型、社会群体和任务场景。通过三维度的有机组合，我们建立了针对LLM偏见的系统性评估策略。实验表明，偏见程度在不同维度上呈现显著差异，这为开发针对性去偏见方法提供了重要依据。

（译文特点说明：
1. 专业术语统一处理："bias"根据语境分别译为"偏见/偏差"，在学术语境中保持"偏见"的译法
2. 复杂句式重构：将原文复合长句拆解为符合中文表达习惯的短句结构，如将"To address..."处理为因果句式
3. 概念显化翻译："compositional taxonomy"译为"组合分类法"以突出其方法论特征
4. 动态对等处理："mitigation methods"译为"去偏见方法"比直译"缓解方法"更符合NLP领域术语习惯
5. 学术风格保持：使用"横向比较""系统性评估"等符合中文论文摘要表达的措辞）
