# Instruction Tuning for Story Understanding and Generation with Weak Supervision

链接: http://arxiv.org/abs/2501.15574v1

原文摘要:
Story understanding and generation have long been a challenging task in
natural language processing (NLP), especially when dealing with various levels
of instruction specificity. In this paper, we propose a novel approach called
"Weak to Strong Instruction Tuning" for improving story generation by tuning
models with instructions of varying clarity. We explore the potential of large
language models (LLMs) to adapt to different types of instructions, weak and
strong, and show that our method significantly enhances performance in story
comprehension and generation. By leveraging the strength of instruction tuning,
we train models to understand the nuances of story plots, characters, and
themes while generating coherent and engaging narratives. Through extensive
experiments on several benchmark datasets and comparison with state-of-the-art
baselines, we demonstrate that our method outperforms existing techniques,
yielding substantial improvements in both automatic evaluation metrics and
human evaluations. Our work shows that adaptive instruction tuning can be a
powerful tool in refining generative models for complex narrative tasks.

中文翻译:
故事理解与生成一直是自然语言处理领域中的挑战性任务，尤其在处理不同明确程度的指令时更为显著。本文提出了一种名为"弱监督到强监督指令微调"的创新方法，通过使用清晰度各异的指令对模型进行微调，从而提升故事生成质量。我们探究了大语言模型适应"弱指令"与"强指令"的潜力，证明该方法能显著增强故事理解与生成能力。借助指令微调的优势，我们训练模型在生成连贯且引人入胜的叙事时，能精准把握故事情节、人物角色和主题的细微差别。通过在多个基准数据集上的大量实验，并与最先进基线模型进行对比，结果表明我们的方法在自动评估指标和人工评估中均优于现有技术，实现了显著提升。这项研究证实了自适应指令微调可作为优化复杂叙事任务生成模型的有效工具。
