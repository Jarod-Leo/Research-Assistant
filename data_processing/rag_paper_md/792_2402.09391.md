# LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset

链接: http://arxiv.org/abs/2402.09391v1

原文摘要:
Chemistry plays a crucial role in many domains, such as drug discovery and
material science. While large language models (LLMs) such as GPT-4 exhibit
remarkable capabilities on natural language processing tasks, existing research
indicates that their performance on chemistry tasks is discouragingly low. In
this paper, however, we demonstrate that our developed LLMs can achieve very
strong results on a comprehensive set of chemistry tasks, outperforming the
most advanced GPT-4 and Claude 3 Opus by a substantial margin. To accomplish
this, we propose SMolInstruct, a large-scale, comprehensive, and high-quality
dataset for instruction tuning. It contains 14 selected chemistry tasks and
over three million samples, laying a solid foundation for training and
evaluating LLMs for chemistry. Using SMolInstruct, we fine-tune a set of
open-source LLMs, among which, we find that Mistral serves as the best base
model for chemistry tasks. Our analysis further demonstrates the critical role
of the proposed dataset in driving the performance improvements.

中文翻译:
化学在药物研发与材料科学等诸多领域扮演着关键角色。尽管GPT-4等大语言模型在自然语言处理任务中展现出卓越能力，但现有研究表明其在化学任务上的表现不尽如人意。本文通过实证表明，我们开发的大语言模型能在综合性化学任务集上取得显著优于最先进的GPT-4和Claude 3 Opus的突破性成果。为此，我们构建了SMolInstruct——一个大规模、全方位、高质量的指令微调数据集，涵盖14项精选化学任务及超300万样本，为化学领域大语言模型的训练与评估奠定了坚实基础。基于该数据集对开源大语言模型进行微调后，我们发现Mistral展现出最佳的化学任务适配性。进一步分析证实，所构建数据集对模型性能提升具有决定性作用。
