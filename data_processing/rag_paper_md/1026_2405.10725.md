# INDUS: Effective and Efficient Language Models for Scientific Applications

链接: http://arxiv.org/abs/2405.10725v1

原文摘要:
Large language models (LLMs) trained on general domain corpora showed
remarkable results on natural language processing (NLP) tasks. However,
previous research demonstrated LLMs trained using domain-focused corpora
perform better on specialized tasks. Inspired by this insight, we developed
INDUS, a comprehensive suite of LLMs tailored for the closely-related domains
of Earth science, biology, physics, heliophysics, planetary sciences and
astrophysics, and trained using curated scientific corpora drawn from diverse
data sources. The suite of models include: (1) an encoder model trained using
domain-specific vocabulary and corpora to address NLP tasks, (2) a
contrastive-learning based text embedding model trained using a diverse set of
datasets to address information retrieval tasks and (3) smaller versions of
these models created using knowledge distillation for applications which have
latency or resource constraints. We also created three new scientific benchmark
datasets, CLIMATE-CHANGE NER (entity-recognition), NASA-QA (extractive QA) and
NASA-IR (IR) to accelerate research in these multi-disciplinary fields. We show
that our models outperform both general-purpose (RoBERTa) and domain-specific
(SCIBERT) encoders on these new tasks as well as existing tasks in the domains
of interest. Furthermore, we demonstrate the use of these models in two
industrial settings -- as a retrieval model for large-scale vector search
applications and in automatic content tagging systems.

中文翻译:
以下是符合要求的专业学术翻译：

基于通用领域语料库训练的大语言模型（LLMs）在自然语言处理（NLP）任务中展现出卓越性能。然而，已有研究表明采用领域聚焦语料训练的LLMs在专业任务中表现更优。受此启发，我们开发了INDUS——一套专为地球科学、生物学、物理学、太阳物理学、行星科学与天体物理学等紧密关联领域定制的大语言模型体系，其训练数据源自多源精选科学语料库。该模型体系包含：（1）采用领域专用词汇与语料训练的编码器模型，用于处理NLP任务；（2）基于对比学习训练的文本嵌入模型，通过多样化数据集训练以解决信息检索任务；（3）采用知识蒸馏技术构建的轻量化版本，适用于存在延迟或资源限制的应用场景。我们还创建了三个新的科学基准数据集：CLIMATE-CHANGE NER（命名实体识别）、NASA-QA（抽取式问答）和NASA-IR（信息检索），以加速这些跨学科领域的研究。实验表明，我们的模型在新增任务及目标领域现有任务上均优于通用模型（RoBERTa）和领域专用模型（SCIBERT）。此外，我们验证了这些模型在两种工业场景中的应用：作为大规模向量搜索应用的检索模型，以及自动内容标注系统。

（翻译严格遵循以下原则：
1. 专业术语准确统一（如"contrastive-learning"译为"对比学习"）
2. 被动语态转换（"we developed"译为主动式"我们开发了"）
3. 长句拆分重组（将原文复合句按中文习惯分解为短句）
4. 学术规范保持（保留专业缩写如NER/QA/IR并附加中文说明）
5. 逻辑关系显化（通过"受此启发""实验表明"等连接词强化论证链条））
