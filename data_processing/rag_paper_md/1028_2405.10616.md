# Feature-based Low-Rank Compression of Large Language Models via Bayesian Optimization

链接: http://arxiv.org/abs/2405.10616v1

原文摘要:
In recent years, large language models (LLMs) have driven advances in natural
language processing. Still, their growing scale has increased the computational
burden, necessitating a balance between efficiency and performance. Low-rank
compression, a promising technique, reduces non-essential parameters by
decomposing weight matrices into products of two low-rank matrices. Yet, its
application in LLMs has not been extensively studied. The key to low-rank
compression lies in low-rank factorization and low-rank dimensions allocation.
To address the challenges of low-rank compression in LLMs, we conduct empirical
research on the low-rank characteristics of large models. We propose a low-rank
compression method suitable for LLMs. This approach involves precise estimation
of feature distributions through pooled covariance matrices and a Bayesian
optimization strategy for allocating low-rank dimensions. Experiments on the
LLaMA-2 models demonstrate that our method outperforms existing strong
structured pruning and low-rank compression techniques in maintaining model
performance at the same compression ratio.

中文翻译:
近年来，大型语言模型（LLM）推动了自然语言处理领域的进步。然而，模型规模的持续增长带来了显著的计算负担，需要在效率与性能之间寻求平衡。低秩压缩作为一种前景广阔的技术，通过将权重矩阵分解为两个低秩矩阵的乘积来削减非必要参数，但该技术在LLM中的应用尚未得到充分研究。低秩压缩的核心在于低秩分解策略和秩维度分配方案。

针对LLM低秩压缩的技术挑战，我们对大模型的低秩特性展开实证研究，提出了一种适用于LLM的低秩压缩方法。该方法通过协方差矩阵池化实现特征分布的精确估计，并采用贝叶斯优化策略进行秩维度分配。在LLaMA-2模型上的实验表明，在相同压缩率下，我们的方法在保持模型性能方面优于现有主流的结构化剪枝和低秩压缩技术。

（译文说明：采用技术论文的严谨表述风格，对长句进行合理切分，确保专业术语准确统一。通过"实证研究""协方差矩阵池化"等表述体现学术规范性，使用"前景广阔""显著"等程度副词保持原文的强调语气。将"Bayesian optimization strategy"译为专业术语"贝叶斯优化策略"，"structured pruning"译为业界通用表述"结构化剪枝"，确保领域适应性。）
