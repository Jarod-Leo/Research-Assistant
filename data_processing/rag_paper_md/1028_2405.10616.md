# Feature-based Low-Rank Compression of Large Language Models via Bayesian Optimization

链接: http://arxiv.org/abs/2405.10616v1

原文摘要:
In recent years, large language models (LLMs) have driven advances in natural
language processing. Still, their growing scale has increased the computational
burden, necessitating a balance between efficiency and performance. Low-rank
compression, a promising technique, reduces non-essential parameters by
decomposing weight matrices into products of two low-rank matrices. Yet, its
application in LLMs has not been extensively studied. The key to low-rank
compression lies in low-rank factorization and low-rank dimensions allocation.
To address the challenges of low-rank compression in LLMs, we conduct empirical
research on the low-rank characteristics of large models. We propose a low-rank
compression method suitable for LLMs. This approach involves precise estimation
of feature distributions through pooled covariance matrices and a Bayesian
optimization strategy for allocating low-rank dimensions. Experiments on the
LLaMA-2 models demonstrate that our method outperforms existing strong
structured pruning and low-rank compression techniques in maintaining model
performance at the same compression ratio.

中文翻译:
近年来，大型语言模型（LLMs）推动了自然语言处理领域的进步。然而，其日益增长的规模也带来了更大的计算负担，需要在效率与性能之间寻求平衡。低秩压缩作为一种前景广阔的技术，通过将权重矩阵分解为两个低秩矩阵的乘积来削减非必要参数，但该技术在LLMs中的应用尚未得到充分研究。低秩压缩的核心在于低秩分解策略和秩维度分配方案。

针对LLMs低秩压缩面临的挑战，本研究对大模型的低秩特性展开实证分析，提出了一种适配LLMs的低秩压缩方法。该方法通过池化协方差矩阵实现特征分布的精确估计，并采用贝叶斯优化策略进行秩维度分配。在LLaMA-2系列模型上的实验表明，在相同压缩率下，本方法在保持模型性能方面优于现有主流结构化剪枝和低秩压缩技术。
