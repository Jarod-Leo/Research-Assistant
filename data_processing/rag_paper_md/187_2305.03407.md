# Online Gesture Recognition using Transformer and Natural Language Processing

链接: http://arxiv.org/abs/2305.03407v1

原文摘要:
The Transformer architecture is shown to provide a powerful machine
transduction framework for online handwritten gestures corresponding to glyph
strokes of natural language sentences. The attention mechanism is successfully
used to create latent representations of an end-to-end encoder-decoder model,
solving multi-level segmentation while also learning some language features and
syntax rules. The additional use of a large decoding space with some learned
Byte-Pair-Encoding (BPE) is shown to provide robustness to ablated inputs and
syntax rules. The encoder stack was directly fed with spatio-temporal data
tokens potentially forming an infinitely large input vocabulary, an approach
that finds applications beyond that of this work. Encoder transfer learning
capabilities is also demonstrated on several languages resulting in faster
optimisation and shared parameters. A new supervised dataset of online
handwriting gestures suitable for generic handwriting recognition tasks was
used to successfully train a small transformer model to an average normalised
Levenshtein accuracy of 96% on English or German sentences and 94% in French.

中文翻译:
研究表明，Transformer架构为在线手写笔迹（对应自然语言句子的字符笔画）提供了强大的机器转导框架。该模型成功利用注意力机制构建端到端编码器-解码器的潜在表征，在解决多层级分割问题的同时，还能学习部分语言特征和句法规则。实验证明，结合大容量解码空间与经过训练的字节对编码（BPE）能有效提升模型对残缺输入及句法规则的鲁棒性。编码器堆栈直接处理可能形成无限大输入词汇表的时空数据标记，这种方法具有超越本研究的应用潜力。跨语言编码器迁移学习实验表明，该架构能实现参数共享并加速优化过程。研究采用新型在线手写笔迹监督数据集（适用于通用手写识别任务），成功训练出的小型Transformer模型在英语/德语句子上达到96%的平均归一化Levenshtein准确率，法语准确率为94%。
