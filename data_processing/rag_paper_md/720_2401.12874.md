# From Understanding to Utilization: A Survey on Explainability for Large Language Models

链接: http://arxiv.org/abs/2401.12874v1

原文摘要:
Explainability for Large Language Models (LLMs) is a critical yet challenging
aspect of natural language processing. As LLMs are increasingly integral to
diverse applications, their "black-box" nature sparks significant concerns
regarding transparency and ethical use. This survey underscores the imperative
for increased explainability in LLMs, delving into both the research on
explainability and the various methodologies and tasks that utilize an
understanding of these models. Our focus is primarily on pre-trained
Transformer-based LLMs, such as LLaMA family, which pose distinctive
interpretability challenges due to their scale and complexity. In terms of
existing methods, we classify them into local and global analyses, based on
their explanatory objectives. When considering the utilization of
explainability, we explore several compelling methods that concentrate on model
editing, control generation, and model enhancement. Additionally, we examine
representative evaluation metrics and datasets, elucidating their advantages
and limitations. Our goal is to reconcile theoretical and empirical
understanding with practical implementation, proposing exciting avenues for
explanatory techniques and their applications in the LLMs era.

中文翻译:
大语言模型(LLMs)的可解释性作为自然语言处理领域的关键挑战，正随着这类模型在多元化应用中的深度整合而引发对透明度与伦理使用的广泛关注。本综述通过系统梳理可解释性研究及其方法论体系，着重探讨了基于预训练Transformer架构的LLMs（如LLaMA系列）所特有的可解释性难题——这些模型因其庞大规模与复杂结构对传统解释方法提出了全新要求。在现有技术层面，我们依据解释目标将方法体系划分为局部分析与全局分析两大范式；在应用维度上，则聚焦模型编辑、可控生成与性能增强三大核心场景的创新性解决方案。研究同时剖析了具有代表性的评估指标与基准数据集，辩证揭示其优势与局限。本文旨在 bridging 理论认知与实践落地的鸿沟，为LLMs时代的解释技术发展及其应用创新提供前瞻性研究路径。
