# Questioning Internal Knowledge Structure of Large Language Models Through the Lens of the Olympic Games

链接: http://arxiv.org/abs/2409.06518v1

原文摘要:
Large language models (LLMs) have become a dominant approach in natural
language processing, yet their internal knowledge structures remain largely
unexplored. In this paper, we analyze the internal knowledge structures of LLMs
using historical medal tallies from the Olympic Games. We task the models with
providing the medal counts for each team and identifying which teams achieved
specific rankings. Our results reveal that while state-of-the-art LLMs perform
remarkably well in reporting medal counts for individual teams, they struggle
significantly with questions about specific rankings. This suggests that the
internal knowledge structures of LLMs are fundamentally different from those of
humans, who can easily infer rankings from known medal counts. To support
further research, we publicly release our code, dataset, and model outputs.

中文翻译:
大型语言模型（LLMs）已成为自然语言处理领域的主流方法，但其内部知识结构仍鲜为人知。本文通过分析奥运会历史奖牌数据，探究LLMs的内部知识组织形式。我们要求模型完成两项任务：提供各代表队的奖牌总数，以及识别特定名次对应的代表队。实验结果表明，尽管最先进的LLMs在单个代表队的奖牌统计上表现优异，但在涉及具体名次的问题上却存在显著困难。这表明LLMs的知识组织结构与人类存在本质差异——人类能轻松从已知奖牌数推导出排名，而模型则不然。为促进后续研究，我们公开了代码、数据集及模型输出结果。
