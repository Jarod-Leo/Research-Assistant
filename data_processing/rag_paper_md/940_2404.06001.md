# Privacy Preserving Prompt Engineering: A Survey

链接: http://arxiv.org/abs/2404.06001v1

原文摘要:
Pre-trained language models (PLMs) have demonstrated significant proficiency
in solving a wide range of general natural language processing (NLP) tasks.
Researchers have observed a direct correlation between the performance of these
models and their sizes. As a result, the sizes of these models have notably
expanded in recent years, persuading researchers to adopt the term large
language models (LLMs) to characterize the larger-sized PLMs. The size
expansion comes with a distinct capability called in-context learning (ICL),
which represents a special form of prompting and allows the models to be
utilized through the presentation of demonstration examples without
modifications to the model parameters. Although interesting, privacy concerns
have become a major obstacle in its widespread usage. Multiple studies have
examined the privacy risks linked to ICL and prompting in general, and have
devised techniques to alleviate these risks. Thus, there is a necessity to
organize these mitigation techniques for the benefit of the community. This
survey provides a systematic overview of the privacy protection methods
employed during ICL and prompting in general. We review, analyze, and compare
different methods under this paradigm. Furthermore, we provide a summary of the
resources accessible for the development of these frameworks. Finally, we
discuss the limitations of these frameworks and offer a detailed examination of
the promising areas that necessitate further exploration.

中文翻译:
预训练语言模型（PLMs）在解决各类通用自然语言处理（NLP）任务中展现出卓越能力。研究发现模型性能与其规模呈直接正相关，这促使近年来模型规模显著扩大，学界因此采用"大语言模型（LLMs）"特指这类超大规模PLMs。规模扩张带来了一项独特能力——上下文学习（ICL），这种特殊的提示形式仅需展示示例即可调用模型，无需调整参数。尽管颇具价值，隐私问题却成为其广泛应用的主要障碍。已有大量研究探讨了ICL及提示技术相关的隐私风险，并提出了多种风险缓解方案。为此，系统梳理这些防护技术对研究社区具有重要意义。本综述首次对ICL及通用提示场景下的隐私保护方法进行了体系化梳理，通过对比分析不同技术路线的优劣，整合了相关开发资源库。最后，我们深入探讨了现有框架的局限性，并对亟待突破的研究方向进行了前瞻性分析。
