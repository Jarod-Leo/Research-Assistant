# Vision-Language Models for Edge Networks: A Comprehensive Survey

链接: http://arxiv.org/abs/2502.07855v1

原文摘要:
Vision Large Language Models (VLMs) combine visual understanding with natural
language processing, enabling tasks like image captioning, visual question
answering, and video analysis. While VLMs show impressive capabilities across
domains such as autonomous vehicles, smart surveillance, and healthcare, their
deployment on resource-constrained edge devices remains challenging due to
processing power, memory, and energy limitations. This survey explores recent
advancements in optimizing VLMs for edge environments, focusing on model
compression techniques, including pruning, quantization, knowledge
distillation, and specialized hardware solutions that enhance efficiency. We
provide a detailed discussion of efficient training and fine-tuning methods,
edge deployment challenges, and privacy considerations. Additionally, we
discuss the diverse applications of lightweight VLMs across healthcare,
environmental monitoring, and autonomous systems, illustrating their growing
impact. By highlighting key design strategies, current challenges, and offering
recommendations for future directions, this survey aims to inspire further
research into the practical deployment of VLMs, ultimately making advanced AI
accessible in resource-limited settings.

中文翻译:
视觉大语言模型（VLMs）将视觉理解与自然语言处理相结合，能够执行图像描述生成、视觉问答及视频分析等任务。尽管这类模型在自动驾驶、智能安防和医疗健康等领域展现出卓越能力，但由于算力、内存和能耗的限制，其在资源受限的边缘设备上的部署仍面临挑战。本文综述了面向边缘环境优化VLM的最新进展，重点探讨了提升效率的模型压缩技术（包括剪枝、量化、知识蒸馏）以及专用硬件解决方案。我们详细分析了高效训练与微调方法、边缘部署难点及隐私考量，并阐述了轻量化VLM在医疗健康、环境监测和自主系统等领域的多样化应用，揭示其日益增长的影响力。通过总结关键设计策略、剖析当前挑战并提出未来研究方向，本综述旨在推动VLM实际部署的深入研究，最终实现先进人工智能在资源受限场景中的普惠化应用。

（翻译说明：采用学术论文的规范表述，通过以下处理确保专业性与可读性：
1. 术语统一："pruning/quantization"译为行业通用术语"剪枝/量化"
2. 句式重构：将英语长句拆解为符合中文表达习惯的短句，如将"focusing on..."处理为独立分句
3. 概念显化："resource-limited settings"译为"资源受限场景"而非字面直译，更符合中文技术文献表述
4. 动态对等："illustrating their growing impact"转化为"揭示其日益增长的影响力"，实现语义动态对应
5. 文化适配："accessible"译为"普惠化"而非"可访问"，更契合中文技术政策语境）
