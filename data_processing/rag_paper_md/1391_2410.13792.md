# Analyzing Deep Transformer Models for Time Series Forecasting via Manifold Learning

链接: http://arxiv.org/abs/2410.13792v1

原文摘要:
Transformer models have consistently achieved remarkable results in various
domains such as natural language processing and computer vision. However,
despite ongoing research efforts to better understand these models, the field
still lacks a comprehensive understanding. This is particularly true for deep
time series forecasting methods, where analysis and understanding work is
relatively limited. Time series data, unlike image and text information, can be
more challenging to interpret and analyze. To address this, we approach the
problem from a manifold learning perspective, assuming that the latent
representations of time series forecasting models lie next to a low-dimensional
manifold. In our study, we focus on analyzing the geometric features of these
latent data manifolds, including intrinsic dimension and principal curvatures.
Our findings reveal that deep transformer models exhibit similar geometric
behavior across layers, and these geometric features are correlated with model
performance. Additionally, we observe that untrained models initially have
different structures, but they rapidly converge during training. By leveraging
our geometric analysis and differentiable tools, we can potentially design new
and improved deep forecasting neural networks. This approach complements
existing analysis studies and contributes to a better understanding of
transformer models in the context of time series forecasting. Code is released
at https://github.com/azencot-group/GATLM.

中文翻译:
Transformer模型在自然语言处理和计算机视觉等多个领域持续取得显著成果。然而，尽管学界不断开展研究以深入理解这些模型，目前仍缺乏全面认知。这一现象在深度时间序列预测方法中尤为突出，相关分析和理解工作相对有限。与图像和文本信息不同，时间序列数据的解释与分析往往更具挑战性。为此，我们从流形学习的角度切入，假设时间序列预测模型的潜在表征邻近于一个低维流形。本研究重点分析了这些潜在数据流形的几何特征，包括本征维度和主曲率。研究发现：深度Transformer模型在不同层级间展现出相似的几何行为，且这些几何特征与模型性能存在关联；未经训练的模型初始结构存在差异，但在训练过程中会快速收敛。通过结合几何分析和可微分工具，我们有望设计出性能更优的新型深度预测神经网络。该方法为现有分析研究提供了有益补充，有助于深化对时间序列预测场景下Transformer模型的理解。代码已发布于https://github.com/azencot-group/GATLM。
