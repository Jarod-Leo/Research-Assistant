# Dolphin: Long Context as a New Modality for Energy-Efficient On-Device Language Models

链接: http://arxiv.org/abs/2408.15518v1

原文摘要:
This paper presents Dolphin, a novel decoder-decoder architecture for
energy-efficient processing of long contexts in language models. Our approach
addresses the significant energy consumption and latency challenges inherent in
on-device models. Dolphin employs a compact 0.5B parameter decoder to distill
extensive contextual information into a memory embedding, substantially
reducing the input length for the primary 7B parameter decoder model. Inspired
by vision-language models, we repurpose the image embedding projector to encode
long textual contexts, effectively treating extended context as a distinct
modality. This innovative method enables processing of substantially longer
contexts without the typical computational overhead associated with extended
input sequences. Empirical evaluations demonstrate a 10-fold improvement in
energy efficiency and a 5-fold reduction in latency compared to conventional
full-length context processing methods without losing quality of the response.
Our work contributes to the development of more sustainable and scalable
language models for on-device applications, addressing the critical need for
energy-efficient and responsive AI technologies in resource-constrained
environments while maintaining the accuracy to understand long contexts. This
research has implications for the broader field of natural language processing,
particularly in the domain of efficient model design for resource-limited
settings. By enabling more sophisticated AI capabilities on edge devices,
Dolphin paves the way for advanced language processing in a wide range of
applications where computational resources are at a premium. The Dolphin model
is publicly available at https://huggingface.co/NexaAIDev/Dolphin.

中文翻译:
本文提出了一种名为Dolphin的新型解码器-解码器架构，旨在实现语言模型长上下文处理的高能效运行。该研究针对设备端模型固有的高能耗与延迟挑战，采用0.5B参数的紧凑型解码器将庞杂的上下文信息蒸馏为记忆嵌入向量，从而将主7B参数解码器模型的输入长度大幅缩减。受视觉语言模型启发，我们创新性地将图像嵌入投影器改造为长文本上下文编码器，将扩展上下文视为独立模态进行处理。这种方法可在不增加扩展输入序列典型计算开销的前提下，显著提升上下文处理长度。实证评估表明，相较传统全长度上下文处理方法，新方案在保持响应质量的同时实现了10倍的能效提升和5倍的延迟降低。本研究为开发可持续、可扩展的设备端语言模型作出贡献，在资源受限环境中满足对高能效、低延迟AI技术的迫切需求，同时保持长上下文理解准确性。该成果对自然语言处理领域具有广泛意义，特别是在资源受限场景的高效模型设计方面。通过提升边缘设备的AI处理能力，Dolphin为计算资源受限场景下的高级语言处理应用开辟了新途径。模型已公开发布于https://huggingface.co/NexaAIDev/Dolphin。
