# Transformer-based model for monocular visual odometry: a video understanding approach

链接: http://arxiv.org/abs/2305.06121v1

原文摘要:
Estimating the camera's pose given images from a single camera is a
traditional task in mobile robots and autonomous vehicles. This problem is
called monocular visual odometry and often relies on geometric approaches that
require considerable engineering effort for a specific scenario. Deep learning
methods have been shown to be generalizable after proper training and with a
large amount of available data. Transformer-based architectures have dominated
the state-of-the-art in natural language processing and computer vision tasks,
such as image and video understanding. In this work, we deal with the monocular
visual odometry as a video understanding task to estimate the 6 degrees of
freedom of a camera's pose. We contribute by presenting the TSformer-VO model
based on spatio-temporal self-attention mechanisms to extract features from
clips and estimate the motions in an end-to-end manner. Our approach achieved
competitive state-of-the-art performance compared with geometry-based and deep
learning-based methods on the KITTI visual odometry dataset, outperforming the
DeepVO implementation highly accepted in the visual odometry community. The
code is publicly available at https://github.com/aofrancani/TSformer-VO.

中文翻译:
基于单摄像头图像序列估计相机位姿是移动机器人与自动驾驶领域的经典任务，这一过程被称为单目视觉里程计。传统方法通常依赖针对特定场景精心设计的几何算法，需要大量工程调优。而深度学习方法通过充分训练和海量数据，展现出优异的泛化能力。当前，基于Transformer的架构已在自然语言处理及图像视频理解等计算机视觉任务中占据主导地位。本研究创新性地将单目视觉里程计视为视频理解任务，提出基于时空自注意力机制的TSformer-VO模型，通过端到端方式从视频片段提取特征并预测相机的六自由度运动。在KITTI视觉里程计基准测试中，本方法相较基于几何和深度学习的现有方案展现出竞争优势，其性能显著超越视觉里程计领域广泛认可的DeepVO实现。相关代码已开源发布于https://github.com/aofrancani/TSformer-VO。
