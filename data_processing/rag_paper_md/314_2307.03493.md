# ITA: An Energy-Efficient Attention and Softmax Accelerator for Quantized Transformers

链接: http://arxiv.org/abs/2307.03493v1

原文摘要:
Transformer networks have emerged as the state-of-the-art approach for
natural language processing tasks and are gaining popularity in other domains
such as computer vision and audio processing. However, the efficient hardware
acceleration of transformer models poses new challenges due to their high
arithmetic intensities, large memory requirements, and complex dataflow
dependencies. In this work, we propose ITA, a novel accelerator architecture
for transformers and related models that targets efficient inference on
embedded systems by exploiting 8-bit quantization and an innovative softmax
implementation that operates exclusively on integer values. By computing
on-the-fly in streaming mode, our softmax implementation minimizes data
movement and energy consumption. ITA achieves competitive energy efficiency
with respect to state-of-the-art transformer accelerators with 16.9 TOPS/W,
while outperforming them in area efficiency with 5.93 TOPS/mm$^2$ in 22 nm
fully-depleted silicon-on-insulator technology at 0.8 V.

中文翻译:
Transformer网络已成为自然语言处理任务的最先进方法，并逐渐在计算机视觉、音频处理等其他领域崭露头角。然而，由于Transformer模型具有高算术强度、大内存需求和复杂的数据流依赖性，其高效硬件加速面临着新的挑战。本文提出ITA——一种面向Transformer及相关模型的创新加速器架构，通过采用8位量化和仅支持整数值运算的创新型softmax实现，致力于在嵌入式系统上实现高效推理。我们的流式处理softmax方案通过实时计算，显著减少了数据移动和能耗。在22纳米全耗尽绝缘体上硅工艺（0.8V电压）下，ITA以16.9 TOPS/W的能效比达到与最先进Transformer加速器相当的水平，同时以5.93 TOPS/mm²的面积效率实现超越。

（注：TOPS/W表示每瓦特万亿次操作，TOPS/mm²表示每平方毫米万亿次操作）
