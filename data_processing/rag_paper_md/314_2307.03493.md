# ITA: An Energy-Efficient Attention and Softmax Accelerator for Quantized Transformers

链接: http://arxiv.org/abs/2307.03493v1

原文摘要:
Transformer networks have emerged as the state-of-the-art approach for
natural language processing tasks and are gaining popularity in other domains
such as computer vision and audio processing. However, the efficient hardware
acceleration of transformer models poses new challenges due to their high
arithmetic intensities, large memory requirements, and complex dataflow
dependencies. In this work, we propose ITA, a novel accelerator architecture
for transformers and related models that targets efficient inference on
embedded systems by exploiting 8-bit quantization and an innovative softmax
implementation that operates exclusively on integer values. By computing
on-the-fly in streaming mode, our softmax implementation minimizes data
movement and energy consumption. ITA achieves competitive energy efficiency
with respect to state-of-the-art transformer accelerators with 16.9 TOPS/W,
while outperforming them in area efficiency with 5.93 TOPS/mm$^2$ in 22 nm
fully-depleted silicon-on-insulator technology at 0.8 V.

中文翻译:
Transformer网络已成为自然语言处理任务中的前沿方法，并在计算机视觉和音频处理等其他领域日益普及。然而，由于Transformer模型具有高算术强度、大内存需求和复杂数据流依赖性，其高效硬件加速面临新挑战。本文提出ITA——一种面向Transformer及相关模型的创新加速器架构，通过采用8位量化和仅对整数值运算的创新型softmax实现，针对嵌入式系统的高效推理进行了优化。我们的流式处理即时计算softmax方案，最大限度地减少了数据移动和能耗。在22纳米全耗尽绝缘体上硅技术、0.8V工作电压下，ITA以16.9 TOPS/W的能效比达到与最先进Transformer加速器相当的水平，同时以5.93 TOPS/mm²的面积效率超越同类方案。
