# LoRA-GGPO: Mitigating Double Descent in LoRA Fine-Tuning via Gradient-Guided Perturbation Optimization

链接: http://arxiv.org/abs/2502.14538v1

原文摘要:
Large Language Models (LLMs) have achieved remarkable success in natural
language processing, but their full fine-tuning remains resource-intensive.
Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation
(LoRA), have emerged as a practical solution by approximating parameter updates
with low-rank matrices. However, LoRA often exhibits a "double descent"
phenomenon during fine-tuning, where model performance degrades due to
overfitting and limited expressiveness caused by low-rank constraints. To
address this issue, we propose LoRA-GGPO (Gradient-Guided Perturbation
Optimization), a novel method that leverages gradient and weight norms to
generate targeted perturbations. By optimizing the sharpness of the loss
landscape, LoRA-GGPO guides the model toward flatter minima, mitigating the
double descent problem and improving generalization. Extensive experiments on
natural language understanding (NLU) and generation (NLG) tasks demonstrate
that LoRA-GGPO outperforms LoRA and its state-of-the-art variants. Furthermore,
extended experiments specifically designed to analyze the double descent
phenomenon confirm that LoRA-GGPO effectively alleviates this issue, producing
more robust and generalizable models. Our work provides a robust and efficient
solution for fine-tuning LLMs, with broad applicability in real-world
scenarios. The code is available at https://github.com/llm172/LoRA-GGPO.

中文翻译:
大语言模型（LLMs）在自然语言处理领域取得了显著成就，但其全参数微调仍存在资源消耗过高的问题。参数高效微调方法（PEFT）如低秩自适应（LoRA）通过低秩矩阵近似参数更新，提供了实用解决方案。然而，LoRA在微调过程中常出现"双下降"现象——由于低秩约束导致的过拟合和表达能力受限，模型性能会出现退化。针对这一问题，我们提出LoRA-GGPO（梯度引导扰动优化），该方法创新性地利用梯度和权重范数生成定向扰动。通过优化损失函数的平坦性，LoRA-GGPO引导模型收敛至更平坦的极小值点，有效缓解双下降问题并提升泛化能力。在自然语言理解（NLU）和生成（NLG）任务上的大量实验表明，LoRA-GGPO性能优于原始LoRA及其先进变体。特别设计的双下降现象分析实验进一步证实，该方法能有效缓解该问题，生成更具鲁棒性和泛化能力的模型。本研究为LLMs微调提供了高效稳健的解决方案，具有广泛的实际应用价值。代码已开源于https://github.com/llm172/LoRA-GGPO。
