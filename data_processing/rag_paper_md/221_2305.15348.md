# READ: Recurrent Adaptation of Large Transformers

链接: http://arxiv.org/abs/2305.15348v1

原文摘要:
Fine-tuning large-scale Transformers has led to the explosion of many AI
applications across Natural Language Processing and Computer Vision tasks.
However, fine-tuning all pre-trained model parameters becomes impractical as
the model size and number of tasks increase. Parameter-efficient transfer
learning (PETL) methods aim to address these challenges. While effective in
reducing the number of trainable parameters, PETL methods still require
significant energy and computational resources to fine-tune. In this paper, we
introduce \textbf{RE}current \textbf{AD}aption (READ) -- a lightweight and
memory-efficient fine-tuning method -- to overcome the limitations of the
current PETL approaches. Specifically, READ inserts a small RNN network
alongside the backbone model so that the model does not have to back-propagate
through the large backbone network. Through comprehensive empirical evaluation
of the GLUE benchmark, we demonstrate READ can achieve a $56\%$ reduction in
the training memory consumption and an $84\%$ reduction in the GPU energy usage
while retraining high model quality compared to full-tuning. Additionally, the
model size of READ does not grow with the backbone model size, making it a
highly scalable solution for fine-tuning large Transformers.

中文翻译:
微调大规模Transformer模型已推动自然语言处理与计算机视觉领域众多AI应用的爆发式增长。然而随着模型规模和任务数量的增加，全参数微调变得难以实施。参数高效迁移学习（PETL）方法应运而生以应对这些挑战。尽管PETL方法能有效减少可训练参数量，但仍需消耗大量能源和计算资源进行微调。本文提出**循环自适应（READ）**——一种轻量级内存高效的微调方法，旨在突破当前PETL技术的局限。具体而言，READ在主干模型旁插入小型RNN网络，从而无需通过庞大主干网络进行反向传播。基于GLUE基准的全面实验评估表明，与全参数微调相比，READ能在保持模型质量的同时降低56%的训练内存消耗，并减少84%的GPU能耗。此外，READ的模型规模不会随主干模型增大而增长，这使其成为微调大型Transformer模型的高度可扩展解决方案。
