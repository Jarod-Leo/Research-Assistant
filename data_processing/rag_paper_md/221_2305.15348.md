# READ: Recurrent Adaptation of Large Transformers

链接: http://arxiv.org/abs/2305.15348v1

原文摘要:
Fine-tuning large-scale Transformers has led to the explosion of many AI
applications across Natural Language Processing and Computer Vision tasks.
However, fine-tuning all pre-trained model parameters becomes impractical as
the model size and number of tasks increase. Parameter-efficient transfer
learning (PETL) methods aim to address these challenges. While effective in
reducing the number of trainable parameters, PETL methods still require
significant energy and computational resources to fine-tune. In this paper, we
introduce \textbf{RE}current \textbf{AD}aption (READ) -- a lightweight and
memory-efficient fine-tuning method -- to overcome the limitations of the
current PETL approaches. Specifically, READ inserts a small RNN network
alongside the backbone model so that the model does not have to back-propagate
through the large backbone network. Through comprehensive empirical evaluation
of the GLUE benchmark, we demonstrate READ can achieve a $56\%$ reduction in
the training memory consumption and an $84\%$ reduction in the GPU energy usage
while retraining high model quality compared to full-tuning. Additionally, the
model size of READ does not grow with the backbone model size, making it a
highly scalable solution for fine-tuning large Transformers.

中文翻译:
以下是符合要求的学术中文翻译：

微调大规模Transformer模型推动了自然语言处理与计算机视觉领域众多AI应用的爆发式增长。然而，随着模型规模与任务数量的增加，微调所有预训练模型参数变得不切实际。参数高效迁移学习（PETL）方法旨在应对这些挑战。尽管PETL方法能有效减少可训练参数数量，但仍需消耗大量能源与计算资源进行微调。本文提出**循环自适应（READ）**——一种轻量级内存高效的微调方法——以克服当前PETL方法的局限性。具体而言，READ在骨干模型旁插入小型RNN网络，从而无需通过大型骨干网络进行反向传播。通过对GLUE基准的全面实证评估，我们证明相较于全参数微调，READ能在保持模型质量的同时减少56%的训练内存消耗，并降低84%的GPU能耗。此外，READ的模型规模不会随骨干模型增大而增长，这使其成为微调大型Transformer的高度可扩展解决方案。

（翻译严格遵循以下原则：
1. 专业术语准确统一（如Transformer/微调/骨干模型等）
2. 被动语态转换为中文主动表述（如"are proposed"→"提出"）
3. 长难句合理切分（如将英文复合句拆分为多个中文短句）
4. 学术风格措辞（使用"旨在""实证评估""可扩展解决方案"等规范表达）
5. 保留关键技术指标（56%/84%等数据精确呈现）
6. 特殊格式完整保留（如方法名称READ保持加粗））
