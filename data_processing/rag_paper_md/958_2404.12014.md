# Enhance Robustness of Language Models Against Variation Attack through Graph Integration

链接: http://arxiv.org/abs/2404.12014v1

原文摘要:
The widespread use of pre-trained language models (PLMs) in natural language
processing (NLP) has greatly improved performance outcomes. However, these
models' vulnerability to adversarial attacks (e.g., camouflaged hints from drug
dealers), particularly in the Chinese language with its rich character
diversity/variation and complex structures, hatches vital apprehension. In this
study, we propose a novel method, CHinese vAriatioN Graph Enhancement (CHANGE),
to increase the robustness of PLMs against character variation attacks in
Chinese content. CHANGE presents a novel approach for incorporating a Chinese
character variation graph into the PLMs. Through designing different
supplementary tasks utilizing the graph structure, CHANGE essentially enhances
PLMs' interpretation of adversarially manipulated text. Experiments conducted
in a multitude of NLP tasks show that CHANGE outperforms current language
models in combating against adversarial attacks and serves as a valuable
contribution to robust language model research. These findings contribute to
the groundwork on robust language models and highlight the substantial
potential of graph-guided pre-training strategies for real-world applications.

中文翻译:
预训练语言模型（PLMs）在自然语言处理（NLP）中的广泛应用显著提升了任务性能。然而，这些模型在面对对抗性攻击（例如毒品交易者使用的隐蔽暗号）时表现脆弱，尤其在汉字具有丰富字形变体与复杂结构的汉语场景中，这一缺陷引发了重大隐忧。本研究提出创新方法"汉字变体图谱增强"（CHANGE），通过将汉字变体图谱整合至PLMs框架，有效提升模型对中文对抗文本的鲁棒性。该方法利用图谱结构设计多种辅助训练任务，从根本上增强了PLMs对对抗性篡改文本的解析能力。在多领域NLP任务中的实验表明，CHANGE在抵御对抗攻击方面优于现有语言模型，为鲁棒语言模型研究提供了重要贡献。研究成果不仅夯实了鲁棒语言模型的理论基础，更揭示了图引导预训练策略在实际应用中的巨大潜力。
