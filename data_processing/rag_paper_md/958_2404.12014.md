# Enhance Robustness of Language Models Against Variation Attack through Graph Integration

链接: http://arxiv.org/abs/2404.12014v1

原文摘要:
The widespread use of pre-trained language models (PLMs) in natural language
processing (NLP) has greatly improved performance outcomes. However, these
models' vulnerability to adversarial attacks (e.g., camouflaged hints from drug
dealers), particularly in the Chinese language with its rich character
diversity/variation and complex structures, hatches vital apprehension. In this
study, we propose a novel method, CHinese vAriatioN Graph Enhancement (CHANGE),
to increase the robustness of PLMs against character variation attacks in
Chinese content. CHANGE presents a novel approach for incorporating a Chinese
character variation graph into the PLMs. Through designing different
supplementary tasks utilizing the graph structure, CHANGE essentially enhances
PLMs' interpretation of adversarially manipulated text. Experiments conducted
in a multitude of NLP tasks show that CHANGE outperforms current language
models in combating against adversarial attacks and serves as a valuable
contribution to robust language model research. These findings contribute to
the groundwork on robust language models and highlight the substantial
potential of graph-guided pre-training strategies for real-world applications.

中文翻译:
预训练语言模型（PLMs）在自然语言处理（NLP）中的广泛应用显著提升了任务性能。然而，这些模型对对抗性攻击（如毒贩使用的隐蔽暗示）的脆弱性，尤其在汉字具有丰富字形变体与复杂结构的特性下，引发了重大隐忧。本研究提出创新方法"汉字变体图增强"（CHANGE），通过将汉字变体图谱融入PLMs，有效提升模型对中文对抗文本的鲁棒性。该方法利用图结构设计多种辅助任务，从根本上增强了PLMs对对抗性篡改文本的解析能力。在多类NLP任务实验中，CHANGE展现出优于现有语言模型的抗攻击性能，为鲁棒语言模型研究提供了重要贡献。这些发现不仅夯实了鲁棒语言模型的研究基础，更揭示了图引导预训练策略在实际应用中的巨大潜力。
