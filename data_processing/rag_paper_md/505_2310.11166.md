# ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing

链接: http://arxiv.org/abs/2310.11166v1

原文摘要:
English and Chinese, known as resource-rich languages, have witnessed the
strong development of transformer-based language models for natural language
processing tasks. Although Vietnam has approximately 100M people speaking
Vietnamese, several pre-trained models, e.g., PhoBERT, ViBERT, and vELECTRA,
performed well on general Vietnamese NLP tasks, including POS tagging and named
entity recognition. These pre-trained language models are still limited to
Vietnamese social media tasks. In this paper, we present the first monolingual
pre-trained language model for Vietnamese social media texts, ViSoBERT, which
is pre-trained on a large-scale corpus of high-quality and diverse Vietnamese
social media texts using XLM-R architecture. Moreover, we explored our
pre-trained model on five important natural language downstream tasks on
Vietnamese social media texts: emotion recognition, hate speech detection,
sentiment analysis, spam reviews detection, and hate speech spans detection.
Our experiments demonstrate that ViSoBERT, with far fewer parameters, surpasses
the previous state-of-the-art models on multiple Vietnamese social media tasks.
Our ViSoBERT model is available only for research purposes.

中文翻译:
以下是符合学术规范的中文翻译：

英语和汉语作为资源丰富的语言，其基于Transformer架构的自然语言处理模型已取得显著发展。尽管越南拥有约1亿越南语使用者，且PhoBERT、ViBERT和vELECTRA等预训练模型在词性标注、命名实体识别等通用越南语NLP任务中表现良好，但这些模型在越南社交媒体文本处理领域仍存在局限。本文首次提出面向越南社交媒体文本的单语预训练语言模型ViSoBERT，该模型基于XLM-R架构，通过大规模高质量且多样化的越南社交媒体语料库进行预训练。我们进一步在五项关键越南社交媒体自然语言下游任务上验证模型性能：情绪识别、仇恨言论检测、情感分析、垃圾评论检测和仇恨言论片段检测。实验结果表明，ViSoBERT在参数量大为减少的情况下，仍能超越现有最优模型在多项越南社交媒体任务中的表现。本研究的ViSoBERT模型仅限学术研究用途。

（说明：翻译过程中进行了以下专业处理：
1. 将"resource-rich languages"译为"资源丰富的语言"符合计算语言学领域术语
2. "transformer-based"统一译为"基于Transformer架构"保持技术一致性
3. 专业模型名称PhoBERT/ViBERT等保留英文原名
4. "downstream tasks"规范译为"下游任务"
5. 被动语态转换为中文主动句式（如"are still limited to"→"仍存在局限"）
6. 保持数值单位规范（100M→1亿）
7. 技术指标"far fewer parameters"准确译为"参数量大为减少"）
