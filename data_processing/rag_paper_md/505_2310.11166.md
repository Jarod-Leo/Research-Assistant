# ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing

链接: http://arxiv.org/abs/2310.11166v1

原文摘要:
English and Chinese, known as resource-rich languages, have witnessed the
strong development of transformer-based language models for natural language
processing tasks. Although Vietnam has approximately 100M people speaking
Vietnamese, several pre-trained models, e.g., PhoBERT, ViBERT, and vELECTRA,
performed well on general Vietnamese NLP tasks, including POS tagging and named
entity recognition. These pre-trained language models are still limited to
Vietnamese social media tasks. In this paper, we present the first monolingual
pre-trained language model for Vietnamese social media texts, ViSoBERT, which
is pre-trained on a large-scale corpus of high-quality and diverse Vietnamese
social media texts using XLM-R architecture. Moreover, we explored our
pre-trained model on five important natural language downstream tasks on
Vietnamese social media texts: emotion recognition, hate speech detection,
sentiment analysis, spam reviews detection, and hate speech spans detection.
Our experiments demonstrate that ViSoBERT, with far fewer parameters, surpasses
the previous state-of-the-art models on multiple Vietnamese social media tasks.
Our ViSoBERT model is available only for research purposes.

中文翻译:
英语和汉语作为资源丰富的语言，其基于Transformer架构的自然语言处理模型已取得长足发展。尽管越南拥有约1亿越南语使用者，且PhoBERT、ViBERT和vELECTRA等预训练模型在词性标注、命名实体识别等通用越南语NLP任务中表现优异，但这些模型在越南社交媒体文本处理领域仍存在局限。本文首次提出专为越南社交媒体文本设计的单语预训练语言模型ViSoBERT，该模型采用XLM-R架构，基于大规模高质量越南社交媒体语料库进行预训练。我们进一步在五项关键越南社交媒体下游任务中验证模型性能：情绪识别、仇恨言论检测、情感分析、垃圾评论检测及仇恨言论片段检测。实验结果表明，ViSoBERT在参数规模显著缩减的情况下，多项任务性能超越现有最优模型。本研究成果仅限学术研究用途。
