# Enhancing Speaker Diarization with Large Language Models: A Contextual Beam Search Approach

链接: http://arxiv.org/abs/2309.05248v1

原文摘要:
Large language models (LLMs) have shown great promise for capturing
contextual information in natural language processing tasks. We propose a novel
approach to speaker diarization that incorporates the prowess of LLMs to
exploit contextual cues in human dialogues. Our method builds upon an
acoustic-based speaker diarization system by adding lexical information from an
LLM in the inference stage. We model the multi-modal decoding process
probabilistically and perform joint acoustic and lexical beam search to
incorporate cues from both modalities: audio and text. Our experiments
demonstrate that infusing lexical knowledge from the LLM into an acoustics-only
diarization system improves overall speaker-attributed word error rate
(SA-WER). The experimental results show that LLMs can provide complementary
information to acoustic models for the speaker diarization task via proposed
beam search decoding approach showing up to 39.8% relative delta-SA-WER
improvement from the baseline system. Thus, we substantiate that the proposed
technique is able to exploit contextual information that is inaccessible to
acoustics-only systems which is represented by speaker embeddings. In addition,
these findings point to the potential of using LLMs to improve speaker
diarization and other speech processing tasks by capturing semantic and
contextual cues.

中文翻译:
大型语言模型（LLM）在自然语言处理任务中展现出捕捉上下文信息的巨大潜力。我们提出了一种创新的说话人日志化方法，通过利用LLM在人类对话中挖掘上下文线索的能力。该方法在基于声学的说话人日志化系统基础上，于推理阶段引入LLM的词汇信息。我们通过概率建模实现多模态解码过程，并执行声学与词汇的联合束搜索，以融合音频和文本双模态线索。实验表明，将LLM的词汇知识注入纯声学日志化系统可显著降低说话人归属词错误率（SA-WER）。结果显示，通过提出的束搜索解码方法，LLM能为声学模型提供互补信息，相比基线系统最高可获得39.8%的相对SA-WER提升。由此证实，该技术能有效利用纯声学系统（以说话人嵌入为代表）无法获取的上下文信息。这些发现进一步揭示了LLM通过捕捉语义和上下文线索来改进说话人日志化及其他语音处理任务的潜在价值。
