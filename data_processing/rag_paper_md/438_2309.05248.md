# Enhancing Speaker Diarization with Large Language Models: A Contextual Beam Search Approach

链接: http://arxiv.org/abs/2309.05248v1

原文摘要:
Large language models (LLMs) have shown great promise for capturing
contextual information in natural language processing tasks. We propose a novel
approach to speaker diarization that incorporates the prowess of LLMs to
exploit contextual cues in human dialogues. Our method builds upon an
acoustic-based speaker diarization system by adding lexical information from an
LLM in the inference stage. We model the multi-modal decoding process
probabilistically and perform joint acoustic and lexical beam search to
incorporate cues from both modalities: audio and text. Our experiments
demonstrate that infusing lexical knowledge from the LLM into an acoustics-only
diarization system improves overall speaker-attributed word error rate
(SA-WER). The experimental results show that LLMs can provide complementary
information to acoustic models for the speaker diarization task via proposed
beam search decoding approach showing up to 39.8% relative delta-SA-WER
improvement from the baseline system. Thus, we substantiate that the proposed
technique is able to exploit contextual information that is inaccessible to
acoustics-only systems which is represented by speaker embeddings. In addition,
these findings point to the potential of using LLMs to improve speaker
diarization and other speech processing tasks by capturing semantic and
contextual cues.

中文翻译:
大型语言模型（LLMs）在自然语言处理任务中展现出捕捉上下文信息的强大潜力。我们提出了一种新颖的说话人日志化方法，通过利用LLMs解析人类对话中的上下文线索。该方法在基于声学的说话人日志化系统基础上，于推理阶段引入LLM提供的词汇信息。我们以概率模型构建多模态解码流程，通过联合声学与词汇束搜索融合音频和文本双模态线索。实验表明，将LLM的词汇知识注入纯声学日志化系统可显著降低说话人相关词错误率（SA-WER）。结果显示，采用所提出的束搜索解码方法时，LLM能为声学模型提供互补信息，相较基线系统最高可获得39.8%的相对SA-WER提升。这证实了该技术能有效利用纯声学系统（以说话人嵌入为代表）无法获取的上下文信息。此外，这些发现揭示了LLMs通过捕捉语义和上下文线索来改进说话人日志化及其他语音处理任务的广阔前景。
