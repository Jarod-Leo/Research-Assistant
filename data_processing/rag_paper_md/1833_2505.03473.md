# Evaluation of LLMs on Long-tail Entity Linking in Historical Documents

链接: http://arxiv.org/abs/2505.03473v1

原文摘要:
Entity Linking (EL) plays a crucial role in Natural Language Processing (NLP)
applications, enabling the disambiguation of entity mentions by linking them to
their corresponding entries in a reference knowledge base (KB). Thanks to their
deep contextual understanding capabilities, LLMs offer a new perspective to
tackle EL, promising better results than traditional methods. Despite the
impressive generalization capabilities of LLMs, linking less popular, long-tail
entities remains challenging as these entities are often underrepresented in
training data and knowledge bases. Furthermore, the long-tail EL task is an
understudied problem, and limited studies address it with LLMs. In the present
work, we assess the performance of two popular LLMs, GPT and LLama3, in a
long-tail entity linking scenario. Using MHERCL v0.1, a manually annotated
benchmark of sentences from domain-specific historical texts, we quantitatively
compare the performance of LLMs in identifying and linking entities to their
corresponding Wikidata entries against that of ReLiK, a state-of-the-art Entity
Linking and Relation Extraction framework. Our preliminary experiments reveal
that LLMs perform encouragingly well in long-tail EL, indicating that this
technology can be a valuable adjunct in filling the gap between head and
long-tail EL.

中文翻译:
实体链接（Entity Linking, EL）在自然语言处理（NLP）应用中具有关键作用，其通过将文本中的实体指称项关联到参考知识库（KB）中的对应条目，实现实体消歧。得益于大语言模型（LLMs）强大的上下文理解能力，其为解决实体链接任务提供了新思路，有望超越传统方法的表现。尽管LLMs展现出卓越的泛化能力，但链接冷门长尾实体仍具挑战性——这类实体在训练数据和知识库中往往存在表征不足的问题。此外，长尾实体链接目前仍是一个研究不足的领域，针对LLMs在此任务中的应用探索尤为有限。本研究评估了GPT和LLama3两种主流大语言模型在长尾实体链接场景中的表现。通过使用MHERCL v0.1（一个基于领域特异性历史文本构建的人工标注评测集），我们定量对比了LLMs与前沿实体链接及关系抽取框架ReLiK在识别实体并链接至Wikidata对应条目任务中的性能。初步实验表明，LLMs在长尾实体链接中表现优异，这预示着该技术有望成为弥合头部实体与长尾实体链接效能差距的重要辅助手段。

（翻译说明：采用学术论文摘要的规范表述，通过以下处理确保专业性：
1. 术语统一："reference knowledge base"规范译为"参考知识库"，"long-tail entities"采用领域通用译法"长尾实体"
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句（如第一句拆分为因果逻辑链）
3. 被动语态转化："are often underrepresented"译为主动式"存在表征不足"
4. 概念显化："head EL"补充译为"头部实体链接"以形成对比
5. 技术名词保留：LLMs、GPT等缩写首次出现时标注全称
6. 数据名称处理：MHERCL v0.1保留原名并添加说明性括号注释）
