# Evaluation of LLMs on Long-tail Entity Linking in Historical Documents

链接: http://arxiv.org/abs/2505.03473v1

原文摘要:
Entity Linking (EL) plays a crucial role in Natural Language Processing (NLP)
applications, enabling the disambiguation of entity mentions by linking them to
their corresponding entries in a reference knowledge base (KB). Thanks to their
deep contextual understanding capabilities, LLMs offer a new perspective to
tackle EL, promising better results than traditional methods. Despite the
impressive generalization capabilities of LLMs, linking less popular, long-tail
entities remains challenging as these entities are often underrepresented in
training data and knowledge bases. Furthermore, the long-tail EL task is an
understudied problem, and limited studies address it with LLMs. In the present
work, we assess the performance of two popular LLMs, GPT and LLama3, in a
long-tail entity linking scenario. Using MHERCL v0.1, a manually annotated
benchmark of sentences from domain-specific historical texts, we quantitatively
compare the performance of LLMs in identifying and linking entities to their
corresponding Wikidata entries against that of ReLiK, a state-of-the-art Entity
Linking and Relation Extraction framework. Our preliminary experiments reveal
that LLMs perform encouragingly well in long-tail EL, indicating that this
technology can be a valuable adjunct in filling the gap between head and
long-tail EL.

中文翻译:
实体链接（Entity Linking, EL）在自然语言处理（NLP）应用中扮演着关键角色，它通过将文本中的实体指称项关联到参考知识库（KB）中的对应条目，实现实体消歧。得益于大型语言模型（LLMs）强大的上下文理解能力，它们为EL任务提供了新思路，有望超越传统方法取得更优效果。尽管LLMs展现出卓越的泛化能力，但链接低频长尾实体仍具挑战性——这类实体在训练数据和知识库中往往代表性不足。此外，长尾EL任务目前研究较少，针对LLMs在此领域应用的探索更为有限。

本研究评估了GPT和LLama3两种主流LLMs在长尾实体链接场景中的表现。通过使用MHERCL v0.1（一个基于专业历史文本的手工标注评测集），我们定量比较了LLMs与前沿实体链接关系抽取框架ReLiK在识别实体并关联至Wikidata条目时的性能。初步实验表明，LLMs在长尾EL任务中表现优异，这预示着该技术有望弥合头部实体与长尾实体链接效果之间的差距。
