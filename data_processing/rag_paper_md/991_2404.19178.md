# Revenge of the Fallen? Recurrent Models Match Transformers at Predicting Human Language Comprehension Metrics

链接: http://arxiv.org/abs/2404.19178v1

原文摘要:
Transformers have generally supplanted recurrent neural networks as the
dominant architecture for both natural language processing tasks and for
modelling the effect of predictability on online human language comprehension.
However, two recently developed recurrent model architectures, RWKV and Mamba,
appear to perform natural language tasks comparably to or better than
transformers of equivalent scale. In this paper, we show that contemporary
recurrent models are now also able to match - and in some cases, exceed - the
performance of comparably sized transformers at modeling online human language
comprehension. This suggests that transformer language models are not uniquely
suited to this task, and opens up new directions for debates about the extent
to which architectural features of language models make them better or worse
models of human language comprehension.

中文翻译:
Transformer模型已普遍取代循环神经网络，成为自然语言处理任务及模拟可预测性对人类在线语言理解影响的主导架构。然而，近期开发的两种循环模型架构RWKV和Mamba，在同等规模下处理自然语言任务的表现已可比拟甚至超越Transformer。本文通过实验证明，当代循环模型在模拟人类在线语言理解任务中，其性能已能与同体量Transformer模型匹敌——某些情况下甚至更优。这一发现表明Transformer并非该任务的唯一适用架构，同时也为探讨语言模型架构特征如何影响其模拟人类语言理解的优劣开辟了新视角。
