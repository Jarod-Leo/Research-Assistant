# COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability

链接: http://arxiv.org/abs/2402.08679v1

原文摘要:
Jailbreaks on large language models (LLMs) have recently received increasing
attention. For a comprehensive assessment of LLM safety, it is essential to
consider jailbreaks with diverse attributes, such as contextual coherence and
sentiment/stylistic variations, and hence it is beneficial to study
controllable jailbreaking, i.e. how to enforce control on LLM attacks. In this
paper, we formally formulate the controllable attack generation problem, and
build a novel connection between this problem and controllable text generation,
a well-explored topic of natural language processing. Based on this connection,
we adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a
state-of-the-art, highly efficient algorithm in controllable text generation,
and introduce the COLD-Attack framework which unifies and automates the search
of adversarial LLM attacks under a variety of control requirements such as
fluency, stealthiness, sentiment, and left-right-coherence. The controllability
enabled by COLD-Attack leads to diverse new jailbreak scenarios which not only
cover the standard setting of generating fluent (suffix) attack with
continuation constraint, but also allow us to address new controllable attack
settings such as revising a user query adversarially with paraphrasing
constraint, and inserting stealthy attacks in context with position constraint.
Our extensive experiments on various LLMs (Llama-2, Mistral, Vicuna, Guanaco,
GPT-3.5, and GPT-4) show COLD-Attack's broad applicability, strong
controllability, high success rate, and attack transferability. Our code is
available at https://github.com/Yu-Fangxu/COLD-Attack.

中文翻译:
近期，大型语言模型（LLM）的越狱行为日益受到关注。为全面评估LLM安全性，需考量具有多样化属性的越狱方式，例如上下文连贯性及情感/风格变异，因此研究可控越狱技术——即如何对LLM攻击实施精准控制——具有重要意义。本文首次将可控攻击生成问题形式化，并建立了该问题与自然语言处理领域深入研究的可控文本生成之间的理论关联。基于此关联，我们改进了基于能量的朗之万动力学约束解码（COLD）这一可控文本生成领域的先进高效算法，提出COLD-Attack框架，实现了在流畅性、隐蔽性、情感倾向、左右连贯性等多重约束条件下自动化搜索对抗性LLM攻击的统一方案。

该框架的可控性催生了多样化的新型越狱场景：不仅涵盖生成符合延续性约束的流畅（后缀）攻击的标准设定，还能支持诸如在改写约束下对抗性重构用户查询、在位置约束下向上下文植入隐蔽攻击等全新可控攻击场景。我们在多种LLM（Llama-2、Mistral、Vicuna、Guanaco、GPT-3.5和GPT-4）上的实验表明，COLD-Attack具有广泛适用性、强可控性、高成功率及优秀的攻击迁移能力。代码已开源：https://github.com/Yu-Fangxu/COLD-Attack。
