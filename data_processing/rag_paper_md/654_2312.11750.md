# A Heterogeneous Chiplet Architecture for Accelerating End-to-End Transformer Models

链接: http://arxiv.org/abs/2312.11750v1

原文摘要:
Transformers have revolutionized deep learning and generative modeling,
enabling advancements in natural language processing tasks. However, the size
of transformer models is increasing continuously, driven by enhanced
capabilities across various deep learning tasks. This trend of ever-increasing
model size has given rise to new challenges in terms of memory and compute
requirements. Conventional computing platforms, including GPUs, suffer from
suboptimal performance due to the memory demands imposed by models with
millions/billions of parameters. The emerging chiplet-based platforms provide a
new avenue for compute- and data-intensive machine learning (ML) applications
enabled by a Network-on-Interposer (NoI). However, designing suitable hardware
accelerators for executing Transformer inference workloads is challenging due
to a wide variety of complex computing kernels in the Transformer architecture.
In this paper, we leverage chiplet-based heterogeneous integration (HI) to
design a high-performance and energy-efficient multi-chiplet platform to
accelerate transformer workloads. We demonstrate that the proposed NoI
architecture caters to the data access patterns inherent in a transformer
model. The optimized placement of the chiplets and the associated NoI links and
routers enable superior performance compared to the state-of-the-art hardware
accelerators. The proposed NoI-based architecture demonstrates scalability
across varying transformer models and improves latency and energy efficiency by
up to 11.8x and 2.36x, respectively when compared with the existing
state-of-the-art architecture HAIMA.

中文翻译:
Transformer模型已彻底革新了深度学习与生成式建模领域，推动了自然语言处理任务的发展。然而，随着各类深度学习任务性能需求的提升，Transformer模型的规模持续膨胀，这种模型尺寸的无限增长趋势引发了内存与计算资源的新挑战。传统计算平台（包括GPU）因需处理百万/十亿级参数模型的内存需求而面临性能瓶颈。基于小芯片（chiplet）的新兴平台通过硅中介层互连网络（NoI），为计算密集型和数据密集型机器学习应用开辟了新途径。但由于Transformer架构中多样化的复杂计算核心，设计适配的硬件加速器面临巨大挑战。本文利用基于小芯片的异质集成技术（HI），设计出高性能、高能效的多小芯片平台以加速Transformer推理任务。研究表明，所提出的NoI架构能有效适配Transformer模型固有的数据访问模式。通过优化小芯片布局及配套NoI链路与路由器，该方案在性能上显著优于现有先进硬件加速器。相较于当前最优架构HAIMA，基于NoI的设计方案在不同规模Transformer模型中均展现出卓越的可扩展性，延迟和能效分别提升最高达11.8倍和2.36倍。
