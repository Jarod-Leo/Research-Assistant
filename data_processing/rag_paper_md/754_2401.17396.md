# Fine-tuning Transformer-based Encoder for Turkish Language Understanding Tasks

链接: http://arxiv.org/abs/2401.17396v1

原文摘要:
Deep learning-based and lately Transformer-based language models have been
dominating the studies of natural language processing in the last years. Thanks
to their accurate and fast fine-tuning characteristics, they have outperformed
traditional machine learning-based approaches and achieved state-of-the-art
results for many challenging natural language understanding (NLU) problems.
Recent studies showed that the Transformer-based models such as BERT, which is
Bidirectional Encoder Representations from Transformers, have reached
impressive achievements on many tasks. Moreover, thanks to their transfer
learning capacity, these architectures allow us to transfer pre-built models
and fine-tune them to specific NLU tasks such as question answering. In this
study, we provide a Transformer-based model and a baseline benchmark for the
Turkish Language. We successfully fine-tuned a Turkish BERT model, namely
BERTurk that is trained with base settings, to many downstream tasks and
evaluated with a the Turkish Benchmark dataset. We showed that our studies
significantly outperformed other existing baseline approaches for Named-Entity
Recognition, Sentiment Analysis, Question Answering and Text Classification in
Turkish Language. We publicly released these four fine-tuned models and
resources in reproducibility and with the view of supporting other Turkish
researchers and applications.

中文翻译:
近年来，基于深度学习尤其是Transformer架构的语言模型在自然语言处理研究中占据主导地位。凭借其精准高效的微调特性，这些模型不仅超越了传统机器学习方法，更在众多具有挑战性的自然语言理解任务中取得了最先进的成果。研究表明，以BERT（来自Transformer的双向编码器表示）为代表的Transformer模型已在多项任务中展现出卓越性能。得益于迁移学习能力，此类架构允许我们将预训练模型迁移并微调至特定NLU任务（如问答系统）。

本研究针对土耳其语构建了基于Transformer的模型及基准测试体系。我们成功对基础配置训练的土耳其语BERT模型（命名为BERTurk）进行多任务微调，并采用土耳其语基准数据集进行评估。实验证明，该模型在土耳其语命名实体识别、情感分析、问答系统和文本分类任务中显著优于现有基线方法。为促进研究可复现性并支持土耳其语相关研究与应用，我们已公开这四种微调模型及相关资源。
