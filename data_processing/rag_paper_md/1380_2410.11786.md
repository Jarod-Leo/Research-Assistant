# Selection-p: Self-Supervised Task-Agnostic Prompt Compression for Faithfulness and Transferability

链接: http://arxiv.org/abs/2410.11786v1

原文摘要:
Large Language Models (LLMs) have demonstrated impressive capabilities in a
wide range of natural language processing tasks when leveraging in-context
learning. To mitigate the additional computational and financial costs
associated with in-context learning, several prompt compression methods have
been proposed to compress the in-context learning prompts. Despite their
success, these methods face challenges with transferability due to
model-specific compression, or rely on external training data, such as GPT-4.
In this paper, we investigate the ability of LLMs to develop a unified
compression method that discretizes uninformative tokens, utilizing a
self-supervised pre-training technique. By introducing a small number of
parameters during the continual pre-training, the proposed Selection-p produces
a probability for each input token, indicating whether to preserve or discard
it. Experiments show Selection-p achieves state-of-the-art performance across
numerous classification tasks, achieving compression rates of up to 10 times
while experiencing only a marginal 0.8% decrease in performance. Moreover, it
exhibits superior transferability to different models compared to prior work.
Additionally, we further analyze how Selection-p helps maintain performance on
in-context learning with long contexts.

中文翻译:
大型语言模型（LLMs）在利用上下文学习时，已展现出处理各类自然语言任务的卓越能力。为降低上下文学习带来的额外计算与成本负担，研究者提出了多种提示压缩方法以精简上下文学习提示。尽管现有方法取得了一定成效，但其仍面临模型特异性压缩导致的迁移性不足问题，或需依赖外部训练数据（如GPT-4）。本文探索了LLMs通过自监督预训练技术开发统一压缩方法的能力，该方法通过离散化非信息性标记来实现。通过在持续预训练阶段引入少量参数，所提出的Selection-p方法能为每个输入标记生成概率值，据此决定保留或舍弃该标记。实验表明，Selection-p在多项分类任务中达到最先进性能，最高可实现10倍压缩率，性能仅下降0.8%。相较于先前研究，该方法展现出更优的跨模型迁移能力。此外，我们还深入分析了Selection-p如何帮助模型在长上下文场景下保持上下文学习性能。
