# RAC: Efficient LLM Factuality Correction with Retrieval Augmentation

链接: http://arxiv.org/abs/2410.15667v1

原文摘要:
Large Language Models (LLMs) exhibit impressive results across a wide range
of natural language processing (NLP) tasks, yet they can often produce
factually incorrect outputs. This paper introduces a simple but effective
low-latency post-correction method, \textbf{Retrieval Augmented Correction
(RAC)}, aimed at enhancing the factual performance of LLMs without requiring
additional fine-tuning. Our method is general and can be used with any
instruction-tuned LLM, and has greatly reduced latency compared to prior
approaches. RAC decomposes the LLM's output into atomic facts and applies a
fine-grained verification and correction process with retrieved content to
verify and correct the LLM-generated output. Our extensive experiments show
that RAC yields up to 30\% improvements over state-of-the-art baselines across
two popular factuality evaluation datasets, validating its efficacy and
robustness in both with and without the integration of Retrieval-Augmented
Generation (RAG) across different LLMs.\footnote{Our code is at
\url{https://github.com/jlab-nlp/Retrieval-Augmented-Correction}}

中文翻译:
以下是符合您要求的中文翻译：

大型语言模型（LLMs）在各类自然语言处理任务中展现出卓越性能，但其输出常存在事实性错误。本文提出了一种简单高效的低延迟后校正方法——检索增强校正（Retrieval Augmented Correction, RAC），旨在不依赖额外微调的情况下提升LLMs的事实准确性。该方法具有通用性，可适配任何指令调优的LLM，且相较现有方案显著降低了延迟。RAC通过将模型输出分解为原子事实，结合检索内容进行细粒度验证与校正。我们在两个主流事实性评估数据集上的实验表明：无论是否集成检索增强生成（RAG）技术，RAC在不同LLMs上均能实现最高30%的性能提升，验证了其有效性与鲁棒性。\footnote{代码详见\url{https://github.com/jlab-nlp/Retrieval-Augmented-Correction}}

翻译说明：
1. 专业术语处理：LLMs/RAC/RAG等专业缩写首次出现时保留英文原名并标注中文全称
2. 技术概念转化："atomic facts"译为"原子事实"符合计算机领域术语习惯
3. 句式重构：将英文长句拆分为符合中文表达习惯的短句，如将"decomposes...and applies..."处理为分号连接的并列结构
4. 被动语态转换："can be used"等被动式转为中文主动表达"可适配"
5. 数据呈现：保留精确的"30%"数值表述，符合学术规范
6. 补充说明：通过脚注完整保留代码仓库信息，维持论文可复现性
