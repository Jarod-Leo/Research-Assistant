# BiasDPO: Mitigating Bias in Language Models through Direct Preference Optimization

链接: http://arxiv.org/abs/2407.13928v1

原文摘要:
Large Language Models (LLMs) have become pivotal in advancing natural
language processing, yet their potential to perpetuate biases poses significant
concerns. This paper introduces a new framework employing Direct Preference
Optimization (DPO) to mitigate gender, racial, and religious biases in
LLM-generated English text. By developing a loss function that favors less
biased over biased completions, our approach cultivates a preference for
respectful and non-discriminatory language in LLMs. We also contribute a
manually designed dataset for training LLMs to recognize and correct biases.
This dataset encompasses a diverse range of prompts paired with both biased and
unbiased completions. Implementing this approach on the Microsoft Phi-2 model,
we demonstrate substantial reductions in biased outputs as our model
outperforms the baseline model on almost all bias benchmarks. Our model also
achieves better performance compared to other open-source models on most
benchmarks. By reducing biases in the language generated by the model, our
study marks a significant step towards developing more ethical and socially
responsible LLMs. We publicly release BiasDPO dataset on HuggingFace.

中文翻译:
大型语言模型（LLMs）已成为推动自然语言处理发展的关键力量，但其可能延续社会偏见的风险引发了广泛关注。本文提出了一种基于直接偏好优化（DPO）的新框架，用于减少LLM生成的英语文本中存在的性别、种族和宗教偏见。通过设计一个倾向于选择低偏见文本的损失函数，我们的方法培养了LLM对尊重性、非歧视性语言的偏好。我们还贡献了一个人工构建的数据集，用于训练LLMs识别和纠正偏见，该数据集包含多样化提示及其对应的偏见/无偏见文本组合。在微软Phi-2模型上的实验表明，该方法显著降低了偏见输出，我们的模型在几乎所有偏见基准测试中都优于基线模型，与其它开源模型相比也在多数基准上表现更优。通过减少模型生成语言中的偏见，本研究朝着开发更具伦理和社会责任感的LLMs迈出了重要一步。我们已在HuggingFace平台公开BiasDPO数据集。
