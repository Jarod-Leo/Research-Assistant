# LLM2KB: Constructing Knowledge Bases using instruction tuned context aware Large Language Models

链接: http://arxiv.org/abs/2308.13207v1

原文摘要:
The advent of Large Language Models (LLM) has revolutionized the field of
natural language processing, enabling significant progress in various
applications. One key area of interest is the construction of Knowledge Bases
(KB) using these powerful models. Knowledge bases serve as repositories of
structured information, facilitating information retrieval and inference tasks.
Our paper proposes LLM2KB, a system for constructing knowledge bases using
large language models, with a focus on the Llama 2 architecture and the
Wikipedia dataset. We perform parameter efficient instruction tuning for
Llama-2-13b-chat and StableBeluga-13B by training small injection models that
have only 0.05 % of the parameters of the base models using the Low Rank
Adaptation (LoRA) technique. These injection models have been trained with
prompts that are engineered to utilize Wikipedia page contexts of subject
entities fetched using a Dense Passage Retrieval (DPR) algorithm, to answer
relevant object entities for a given subject entity and relation. Our best
performing model achieved an average F1 score of 0.6185 across 21 relations in
the LM-KBC challenge held at the ISWC 2023 conference.

中文翻译:
大型语言模型（LLM）的出现彻底改变了自然语言处理领域，推动各类应用取得重大进展。其中，利用这类强大模型构建知识库（KB）成为关键研究方向。知识库作为结构化信息的存储库，能有效支持信息检索与推理任务。本文提出LLM2KB系统，专注于基于Llama 2架构和维基百科数据集的大模型知识库构建。我们采用参数高效指令调优方法，通过低秩自适应（LoRA）技术训练仅含基础模型0.05%参数的小型注入模型，对Llama-2-13b-chat和StableBeluga-13B进行优化。这些注入模型使用特定设计的提示进行训练，这些提示通过密集段落检索（DPR）算法获取主题实体的维基百科页面上下文，从而回答给定主题实体与关系的相关客体实体。在ISWC 2023会议举办的LM-KBC挑战赛中，我们的最佳模型在21个关系上平均F1得分达到0.6185。
