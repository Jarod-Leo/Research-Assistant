# Adaptive BPE Tokenization for Enhanced Vocabulary Adaptation in Finetuning Pretrained Language Models

链接: http://arxiv.org/abs/2410.03258v1

原文摘要:
In this work, we show a fundamental limitation in vocabulary adaptation
approaches that use Byte-Pair Encoding (BPE) tokenization scheme for
fine-tuning pretrained language models (PLMs) to expert domains. Current
approaches trivially append the target domain-specific vocabulary at the end of
the PLM vocabulary. This approach leads to a lower priority score and causes
sub-optimal tokenization in BPE that iteratively uses merge rules to tokenize a
given text. To mitigate this issue, we propose AdaptBPE where the BPE
tokenization initialization phase is modified to first perform the longest
string matching on the added (target) vocabulary before tokenizing at the
character level. We perform an extensive evaluation of AdaptBPE versus the
standard BPE over various classification and summarization tasks; AdaptBPE
improves by 3.57% (in terms of accuracy) and 1.87% (in terms of Rouge-L),
respectively. AdaptBPE for MEDVOC works particularly well when reference
summaries have high OOV concentration or are longer in length. We also conduct
a human evaluation, revealing that AdaptBPE generates more relevant and more
faithful summaries as compared to MEDVOC. We make our codebase publicly
available at https://github.com/gb-kgp/adaptbpe.

中文翻译:
本研究揭示了基于字节对编码（BPE）的分词方案在预训练语言模型（PLM）领域适配中的固有缺陷。现有方法简单地将目标领域词汇追加至PLM词表末尾，导致这些词汇在BPE的迭代合并规则中优先级得分较低，从而产生次优分词效果。为此，我们提出AdaptBPE方法，通过改进BPE初始化阶段：在字符级分词前优先对新增（目标领域）词汇执行最长字符串匹配。在多类分类和摘要任务上的实验表明，AdaptBPE相较标准BPE分别取得3.57%（准确率）和1.87%（Rouge-L）的性能提升。当参考摘要包含大量未登录词或篇幅较长时，针对医学词汇优化的MEDVOC版AdaptBPE表现尤为突出。人工评估进一步证实，AdaptBPE生成的摘要更具相关性和忠实度。相关代码已开源在https://github.com/gb-kgp/adaptbpe。
