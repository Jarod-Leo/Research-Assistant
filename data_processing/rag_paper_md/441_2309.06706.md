# Simultaneous Machine Translation with Large Language Models

链接: http://arxiv.org/abs/2309.06706v1

原文摘要:
Real-world simultaneous machine translation (SimulMT) systems face more
challenges than just the quality-latency trade-off. They also need to address
issues related to robustness with noisy input, processing long contexts, and
flexibility for knowledge injection. These challenges demand models with strong
language understanding and generation capabilities which may not often equipped
by dedicated MT models. In this paper, we investigate the possibility of
applying Large Language Models (LLM) to SimulMT tasks by using existing
incremental-decoding methods with a newly proposed RALCP algorithm for latency
reduction. We conducted experiments using the \texttt{Llama2-7b-chat} model on
nine different languages from the MUST-C dataset. The results show that LLM
outperforms dedicated MT models in terms of BLEU and LAAL metrics. Further
analysis indicates that LLM has advantages in terms of tuning efficiency and
robustness. However, it is important to note that the computational cost of LLM
remains a significant obstacle to its application in SimulMT.\footnote{We will
release our code, weights, and data with publication.}

中文翻译:
现实世界中的同步机器翻译（SimulMT）系统面临的挑战远不止质量与延迟的权衡。它们还需应对含噪声输入的鲁棒性处理、长上下文理解以及知识注入的灵活性等问题。这些挑战要求模型具备强大的语言理解和生成能力，而专用机器翻译模型往往难以满足。本文通过结合现有增量解码方法与新提出的RALCP延迟优化算法，探索将大语言模型（LLM）应用于SimulMT任务的可能性。我们在MUST-C数据集的九种语言上使用\texttt{Llama2-7b-chat}模型进行实验，结果表明LLM在BLEU和LAAL指标上均优于专用机器翻译模型。进一步分析显示，LLM在调优效率和鲁棒性方面具有优势。但需注意，LLM的高计算成本仍是其在SimulMT领域应用的主要障碍。\footnote{我们将在论文发表时同步公开代码、权重及数据。}
