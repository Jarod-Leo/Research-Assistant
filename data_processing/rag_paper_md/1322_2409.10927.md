# Propulsion: Steering LLM with Tiny Fine-Tuning

链接: http://arxiv.org/abs/2409.10927v2

原文摘要:
The rapid advancements in Large Language Models (LLMs) have revolutionized
natural language processing (NLP) and related fields. However, fine-tuning
these models for specific tasks remains computationally expensive and risks
degrading pre-learned features. To address these challenges, we propose
Propulsion, a novel parameter efficient fine-tuning (PEFT) method designed to
optimize task-specific performance while drastically reducing computational
overhead. Inspired by the concept of controlled adjustments in physical motion,
Propulsion selectively re-scales specific dimensions of a pre-trained model,
guiding output predictions toward task objectives without modifying the model's
parameters. By introducing lightweight, trainable Propulsion parameters at the
pre-trained layer, we minimize the number of parameters updated during
fine-tuning, preventing overfitting or overwriting of existing knowledge. Our
theoretical analysis, supported by Neural Tangent Kernel (NTK) theory, shows
that Propulsion approximates the performance of full fine-tuning with far fewer
trainable parameters. Empirically, Propulsion reduces the parameter count from
355.3 million to just 0.086 million, achieving over a 10x reduction compared to
standard approaches like LoRA while maintaining competitive performance across
benchmarks.

中文翻译:
以下是符合要求的学术论文摘要中文翻译：

大型语言模型（LLMs）的快速发展为自然语言处理（NLP）及相关领域带来了革命性变革。然而，针对特定任务微调这些模型仍存在计算成本高昂且可能损害预训练特征的缺陷。为解决这些问题，我们提出"推进器"（Propulsion）——一种新型参数高效微调（PEFT）方法，旨在显著降低计算开销的同时优化任务特定性能。该方法受物理运动中可控调节概念的启发，通过选择性重缩放预训练模型的特定维度来引导输出预测朝向任务目标，而无需修改模型原始参数。我们在预训练层引入轻量级、可训练的推进参数，将微调过程中更新的参数量降至最低，从而避免过拟合或既有知识的覆盖。基于神经正切核（NTK）理论的理论分析表明，推进器能以极少的可训练参数逼近全参数微调的性能。实验数据显示，该方法将参数量从3.553亿压缩至仅8.6万，相比LoRA等标准方法实现了10倍以上的参数缩减，同时在基准测试中保持具有竞争力的性能。

（译文严格遵循学术规范，采用专业术语统一原则："fine-tuning"统一译为"微调"，"pre-trained"译为"预训练"，"overfitting"译为"过拟合"等。通过拆分英文长句为中文短句结构，如将"inspired by..."独立译为原因状语分句。关键方法名"Propulsion"保留英文原名并添加中文译名"推进器"符合计算机领域术语处理惯例。数值表达统一转换为中文计数单位"亿/万"，技术缩写首次出现时标注全称。）
