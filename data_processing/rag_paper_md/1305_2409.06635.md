# MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders

链接: http://arxiv.org/abs/2409.06635v1

原文摘要:
The rapid advancements in large language models (LLMs) have significantly
enhanced natural language processing capabilities, facilitating the development
of AudioLLMs that process and understand speech and audio inputs alongside
text. Existing AudioLLMs typically combine a pre-trained audio encoder with a
pre-trained LLM, which are subsequently finetuned on specific audio tasks.
However, the pre-trained audio encoder has constrained capacity to capture
features for new tasks and datasets. To address this, we propose to incorporate
mixtures of `weak' encoders (MoWE) into the AudioLLM framework. MoWE
supplements a base encoder with a pool of relatively light weight encoders,
selectively activated based on the audio input to enhance feature extraction
without significantly increasing model size. Our empirical results demonstrate
that MoWE effectively improves multi-task performance, broadening the
applicability of AudioLLMs to more diverse audio tasks.

中文翻译:
大型语言模型（LLM）的快速发展显著提升了自然语言处理能力，推动了能够同时处理语音、音频输入与文本的AudioLLM技术发展。现有AudioLLM通常将预训练音频编码器与预训练LLM结合，再针对特定音频任务进行微调。然而预训练音频编码器在捕捉新任务和数据集特征方面存在局限。为此，我们提出在AudioLLM框架中引入"弱"编码器混合机制（MoWE），通过基础编码器与轻量级编码器池的协同工作，根据音频输入选择性激活增强特征提取，且不显著增加模型规模。实验表明，MoWE能有效提升多任务性能，使AudioLLM适用于更广泛的音频任务场景。
