# APAM: Adaptive Pre-training and Adaptive Meta Learning in Language Model for Noisy Labels and Long-tailed Learning

链接: http://arxiv.org/abs/2302.03488v1

原文摘要:
Practical natural language processing (NLP) tasks are commonly long-tailed
with noisy labels. Those problems challenge the generalization and robustness
of complex models such as Deep Neural Networks (DNNs). Some commonly used
resampling techniques, such as oversampling or undersampling, could easily lead
to overfitting. It is growing popular to learn the data weights leveraging a
small amount of metadata. Besides, recent studies have shown the advantages of
self-supervised pre-training, particularly to the under-represented data. In
this work, we propose a general framework to handle the problem of both
long-tail and noisy labels. The model is adapted to the domain of problems in a
contrastive learning manner. The re-weighting module is a feed-forward network
that learns explicit weighting functions and adapts weights according to
metadata. The framework further adapts weights of terms in the loss function
through a combination of the polynomial expansion of cross-entropy loss and
focal loss. Our extensive experiments show that the proposed framework
consistently outperforms baseline methods. Lastly, our sensitive analysis
emphasizes the capability of the proposed framework to handle the long-tailed
problem and mitigate the negative impact of noisy labels.

中文翻译:
实际的自然语言处理（NLP）任务往往面临长尾分布与噪声标签的双重挑战，这对深度神经网络（DNNs）等复杂模型的泛化能力和鲁棒性提出了严峻考验。传统的重采样技术（如过采样或欠采样）极易引发过拟合问题。当前研究趋势表明，利用少量元数据学习数据权重的方法日益受到关注，同时自监督预训练技术——尤其是针对低代表性数据——已展现出显著优势。为此，我们提出一个通用框架来协同解决长尾分布与噪声标签问题：该模型通过对比学习方式自适应问题领域，其重加权模块采用前馈网络结构，可根据元数据学习显式权重函数并动态调整权重。框架还创新性地结合多项式展开的交叉熵损失与焦点损失，实现对损失函数各项权重的自适应调节。大量实验证明，该框架在各项基准测试中均稳定优于基线方法。最终的敏感性分析进一步验证了框架处理长尾问题的卓越能力，以及有效缓解噪声标签负面影响的特性。
