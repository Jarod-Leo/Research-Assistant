# Reinforcement Retrieval Leveraging Fine-grained Feedback for Fact Checking News Claims with Black-Box LLM

链接: http://arxiv.org/abs/2404.17283v1

原文摘要:
Retrieval-augmented language models have exhibited promising performance
across various areas of natural language processing (NLP), including
fact-critical tasks. However, due to the black-box nature of advanced large
language models (LLMs) and the non-retrieval-oriented supervision signal of
specific tasks, the training of retrieval model faces significant challenges
under the setting of black-box LLM. We propose an approach leveraging
Fine-grained Feedback with Reinforcement Retrieval (FFRR) to enhance
fact-checking on news claims by using black-box LLM. FFRR adopts a two-level
strategy to gather fine-grained feedback from the LLM, which serves as a reward
for optimizing the retrieval policy, by rating the retrieved documents based on
the non-retrieval ground truth of the task. We evaluate our model on two public
datasets for real-world news claim verification, and the results demonstrate
that FFRR achieves significant improvements over strong LLM-enabled and non-LLM
baselines.

中文翻译:
检索增强型语言模型在自然语言处理（NLP）多个领域展现出卓越性能，尤其在事实核查任务中表现突出。然而，由于先进大语言模型（LLM）的黑箱特性及特定任务非检索导向的监督信号，在黑箱LLM框架下训练检索模型面临重大挑战。本研究提出一种基于细粒度反馈与强化检索（FFRR）的方法，通过利用黑箱LLM来提升新闻声明的事实核查能力。FFRR采用双层策略：首先根据任务非检索标准答案对检索文档进行评分，进而从LLM获取细粒度反馈作为优化检索策略的奖励信号。我们在两个真实新闻声明验证的公开数据集上进行评估，结果表明FFRR相较于强大的LLM基准模型和非LLM基线均取得显著提升。
