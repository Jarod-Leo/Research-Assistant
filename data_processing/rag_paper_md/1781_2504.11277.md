# From Misleading Queries to Accurate Answers: A Three-Stage Fine-Tuning Method for LLMs

链接: http://arxiv.org/abs/2504.11277v1

原文摘要:
Large language models (LLMs) exhibit excellent performance in natural
language processing (NLP), but remain highly sensitive to the quality of input
queries, especially when these queries contain misleading or inaccurate
information. Existing methods focus on correcting the output, but they often
overlook the potential of improving the ability of LLMs to detect and correct
misleading content in the input itself. In this paper, we propose a novel
three-stage fine-tuning method that enhances the ability of LLMs to detect and
correct misleading information in the input, further improving response
accuracy and reducing hallucinations. Specifically, the three stages include
(1) training LLMs to identify misleading information, (2) training LLMs to
correct the misleading information using built-in or external knowledge, and
(3) training LLMs to generate accurate answers based on the corrected queries.
To evaluate our method, we conducted experiments on three datasets for the
hallucination detection task and the question answering (QA) task, as well as
two datasets containing misleading information that we constructed. The
experimental results demonstrate that our method significantly improves the
accuracy and factuality of LLM responses, while also enhancing the ability to
detect hallucinations and reducing the generation of hallucinations in the
output, particularly when the query contains misleading information. We will
publicly release our code upon acceptance.

中文翻译:
大型语言模型（LLMs）在自然语言处理（NLP）任务中展现出卓越性能，但对输入查询的质量极为敏感，尤其是当查询包含误导性或错误信息时。现有方法主要集中于修正输出结果，却往往忽视了提升模型自身检测与纠正输入中误导内容的能力。本文提出一种新颖的三阶段微调方法，通过增强LLMs识别并修正输入中误导信息的能力，进一步提升响应准确性并减少幻觉现象。具体而言，三阶段包括：（1）训练模型识别误导信息；（2）训练模型利用内置或外部知识修正误导信息；（3）训练模型基于修正后的查询生成准确答案。为验证方法有效性，我们在三个标准数据集上进行了幻觉检测任务和问答任务（QA）实验，同时使用自建的两个含误导信息数据集进行测试。实验结果表明，该方法显著提高了LLM响应的准确性与事实性，同时增强了幻觉检测能力并减少输出中的幻觉生成，尤其在查询包含误导信息时效果更为突出。代码将在论文录用后公开。
