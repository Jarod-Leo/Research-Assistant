# FltLM: An Intergrated Long-Context Large Language Model for Effective Context Filtering and Understanding

链接: http://arxiv.org/abs/2410.06886v1

原文摘要:
The development of Long-Context Large Language Models (LLMs) has markedly
advanced natural language processing by facilitating the process of textual
data across long documents and multiple corpora. However, Long-Context LLMs
still face two critical challenges: The lost in the middle phenomenon, where
crucial middle-context information is likely to be missed, and the distraction
issue that the models lose focus due to overly extended contexts. To address
these challenges, we propose the Context Filtering Language Model (FltLM), a
novel integrated Long-Context LLM which enhances the ability of the model on
multi-document question-answering (QA) tasks. Specifically, FltLM innovatively
incorporates a context filter with a soft mask mechanism, identifying and
dynamically excluding irrelevant content to concentrate on pertinent
information for better comprehension and reasoning. Our approach not only
mitigates these two challenges, but also enables the model to operate
conveniently in a single forward pass. Experimental results demonstrate that
FltLM significantly outperforms supervised fine-tuning and retrieval-based
methods in complex QA scenarios, suggesting a promising solution for more
accurate and reliable long-context natural language understanding applications.

中文翻译:
长文本大语言模型（Long-Context LLMs）的发展通过处理长文档和多源语料库的文本数据，显著推动了自然语言处理的进步。然而，这类模型仍面临两大关键挑战：一是"中间信息丢失"现象，即关键的中段上下文信息容易被忽略；二是"注意力分散"问题，即过长的上下文会导致模型失去焦点。为解决这些问题，我们提出了上下文过滤语言模型（FltLM），这是一种创新的集成式长文本大语言模型，可显著提升模型在多文档问答任务中的表现。具体而言，FltLM创新性地融合了具有软掩码机制的上下文过滤器，能够识别并动态排除无关内容，使模型专注于相关信息以提升理解与推理能力。该方法不仅有效缓解了上述两大挑战，还实现了单次前向传播即可完成处理的便捷操作。实验结果表明，在复杂问答场景中，FltLM显著优于监督微调和基于检索的方法，为更精准可靠的长文本自然语言理解应用提供了有前景的解决方案。
