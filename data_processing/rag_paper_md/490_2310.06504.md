# Revisit Input Perturbation Problems for LLMs: A Unified Robustness Evaluation Framework for Noisy Slot Filling Task

链接: http://arxiv.org/abs/2310.06504v1

原文摘要:
With the increasing capabilities of large language models (LLMs), these
high-performance models have achieved state-of-the-art results on a wide range
of natural language processing (NLP) tasks. However, the models' performance on
commonly-used benchmark datasets often fails to accurately reflect their
reliability and robustness when applied to real-world noisy data. To address
these challenges, we propose a unified robustness evaluation framework based on
the slot-filling task to systematically evaluate the dialogue understanding
capability of LLMs in diverse input perturbation scenarios. Specifically, we
construct a input perturbation evaluation dataset, Noise-LLM, which contains
five types of single perturbation and four types of mixed perturbation data.
Furthermore, we utilize a multi-level data augmentation method (character,
word, and sentence levels) to construct a candidate data pool, and carefully
design two ways of automatic task demonstration construction strategies
(instance-level and entity-level) with various prompt templates. Our aim is to
assess how well various robustness methods of LLMs perform in real-world noisy
scenarios. The experiments have demonstrated that the current open-source LLMs
generally achieve limited perturbation robustness performance. Based on these
experimental observations, we make some forward-looking suggestions to fuel the
research in this direction.

中文翻译:
随着大语言模型（LLM）能力的不断提升，这些高性能模型已在众多自然语言处理（NLP）任务中取得了最先进的成果。然而，当应用于现实世界中的噪声数据时，这些模型在常用基准数据集上的表现往往无法准确反映其可靠性和鲁棒性。为应对这些挑战，我们提出一个基于槽填充任务的统一鲁棒性评估框架，用以系统评估LLM在多样化输入扰动场景下的对话理解能力。具体而言，我们构建了包含五类单一扰动和四类混合扰动数据的输入扰动评估数据集Noise-LLM，并采用多层级（字符级、词级和句子级）数据增强方法构建候选数据池。同时，我们精心设计了两类自动任务演示构建策略（实例级与实体级）及多种提示模板，旨在评估LLM各类鲁棒性方法在真实噪声场景中的表现。实验表明，当前开源LLM普遍仅能实现有限的扰动鲁棒性性能。基于这些实验发现，我们提出了若干前瞻性建议以推动该领域的研究发展。

（翻译说明：采用学术论文的规范表达，通过以下处理确保专业性：
1. 术语统一："robustness"译为"鲁棒性"，"slot-filling"译为"槽填充"
2. 长句拆分：将原文复合句按中文习惯分解为多个短句
3. 逻辑显化：通过"具体而言""同时"等连接词明确研究步骤
4. 被动语态转换："are constructed"译为主动态的"构建"
5. 概念准确："prompt templates"译为专业术语"提示模板"而非直译"提示词模板"）
