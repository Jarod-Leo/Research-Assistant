# Loong: Generating Minute-level Long Videos with Autoregressive Language Models

链接: http://arxiv.org/abs/2410.02757v1

原文摘要:
It is desirable but challenging to generate content-rich long videos in the
scale of minutes. Autoregressive large language models (LLMs) have achieved
great success in generating coherent and long sequences of tokens in the domain
of natural language processing, while the exploration of autoregressive LLMs
for video generation is limited to generating short videos of several seconds.
In this work, we conduct a deep analysis of the challenges that prevent
autoregressive LLM-based video generators from generating long videos. Based on
the observations and analysis, we propose Loong, a new autoregressive LLM-based
video generator that can generate minute-long videos. Specifically, we model
the text tokens and video tokens as a unified sequence for autoregressive LLMs
and train the model from scratch. We propose progressive short-to-long training
with a loss re-weighting scheme to mitigate the loss imbalance problem for long
video training. We further investigate inference strategies, including video
token re-encoding and sampling strategies, to diminish error accumulation
during inference. Our proposed Loong can be trained on 10-second videos and be
extended to generate minute-level long videos conditioned on text prompts, as
demonstrated by the results. More samples are available at:
https://yuqingwang1029.github.io/Loong-video.

中文翻译:
生成内容丰富、时长达到分钟级别的长视频是一个理想但极具挑战性的目标。自回归大语言模型（LLMs）在自然语言处理领域已成功生成长序列且连贯的文本标记，然而基于自回归LLM的视频生成研究目前仅限于生成数秒的短视频。本研究深入分析了阻碍自回归LLM视频生成器生成长视频的关键挑战，并基于观察与分析提出了Loong——一种新型自回归LLM视频生成框架，能够生成长达分钟级别的视频。具体而言，我们将文本标记与视频标记统一建模为自回归LLM的序列输入，并从头开始训练模型。通过提出渐进式短到长训练策略配合损失重加权机制，有效缓解了长视频训练中的损失失衡问题。此外，我们研究了包括视频标记重编码和采样策略在内的推理优化方法，以减少推理过程中的误差累积。实验结果表明，Loong模型在10秒视频片段上训练后，能够根据文本提示生成分钟级的长视频。更多示例请访问：https://yuqingwang1029.github.io/Loong-video。
