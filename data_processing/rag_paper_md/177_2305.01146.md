# RadAdapt: Radiology Report Summarization via Lightweight Domain Adaptation of Large Language Models

链接: http://arxiv.org/abs/2305.01146v1

原文摘要:
We systematically investigate lightweight strategies to adapt large language
models (LLMs) for the task of radiology report summarization (RRS).
Specifically, we focus on domain adaptation via pretraining (on natural
language, biomedical text, or clinical text) and via discrete prompting or
parameter-efficient fine-tuning. Our results consistently achieve best
performance by maximally adapting to the task via pretraining on clinical text
and fine-tuning on RRS examples. Importantly, this method fine-tunes a mere
0.32% of parameters throughout the model, in contrast to end-to-end fine-tuning
(100% of parameters). Additionally, we study the effect of in-context examples
and out-of-distribution (OOD) training before concluding with a radiologist
reader study and qualitative analysis. Our findings highlight the importance of
domain adaptation in RRS and provide valuable insights toward developing
effective natural language processing solutions for clinical tasks.

中文翻译:
我们系统性地探索了轻量化策略，以适配大语言模型（LLMs）执行放射学报告摘要（RRS）任务。具体而言，我们聚焦于通过预训练（基于自然语言、生物医学文本或临床文本）以及离散提示或参数高效微调实现领域适应。研究结果表明，通过在临床文本上进行预训练并结合RRS样本微调实现任务最大化适配时，模型性能始终最优。值得注意的是，该方法仅需微调全模型0.32%的参数，而传统端到端微调需调整100%参数。此外，我们研究了上下文示例和分布外（OOD）训练的影响，最终通过放射科医师阅片研究及定性分析完成验证。本研究不仅强调了领域适应在RRS中的关键作用，更为开发面向临床任务的高效自然语言处理解决方案提供了重要见解。
