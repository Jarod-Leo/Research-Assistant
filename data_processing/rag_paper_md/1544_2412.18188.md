# On the Applicability of Zero-Shot Cross-Lingual Transfer Learning for Sentiment Classification in Distant Language Pairs

链接: http://arxiv.org/abs/2412.18188v1

原文摘要:
This research explores the applicability of cross-lingual transfer learning
from English to Japanese and Indonesian using the XLM-R pre-trained model. The
results are compared with several previous works, either by models using a
similar zero-shot approach or a fully-supervised approach, to provide an
overview of the zero-shot transfer learning approach's capability using XLM-R
in comparison with existing models. Our models achieve the best result in one
Japanese dataset and comparable results in other datasets in Japanese and
Indonesian languages without being trained using the target language.
Furthermore, the results suggest that it is possible to train a multi-lingual
model, instead of one model for each language, and achieve promising results.

中文翻译:
本研究探讨了利用XLM-R预训练模型实现从英语到日语及印度尼西亚语的跨语言迁移学习适用性。通过对比采用类似零样本方法或全监督方法的现有模型，我们系统评估了XLM-R在零样本迁移学习中的性能表现。实验结果表明，在未经目标语言训练的情况下，我们的模型在一个日语数据集上取得了最优性能，在其他日语和印度尼西亚语数据集上也获得了可比结果。研究进一步证实，采用单一多语言模型（而非针对每种语言单独训练模型）的训练策略具有可行性，并能取得理想效果。
