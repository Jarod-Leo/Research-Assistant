# Legal Documents Drafting with Fine-Tuned Pre-Trained Large Language Model

链接: http://arxiv.org/abs/2406.04202v1

原文摘要:
With the development of large-scale Language Models (LLM), fine-tuning
pre-trained LLM has become a mainstream paradigm for solving downstream tasks
of natural language processing. However, training a language model in the legal
field requires a large number of legal documents so that the language model can
learn legal terminology and the particularity of the format of legal documents.
The typical NLP approaches usually rely on many manually annotated data sets
for training. However, in the legal field application, it is difficult to
obtain a large number of manually annotated data sets, which restricts the
typical method applied to the task of drafting legal documents. The
experimental results of this paper show that not only can we leverage a large
number of annotation-free legal documents without Chinese word segmentation to
fine-tune a large-scale language model, but more importantly, it can fine-tune
a pre-trained LLM on the local computer to achieve the generating legal
document drafts task, and at the same time achieve the protection of
information privacy and to improve information security issues.

中文翻译:
随着大规模语言模型(LLM)的发展，对预训练LLM进行微调已成为解决自然语言处理下游任务的主流范式。然而在法律领域训练语言模型需要大量法律文书，才能使语言模型学习法律专业术语和法律文书格式的特殊性。传统NLP方法通常依赖大量人工标注数据集进行训练，但在法律领域应用中，难以获取大规模人工标注数据集，这制约了传统方法在法律文书起草任务中的应用。本文实验结果表明，我们不仅能够利用无需中文分词的大规模无标注法律文书对大型语言模型进行微调，更重要的是可以在本地计算机上对预训练LLM进行微调，实现法律文书草稿的生成任务，同时达到保护信息隐私和提升信息安全的目的。
