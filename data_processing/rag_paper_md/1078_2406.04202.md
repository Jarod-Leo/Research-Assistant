# Legal Documents Drafting with Fine-Tuned Pre-Trained Large Language Model

链接: http://arxiv.org/abs/2406.04202v1

原文摘要:
With the development of large-scale Language Models (LLM), fine-tuning
pre-trained LLM has become a mainstream paradigm for solving downstream tasks
of natural language processing. However, training a language model in the legal
field requires a large number of legal documents so that the language model can
learn legal terminology and the particularity of the format of legal documents.
The typical NLP approaches usually rely on many manually annotated data sets
for training. However, in the legal field application, it is difficult to
obtain a large number of manually annotated data sets, which restricts the
typical method applied to the task of drafting legal documents. The
experimental results of this paper show that not only can we leverage a large
number of annotation-free legal documents without Chinese word segmentation to
fine-tune a large-scale language model, but more importantly, it can fine-tune
a pre-trained LLM on the local computer to achieve the generating legal
document drafts task, and at the same time achieve the protection of
information privacy and to improve information security issues.

中文翻译:
随着大规模语言模型（LLM）的发展，对预训练LLM进行微调已成为解决自然语言处理下游任务的主流范式。然而在法律领域训练语言模型需要大量法律文书，以使模型能够习得法律专业术语和法律文书格式的特殊性。传统自然语言处理方法通常依赖大量人工标注数据集进行训练，但在法律领域应用中，难以获取大规模人工标注数据集，这制约了传统方法在法律文书起草任务中的应用。本文实验结果表明：我们不仅能够利用无需中文分词的大规模无标注法律文书对LLM进行微调，更重要的是可在本地计算机上对预训练LLM进行微调，实现法律文书草案生成任务，同时达成信息隐私保护与信息安全问题的提升。

（翻译说明：
1. 专业术语处理："fine-tuning"统一译为"微调"，"pre-trained LLM"译为"预训练LLM"，保持技术术语一致性
2. 长句拆分：将原文复合长句拆分为符合中文表达习惯的短句，如将"However..."部分拆分为两个逻辑层次
3. 被动语态转换："it is difficult to obtain"转化为主动式"难以获取"
4. 概念显化处理："annotation-free"译为"无标注"而非字面直译"免标注"，更符合机器学习领域表述
5. 逻辑衔接强化：通过"不仅...更重要的是..."的递进结构准确再现原文论证层次
6. 文化适配："local computer"译为"本地计算机"而非"本地电脑"，保持学术文本的正式性）
