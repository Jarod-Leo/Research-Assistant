# Breaking the Stage Barrier: A Novel Single-Stage Approach to Long Context Extension for Large Language Models

链接: http://arxiv.org/abs/2412.07171v1

原文摘要:
Recently, Large language models (LLMs) have revolutionized Natural Language
Processing (NLP). Pretrained LLMs, due to limited training context size,
struggle with handling long token sequences, limiting their performance on
various downstream tasks. Current solutions toward long context modeling often
employ multi-stage continual pertaining, which progressively increases the
effective context length through several continual pretraining stages. However,
those approaches require extensive manual tuning and human expertise. In this
paper, we introduce a novel single-stage continual pretraining method,
Head-Adaptive Rotary Position Encoding (HARPE), to equip LLMs with long context
modeling capabilities while simplifying the training process. Our HARPE
leverages different Rotary Position Encoding (RoPE) base frequency values
across different attention heads and directly trains LLMs on the target context
length. Extensive experiments on 4 language modeling benchmarks, including the
latest RULER benchmark, demonstrate that HARPE excels in understanding and
integrating long-context tasks with single-stage training, matching and even
outperforming existing multi-stage methods. Our results highlight that HARPE
successfully breaks the stage barrier for training LLMs with long context
modeling capabilities.

中文翻译:
近年来，大型语言模型（LLMs）彻底改变了自然语言处理（NLP）领域。然而，由于预训练上下文长度的限制，现有LLMs难以处理长令牌序列，这制约了其在各类下游任务中的表现。当前的长上下文建模解决方案通常采用多阶段持续预训练策略，通过分阶段逐步扩展有效上下文长度。但这类方法需要大量人工调参与领域专业知识。本文提出一种创新的单阶段持续预训练方法——自适应头部旋转位置编码（HARPE），在简化训练流程的同时赋予LLMs长上下文建模能力。HARPE通过在不同注意力头部应用差异化的旋转位置编码基频值，直接在目标上下文长度上训练模型。在包含最新RULER基准在内的4个语言建模基准测试中，实验表明HARPE仅需单阶段训练即可在长上下文理解与整合任务上达到甚至超越现有多阶段方法的性能。研究结果证明HARPE成功突破了长上下文能力训练的阶段壁垒。
