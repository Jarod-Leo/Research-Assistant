# Learning to Generalize for Cross-domain QA

链接: http://arxiv.org/abs/2305.08208v1

原文摘要:
There have been growing concerns regarding the out-of-domain generalization
ability of natural language processing (NLP) models, particularly in
question-answering (QA) tasks. Current synthesized data augmentation methods
for QA are hampered by increased training costs. To address this issue, we
propose a novel approach that combines prompting methods and linear probing
then fine-tuning strategy, which does not entail additional cost. Our method
has been theoretically and empirically shown to be effective in enhancing the
generalization ability of both generative and discriminative models. Our
approach outperforms state-of-the-art baselines, with an average increase in F1
score of 4.5%-7.9%. Furthermore, our method can be easily integrated into any
pre-trained models and offers a promising solution to the under-explored
cross-domain QA task. We release our source code at GitHub*.

中文翻译:
近年来，自然语言处理模型在领域外泛化能力方面的问题日益受到关注，尤其在问答任务中表现突出。当前针对问答任务的合成数据增强方法普遍面临训练成本增加的瓶颈。为解决这一问题，我们提出了一种创新性方案：通过结合提示学习方法和线性探测-微调策略，在不增加额外成本的前提下提升模型性能。理论分析与实验验证表明，该方法能有效增强生成式与判别式模型的泛化能力，在F1分数上平均提升4.5%-7.9%，优于现有最优基线模型。该方案具备良好的普适性，可无缝集成到各类预训练模型中，为尚未充分探索的跨领域问答任务提供了可行解决方案。相关源代码已发布于GitHub*平台。
