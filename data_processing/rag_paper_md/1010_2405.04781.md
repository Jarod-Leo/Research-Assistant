# CourseGPT-zh: an Educational Large Language Model Based on Knowledge Distillation Incorporating Prompt Optimization

链接: http://arxiv.org/abs/2405.04781v1

原文摘要:
Large language models (LLMs) have demonstrated astonishing capabilities in
natural language processing (NLP) tasks, sparking interest in their application
to professional domains with higher specialized requirements. However,
restricted access to closed-source LLMs via APIs and the difficulty in
collecting massive high-quality datasets pose obstacles to the development of
large language models in education fields of various courses. Given these
challenges, we propose CourseGPT-zh, a course-oriented education LLM that
supports customization and low-cost deployment. To address the
comprehensiveness and diversity requirements of course-specific corpora, we
design a high-quality question-answering corpus distillation framework
incorporating prompt optimization, which effectively mines textbook knowledge
and enhances its diversity. Moreover, considering the alignment of LLM
responses with user needs, a novel method for discrete prompt optimization
based on LLM-as-Judge is introduced. During optimization, this framework
leverages the LLM's ability to reflect on and exploit error feedback and
patterns, allowing for prompts that meet user needs and preferences while
saving response length. Lastly, we obtain CourseGPT-zh based on the open-source
LLM using parameter-efficient fine-tuning. Experimental results show that our
discrete prompt optimization framework effectively improves the response
quality of ChatGPT, and CourseGPT-zh exhibits strong professional capabilities
in specialized knowledge question-answering, significantly outperforming
comparable open-source models.

中文翻译:
大型语言模型（LLM）在自然语言处理（NLP）任务中展现出惊人能力，引发了对其应用于专业化要求更高领域的兴趣。然而，通过API访问闭源模型的限制及海量高质量数据集收集的困难，阻碍了各课程教育领域大模型的发展。针对这些挑战，我们提出CourseGPT-zh——支持定制化与低成本部署的课程教育大模型。为解决课程语料全面性与多样性需求，设计了融合提示优化的高质量问答语料蒸馏框架，有效挖掘教材知识并增强多样性。此外，考虑到模型响应与用户需求的对齐性，创新性地提出基于LLM-as-Judge的离散提示优化方法。该框架在优化过程中利用大模型对错误反馈与模式的反思挖掘能力，使提示词在满足用户需求偏好的同时节省响应长度。最终基于开源大模型通过参数高效微调获得CourseGPT-zh。实验表明，我们的离散提示优化框架能有效提升ChatGPT响应质量，且CourseGPT-zh在专业知识问答中表现出色，显著优于同类开源模型。
