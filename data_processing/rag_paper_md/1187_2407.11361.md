# Graph Structure Prompt Learning: A Novel Methodology to Improve Performance of Graph Neural Networks

链接: http://arxiv.org/abs/2407.11361v1

原文摘要:
Graph neural networks (GNNs) are widely applied in graph data modeling.
However, existing GNNs are often trained in a task-driven manner that fails to
fully capture the intrinsic nature of the graph structure, resulting in
sub-optimal node and graph representations. To address this limitation, we
propose a novel Graph structure Prompt Learning method (GPL) to enhance the
training of GNNs, which is inspired by prompt mechanisms in natural language
processing. GPL employs task-independent graph structure losses to encourage
GNNs to learn intrinsic graph characteristics while simultaneously solving
downstream tasks, producing higher-quality node and graph representations. In
extensive experiments on eleven real-world datasets, after being trained by
GPL, GNNs significantly outperform their original performance on node
classification, graph classification, and edge prediction tasks (up to 10.28%,
16.5%, and 24.15%, respectively). By allowing GNNs to capture the inherent
structural prompts of graphs in GPL, they can alleviate the issue of
over-smooth and achieve new state-of-the-art performances, which introduces a
novel and effective direction for GNN research with potential applications in
various domains.

中文翻译:
图神经网络（GNNs）在图形数据建模中应用广泛。然而，现有GNNs通常采用任务驱动的方式进行训练，未能充分捕捉图结构的本质特征，导致节点与图表示效果欠佳。为解决这一局限，受自然语言处理中提示机制的启发，我们提出了一种新颖的图结构提示学习方法（GPL）来增强GNNs的训练。GPL通过任务无关的图结构损失函数，促使GNNs在解决下游任务的同时学习图的固有特性，从而生成更高质量的节点与图表示。在11个真实数据集上的大量实验表明，经GPL训练后的GNNs在节点分类、图分类和边预测任务中性能显著提升（最高分别达10.28%、16.5%和24.15%）。该方法通过让GNNs捕获图的内在结构提示，既能缓解过平滑问题，又能实现新的性能突破，为GNN研究开辟了具有多领域应用潜力的创新方向。
