# Can Pre-trained Language Models Understand Chinese Humor?

链接: http://arxiv.org/abs/2407.04105v1

原文摘要:
Humor understanding is an important and challenging research in natural
language processing. As the popularity of pre-trained language models (PLMs),
some recent work makes preliminary attempts to adopt PLMs for humor recognition
and generation. However, these simple attempts do not substantially answer the
question: {\em whether PLMs are capable of humor understanding?} This paper is
the first work that systematically investigates the humor understanding ability
of PLMs. For this purpose, a comprehensive framework with three evaluation
steps and four evaluation tasks is designed. We also construct a comprehensive
Chinese humor dataset, which can fully meet all the data requirements of the
proposed evaluation framework. Our empirical study on the Chinese humor dataset
yields some valuable observations, which are of great guiding value for future
optimization of PLMs in humor understanding and generation.

中文翻译:
幽默理解是自然语言处理中一项重要且富有挑战性的研究。随着预训练语言模型(PLMs)的普及，近期有研究初步尝试将其应用于幽默识别与生成任务。然而这些简单尝试并未从根本上回答"预训练语言模型是否具备幽默理解能力"这一核心问题。本文首次系统性地探究了PLMs的幽默理解能力，为此设计了一个包含三个评估阶段、四项评测任务的完整框架，并构建了能全面满足该框架数据需求的中文幽默语料库。基于该语料库的实证研究得出了若干重要发现，这些发现对未来优化PLMs的幽默理解与生成能力具有重要指导价值。
