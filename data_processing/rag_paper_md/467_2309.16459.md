# Augmenting LLMs with Knowledge: A survey on hallucination prevention

链接: http://arxiv.org/abs/2309.16459v1

原文摘要:
Large pre-trained language models have demonstrated their proficiency in
storing factual knowledge within their parameters and achieving remarkable
results when fine-tuned for downstream natural language processing tasks.
Nonetheless, their capacity to access and manipulate knowledge with precision
remains constrained, resulting in performance disparities on
knowledge-intensive tasks when compared to task-specific architectures.
Additionally, the challenges of providing provenance for model decisions and
maintaining up-to-date world knowledge persist as open research frontiers. To
address these limitations, the integration of pre-trained models with
differentiable access mechanisms to explicit non-parametric memory emerges as a
promising solution. This survey delves into the realm of language models (LMs)
augmented with the ability to tap into external knowledge sources, including
external knowledge bases and search engines. While adhering to the standard
objective of predicting missing tokens, these augmented LMs leverage diverse,
possibly non-parametric external modules to augment their contextual processing
capabilities, departing from the conventional language modeling paradigm.
Through an exploration of current advancements in augmenting large language
models with knowledge, this work concludes that this emerging research
direction holds the potential to address prevalent issues in traditional LMs,
such as hallucinations, un-grounded responses, and scalability challenges.

中文翻译:
大规模预训练语言模型已展现出在参数中存储事实性知识的能力，并在下游自然语言处理任务微调后取得卓越表现。然而，其精确访问与操作知识的能力仍存在局限，导致在知识密集型任务上的性能落后于专用架构模型。此外，为模型决策提供溯源依据及保持世界知识时效性等挑战，仍是待突破的研究前沿。为解决这些限制，将预训练模型与可微分显式非参数记忆访问机制相结合成为颇具前景的解决方案。本文系统研究了具备外部知识源访问能力的增强型语言模型，涵盖外部知识库与搜索引擎等资源。这些模型在保持预测缺失标记标准目标的同时，通过调用多样化（可能为非参数化）的外部模块来扩展上下文处理能力，突破了传统语言建模范式。通过对知识增强大语言模型最新进展的梳理，本研究认为这一新兴研究方向有望解决传统语言模型普遍存在的幻觉生成、无依据响应及可扩展性难题。
