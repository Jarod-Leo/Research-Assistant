# Leaving the barn door open for Clever Hans: Simple features predict LLM benchmark answers

链接: http://arxiv.org/abs/2410.11672v1

原文摘要:
The integrity of AI benchmarks is fundamental to accurately assess the
capabilities of AI systems. The internal validity of these benchmarks - i.e.,
making sure they are free from confounding factors - is crucial for ensuring
that they are measuring what they are designed to measure. In this paper, we
explore a key issue related to internal validity: the possibility that AI
systems can solve benchmarks in unintended ways, bypassing the capability being
tested. This phenomenon, widely known in human and animal experiments, is often
referred to as the 'Clever Hans' effect, where tasks are solved using spurious
cues, often involving much simpler processes than those putatively assessed.
Previous research suggests that language models can exhibit this behaviour as
well. In several older Natural Language Processing (NLP) benchmarks, individual
$n$-grams like "not" have been found to be highly predictive of the correct
labels, and supervised NLP models have been shown to exploit these patterns. In
this work, we investigate the extent to which simple $n$-grams extracted from
benchmark instances can be combined to predict labels in modern multiple-choice
benchmarks designed for LLMs, and whether LLMs might be using such $n$-gram
patterns to solve these benchmarks. We show how simple classifiers trained on
these $n$-grams can achieve high scores on several benchmarks, despite lacking
the capabilities being tested. Additionally, we provide evidence that modern
LLMs might be using these superficial patterns to solve benchmarks. This
suggests that the internal validity of these benchmarks may be compromised and
caution should be exercised when interpreting LLM performance results on them.

中文翻译:
人工智能基准测试的完整性是准确评估AI系统能力的基础。这些基准的内部效度——即确保其不受混杂因素影响——对于保证其真正测量目标能力至关重要。本文探讨了一个与内部效度相关的核心问题：AI系统可能通过非预期方式解决基准测试，绕过被评估的真实能力。这种现象在人类与动物实验中广为人知，常被称为"聪明汉斯效应"——即利用虚假线索（通常比假定评估的认知过程简单得多）来解决问题。已有研究表明语言模型同样会表现出此类行为：在早期自然语言处理（NLP）基准中，诸如"not"之类的单个n元语法被证明能高度预测正确答案，监督式NLP模型确实会利用这些模式。本研究量化分析了从基准实例中提取的简单n元语法组合对现代大语言模型（LLM）多选题基准标签的预测能力，并探究LLM是否可能利用此类n元语法模式解题。实验表明，基于这些n元语法训练的简单分类器虽不具备被测能力，却能在多个基准上获得高分。进一步证据显示现代LLM可能正在利用这些表层模式解题。这表明相关基准的内部效度可能存在问题，在解读LLM性能结果时应保持审慎态度。
