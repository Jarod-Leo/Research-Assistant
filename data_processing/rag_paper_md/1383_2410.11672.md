# Leaving the barn door open for Clever Hans: Simple features predict LLM benchmark answers

链接: http://arxiv.org/abs/2410.11672v1

原文摘要:
The integrity of AI benchmarks is fundamental to accurately assess the
capabilities of AI systems. The internal validity of these benchmarks - i.e.,
making sure they are free from confounding factors - is crucial for ensuring
that they are measuring what they are designed to measure. In this paper, we
explore a key issue related to internal validity: the possibility that AI
systems can solve benchmarks in unintended ways, bypassing the capability being
tested. This phenomenon, widely known in human and animal experiments, is often
referred to as the 'Clever Hans' effect, where tasks are solved using spurious
cues, often involving much simpler processes than those putatively assessed.
Previous research suggests that language models can exhibit this behaviour as
well. In several older Natural Language Processing (NLP) benchmarks, individual
$n$-grams like "not" have been found to be highly predictive of the correct
labels, and supervised NLP models have been shown to exploit these patterns. In
this work, we investigate the extent to which simple $n$-grams extracted from
benchmark instances can be combined to predict labels in modern multiple-choice
benchmarks designed for LLMs, and whether LLMs might be using such $n$-gram
patterns to solve these benchmarks. We show how simple classifiers trained on
these $n$-grams can achieve high scores on several benchmarks, despite lacking
the capabilities being tested. Additionally, we provide evidence that modern
LLMs might be using these superficial patterns to solve benchmarks. This
suggests that the internal validity of these benchmarks may be compromised and
caution should be exercised when interpreting LLM performance results on them.

中文翻译:
人工智能基准测试的完整性是准确评估AI系统能力的基础。这些基准测试的内部效度——即确保其不受混杂因素干扰——对于保证测试结果真实反映被测量能力至关重要。本文探讨了一个与内部效度相关的核心问题：AI系统可能通过非预期方式解决基准测试，从而绕过被测试能力的现象。这一在人类与动物实验中广为人知的现象常被称为"聪明汉斯效应"——测试对象利用虚假线索（通常涉及比评估目标简单得多的处理过程）来解决问题。已有研究表明，语言模型同样可能表现出此类行为。在早期自然语言处理（NLP）基准测试中，研究者发现单个n元语法（如"not"）对正确答案具有高度预测性，监督式NLP模型已被证实会利用这些模式。本研究通过现代面向大语言模型（LLM）设计的多项选择基准测试，探究从测试实例中提取的简单n元语法组合对答案标签的预测能力，以及LLM是否可能利用此类n元语法模式来解题。我们证明：尽管缺乏被测试的真实能力，基于这些n元语法训练的简单分类器仍能在多个基准测试中获得高分。此外，我们提供证据表明现代LLM可能正在利用这些表层模式来解题。这表明相关基准测试的内部效度可能存在问题，在解读LLM测试结果时应保持审慎态度。
