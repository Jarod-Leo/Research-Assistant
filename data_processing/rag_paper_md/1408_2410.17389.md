# Navigating Noisy Feedback: Enhancing Reinforcement Learning with Error-Prone Language Models

链接: http://arxiv.org/abs/2410.17389v1

原文摘要:
The correct specification of reward models is a well-known challenge in
reinforcement learning. Hand-crafted reward functions often lead to inefficient
or suboptimal policies and may not be aligned with user values. Reinforcement
learning from human feedback is a successful technique that can mitigate such
issues, however, the collection of human feedback can be laborious. Recent
works have solicited feedback from pre-trained large language models rather
than humans to reduce or eliminate human effort, however, these approaches
yield poor performance in the presence of hallucination and other errors. This
paper studies the advantages and limitations of reinforcement learning from
large language model feedback and proposes a simple yet effective method for
soliciting and applying feedback as a potential-based shaping function. We
theoretically show that inconsistent rankings, which approximate ranking
errors, lead to uninformative rewards with our approach. Our method empirically
improves convergence speed and policy returns over commonly used baselines even
with significant ranking errors, and eliminates the need for complex
post-processing of reward functions.

中文翻译:
以下是符合您要求的中文翻译：

奖励模型的正确设定一直是强化学习领域公认的挑战。人工设计的奖励函数往往导致策略效率低下或次优，且可能与用户价值观不一致。基于人类反馈的强化学习技术虽能有效缓解这些问题，但人工反馈收集过程耗时费力。近期研究尝试用预训练大语言模型替代人类提供反馈以降低人力成本，然而当出现幻觉或其他错误时，这些方法表现欠佳。本文系统研究了基于大语言模型反馈的强化学习的优势与局限，提出一种简洁有效的反馈获取与应用方法——将其作为基于势能的塑形函数。理论分析表明，近似排序错误导致的不一致排名会产生无信息量的奖励信号。实验证明，即使存在显著排序误差，我们的方法仍能提升收敛速度和策略回报，优于常用基线方案，同时无需对奖励函数进行复杂后处理。

（翻译严格遵循以下原则：
1. 专业术语准确统一："reward models"译为"奖励模型"，"potential-based shaping function"译为"基于势能的塑形函数"
2. 被动语态转化：将英文被动结构转换为中文主动表达，如"are aligned with"处理为"与...一致"
3. 长句拆分重组：将原文复合句按中文表达习惯分解为多个短句，如理论分析部分
4. 概念显化处理："hallucination"译为专业术语"幻觉"，"uninformative rewards"意译为"无信息量的奖励信号"
5. 学术风格保持：使用"本文""研究表明"等学术用语，避免口语化表达）
