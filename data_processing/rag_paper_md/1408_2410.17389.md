# Navigating Noisy Feedback: Enhancing Reinforcement Learning with Error-Prone Language Models

链接: http://arxiv.org/abs/2410.17389v1

原文摘要:
The correct specification of reward models is a well-known challenge in
reinforcement learning. Hand-crafted reward functions often lead to inefficient
or suboptimal policies and may not be aligned with user values. Reinforcement
learning from human feedback is a successful technique that can mitigate such
issues, however, the collection of human feedback can be laborious. Recent
works have solicited feedback from pre-trained large language models rather
than humans to reduce or eliminate human effort, however, these approaches
yield poor performance in the presence of hallucination and other errors. This
paper studies the advantages and limitations of reinforcement learning from
large language model feedback and proposes a simple yet effective method for
soliciting and applying feedback as a potential-based shaping function. We
theoretically show that inconsistent rankings, which approximate ranking
errors, lead to uninformative rewards with our approach. Our method empirically
improves convergence speed and policy returns over commonly used baselines even
with significant ranking errors, and eliminates the need for complex
post-processing of reward functions.

中文翻译:
奖励模型的正确设定是强化学习领域公认的挑战。人工设计的奖励函数常导致策略效率低下或次优，且可能与用户价值观不符。基于人类反馈的强化学习虽能有效缓解这些问题，但人工反馈收集过程往往费时费力。近期研究尝试用预训练大语言模型替代人类提供反馈以减少人力投入，然而在存在幻觉等错误时，这些方法表现欠佳。本文系统研究了基于大语言模型反馈的强化学习的优势与局限，提出一种简洁有效的反馈获取与应用方法——通过势能函数实现反馈转化。理论分析表明，近似排序错误导致的不一致排名会使奖励信号失去信息价值。实证研究表明，即使在显著排序误差下，我们的方法仍能提升收敛速度和策略回报，优于常用基线方案，且无需对奖励函数进行复杂后处理。
