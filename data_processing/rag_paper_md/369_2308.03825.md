# "Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models

链接: http://arxiv.org/abs/2308.03825v1

原文摘要:
The misuse of large language models (LLMs) has drawn significant attention
from the general public and LLM vendors. One particular type of adversarial
prompt, known as jailbreak prompt, has emerged as the main attack vector to
bypass the safeguards and elicit harmful content from LLMs. In this paper,
employing our new framework JailbreakHub, we conduct a comprehensive analysis
of 1,405 jailbreak prompts spanning from December 2022 to December 2023. We
identify 131 jailbreak communities and discover unique characteristics of
jailbreak prompts and their major attack strategies, such as prompt injection
and privilege escalation. We also observe that jailbreak prompts increasingly
shift from online Web communities to prompt-aggregation websites and 28 user
accounts have consistently optimized jailbreak prompts over 100 days. To assess
the potential harm caused by jailbreak prompts, we create a question set
comprising 107,250 samples across 13 forbidden scenarios. Leveraging this
dataset, our experiments on six popular LLMs show that their safeguards cannot
adequately defend jailbreak prompts in all scenarios. Particularly, we identify
five highly effective jailbreak prompts that achieve 0.95 attack success rates
on ChatGPT (GPT-3.5) and GPT-4, and the earliest one has persisted online for
over 240 days. We hope that our study can facilitate the research community and
LLM vendors in promoting safer and regulated LLMs.

中文翻译:
大型语言模型（LLM）的滥用问题已引发公众与模型供应商的高度关注。其中一类名为"越狱提示"（jailbreak prompt）的对抗性提示，正成为突破安全防护、诱导模型生成有害内容的主要攻击手段。本研究通过自建的JailbreakHub分析框架，对2022年12月至2023年12月期间的1,405个越狱提示进行了系统分析，发现：1）存在131个活跃的越狱社区；2）越狱提示具有独特演化特征，主要采用提示注入、权限提升等攻击策略；3）攻击阵地正从网络社区向提示聚合网站转移，28个用户账号持续优化越狱提示超100天。为评估危害性，我们构建了覆盖13类禁忌场景的107,250个测试样本库。在六大主流LLM上的实验表明，现有防护机制无法全面抵御越狱攻击——尤其识别出5个攻击成功率高达95%的高效提示（针对ChatGPT/GPT-4），其中最早出现的已存活240余天。本研究旨在为学术界和厂商提升LLM安全性提供参考。
