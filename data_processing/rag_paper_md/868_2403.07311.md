# Knowledge Graph Large Language Model (KG-LLM) for Link Prediction

链接: http://arxiv.org/abs/2403.07311v1

原文摘要:
The task of multi-hop link prediction within knowledge graphs (KGs) stands as
a challenge in the field of knowledge graph analysis, as it requires the model
to reason through and understand all intermediate connections before making a
prediction. In this paper, we introduce the Knowledge Graph Large Language
Model (KG-LLM), a novel framework that leverages large language models (LLMs)
for knowledge graph tasks. We first convert structured knowledge graph data
into natural language and then use these natural language prompts to fine-tune
LLMs to enhance multi-hop link prediction in KGs. By converting the KG to
natural language prompts, our framework is designed to learn the latent
representations of entities and their interrelations. To show the efficacy of
the KG-LLM Framework, we fine-tune three leading LLMs within this framework,
including Flan-T5, LLaMa2 and Gemma. Further, we explore the framework's
potential to provide LLMs with zero-shot capabilities for handling previously
unseen prompts. Experimental results show that KG-LLM significantly improves
the models' generalization capabilities, leading to more accurate predictions
in unfamiliar scenarios.

中文翻译:
知识图谱（KGs）中的多跳链接预测任务是知识图谱分析领域的一项挑战，因为该任务要求模型在做出预测前必须推理并理解所有中间连接关系。本文提出知识图谱大语言模型（KG-LLM）这一创新框架，通过利用大语言模型（LLMs）来处理知识图谱任务。我们首先将结构化的知识图谱数据转换为自然语言，随后利用这些自然语言提示对LLMs进行微调，以增强知识图谱中的多跳链接预测能力。通过将知识图谱转化为自然语言提示，该框架能够学习实体及其相互关系的潜在表征。为验证KG-LLM框架的有效性，我们在该框架下对Flan-T5、LLaMa2和Gemma三种前沿大语言模型进行了微调。此外，我们还探索了该框架赋予LLMs处理未见提示的零样本能力潜力。实验结果表明，KG-LLM显著提升了模型的泛化能力，使其在陌生场景中能做出更精准的预测。
