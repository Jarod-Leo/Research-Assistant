# Interpreting Pretrained Language Models via Concept Bottlenecks

链接: http://arxiv.org/abs/2311.05014v1

原文摘要:
Pretrained language models (PLMs) have made significant strides in various
natural language processing tasks. However, the lack of interpretability due to
their ``black-box'' nature poses challenges for responsible implementation.
Although previous studies have attempted to improve interpretability by using,
e.g., attention weights in self-attention layers, these weights often lack
clarity, readability, and intuitiveness. In this research, we propose a novel
approach to interpreting PLMs by employing high-level, meaningful concepts that
are easily understandable for humans. For example, we learn the concept of
``Food'' and investigate how it influences the prediction of a model's
sentiment towards a restaurant review. We introduce C$^3$M, which combines
human-annotated and machine-generated concepts to extract hidden neurons
designed to encapsulate semantically meaningful and task-specific concepts.
Through empirical evaluations on real-world datasets, we manifest that our
approach offers valuable insights to interpret PLM behavior, helps diagnose
model failures, and enhances model robustness amidst noisy concept labels.

中文翻译:
预训练语言模型（PLMs）在各类自然语言处理任务中取得了显著进展。然而，其"黑盒"特性导致的解释性缺失为负责任的应用带来了挑战。尽管先前研究尝试通过自注意力层中的注意力权重等方法来提升可解释性，但这些权重往往缺乏清晰性、可读性和直观性。本研究提出了一种创新方法，通过采用人类易于理解的高层次语义概念来解释PLMs。例如，我们学习"食物"这一概念，并探究其如何影响模型对餐厅评论情感倾向的预测。我们提出的C$^3$M框架融合了人工标注与机器生成的概念，旨在提取能够封装语义明确且任务相关概念的隐藏神经元。基于真实数据集的实证评估表明，该方法能有效揭示PLMs的行为机制，辅助诊断模型故障，并在存在噪声概念标签的情况下增强模型鲁棒性。
