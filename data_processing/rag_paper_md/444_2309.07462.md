# Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?

链接: http://arxiv.org/abs/2309.07462v1

原文摘要:
Large Language Models (LLMs) excel in various Natural Language Processing
(NLP) tasks, yet their evaluation, particularly in languages beyond the top
$20$, remains inadequate due to existing benchmarks and metrics limitations.
Employing LLMs as evaluators to rank or score other models' outputs emerges as
a viable solution, addressing the constraints tied to human annotators and
established benchmarks. In this study, we explore the potential of LLM-based
evaluators, specifically GPT-4 in enhancing multilingual evaluation by
calibrating them against $20$K human judgments across three text-generation
tasks, five metrics, and eight languages. Our analysis reveals a bias in
GPT4-based evaluators towards higher scores, underscoring the necessity of
calibration with native speaker judgments, especially in low-resource and
non-Latin script languages, to ensure accurate evaluation of LLM performance
across diverse languages.

中文翻译:
大型语言模型（LLMs）在各类自然语言处理任务中表现卓越，但受限于现有评估基准与指标的不足，其在对前20名以外语言的评估仍不充分。采用LLMs作为评估者对其他模型输出进行排名或评分，成为解决人类标注者与既有基准约束的有效方案。本研究通过将基于GPT-4的评估器与20,000条人工评判进行校准（涵盖三项文本生成任务、五项评估指标及八种语言），探索了LLM评估器在多语言评估中的提升潜力。分析表明：基于GPT-4的评估器存在分数偏高倾向，强调必须结合母语者评判进行校准——尤其在资源匮乏型语言与非拉丁文字语言中——方能确保LLMs跨语言性能评估的准确性。

（翻译说明：采用学术论文摘要的简洁风格，通过句式重组实现中英文语序转换；专业术语如"low-resource languages"译为"资源匮乏型语言"符合中文文献惯例；长句拆解为符合中文表达习惯的短句结构；破折号与括号的使用既保持原文强调效果，又符合中文标点规范；"non-Latin script"译为"非拉丁文字"确保技术准确性）
