# Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?

链接: http://arxiv.org/abs/2309.07462v1

原文摘要:
Large Language Models (LLMs) excel in various Natural Language Processing
(NLP) tasks, yet their evaluation, particularly in languages beyond the top
$20$, remains inadequate due to existing benchmarks and metrics limitations.
Employing LLMs as evaluators to rank or score other models' outputs emerges as
a viable solution, addressing the constraints tied to human annotators and
established benchmarks. In this study, we explore the potential of LLM-based
evaluators, specifically GPT-4 in enhancing multilingual evaluation by
calibrating them against $20$K human judgments across three text-generation
tasks, five metrics, and eight languages. Our analysis reveals a bias in
GPT4-based evaluators towards higher scores, underscoring the necessity of
calibration with native speaker judgments, especially in low-resource and
non-Latin script languages, to ensure accurate evaluation of LLM performance
across diverse languages.

中文翻译:
大型语言模型（LLMs）在各类自然语言处理（NLP）任务中表现卓越，然而受限于现有基准与评估指标的不足，其评价体系——尤其是针对前20名以外语种的表现——仍显薄弱。采用LLMs作为评估者对其他模型输出进行排名或打分，成为解决人工标注与固定基准约束的可行方案。本研究探索了基于LLM的评估器（特别是GPT-4）通过校准20,000条人类判断（涵盖三项文本生成任务、五项指标及八种语言）来增强多语言评估的潜力。分析发现GPT-4评估器存在分数偏高倾向，强调必须结合母语者判断进行校准，尤其在资源匮乏语言和非拉丁文字语种中，以确保跨语言LLM性能评估的准确性。
