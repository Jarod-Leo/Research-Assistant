# K-ON: Stacking Knowledge On the Head Layer of Large Language Model

链接: http://arxiv.org/abs/2502.06257v1

原文摘要:
Recent advancements in large language models (LLMs) have significantly
improved various natural language processing (NLP) tasks. Typically, LLMs are
trained to predict the next token, aligning well with many NLP tasks. However,
in knowledge graph (KG) scenarios, entities are the fundamental units and
identifying an entity requires at least several tokens. This leads to a
granularity mismatch between KGs and natural languages. To address this issue,
we propose K-ON, which integrates KG knowledge into the LLM by employing
multiple head layers for next k-step prediction. K-ON can not only generate
entity-level results in one step, but also enables contrastive loss against
entities, which is the most powerful tool in KG representation learning.
Experimental results show that K-ON outperforms state-of-the-art methods that
incorporate text and even the other modalities.

中文翻译:
近期大语言模型（LLM）的突破性进展显著提升了各类自然语言处理（NLP）任务的性能。传统LLM通过预测下一词元进行训练，这与多数NLP任务高度契合。然而在知识图谱（KG）场景中，实体作为基本单元往往需要多个词元才能完整表征，导致KG与自然语言存在粒度失配问题。为此，我们提出K-ON模型，通过引入多头部层实现k步连续预测，将KG知识深度整合至LLM框架。该方案不仅能单步生成实体级结果，更支持针对实体的对比损失计算——这正是知识图谱表示学习中最具效力的训练手段。实验表明，K-ON在融合文本乃至多模态信息的现有最优方法中展现出显著优势。
