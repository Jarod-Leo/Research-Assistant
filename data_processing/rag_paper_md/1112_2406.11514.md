# Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs

链接: http://arxiv.org/abs/2406.11514v1

原文摘要:
Large Language Models (LLMs) excel in various natural language processing
tasks but struggle with hallucination issues. Existing solutions have
considered utilizing LLMs' inherent reasoning abilities to alleviate
hallucination, such as self-correction and diverse sampling methods. However,
these methods often overtrust LLMs' initial answers due to inherent biases. The
key to alleviating this issue lies in overriding LLMs' inherent biases for
answer inspection. To this end, we propose a CounterFactual Multi-Agent Debate
(CFMAD) framework. CFMAD presets the stances of LLMs to override their inherent
biases by compelling LLMs to generate justifications for a predetermined
answer's correctness. The LLMs with different predetermined stances are engaged
with a skeptical critic for counterfactual debate on the rationality of
generated justifications. Finally, the debate process is evaluated by a
third-party judge to determine the final answer. Extensive experiments on four
datasets of three tasks demonstrate the superiority of CFMAD over existing
methods.

中文翻译:
大型语言模型（LLMs）在各类自然语言处理任务中表现卓越，却饱受幻觉问题困扰。现有解决方案尝试利用LLMs固有的推理能力来缓解幻觉现象，例如自我纠错和多样化采样方法。然而这些方法因模型固有偏见往往过度信任初始答案。解决该问题的关键在于突破LLMs的固有偏见进行答案校验。为此，我们提出反事实多智能体辩论框架（CFMAD）。该框架通过预设LLMs立场来覆盖其固有偏见——强制模型为预设答案的正确性生成论证依据，使持有不同预设立场的LLMs与质疑型评论者展开反事实辩论，对生成论证的合理性进行交锋。最终由第三方裁判评估辩论过程以确定最终答案。在三个任务的四个数据集上的大量实验表明，CFMAD显著优于现有方法。
