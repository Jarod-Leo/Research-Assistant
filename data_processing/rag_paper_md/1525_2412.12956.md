# SnakModel: Lessons Learned from Training an Open Danish Large Language Model

链接: http://arxiv.org/abs/2412.12956v1

原文摘要:
We present SnakModel, a Danish large language model (LLM) based on Llama2-7B,
which we continuously pre-train on 13.6B Danish words, and further tune on 3.7M
Danish instructions. As best practices for creating LLMs for smaller language
communities have yet to be established, we examine the effects of early
modeling and training decisions on downstream performance throughout the entire
training pipeline, including (1) the creation of a strictly curated corpus of
Danish text from diverse sources; (2) the language modeling and
instruction-tuning training process itself, including the analysis of
intermediate training dynamics, and ablations across different hyperparameters;
(3) an evaluation on eight language and culturally-specific tasks. Across these
experiments SnakModel achieves the highest overall performance, outperforming
multiple contemporary Llama2-7B-based models. By making SnakModel, the majority
of our pre-training corpus, and the associated code available under open
licenses, we hope to foster further research and development in Danish Natural
Language Processing, and establish training guidelines for languages with
similar resource constraints.

中文翻译:
我们推出了SnakModel，这是一款基于Llama2-7B架构的丹麦语大语言模型（LLM）。该模型在136亿丹麦语词汇上进行了持续预训练，并进一步针对370万条丹麦语指令进行了微调。鉴于目前针对小语种社区构建大语言模型的最佳实践尚未确立，我们系统考察了整个训练流程中早期建模与训练决策对下游性能的影响，具体包括：（1）从多源数据中严格筛选构建丹麦语文本语料库；（2）语言建模与指令微调的训练过程本身，涵盖中间训练动态分析及不同超参数的消融实验；（3）在八项涉及语言与文化特性的任务上进行评估。实验表明SnakModel在综合性能上表现最优，优于多个同期基于Llama2-7B的模型。通过将SnakModel、大部分预训练语料库及相关代码以开放许可形式发布，我们期望能推动丹麦自然语言处理领域的深入研究，并为资源条件相似的其他语言建立训练范式。
