# Robustness via Referencing: Defending against Prompt Injection Attacks by Referencing the Executed Instruction

链接: http://arxiv.org/abs/2504.20472v1

原文摘要:
Large language models (LLMs) have demonstrated impressive performance and
have come to dominate the field of natural language processing (NLP) across
various tasks. However, due to their strong instruction-following capabilities
and inability to distinguish between instructions and data content, LLMs are
vulnerable to prompt injection attacks. These attacks manipulate LLMs into
deviating from the original input instructions and executing maliciously
injected instructions within data content, such as web documents retrieved from
search engines. Existing defense methods, including prompt-engineering and
fine-tuning approaches, typically instruct models to follow the original input
instructions while suppressing their tendencies to execute injected
instructions. However, our experiments reveal that suppressing
instruction-following tendencies is challenging. Through analyzing failure
cases, we observe that although LLMs tend to respond to any recognized
instructions, they are aware of which specific instructions they are executing
and can correctly reference them within the original prompt. Motivated by these
findings, we propose a novel defense method that leverages, rather than
suppresses, the instruction-following abilities of LLMs. Our approach prompts
LLMs to generate responses that include both answers and their corresponding
instruction references. Based on these references, we filter out answers not
associated with the original input instructions. Comprehensive experiments
demonstrate that our method outperforms prompt-engineering baselines and
achieves performance comparable to fine-tuning methods, reducing the attack
success rate (ASR) to 0 percent in some scenarios. Moreover, our approach has
minimal impact on overall utility.

中文翻译:
大型语言模型（LLMs）已展现出卓越性能，并在自然语言处理（NLP）领域的各类任务中占据主导地位。然而，由于其强大的指令跟随能力且无法区分指令与数据内容，LLMs易受提示注入攻击。这类攻击通过操纵模型使其偏离原始输入指令，转而执行数据内容中恶意注入的指令（例如从搜索引擎检索的网页文档）。现有防御方法（包括提示工程和微调技术）通常要求模型遵循原始输入指令，同时抑制其执行注入指令的倾向。但实验表明，抑制指令跟随倾向具有挑战性。通过分析失败案例，我们发现尽管LLMs倾向于响应任何识别到的指令，但它们能明确知晓当前执行的特定指令，并可在原始提示中正确引用这些指令。基于此发现，我们提出一种创新防御方法——该方法并非抑制而是充分利用LLMs的指令跟随能力。我们的策略引导模型生成包含答案及其对应指令引用的响应，随后根据这些引用过滤掉与原始输入指令无关的答案。全面实验表明，该方法优于提示工程基线方案，其性能与微调方法相当，在某些场景下能将攻击成功率（ASR）降至0%。此外，该方法对模型整体效用影响极小。
