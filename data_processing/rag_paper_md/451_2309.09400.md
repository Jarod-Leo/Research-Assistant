# CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages

链接: http://arxiv.org/abs/2309.09400v1

原文摘要:
The driving factors behind the development of large language models (LLMs)
with impressive learning capabilities are their colossal model sizes and
extensive training datasets. Along with the progress in natural language
processing, LLMs have been frequently made accessible to the public to foster
deeper investigation and applications. However, when it comes to training
datasets for these LLMs, especially the recent state-of-the-art models, they
are often not fully disclosed. Creating training data for high-performing LLMs
involves extensive cleaning and deduplication to ensure the necessary level of
quality. The lack of transparency for training data has thus hampered research
on attributing and addressing hallucination and bias issues in LLMs, hindering
replication efforts and further advancements in the community. These challenges
become even more pronounced in multilingual learning scenarios, where the
available multilingual text datasets are often inadequately collected and
cleaned. Consequently, there is a lack of open-source and readily usable
dataset to effectively train LLMs in multiple languages. To overcome this
issue, we present CulturaX, a substantial multilingual dataset with 6.3
trillion tokens in 167 languages, tailored for LLM development. Our dataset
undergoes meticulous cleaning and deduplication through a rigorous pipeline of
multiple stages to accomplish the best quality for model training, including
language identification, URL-based filtering, metric-based cleaning, document
refinement, and data deduplication. CulturaX is fully released to the public in
HuggingFace to facilitate research and advancements in multilingual LLMs:
https://huggingface.co/datasets/uonlp/CulturaX.

中文翻译:
驱动大型语言模型（LLMs）具备卓越学习能力的核心因素在于其庞大的参数量与海量训练数据。随着自然语言处理技术的进步，LLMs已频繁向公众开放以促进深度研究和应用。然而，这些模型（尤其是当前最先进版本）的训练数据集往往未被完整公开。构建高性能LLMs的训练数据需要进行大量清洗与去重工作以确保质量达标，训练数据的非透明性阻碍了针对模型幻觉与偏见问题的归因研究，也制约了学术界的复现尝试与技术突破。这些挑战在多语言学习场景中尤为突出——现有多语言文本数据集普遍存在采集不充分与清洗不足的问题，导致缺乏可直接用于有效训练多语言LLMs的开源数据集。

为解决这一难题，我们推出CulturaX：一个包含167种语言、总规模达6.3万亿token的专用多语言数据集。该数据集通过语言识别、基于URL的过滤、指标清洗、文档优化及数据去重等多阶段严格流程实现精细化处理，为模型训练提供最优质量保障。CulturaX已在HuggingFace平台完整开源以推动多语言LLMs研究：https://huggingface.co/datasets/uonlp/CulturaX。
