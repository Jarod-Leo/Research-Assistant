# Automating Code-Related Tasks Through Transformers: The Impact of Pre-training

链接: http://arxiv.org/abs/2302.04048v1

原文摘要:
Transformers have gained popularity in the software engineering (SE)
literature. These deep learning models are usually pre-trained through a
self-supervised objective, meant to provide the model with basic knowledge
about a language of interest (e.g., Java). A classic pre-training objective is
the masked language model (MLM), in which a percentage of tokens from the input
(e.g., a Java method) is masked, with the model in charge of predicting them.
Once pre-trained, the model is then fine-tuned to support the specific
downstream task of interest (e.g., code summarization). While there is evidence
suggesting the boost in performance provided by pre-training, little is known
about the impact of the specific pre-training objective(s) used. Indeed, MLM is
just one of the possible pre-training objectives and recent work from the
natural language processing field suggest that pre-training objectives tailored
for the specific downstream task of interest may substantially boost the
model's performance. In this study, we focus on the impact of pre-training
objectives on the performance of transformers when automating code-related
tasks. We start with a systematic literature review aimed at identifying the
pre-training objectives used in SE. Then, we pre-train 32 transformers using
both (i) generic pre-training objectives usually adopted in SE; and (ii)
pre-training objectives tailored to specific code-related tasks subject of our
experimentation, namely bug-fixing, code summarization, and code completion. We
also compare the pre-trained models with non pre-trained ones. Our results show
that: (i) pre-training helps in boosting performance only if the amount of
fine-tuning data available is small; (ii) the MLM objective is usually
sufficient to maximize the prediction performance of the model, even when
comparing it with pre-training objectives specialized for the downstream task
at hand.

中文翻译:
Transformer模型在软件工程（SE）研究领域日益受到青睐。这类深度学习模型通常通过自监督目标进行预训练，旨在使其掌握目标语言（如Java）的基础知识。经典的预训练目标包括掩码语言模型（MLM），即从输入文本（如Java方法）中随机遮蔽部分标记，由模型预测这些被遮蔽的内容。预训练完成后，模型会通过微调来适应特定的下游任务（如代码摘要生成）。尽管已有证据表明预训练能显著提升模型性能，但针对不同预训练目标的具体影响机制仍不明确。事实上，MLM仅是众多预训练目标中的一种，自然语言处理领域的最新研究表明：为下游任务量身定制的预训练目标可能大幅提升模型表现。

本研究重点探究预训练目标对自动化代码任务中Transformer性能的影响。我们首先通过系统性文献综述识别SE领域常用的预训练目标，随后对32个Transformer模型进行两组对比实验：（i）采用SE领域常规的通用预训练目标；（ii）使用针对特定代码任务（缺陷修复、代码摘要生成和代码补全）专门设计的预训练目标。同时将预训练模型与未经预训练的模型进行对比。研究发现：（i）仅当微调数据量较小时，预训练才能显著提升性能；（ii）即使与针对下游任务专门设计的预训练目标相比，MLM目标通常已足以使模型达到最佳预测性能。

（注：译文在保持学术严谨性的基础上，对原文进行了以下优化处理：
1. 将长句拆分为符合中文表达习惯的短句结构
2. 专业术语如"fine-tuning"统一译为"微调"，"downstream task"译为"下游任务"
3. 被动语态转换为主动表述（如"are usually pre-trained"→"通常通过...进行预训练"）
4. 补充逻辑连接词（如"事实上""随后"）增强行文连贯性
5. 技术概念首次出现时保留英文缩写（如MLM），后文统一使用中文译名）
