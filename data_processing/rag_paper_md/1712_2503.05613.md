# A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models

链接: http://arxiv.org/abs/2503.05613v1

原文摘要:
Large Language Models (LLMs) have revolutionized natural language processing,
yet their internal mechanisms remain largely opaque. Recently, mechanistic
interpretability has attracted significant attention from the research
community as a means to understand the inner workings of LLMs. Among various
mechanistic interpretability approaches, Sparse Autoencoders (SAEs) have
emerged as a particularly promising method due to their ability to disentangle
the complex, superimposed features within LLMs into more interpretable
components. This paper presents a comprehensive examination of SAEs as a
promising approach to interpreting and understanding LLMs. We provide a
systematic overview of SAE principles, architectures, and applications
specifically tailored for LLM analysis, covering theoretical foundations,
implementation strategies, and recent developments in sparsity mechanisms. We
also explore how SAEs can be leveraged to explain the internal workings of
LLMs, steer model behaviors in desired directions, and develop more transparent
training methodologies for future models. Despite the challenges that remain
around SAE implementation and scaling, they continue to provide valuable tools
for understanding the internal mechanisms of large language models.

中文翻译:
大语言模型（LLMs）彻底改变了自然语言处理领域，但其内部工作机制仍存在显著的不透明性。近年来，机械可解释性作为理解LLMs内在运作机理的重要手段，已引起研究界的广泛关注。在各种机械可解释性方法中，稀疏自编码器（SAEs）因其能将LLMs中复杂叠加的特征解耦为更具可解释性的组件，成为极具前景的研究方向。本文系统性地探讨了SAEs作为LLMs解释与理解方法的潜力，详细阐述了面向LLM分析的特化SAE原理、架构与应用，涵盖理论基础、实施策略及稀疏机制的最新进展。我们深入研究了如何利用SAEs揭示LLMs的内部运作机制、引导模型行为朝向预期方向演进，以及为未来模型开发更透明的训练方法。尽管SAE在实现与扩展方面仍存在挑战，该方法持续为理解大语言模型内部机制提供了宝贵工具。
