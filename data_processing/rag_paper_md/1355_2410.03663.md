# Enhance Reasoning by Learning from Mistakes: Peer-Review Knowledge Distillation from Multiple Large Language Models

链接: http://arxiv.org/abs/2410.03663v1

原文摘要:
While reasoning capabilities typically emerge in large language models (LLMs)
with tens of billions of parameters, recent research focuses on improving
smaller open-source models through knowledge distillation (KD) from commercial
LLMs. However, many of these studies rely solely on responses from a single LLM
as the gold rationale, unlike the natural human learning process, which
involves understanding both the correct answers and the reasons behind
mistakes. In this paper, we introduce a novel Fault-Aware DistIllation via
Peer-Review (FAIR) approach: 1) Instead of merely obtaining rationales from
teachers, our method asks teachers to identify and explain the student's
mistakes, providing customized instruction learning data. 2) We design a
simulated peer-review process between teacher LLMs, which selects only the
generated rationales above the acceptance threshold. This reduces the chance of
teachers guessing correctly with flawed rationale, improving instructional data
quality. Comprehensive experiments and analysis on mathematical, commonsense,
and logical reasoning tasks demonstrate the effectiveness of our method.

中文翻译:
尽管推理能力通常出现在具有数百亿参数的大型语言模型（LLMs）中，但近期研究致力于通过从商用LLMs进行知识蒸馏（KD）来提升小型开源模型的性能。然而，与人类自然学习过程不同——后者需同时理解正确答案和错误背后的原因——许多现有研究仅依赖单一LLM的响应作为黄金标准逻辑链。本文提出了一种创新的"基于同行评审的容错蒸馏"（FAIR）方法：1）不同于仅从教师模型获取逻辑链，我们的方法要求教师模型识别并解释学生模型的错误，从而提供定制化的指导学习数据；2）我们设计了教师LLMs之间的模拟同行评审流程，仅筛选超过接受阈值的生成逻辑链。这降低了教师模型通过错误逻辑链猜测正确的可能性，提升了教学数据质量。在数学、常识和逻辑推理任务上的全面实验与分析验证了本方法的有效性。
