# Transforming Graphs for Enhanced Attribute-Based Clustering: An Innovative Graph Transformer Method

链接: http://arxiv.org/abs/2306.11307v1

原文摘要:
Graph Representation Learning (GRL) is an influential methodology, enabling a
more profound understanding of graph-structured data and aiding graph
clustering, a critical task across various domains. The recent incursion of
attention mechanisms, originally an artifact of Natural Language Processing
(NLP), into the realm of graph learning has spearheaded a notable shift in
research trends. Consequently, Graph Attention Networks (GATs) and Graph
Attention Auto-Encoders have emerged as preferred tools for graph clustering
tasks. Yet, these methods primarily employ a local attention mechanism, thereby
curbing their capacity to apprehend the intricate global dependencies between
nodes within graphs. Addressing these impediments, this study introduces an
innovative method known as the Graph Transformer Auto-Encoder for Graph
Clustering (GTAGC). By melding the Graph Auto-Encoder with the Graph
Transformer, GTAGC is adept at capturing global dependencies between nodes.
This integration amplifies the graph representation and surmounts the
constraints posed by the local attention mechanism. The architecture of GTAGC
encompasses graph embedding, integration of the Graph Transformer within the
autoencoder structure, and a clustering component. It strategically alternates
between graph embedding and clustering, thereby tailoring the Graph Transformer
for clustering tasks, whilst preserving the graph's global structural
information. Through extensive experimentation on diverse benchmark datasets,
GTAGC has exhibited superior performance against existing state-of-the-art
graph clustering methodologies.

中文翻译:
图表示学习（Graph Representation Learning, GRL）作为一种具有影响力的方法论，能够深化对图结构数据的理解，并助力图聚类这一跨领域关键任务。近年来，源自自然语言处理（NLP）的注意力机制被引入图学习领域，显著改变了研究趋势。由此诞生的图注意力网络（GATs）和图注意力自编码器已成为图聚类任务的首选工具。然而，这些方法主要采用局部注意力机制，限制了其捕捉图中节点间复杂全局依赖关系的能力。

针对这些局限性，本研究提出了一种创新方法——面向图聚类的图Transformer自编码器（GTAGC）。该方法通过将图自编码器与图Transformer相融合，能够有效捕获节点间的全局依赖关系。这种集成不仅增强了图表示能力，还突破了局部注意力机制的约束。GTAGC架构包含图嵌入层、自编码器框架内的图Transformer模块以及聚类组件，通过策略性地交替进行图嵌入与聚类操作，在保持图全局结构信息的同时，使图Transformer适配于聚类任务。

在多个基准数据集上的大量实验表明，GTAGC相较于现有最先进的图聚类方法展现出更优越的性能。
