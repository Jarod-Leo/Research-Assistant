# Transformer-based Single-Cell Language Model: A Survey

链接: http://arxiv.org/abs/2407.13205v1

原文摘要:
The transformers have achieved significant accomplishments in the natural
language processing as its outstanding parallel processing capabilities and
highly flexible attention mechanism. In addition, increasing studies based on
transformers have been proposed to model single-cell data. In this review, we
attempt to systematically summarize the single-cell language models and
applications based on transformers. First, we provide a detailed introduction
about the structure and principles of transformers. Then, we review the
single-cell language models and large language models for single-cell data
analysis. Moreover, we explore the datasets and applications of single-cell
language models in downstream tasks such as batch correction, cell clustering,
cell type annotation, gene regulatory network inference and perturbation
response. Further, we discuss the challenges of single-cell language models and
provide promising research directions. We hope this review will serve as an
up-to-date reference for researchers interested in the direction of single-cell
language models.

中文翻译:
Transformer凭借其卓越的并行处理能力和高度灵活的自注意力机制，在自然语言处理领域取得了显著成就。与此同时，基于Transformer的单细胞数据建模研究也日益增多。本文系统综述了基于Transformer的单细胞语言模型及其应用：首先详细解析了Transformer的结构原理；继而梳理了面向单细胞分析的语言模型与大型语言模型；进而探讨了单细胞语言模型在批次校正、细胞聚类、细胞类型注释、基因调控网络推断及扰动响应等下游任务中的数据集与应用场景；最后讨论了该技术面临的挑战并展望了潜在研究方向。本综述旨在为单细胞语言模型领域的研究者提供最新的技术参考。
