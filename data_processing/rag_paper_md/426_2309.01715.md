# Prompting or Fine-tuning? A Comparative Study of Large Language Models for Taxonomy Construction

链接: http://arxiv.org/abs/2309.01715v1

原文摘要:
Taxonomies represent hierarchical relations between entities, frequently
applied in various software modeling and natural language processing (NLP)
activities. They are typically subject to a set of structural constraints
restricting their content. However, manual taxonomy construction can be
time-consuming, incomplete, and costly to maintain. Recent studies of large
language models (LLMs) have demonstrated that appropriate user inputs (called
prompting) can effectively guide LLMs, such as GPT-3, in diverse NLP tasks
without explicit (re-)training. However, existing approaches for automated
taxonomy construction typically involve fine-tuning a language model by
adjusting model parameters. In this paper, we present a general framework for
taxonomy construction that takes into account structural constraints. We
subsequently conduct a systematic comparison between the prompting and
fine-tuning approaches performed on a hypernym taxonomy and a novel computer
science taxonomy dataset. Our result reveals the following: (1) Even without
explicit training on the dataset, the prompting approach outperforms
fine-tuning-based approaches. Moreover, the performance gap between prompting
and fine-tuning widens when the training dataset is small. However, (2)
taxonomies generated by the fine-tuning approach can be easily post-processed
to satisfy all the constraints, whereas handling violations of the taxonomies
produced by the prompting approach can be challenging. These evaluation
findings provide guidance on selecting the appropriate method for taxonomy
construction and highlight potential enhancements for both approaches.

中文翻译:
以下是符合要求的学术中文翻译：

分类体系表征实体间的层级关系，广泛应用于各类软件建模与自然语言处理任务中。这类体系通常受限于一系列约束其内容的结构化规则。然而人工构建分类体系存在耗时费力、完整性不足且维护成本高等问题。近期针对大语言模型（LLMs）的研究表明，通过适当的用户输入（即提示工程）可有效引导GPT-3等LLMs完成多种自然语言处理任务，而无需显式的（重新）训练。但现有自动化分类体系构建方法通常需要通过调整模型参数对语言模型进行微调。本文提出一个考虑结构化约束的通用分类体系构建框架，并基于上位词分类体系与新型计算机科学分类数据集，对提示工程与模型微调两种方法展开系统对比。研究发现：（1）即使未对目标数据集进行显式训练，提示工程法的表现仍优于基于微调的方法。当训练数据集较小时，二者性能差距更为显著；（2）微调方法生成的分类体系易于通过后处理满足所有约束条件，而提示工程方法产生的体系若违反约束规则则较难修正。这些评估结果可为分类体系构建方法的选择提供指导，并为两种方法的改进指明方向。

翻译说明：
1. 专业术语处理："Taxonomies"译为"分类体系"符合中文计算机领域术语，"prompting"保留为"提示工程"并补充括号说明
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句，如将"However, manual..."处理为转折并列句
3. 被动语态转换："are typically subject to"译为主动态"受限于"
4. 概念显化："LLMs"首次出现时补充全称"大语言模型"
5. 学术表达："Our result reveals"译为"研究发现"更符合中文论文表述
6. 数据呈现：使用中文惯用的冒号引导研究发现条目，保持学术严谨性
7. 逻辑连接：通过"然而""但"等连接词保持论证逻辑的连贯性
