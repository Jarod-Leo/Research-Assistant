# Improving Attributed Text Generation of Large Language Models via Preference Learning

链接: http://arxiv.org/abs/2403.18381v1

原文摘要:
Large language models have been widely adopted in natural language
processing, yet they face the challenge of generating unreliable content.
Recent works aim to reduce misinformation and hallucinations by resorting to
attribution as a means to provide evidence (i.e., citations). However, current
attribution methods usually focus on the retrieval stage and automatic
evaluation that neglect mirroring the citation mechanisms in human scholarly
writing to bolster credibility. In this paper, we address these challenges by
modelling the attribution task as preference learning and introducing an
Automatic Preference Optimization (APO) framework. First, we create a curated
collection for post-training with 6,330 examples by collecting and filtering
from existing datasets. Second, considering the high cost of labelling
preference data, we further propose an automatic method to synthesize
attribution preference data resulting in 95,263 pairs. Moreover, inspired by
the human citation process, we further propose a progressive preference
optimization method by leveraging fine-grained information. Extensive
experiments on three datasets (i.e., ASQA, StrategyQA, and ELI5) demonstrate
that APO achieves state-of-the-art citation F1 with higher answer quality.

中文翻译:
大型语言模型在自然语言处理领域已得到广泛应用，但仍面临生成内容可信度不足的挑战。近期研究尝试通过归因机制（即提供引用证据）来减少错误信息和幻觉现象。然而，现有归因方法多聚焦于检索阶段和自动评估，未能充分借鉴人类学术写作中的引用机制以增强可信度。本文通过将归因任务建模为偏好学习，提出自动偏好优化（APO）框架应对这些挑战：首先从现有数据集中筛选整理出6,330个训练样本构建后训练数据集；其次针对偏好数据标注成本高的问题，提出自动合成归因偏好数据的方法，生成95,263组配对样本；此外受人类引用过程启发，提出利用细粒度信息进行渐进式偏好优化的方法。在ASQA、StrategyQA和ELI5三个数据集上的实验表明，APO在保持更高回答质量的同时，实现了最先进的引用F1值。
