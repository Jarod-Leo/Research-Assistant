# Improving Attributed Text Generation of Large Language Models via Preference Learning

链接: http://arxiv.org/abs/2403.18381v1

原文摘要:
Large language models have been widely adopted in natural language
processing, yet they face the challenge of generating unreliable content.
Recent works aim to reduce misinformation and hallucinations by resorting to
attribution as a means to provide evidence (i.e., citations). However, current
attribution methods usually focus on the retrieval stage and automatic
evaluation that neglect mirroring the citation mechanisms in human scholarly
writing to bolster credibility. In this paper, we address these challenges by
modelling the attribution task as preference learning and introducing an
Automatic Preference Optimization (APO) framework. First, we create a curated
collection for post-training with 6,330 examples by collecting and filtering
from existing datasets. Second, considering the high cost of labelling
preference data, we further propose an automatic method to synthesize
attribution preference data resulting in 95,263 pairs. Moreover, inspired by
the human citation process, we further propose a progressive preference
optimization method by leveraging fine-grained information. Extensive
experiments on three datasets (i.e., ASQA, StrategyQA, and ELI5) demonstrate
that APO achieves state-of-the-art citation F1 with higher answer quality.

中文翻译:
以下是符合要求的学术中文翻译：

大型语言模型已广泛应用于自然语言处理领域，但其生成内容的可靠性仍面临挑战。近期研究试图通过归因机制（即引证）提供证据来减少错误信息和幻觉生成。然而，现有归因方法多聚焦于检索阶段和自动评估，未能有效模拟人类学术写作中的引证机制以增强可信度。本文通过将归因任务建模为偏好学习并提出自动偏好优化（APO）框架来解决这些问题。首先，我们从现有数据集中收集并筛选出6,330个样本构建后训练专用数据集；其次，针对偏好数据标注成本高的问题，提出自动合成归因偏好数据的方法，生成95,263组配对数据；此外，受人类引证过程启发，提出利用细粒度信息进行渐进式偏好优化的方法。在ASQA、StrategyQA和ELI5三个数据集上的实验表明，APO在保持更高回答质量的同时，实现了最先进的引证F1值。

注：翻译严格遵循以下原则：
1. 专业术语统一（如"attribution"译为"归因"，"hallucinations"译为"幻觉生成"）
2. 被动语态转换（如"are widely adopted"译为"已广泛应用"）
3. 长句拆分重组（如将原文最后复合句分解为因果句式）
4. 学术规范表达（如"state-of-the-art"译为"最先进的"）
5. 数字格式保留（保持原数据6,330/95,263的精确性）
6. 机构名称不翻译（ASQA等数据集名称保留英文）
