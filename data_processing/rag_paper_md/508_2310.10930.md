# Enhanced Transformer Architecture for Natural Language Processing

链接: http://arxiv.org/abs/2310.10930v1

原文摘要:
Transformer is a state-of-the-art model in the field of natural language
processing (NLP). Current NLP models primarily increase the number of
transformers to improve processing performance. However, this technique
requires a lot of training resources such as computing capacity. In this paper,
a novel structure of Transformer is proposed. It is featured by full layer
normalization, weighted residual connection, positional encoding exploiting
reinforcement learning, and zero masked self-attention. The proposed
Transformer model, which is called Enhanced Transformer, is validated by the
bilingual evaluation understudy (BLEU) score obtained with the Multi30k
translation dataset. As a result, the Enhanced Transformer achieves 202.96%
higher BLEU score as compared to the original transformer with the translation
dataset.

中文翻译:
Transformer是自然语言处理（NLP）领域的先进模型。当前NLP模型主要通过增加Transformer数量来提升处理性能，但这种方法需要消耗大量计算资源等训练成本。本文提出了一种新型Transformer结构，其特点包括：全层归一化处理、加权残差连接、基于强化学习的位置编码优化以及零掩码自注意力机制。该增强型Transformer模型（Enhanced Transformer）通过Multi30k翻译数据集的双语评估替补（BLEU）分数进行验证。实验结果表明，在使用相同翻译数据集时，增强型Transformer的BLEU分数较原始Transformer模型提升了202.96%。
