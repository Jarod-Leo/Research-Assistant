# The Importance of Positional Encoding Initialization in Transformers for Relational Reasoning

链接: http://arxiv.org/abs/2406.08272v1

原文摘要:
The attention mechanism is central to the transformer's ability to capture
complex dependencies between tokens of an input sequence. Key to the successful
application of the attention mechanism in transformers is its choice of
positional encoding (PE). The PE provides essential information that
distinguishes the position and order amongst tokens in a sequence. Most prior
investigations of PE effects on generalization were tailored to 1D input
sequences, such as those presented in natural language, where adjacent tokens
(e.g., words) are highly related. In contrast, many real world tasks involve
datasets with highly non-trivial positional arrangements, such as datasets
organized in multiple spatial dimensions, or datasets for which ground truth
positions are not known, such as in biological data. Here we study the
importance of learning accurate PE for problems which rely on a non-trivial
arrangement of input tokens. Critically, we find that the choice of
initialization of a learnable PE greatly influences its ability to learn
accurate PEs that lead to enhanced generalization. We empirically demonstrate
our findings in three experiments: 1) A 2D relational reasoning task; 2) A
nonlinear stochastic network simulation; 3) A real world 3D neuroscience
dataset, applying interpretability analyses to verify the learning of accurate
PEs. Overall, we find that a learned PE initialized from a small-norm
distribution can 1) uncover interpretable PEs that mirror ground truth
positions in multiple dimensions, and 2) lead to improved downstream
generalization in empirical evaluations. Importantly, choosing an ill-suited PE
can be detrimental to both model interpretability and generalization. Together,
our results illustrate the feasibility of learning identifiable and
interpretable PEs for enhanced generalization.

中文翻译:
注意力机制是Transformer模型捕捉输入序列中标记间复杂依赖关系的核心。该机制成功应用于Transformer的关键在于其位置编码（PE）的选择。PE提供了区分序列中标记位置与顺序的重要信息。以往关于PE对泛化能力影响的研究多针对一维输入序列（如自然语言中相邻标记高度相关的词序列），而现实任务常涉及位置排列复杂的多维数据集（如空间多维组织的数据或真实位置未知的生物数据）。本文研究了在输入标记非平凡排列的问题中学习准确PE的重要性，并发现可学习PE的初始化方式对其能否习得提升泛化的精确PE具有决定性影响。我们通过三项实验验证这一发现：1）二维关系推理任务；2）非线性随机网络模拟；3）真实三维神经科学数据集（辅以可解释性分析验证PE学习效果）。结果表明：从小范数分布初始化的可学习PE能够1）在多维空间中还原与真实位置对应的可解释PE；2）显著提升下游任务泛化性能。反之，不当的PE选择会同时损害模型可解释性与泛化能力。本研究证实了通过学习可识别、可解释的PE来增强泛化能力的可行性。
