# Gender Encoding Patterns in Pretrained Language Model Representations

链接: http://arxiv.org/abs/2503.06734v1

原文摘要:
Gender bias in pretrained language models (PLMs) poses significant social and
ethical challenges. Despite growing awareness, there is a lack of comprehensive
investigation into how different models internally represent and propagate such
biases. This study adopts an information-theoretic approach to analyze how
gender biases are encoded within various encoder-based architectures. We focus
on three key aspects: identifying how models encode gender information and
biases, examining the impact of bias mitigation techniques and fine-tuning on
the encoded biases and their effectiveness, and exploring how model design
differences influence the encoding of biases. Through rigorous and systematic
investigation, our findings reveal a consistent pattern of gender encoding
across diverse models. Surprisingly, debiasing techniques often exhibit limited
efficacy, sometimes inadvertently increasing the encoded bias in internal
representations while reducing bias in model output distributions. This
highlights a disconnect between mitigating bias in output distributions and
addressing its internal representations. This work provides valuable guidance
for advancing bias mitigation strategies and fostering the development of more
equitable language models.

中文翻译:
预训练语言模型（PLMs）中的性别偏见带来了重大的社会与伦理挑战。尽管公众意识逐渐增强，但针对不同模型如何内部表征并传播此类偏见的系统性研究仍显不足。本研究采用信息论方法，分析了多种基于编码器架构的模型如何编码性别偏见。我们聚焦三个核心维度：揭示模型编码性别信息与偏见的具体机制；评估偏见缓解技术和微调过程对编码偏见的干预效果；探究模型设计差异对偏见编码的影响规律。通过严谨系统的实验分析，我们发现不同模型呈现出高度一致的性别编码模式。令人意外的是，现有去偏见技术往往效果有限，某些情况下甚至会加剧模型内部表征的编码偏见，尽管其输出分布的偏见有所降低。这凸显出输出分布去偏见与内部表征修正之间存在脱节现象。本研究成果为推进偏见缓解策略、发展更公平的语言模型提供了重要指导。
