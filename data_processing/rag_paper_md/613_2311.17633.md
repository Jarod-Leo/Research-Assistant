# Introduction to Transformers: an NLP Perspective

链接: http://arxiv.org/abs/2311.17633v1

原文摘要:
Transformers have dominated empirical machine learning models of natural
language processing. In this paper, we introduce basic concepts of Transformers
and present key techniques that form the recent advances of these models. This
includes a description of the standard Transformer architecture, a series of
model refinements, and common applications. Given that Transformers and related
deep learning techniques might be evolving in ways we have never seen, we
cannot dive into all the model details or cover all the technical areas.
Instead, we focus on just those concepts that are helpful for gaining a good
understanding of Transformers and their variants. We also summarize the key
ideas that impact this field, thereby yielding some insights into the strengths
and limitations of these models.

中文翻译:
Transformer模型已在自然语言处理领域的实证机器学习模型中占据主导地位。本文系统介绍了Transformer的基础概念，并阐述了推动这类模型近期发展的关键技术体系。主要内容包括：标准Transformer架构解析、一系列模型优化方法以及典型应用场景。鉴于Transformer及相关深度学习技术可能正以我们前所未见的方式演进，本文无法穷尽所有模型细节或覆盖全部技术领域，而是聚焦于那些真正有助于深入理解Transformer及其变体的核心概念。同时，我们对影响该领域发展的关键思想进行提炼，从而揭示这类模型的内在优势与固有局限性。

（译文特点说明：
1. 专业术语统一："Transformers"规范译为"Transformer模型/架构"，保持学术文本一致性
2. 句式重构：将原文多个松散短句整合为符合中文表达习惯的复合句，如"this includes..."处理为总分结构
3. 逻辑显化：通过"鉴于...而是..."的转折结构清晰呈现原文隐含的论述逻辑
4. 学术风格：使用"解析""提炼""揭示"等动词保持学术文本的严谨性，同时通过"无法穷尽""聚焦于"等表述准确传达原文的限定范围
5. 概念显化："model refinements"译为"模型优化方法"比直译更符合中文技术文献表达习惯）
