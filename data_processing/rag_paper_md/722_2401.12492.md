# Comparing Human-Centered Language Modeling: Is it Better to Model Groups, Individual Traits, or Both?

链接: http://arxiv.org/abs/2401.12492v1

原文摘要:
Pre-trained language models consider the context of neighboring words and
documents but lack any author context of the human generating the text.
However, language depends on the author's states, traits, social, situational,
and environmental attributes, collectively referred to as human context (Soni
et al., 2024). Human-centered natural language processing requires
incorporating human context into language models. Currently, two methods exist:
pre-training with 1) group-wise attributes (e.g., over-45-year-olds) or 2)
individual traits. Group attributes are simple but coarse -- not all
45-year-olds write the same way -- while individual traits allow for more
personalized representations, but require more complex modeling and data. It is
unclear which approach benefits what tasks. We compare pre-training models with
human context via 1) group attributes, 2) individual users, and 3) a combined
approach on five user- and document-level tasks. Our results show that there is
no best approach, but that human-centered language modeling holds avenues for
different methods.

中文翻译:
预训练语言模型能够考虑邻近词语及文档的上下文，却忽视了文本生成者——人类作者的相关背景。事实上，语言表达深受作者状态、个性特征、社会关系、情境因素及环境属性等人类语境要素的影响（Soni等人，2024）。以人为中心的自然语言处理需要将人类语境融入语言模型。目前存在两种方法：1）基于群体属性（如45岁以上人群）的预训练；2）基于个体特征的预训练。群体属性方法简单但粗糙（并非所有45岁人群都有相同的表达方式），而个体特征能实现更个性化的表征，却需要更复杂的建模和数据支持。这两种方法对不同任务的具体效益尚不明确。我们通过五类用户及文档级任务，对比了三种预训练方式：1）群体属性、2）个体用户、3）混合方法。结果表明不存在绝对最优方案，但以人为中心的语言建模为不同方法提供了发展路径。
