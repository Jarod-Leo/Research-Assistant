# Memory Is All You Need: An Overview of Compute-in-Memory Architectures for Accelerating Large Language Model Inference

链接: http://arxiv.org/abs/2406.08413v1

原文摘要:
Large language models (LLMs) have recently transformed natural language
processing, enabling machines to generate human-like text and engage in
meaningful conversations. This development necessitates speed, efficiency, and
accessibility in LLM inference as the computational and memory requirements of
these systems grow exponentially. Meanwhile, advancements in computing and
memory capabilities are lagging behind, exacerbated by the discontinuation of
Moore's law. With LLMs exceeding the capacity of single GPUs, they require
complex, expert-level configurations for parallel processing. Memory accesses
become significantly more expensive than computation, posing a challenge for
efficient scaling, known as the memory wall. Here, compute-in-memory (CIM)
technologies offer a promising solution for accelerating AI inference by
directly performing analog computations in memory, potentially reducing latency
and power consumption. By closely integrating memory and compute elements, CIM
eliminates the von Neumann bottleneck, reducing data movement and improving
energy efficiency. This survey paper provides an overview and analysis of
transformer-based models, reviewing various CIM architectures and exploring how
they can address the imminent challenges of modern AI computing systems. We
discuss transformer-related operators and their hardware acceleration schemes
and highlight challenges, trends, and insights in corresponding CIM designs.

中文翻译:
大型语言模型（LLMs）近期彻底改变了自然语言处理领域，使机器能够生成类人文本并参与有意义的对话。随着这些系统的计算与内存需求呈指数级增长，LLM推理的速度、效率和可访问性变得至关重要。与此同时，计算与内存能力的进步却相对滞后，摩尔定律的失效更使这一矛盾加剧。当LLMs超出单个GPU的承载能力时，需要依赖复杂且需专家级配置的并行处理方案。内存访问成本显著高于计算成本，这一高效扩展难题被称为"内存墙"。在此背景下，存内计算（CIM）技术通过直接在内存中执行模拟计算，为加速AI推理提供了颇具前景的解决方案，有望降低延迟与功耗。CIM通过紧密集成内存与计算单元，消除了冯·诺依曼架构瓶颈，减少了数据移动并提升了能效。本综述论文系统分析了基于Transformer的模型，梳理了多种CIM架构，探讨其如何应对现代AI计算系统的紧迫挑战。我们详细讨论了Transformer相关算子及其硬件加速方案，并着重剖析了对应CIM设计中的核心挑战、发展趋势与技术洞见。
