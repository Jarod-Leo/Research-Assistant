# Memory Is All You Need: An Overview of Compute-in-Memory Architectures for Accelerating Large Language Model Inference

链接: http://arxiv.org/abs/2406.08413v1

原文摘要:
Large language models (LLMs) have recently transformed natural language
processing, enabling machines to generate human-like text and engage in
meaningful conversations. This development necessitates speed, efficiency, and
accessibility in LLM inference as the computational and memory requirements of
these systems grow exponentially. Meanwhile, advancements in computing and
memory capabilities are lagging behind, exacerbated by the discontinuation of
Moore's law. With LLMs exceeding the capacity of single GPUs, they require
complex, expert-level configurations for parallel processing. Memory accesses
become significantly more expensive than computation, posing a challenge for
efficient scaling, known as the memory wall. Here, compute-in-memory (CIM)
technologies offer a promising solution for accelerating AI inference by
directly performing analog computations in memory, potentially reducing latency
and power consumption. By closely integrating memory and compute elements, CIM
eliminates the von Neumann bottleneck, reducing data movement and improving
energy efficiency. This survey paper provides an overview and analysis of
transformer-based models, reviewing various CIM architectures and exploring how
they can address the imminent challenges of modern AI computing systems. We
discuss transformer-related operators and their hardware acceleration schemes
and highlight challenges, trends, and insights in corresponding CIM designs.

中文翻译:
近年来，大型语言模型（LLMs）彻底改变了自然语言处理领域，使机器能够生成类人文本并进行有意义的对话。随着这些系统的计算和内存需求呈指数级增长，LLM推理的速度、效率和易用性变得至关重要。然而当前计算与内存能力的进步却相对滞后——摩尔定律的失效进一步加剧了这一矛盾。当LLM规模超过单个GPU的承载极限时，就需要依赖复杂的专家级并行处理配置。此时内存访问的代价已远超计算本身，这种阻碍高效扩展的难题被称为"内存墙"。存内计算（CIM）技术通过直接在内存中执行模拟计算，为加速AI推理提供了颇具前景的解决方案，有望显著降低延迟与功耗。通过紧密集成存储与计算单元，CIM消除了冯·诺依曼架构瓶颈，减少数据搬运并提升能效。本综述论文系统梳理了基于Transformer的模型架构，分析了各类CIM技术方案，探讨其如何应对现代AI计算系统的紧迫挑战。我们详细讨论了Transformer相关算子及其硬件加速机制，并着重剖析了CIM设计中的核心挑战、发展趋势与技术洞见。

（翻译说明：采用技术文献的严谨表述风格，通过以下处理实现专业性与可读性的平衡：
1. 专业术语统一（如"memory wall"译为行业通用术语"内存墙"）
2. 长句拆分重组（如将原文第二句拆分为因果关系的两个分句）
3. 被动语态转化（如"are lagging behind"译为主动式"相对滞后"）
4. 概念显化处理（如"von Neumann bottleneck"补充说明为"冯·诺依曼架构瓶颈"）
5. 保持技术准确性同时增强中文韵律（如"promising solution"译为"颇具前景的解决方案"））
