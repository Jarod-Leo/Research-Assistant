# Assessing Large Language Models in Agentic Multilingual National Bias

链接: http://arxiv.org/abs/2502.17945v1

原文摘要:
Large Language Models have garnered significant attention for their
capabilities in multilingual natural language processing, while studies on
risks associated with cross biases are limited to immediate context
preferences. Cross-language disparities in reasoning-based recommendations
remain largely unexplored, with a lack of even descriptive analysis. This study
is the first to address this gap. We test LLM's applicability and capability in
providing personalized advice across three key scenarios: university
applications, travel, and relocation. We investigate multilingual bias in
state-of-the-art LLMs by analyzing their responses to decision-making tasks
across multiple languages. We quantify bias in model-generated scores and
assess the impact of demographic factors and reasoning strategies (e.g.,
Chain-of-Thought prompting) on bias patterns. Our findings reveal that local
language bias is prevalent across different tasks, with GPT-4 and Sonnet
reducing bias for English-speaking countries compared to GPT-3.5 but failing to
achieve robust multilingual alignment, highlighting broader implications for
multilingual AI agents and applications such as education.

中文翻译:
大型语言模型因其在多语言自然语言处理方面的能力而备受关注，然而关于跨语言偏见的风险研究目前仅局限于即时语境偏好。基于推理的跨语言推荐差异领域仍存在大量空白，甚至缺乏基础描述性分析。本研究首次针对这一空白展开探索。我们测试了LLM在三大关键场景（大学申请、旅行规划及移民定居）中提供个性化建议的适用性与能力，通过分析模型对多语言决策任务的响应来探究前沿LLM的多语言偏见问题。研究量化了模型生成评分中的偏见程度，并评估了人口统计因素与推理策略（如思维链提示）对偏见模式的影响。研究发现：本地语言偏见普遍存在于不同任务中，相比GPT-3.5，GPT-4和Sonnet虽能降低英语国家的偏见水平，却未能实现稳健的多语言对齐，这对多语言AI代理及教育等应用场景具有广泛启示意义。
