# RankMamba, Benchmarking Mamba's Document Ranking Performance in the Era of Transformers

链接: http://arxiv.org/abs/2403.18276v1

原文摘要:
Transformer structure has achieved great success in multiple applied machine
learning communities, such as natural language processing (NLP), computer
vision (CV) and information retrieval (IR). Transformer architecture's core
mechanism -- attention requires $O(n^2)$ time complexity in training and $O(n)$
time complexity in inference. Many works have been proposed to improve the
attention mechanism's scalability, such as Flash Attention and Multi-query
Attention. A different line of work aims to design new mechanisms to replace
attention. Recently, a notable model structure -- Mamba, which is based on
state space models, has achieved transformer-equivalent performance in multiple
sequence modeling tasks.
  In this work, we examine \mamba's efficacy through the lens of a classical IR
task -- document ranking. A reranker model takes a query and a document as
input, and predicts a scalar relevance score. This task demands the language
model's ability to comprehend lengthy contextual inputs and to capture the
interaction between query and document tokens. We find that (1) Mamba models
achieve competitive performance compared to transformer-based models with the
same training recipe; (2) but also have a lower training throughput in
comparison to efficient transformer implementations such as flash attention. We
hope this study can serve as a starting point to explore Mamba models in other
classical IR tasks. Our code implementation and trained checkpoints are made
public to facilitate reproducibility
(https://github.com/zhichaoxu-shufe/RankMamba).

中文翻译:
Transformer结构在多个应用机器学习领域取得了巨大成功，如自然语言处理（NLP）、计算机视觉（CV）和信息检索（IR）。Transformer架构的核心机制——注意力机制在训练时需要O(n²)的时间复杂度，推理时则需要O(n)的时间复杂度。已有许多工作致力于提升注意力机制的可扩展性，例如Flash Attention和多查询注意力机制。另一类研究则试图设计新机制来替代注意力。近期，基于状态空间模型的Mamba结构在多项序列建模任务中达到了与Transformer相当的性能。

本研究通过经典IR任务——文档排序来检验Mamba的有效性。排序模型以查询和文档作为输入，预测相关性分数。该任务要求语言模型具备理解长上下文输入并捕捉查询与文档词元间交互的能力。我们发现：（1）采用相同训练方案时，Mamba模型能达到与基于Transformer的模型相当的性能；（2）但与高效Transformer实现（如Flash Attention）相比，其训练吞吐量较低。希望这项工作能为探索Mamba模型在其他经典IR任务中的应用提供起点。我们公开了代码实现和训练好的检查点以促进可复现性研究（https://github.com/zhichaoxu-shufe/RankMamba）。
