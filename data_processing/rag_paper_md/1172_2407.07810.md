# Transformer Alignment in Large Language Models

链接: http://arxiv.org/abs/2407.07810v1

原文摘要:
Large Language Models (LLMs) have made significant strides in natural
language processing, and a precise understanding of the internal mechanisms
driving their success is essential. In this work, we analyze the trajectories
of token embeddings as they pass through transformer blocks, linearizing the
system along these trajectories through their Jacobian matrices. By examining
the relationships between these block Jacobians, we uncover the phenomenon of
\textbf{transformer block coupling} in a multitude of LLMs, characterized by
the coupling of their top singular vectors across tokens and depth. Our
findings reveal that coupling \textit{positively correlates} with model
performance, and that this relationship is stronger than with other
hyperparameters such as parameter count, model depth, and embedding dimension.
We further investigate how these properties emerge during training, observing a
progressive development of coupling, increased linearity, and layer-wise
exponential growth in token trajectories. Additionally, experiments with Vision
Transformers (ViTs) corroborate the emergence of coupling and its relationship
with generalization, reinforcing our findings in LLMs. Collectively, these
insights offer a novel perspective on token interactions in transformers,
opening new directions for studying their mechanisms as well as improving
training and generalization.

中文翻译:
大语言模型（LLMs）在自然语言处理领域取得了显著进展，而准确理解其成功背后的内部机制至关重要。本研究通过分析词元嵌入在Transformer块间传递的轨迹，利用雅可比矩阵沿这些轨迹对系统进行线性化处理。通过考察块间雅可比矩阵的关系，我们在多种LLMs中发现了**Transformer块耦合**现象，其特征表现为跨词元和深度的顶部奇异向量耦合。研究结果表明，耦合程度与模型性能呈正相关，且这种关联性相较于参数量、模型深度和嵌入维度等其他超参数更为显著。我们进一步探究了这些特性在训练过程中的形成规律，观察到耦合渐进发展、线性度提升以及词元轨迹呈现层间指数级增长的现象。此外，视觉Transformer（ViTs）的实验验证了耦合现象的出现及其与泛化能力的关系，强化了我们在LLMs中的发现。这些发现为理解Transformer中词元交互提供了新视角，为研究其工作机制及改进训练与泛化性能开辟了新方向。
