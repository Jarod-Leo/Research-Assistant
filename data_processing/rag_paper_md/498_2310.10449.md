# Text Summarization Using Large Language Models: A Comparative Study of MPT-7b-instruct, Falcon-7b-instruct, and OpenAI Chat-GPT Models

链接: http://arxiv.org/abs/2310.10449v1

原文摘要:
Text summarization is a critical Natural Language Processing (NLP) task with
applications ranging from information retrieval to content generation.
Leveraging Large Language Models (LLMs) has shown remarkable promise in
enhancing summarization techniques. This paper embarks on an exploration of
text summarization with a diverse set of LLMs, including MPT-7b-instruct,
falcon-7b-instruct, and OpenAI ChatGPT text-davinci-003 models. The experiment
was performed with different hyperparameters and evaluated the generated
summaries using widely accepted metrics such as the Bilingual Evaluation
Understudy (BLEU) Score, Recall-Oriented Understudy for Gisting Evaluation
(ROUGE) Score, and Bidirectional Encoder Representations from Transformers
(BERT) Score. According to the experiment, text-davinci-003 outperformed the
others. This investigation involved two distinct datasets: CNN Daily Mail and
XSum. Its primary objective was to provide a comprehensive understanding of the
performance of Large Language Models (LLMs) when applied to different datasets.
The assessment of these models' effectiveness contributes valuable insights to
researchers and practitioners within the NLP domain. This work serves as a
resource for those interested in harnessing the potential of LLMs for text
summarization and lays the foundation for the development of advanced
Generative AI applications aimed at addressing a wide spectrum of business
challenges.

中文翻译:
文本摘要是一项关键的自然语言处理（NLP）任务，其应用范围涵盖信息检索到内容生成。利用大型语言模型（LLMs）已展现出显著提升摘要技术的潜力。本文通过多种LLMs（包括MPT-7b-instruct、falcon-7b-instruct和OpenAI ChatGPT text-davinci-003模型）对文本摘要展开探索。实验采用不同超参数进行，并使用BLEU评分、ROUGE评分和BERT评分等广泛认可的指标对生成摘要进行评估。实验结果表明，text-davinci-003模型表现最优。研究涉及CNN Daily Mail和XSum两个不同数据集，主要目标是为理解LLMs在不同数据集上的性能提供全面视角。这些模型效能的评估为NLP领域的研究者和从业者提供了宝贵见解。本工作为有意挖掘LLMs文本摘要潜力者提供了参考资源，并为开发旨在解决广泛业务挑战的先进生成式AI应用奠定了基础。
