# P3: A Policy-Driven, Pace-Adaptive, and Diversity-Promoted Framework for Optimizing LLM Training

链接: http://arxiv.org/abs/2408.05541v1

原文摘要:
In the rapidly advancing field of Large Language Models (LLMs), effectively
leveraging existing datasets during fine-tuning to maximize the model's
potential is of paramount importance. This paper introduces P3, an adaptive
framework aimed at optimizing the task-specific fine-tuning process through
iterative data pruning. P3 consists of three key components: (1) Policy-driven
Difficulty Measurement, which dynamically assesses data difficulty based on the
model's real-time performance, replacing static metrics with adaptable
evaluations; (2) Pace-Adaptive Selection, leveraging self-paced learning to
progressively introduce more challenging data, thereby enhancing model
capability; (3) Diversity Promotion, incorporating Determinantal Point Process
(DPP) to ensure data diversity across epochs, enriching the learning process.
We validate P3 on the reasoning scenarios, APPS and MATH, demonstrating
significant improvements over traditional data pruning methods. By advancing
dynamic data selection and utilization strategies, P3 contributes both a
theoretical framework and concrete approach to fully exploit existing data for
LLMs' performance improvement, offering utility across diverse tasks.

中文翻译:
在大语言模型（LLMs）快速发展的领域中，如何通过微调阶段高效利用现有数据集以充分释放模型潜力至关重要。本文提出P3——一种基于迭代式数据剪枝的自适应框架，旨在优化任务导向的微调过程。该框架包含三大核心组件：（1）策略驱动的难度评估机制，通过模型实时表现动态衡量数据难度，替代传统静态评估指标；（2）节奏自适应选择模块，运用自步学习策略逐步引入高难度样本，持续提升模型能力；（3）多样性增强方案，采用行列式点过程（DPP）确保跨训练周期的数据多样性，丰富学习过程。我们在数学推理基准APPS和MATH上验证了P3框架，其性能显著超越传统数据剪枝方法。通过创新动态数据选择与利用策略，P3不仅为充分挖掘现有数据价值提供了理论框架，更贡献了可跨任务迁移的实用方法，推动LLMs性能边界持续拓展。
