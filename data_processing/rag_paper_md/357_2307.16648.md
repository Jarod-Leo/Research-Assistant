# LLMs4OL: Large Language Models for Ontology Learning

链接: http://arxiv.org/abs/2307.16648v2

原文摘要:
We propose the LLMs4OL approach, which utilizes Large Language Models (LLMs)
for Ontology Learning (OL). LLMs have shown significant advancements in natural
language processing, demonstrating their ability to capture complex language
patterns in different knowledge domains. Our LLMs4OL paradigm investigates the
following hypothesis: \textit{Can LLMs effectively apply their language pattern
capturing capability to OL, which involves automatically extracting and
structuring knowledge from natural language text?} To test this hypothesis, we
conduct a comprehensive evaluation using the zero-shot prompting method. We
evaluate nine different LLM model families for three main OL tasks: term
typing, taxonomy discovery, and extraction of non-taxonomic relations.
Additionally, the evaluations encompass diverse genres of ontological
knowledge, including lexicosemantic knowledge in WordNet, geographical
knowledge in GeoNames, and medical knowledge in UMLS.

中文翻译:
我们提出了LLMs4OL方法，该方法利用大语言模型（LLMs）进行本体学习（OL）。LLMs在自然语言处理领域展现出显著进步，其捕捉不同知识领域复杂语言模式的能力已得到验证。我们的LLMs4OL范式探究以下假设：\textit{LLMs能否将其语言模式捕捉能力有效应用于OL任务？该任务涉及从自然语言文本中自动提取并结构化知识。}为验证这一假设，我们采用零样本提示方法进行全面评估，针对术语分类、分类体系发现和非分类关系抽取三大OL核心任务，测试了九个不同的LLM模型家族。评估范围涵盖多种本体知识类型，包括WordNet中的词汇语义知识、GeoNames中的地理知识以及UMLS中的医学知识。
