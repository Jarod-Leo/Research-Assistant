# Training language models for deeper understanding improves brain alignment

链接: http://arxiv.org/abs/2212.10898v1

原文摘要:
Building systems that achieve a deeper understanding of language is one of
the central goals of natural language processing (NLP). Towards this goal,
recent works have begun to train language models on narrative datasets which
require extracting the most critical information by integrating across long
contexts. However, it is still an open question whether these models are
learning a deeper understanding of the text, or if the models are simply
learning a heuristic to complete the task. This work investigates this further
by turning to the one language processing system that truly understands complex
language: the human brain. We show that training language models for deeper
narrative understanding results in richer representations that have improved
alignment to human brain activity. We further find that the improvements in
brain alignment are larger for character names than for other discourse
features, which indicates that these models are learning important narrative
elements. Taken together, these results suggest that this type of training can
indeed lead to deeper language understanding. These findings have consequences
both for cognitive neuroscience by revealing some of the significant factors
behind brain-NLP alignment, and for NLP by highlighting that understanding of
long-range context can be improved beyond language modeling.

中文翻译:
构建能够深入理解语言的系统是自然语言处理（NLP）的核心目标之一。为实现这一目标，近期研究开始尝试在叙事数据集上训练语言模型，这类任务要求通过整合长上下文来提取最关键的信息。然而，这些模型究竟是在学习对文本的深层理解，还是仅掌握了完成任务的经验法则，目前仍无定论。本研究通过转向真正理解复杂语言的语言处理系统——人类大脑——对此问题展开深入探讨。实验表明，经过深度叙事理解训练的语言模型能产生更丰富的表征，这些表征与人类大脑活动的对齐程度显著提升。进一步研究发现，角色名称特征的大脑对齐改善幅度明显大于其他话语特征，这表明模型确实掌握了重要的叙事元素。综合来看，这些结果证实此类训练确实能促进更深层的语言理解。研究发现对认知神经科学和NLP领域均具有启示意义：既揭示了影响大脑-NLP对齐的关键因素，也证明超越语言建模的远距离上下文理解能力可以得到提升。
