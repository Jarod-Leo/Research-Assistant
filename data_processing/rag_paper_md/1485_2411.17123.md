# Advancing Content Moderation: Evaluating Large Language Models for Detecting Sensitive Content Across Text, Images, and Videos

链接: http://arxiv.org/abs/2411.17123v1

原文摘要:
The widespread dissemination of hate speech, harassment, harmful and sexual
content, and violence across websites and media platforms presents substantial
challenges and provokes widespread concern among different sectors of society.
Governments, educators, and parents are often at odds with media platforms
about how to regulate, control, and limit the spread of such content.
Technologies for detecting and censoring the media contents are a key solution
to addressing these challenges. Techniques from natural language processing and
computer vision have been used widely to automatically identify and filter out
sensitive content such as offensive languages, violence, nudity, and addiction
in both text, images, and videos, enabling platforms to enforce content
policies at scale. However, existing methods still have limitations in
achieving high detection accuracy with fewer false positives and false
negatives. Therefore, more sophisticated algorithms for understanding the
context of both text and image may open rooms for improvement in content
censorship to build a more efficient censorship system. In this paper, we
evaluate existing LLM-based content moderation solutions such as OpenAI
moderation model and Llama-Guard3 and study their capabilities to detect
sensitive contents. Additionally, we explore recent LLMs such as GPT, Gemini,
and Llama in identifying inappropriate contents across media outlets. Various
textual and visual datasets like X tweets, Amazon reviews, news articles, human
photos, cartoons, sketches, and violence videos have been utilized for
evaluation and comparison. The results demonstrate that LLMs outperform
traditional techniques by achieving higher accuracy and lower false positive
and false negative rates. This highlights the potential to integrate LLMs into
websites, social media platforms, and video-sharing services for regulatory and
content moderation purposes.

中文翻译:
仇恨言论、骚扰、有害与色情内容及暴力行为在各类网站和媒体平台上的广泛传播，带来了严峻挑战并引发社会各界普遍担忧。政府、教育工作者和家长群体常与媒体平台就如何监管、控制和限制此类内容传播存在分歧。媒体内容检测与审查技术是应对这些挑战的核心解决方案。自然语言处理和计算机视觉技术已被广泛应用于文本、图像及视频中攻击性语言、暴力、裸露和成瘾等敏感内容的自动识别与过滤，使平台能够大规模执行内容审核政策。然而现有方法在实现高检测准确率同时降低误判率方面仍存在局限。因此，更先进的文本与图像上下文理解算法可能为内容审查系统效能提升创造改进空间，以构建更高效的审查体系。本文评估了基于大语言模型（LLM）的现有内容审核方案（如OpenAI审核模型和Llama-Guard3），并研究其检测敏感内容的能力。同时探究了GPT、Gemini和Llama等前沿大语言模型在跨媒体不适宜内容识别中的应用。通过X平台推文、亚马逊评论、新闻文章、人物照片、卡通图像、素描及暴力视频等多模态数据集进行评估比较，结果表明大语言模型以更高准确率和更低误判率超越传统技术。这凸显了将大语言模型整合至网站、社交媒体平台及视频分享服务中以实现监管与内容审核目标的巨大潜力。
