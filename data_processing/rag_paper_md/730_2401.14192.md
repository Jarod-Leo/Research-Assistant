# How Can Large Language Models Understand Spatial-Temporal Data?

链接: http://arxiv.org/abs/2401.14192v1

原文摘要:
While Large Language Models (LLMs) dominate tasks like natural language
processing and computer vision, harnessing their power for spatial-temporal
forecasting remains challenging. The disparity between sequential text and
complex spatial-temporal data hinders this application. To address this issue,
this paper introduces STG-LLM, an innovative approach empowering LLMs for
spatial-temporal forecasting. We tackle the data mismatch by proposing: 1)
STG-Tokenizer: This spatial-temporal graph tokenizer transforms intricate graph
data into concise tokens capturing both spatial and temporal relationships; 2)
STG-Adapter: This minimalistic adapter, consisting of linear encoding and
decoding layers, bridges the gap between tokenized data and LLM comprehension.
By fine-tuning only a small set of parameters, it can effectively grasp the
semantics of tokens generated by STG-Tokenizer, while preserving the original
natural language understanding capabilities of LLMs. Extensive experiments on
diverse spatial-temporal benchmark datasets show that STG-LLM successfully
unlocks LLM potential for spatial-temporal forecasting. Remarkably, our
approach achieves competitive performance on par with dedicated SOTA methods.

中文翻译:
尽管大语言模型（LLMs）在自然语言处理和计算机视觉任务中占据主导地位，但将其能力应用于时空预测领域仍面临挑战。序列文本与复杂时空数据之间的差异阻碍了这一应用。为解决该问题，本文提出创新方法STG-LLM，通过以下设计实现LLMs的时空预测能力：1）STG-Tokenizer：这种时空图标记器将复杂的图数据转化为能同时捕捉时空关系的紧凑标记；2）STG-Adapter：该极简适配器由线性编码层和解码层构成，在标记化数据与LLM理解之间建立桥梁。仅需微调少量参数，即可有效掌握STG-Tokenizer生成标记的语义，同时保留LLMs原有的自然语言理解能力。在多个时空基准数据集上的大量实验表明，STG-LLM成功释放了LLMs在时空预测中的潜力。值得注意的是，我们的方法取得了与专用SOTA方法相媲美的竞争优势。
