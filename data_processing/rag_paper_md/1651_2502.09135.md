# Interpreting and Steering Protein Language Models through Sparse Autoencoders

链接: http://arxiv.org/abs/2502.09135v1

原文摘要:
The rapid advancements in transformer-based language models have
revolutionized natural language processing, yet understanding the internal
mechanisms of these models remains a significant challenge. This paper explores
the application of sparse autoencoders (SAE) to interpret the internal
representations of protein language models, specifically focusing on the ESM-2
8M parameter model. By performing a statistical analysis on each latent
component's relevance to distinct protein annotations, we identify potential
interpretations linked to various protein characteristics, including
transmembrane regions, binding sites, and specialized motifs.
  We then leverage these insights to guide sequence generation, shortlisting
the relevant latent components that can steer the model towards desired targets
such as zinc finger domains. This work contributes to the emerging field of
mechanistic interpretability in biological sequence models, offering new
perspectives on model steering for sequence design.

中文翻译:
基于Transformer的语言模型快速发展，已彻底改变自然语言处理领域，但理解这些模型的内部机制仍面临重大挑战。本文探索稀疏自编码器（SAE）在蛋白质语言模型内部表征解析中的应用，重点研究ESM-2 800万参数模型。通过对各潜在组分与不同蛋白质注释相关性的统计分析，我们识别出与跨膜区域、结合位点及特殊基序等多种蛋白质特征相关的潜在解释。

进一步利用这些发现指导序列生成，筛选出可引导模型朝向特定目标（如锌指结构域）的相关潜在组分。本研究为生物序列模型的可解释性机制这一新兴领域作出贡献，为序列设计的模型调控提供了新视角。
