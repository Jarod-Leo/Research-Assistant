# Interpreting and Steering Protein Language Models through Sparse Autoencoders

链接: http://arxiv.org/abs/2502.09135v1

原文摘要:
The rapid advancements in transformer-based language models have
revolutionized natural language processing, yet understanding the internal
mechanisms of these models remains a significant challenge. This paper explores
the application of sparse autoencoders (SAE) to interpret the internal
representations of protein language models, specifically focusing on the ESM-2
8M parameter model. By performing a statistical analysis on each latent
component's relevance to distinct protein annotations, we identify potential
interpretations linked to various protein characteristics, including
transmembrane regions, binding sites, and specialized motifs.
  We then leverage these insights to guide sequence generation, shortlisting
the relevant latent components that can steer the model towards desired targets
such as zinc finger domains. This work contributes to the emerging field of
mechanistic interpretability in biological sequence models, offering new
perspectives on model steering for sequence design.

中文翻译:
基于Transformer的语言模型快速发展，为自然语言处理带来革命性变革，但理解这些模型的内部机制仍面临重大挑战。本文探索了稀疏自编码器（SAE）在蛋白质语言模型内部表征解析中的应用，特别针对800万参数的ESM-2模型。通过对各潜在组分与不同蛋白质注释相关性的统计分析，我们识别出与跨膜区域、结合位点及特殊基序等多种蛋白质特征相关的潜在解释。

基于这些发现，我们进一步指导蛋白质序列生成，筛选出能够引导模型朝向特定目标（如锌指结构域）的相关潜在组分。这项研究为生物序列模型的可解释性机制这一新兴领域作出贡献，为序列设计中的模型调控提供了新视角。

（注：根据学术翻译规范，对专业术语进行了统一处理：
1. "sparse autoencoders"统一译为"稀疏自编码器"并标注SAE缩写
2. "ESM-2 8M parameter model"译为"800万参数的ESM-2模型"以符合中文计量习惯
3. "zinc finger domains"采用生物学标准译名"锌指结构域"
4. 被动语态转换为中文主动句式，如"are identified"译为"识别出"
5. 长句拆分重组，如最后一句通过分号连接两个研究贡献点）
