# Can Large Language Models Be an Alternative to Human Evaluations?

链接: http://arxiv.org/abs/2305.01937v1

原文摘要:
Human evaluation is indispensable and inevitable for assessing the quality of
texts generated by machine learning models or written by humans. However, human
evaluation is very difficult to reproduce and its quality is notoriously
unstable, hindering fair comparisons among different natural language
processing (NLP) models and algorithms. Recently, large language models (LLMs)
have demonstrated exceptional performance on unseen tasks when only the task
instructions are provided. In this paper, we explore if such an ability of the
LLMs can be used as an alternative to human evaluation. We present the LLMs
with the exact same instructions, samples to be evaluated, and questions used
to conduct human evaluation, and then ask the LLMs to generate responses to
those questions; we dub this LLM evaluation. We use human evaluation and LLM
evaluation to evaluate the texts in two NLP tasks: open-ended story generation
and adversarial attacks. We show that the result of LLM evaluation is
consistent with the results obtained by expert human evaluation: the texts
rated higher by human experts are also rated higher by the LLMs. We also find
that the results of LLM evaluation are stable over different formatting of the
task instructions and the sampling algorithm used to generate the answer. We
are the first to show the potential of using LLMs to assess the quality of
texts and discuss the limitations and ethical considerations of LLM evaluation.

中文翻译:
人工评估对于衡量机器学习模型生成或人类撰写的文本质量而言，既是不可或缺的环节，也是无法回避的步骤。然而，人工评估难以复现且质量 notoriously 不稳定，这阻碍了不同自然语言处理（NLP）模型与算法之间的公平比较。近期研究表明，大型语言模型（LLMs）仅凭任务指令就能在未见任务上展现出卓越性能。本文探究是否可将LLMs的这种能力作为人工评估的替代方案——我们向LLMs提供与人工评估完全相同的任务指令、待评估样本及问题，要求其生成对应回答，并将此过程称为"LLM评估"。我们在开放式故事生成和对抗攻击两个NLP任务中，同步采用人工评估与LLM评估对文本进行评价。实验表明：LLM评估结果与专家人工评估具有一致性，即人类专家评分较高的文本同样获得LLMs更高评价；同时发现LLM评估结果不受任务指令格式差异及答案生成采样算法的影响，表现出稳定性。本研究首次揭示了LLMs在文本质量评估方面的潜力，并探讨了LLM评估的局限性及伦理考量。
