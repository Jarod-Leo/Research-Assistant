# Natural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey

链接: http://arxiv.org/abs/2411.17558v1

原文摘要:
Visual Question Answering (VQA) is a challenge task that combines natural
language processing and computer vision techniques and gradually becomes a
benchmark test task in multimodal large language models (MLLMs). The goal of
our survey is to provide an overview of the development of VQA and a detailed
description of the latest models with high timeliness. This survey gives an
up-to-date synthesis of natural language understanding of images and text, as
well as the knowledge reasoning module based on image-question information on
the core VQA tasks. In addition, we elaborate on recent advances in extracting
and fusing modal information with vision-language pretraining models and
multimodal large language models in VQA. We also exhaustively review the
progress of knowledge reasoning in VQA by detailing the extraction of internal
knowledge and the introduction of external knowledge. Finally, we present the
datasets of VQA and different evaluation metrics and discuss possible
directions for future work.

中文翻译:
视觉问答（Visual Question Answering, VQA）作为融合自然语言处理与计算机视觉技术的挑战性任务，正逐渐成为多模态大语言模型（MLLMs）的基准测试任务。本综述旨在系统梳理VQA领域的发展脉络，并针对具有高时效性的最新模型进行细致解析。文章首先对图像与文本的自然语言理解机制以及基于图像-问题信息的核心VQA任务知识推理模块作出最新研究整合；进而详细阐述视觉语言预训练模型与多模态大语言模型在VQA中模态信息提取与融合的最新进展；通过深度剖析内部知识提取与外部知识引入两大维度，全面评述VQA知识推理的研究进程；最后系统呈现VQA任务数据集与多样化评估指标体系，并探讨未来可能的研究方向。
