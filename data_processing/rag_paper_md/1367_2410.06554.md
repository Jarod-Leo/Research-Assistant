# The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models

链接: http://arxiv.org/abs/2410.06554v1

原文摘要:
Reinforcement Learning from Human Feedback significantly enhances Natural
Language Processing by aligning language models with human expectations. A
critical factor in this alignment is the strength of reward models used during
training. This study explores whether stronger reward models invariably lead to
better language models. In this paper, through experiments on relevance,
factuality, and completeness tasks using the QA-FEEDBACK dataset and reward
models based on Longformer, we uncover a surprising paradox: language models
trained with moderately accurate reward models outperform those guided by
highly accurate ones. This challenges the widely held belief that stronger
reward models always lead to better language models, and opens up new avenues
for future research into the key factors driving model performance and how to
choose the most suitable reward models. Code and additional details are
available at https://github.com/EIT-NLP/AccuracyParadox-RLHF.

中文翻译:
基于人类反馈的强化学习通过使语言模型与人类期望对齐，显著提升了自然语言处理能力。这一对齐过程中的关键因素在于训练时所采用奖励模型的强度。本研究探讨了更强的奖励模型是否必然会产生更优的语言模型。本文通过在QA-FEEDBACK数据集上开展相关性、事实性和完整性任务的实验，并采用基于Longformer架构的奖励模型，揭示了一个令人惊异的悖论：由中等准确度奖励模型训练的语言模型，其表现竟优于由高准确度奖励模型指导的模型。这一发现挑战了"奖励模型越强则语言模型越优"的普遍认知，为未来研究模型性能的核心驱动因素及如何选择最适配的奖励模型开辟了新路径。相关代码及补充材料详见https://github.com/EIT-NLP/AccuracyParadox-RLHF。
