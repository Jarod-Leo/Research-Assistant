# Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization

链接: http://arxiv.org/abs/2311.05161v1

原文摘要:
Large Language Models (LLMs) are proficient in natural language processing
tasks, but their deployment is often restricted by extensive parameter sizes
and computational demands. This paper focuses on post-training quantization
(PTQ) in LLMs, specifically 4-bit weight and 8-bit activation (W4A8)
quantization, to enhance computational efficiency -- a topic less explored
compared to weight-only quantization. We present two innovative techniques:
activation-quantization-aware scaling (AQAS) and sequence-length-aware
calibration (SLAC) to enhance PTQ by considering the combined effects on
weights and activations and aligning calibration sequence lengths to target
tasks. Moreover, we introduce dINT, a hybrid data format combining integer and
denormal representations, to address the underflow issue in W4A8 quantization,
where small values are rounded to zero. Through rigorous evaluations of LLMs,
including OPT and LLaMA, we demonstrate that our techniques significantly boost
task accuracies to levels comparable with full-precision models. By developing
arithmetic units compatible with dINT, we further confirm that our methods
yield a 2$\times$ hardware efficiency improvement compared to 8-bit integer MAC
unit.

中文翻译:
大型语言模型（LLMs）在自然语言处理任务中表现出色，但其部署常受限于庞大的参数量与计算需求。本文聚焦于LLMs的训练后量化（PTQ），特别是4比特权重与8比特激活（W4A8）量化方案，以提升计算效率——该方向相较于仅权重量化的研究尚属空白。我们提出两项创新技术：**激活量化感知缩放（AQAS）**通过协同考虑权重与激活的量化影响优化PTQ过程，**序列长度感知校准（SLAC）**则通过使校准序列长度与目标任务对齐提升效果。此外，针对W4A8量化中小数值被舍入为零的数值下溢问题，我们设计了混合整数与次正规数表示的**dINT数据格式**。通过对OPT、LLaMA等LLMs的严格评估，实验表明这些技术将任务准确率显著提升至与全精度模型相当的水平。通过开发兼容dINT的算术单元，我们进一步验证该方法相比8比特整数乘法累加单元可实现2倍的硬件能效提升。
