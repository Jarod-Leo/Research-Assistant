# Iterative Mask Filling: An Effective Text Augmentation Method Using Masked Language Modeling

链接: http://arxiv.org/abs/2401.01830v1

原文摘要:
Data augmentation is an effective technique for improving the performance of
machine learning models. However, it has not been explored as extensively in
natural language processing (NLP) as it has in computer vision. In this paper,
we propose a novel text augmentation method that leverages the Fill-Mask
feature of the transformer-based BERT model. Our method involves iteratively
masking words in a sentence and replacing them with language model predictions.
We have tested our proposed method on various NLP tasks and found it to be
effective in many cases. Our results are presented along with a comparison to
existing augmentation methods. Experimental results show that our proposed
method significantly improves performance, especially on topic classification
datasets.

中文翻译:
数据增强是一种提升机器学习模型性能的有效技术，然而其在自然语言处理（NLP）领域的探索远不及计算机视觉领域深入。本文提出了一种创新的文本增强方法，该方法基于Transformer架构的BERT模型所具备的填充掩码（Fill-Mask）功能。具体实现过程为：对句子中的词汇进行迭代式掩码处理，并通过语言模型的预测结果进行替换填充。我们在多项NLP任务上测试了该方法的有效性，实验表明其在多数场景下均能取得显著效果。研究结果不仅展示了本方法的优势，还与现有增强技术进行了对比分析。实验数据证实，所提出的方法能显著提升模型性能，尤其在主题分类数据集上表现尤为突出。
