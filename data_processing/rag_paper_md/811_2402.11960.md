# DB-LLM: Accurate Dual-Binarization for Efficient LLMs

链接: http://arxiv.org/abs/2402.11960v1

原文摘要:
Large language models (LLMs) have significantly advanced the field of natural
language processing, while the expensive memory and computation consumption
impede their practical deployment. Quantization emerges as one of the most
effective methods for improving the computational efficiency of LLMs. However,
existing ultra-low-bit quantization always causes severe accuracy drops. In
this paper, we empirically relieve the micro and macro characteristics of
ultra-low bit quantization and present a novel Dual-Binarization method for
LLMs, namely DB-LLM. For the micro-level, we take both the accuracy advantage
of 2-bit-width and the efficiency advantage of binarization into account,
introducing Flexible Dual Binarization (FDB). By splitting 2-bit quantized
weights into two independent sets of binaries, FDB ensures the accuracy of
representations and introduces flexibility, utilizing the efficient bitwise
operations of binarization while retaining the inherent high sparsity of
ultra-low bit quantization. For the macro-level, we find the distortion that
exists in the prediction of LLM after quantization, which is specified as the
deviations related to the ambiguity of samples. We propose the Deviation-Aware
Distillation (DAD) method, enabling the model to focus differently on various
samples. Comprehensive experiments show that our DB-LLM not only significantly
surpasses the current State-of-The-Art (SoTA) in ultra-low bit quantization
(eg, perplexity decreased from 9.64 to 7.23), but also achieves an additional
20\% reduction in computational consumption compared to the SOTA method under
the same bit-width. Our code will be released soon.

中文翻译:
大语言模型（LLM）显著推动了自然语言处理领域的发展，但其高昂的内存与计算开销阻碍了实际部署。量化技术成为提升LLM计算效率最有效的方法之一，然而现有超低位宽量化往往导致严重的精度损失。本文通过实证研究揭示了超低位宽量化的微观与宏观特性，提出了一种创新的双二值化方法DB-LLM。

在微观层面，我们兼顾2比特位宽的精度优势与二值化的效率优势，提出柔性双二值化（FDB）。通过将2比特量化权重拆分为两组独立二进制数，FDB既保证了表征精度又引入灵活性，在利用二值化高效位运算的同时保留了超低位宽量化的固有高稀疏性。在宏观层面，我们发现量化后LLM预测中存在失真现象，具体表现为与样本模糊性相关的偏差。为此提出偏差感知蒸馏（DAD）方法，使模型能够差异化关注不同样本。

综合实验表明，DB-LLM不仅在超低位宽量化指标上大幅超越当前最优水平（如困惑度从9.64降至7.23），在相同位宽下较SOTA方法还额外减少20%计算消耗。代码即将开源。
