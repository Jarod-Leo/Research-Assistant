# Large Language Models Are Not Abstract Reasoners

链接: http://arxiv.org/abs/2305.19555v1

原文摘要:
Large Language Models have shown tremendous performance on a large variety of
natural language processing tasks, ranging from text comprehension to common
sense reasoning. However, the mechanisms responsible for this success remain
opaque, and it is unclear whether LLMs can achieve human-like cognitive
capabilities or whether these models are still fundamentally circumscribed.
Abstract reasoning is a fundamental task for cognition, consisting of finding
and applying a general pattern from few data. Evaluating deep neural
architectures on this task could give insight into their potential limitations
regarding reasoning and their broad generalisation abilities, yet this is
currently an under-explored area. In this paper, we introduce a new benchmark
for evaluating language models beyond memorization on abstract reasoning tasks.
We perform extensive evaluations of state-of-the-art LLMs, showing that they
currently achieve very limited performance in contrast with other natural
language tasks, even when applying techniques that have been shown to improve
performance on other NLP tasks. We argue that guiding LLM generation to follow
causal paths could help improve the generalisation and reasoning abilities of
LLMs.

中文翻译:
大型语言模型在从文本理解到常识推理等各类自然语言处理任务中展现出卓越性能，但其成功的内在机制仍不透明。目前尚不清楚这些模型是否能实现类人认知能力，抑或其能力仍存在根本性局限。抽象推理作为认知的基础任务，要求从少量数据中发现并应用通用模式。通过该任务评估深度神经架构，有助于揭示其在推理能力和泛化性方面的潜在局限，然而当前相关研究仍属空白领域。本文提出一个超越记忆能力的新型基准测试，用于评估语言模型在抽象推理任务中的表现。我们对前沿大型语言模型开展广泛评估，结果表明：即便采用经其他自然语言处理任务验证有效的优化技术，这些模型当前表现仍远逊于其在其他语言任务中的水平。研究指出，引导语言模型遵循因果路径进行生成，或将有效提升其泛化与推理能力。
