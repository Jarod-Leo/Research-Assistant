# Unsupervised Text Style Transfer via LLMs and Attention Masking with Multi-way Interactions

链接: http://arxiv.org/abs/2402.13647v1

原文摘要:
Unsupervised Text Style Transfer (UTST) has emerged as a critical task within
the domain of Natural Language Processing (NLP), aiming to transfer one
stylistic aspect of a sentence into another style without changing its
semantics, syntax, or other attributes. This task is especially challenging
given the intrinsic lack of parallel text pairings. Among existing methods for
UTST tasks, attention masking approach and Large Language Models (LLMs) are
deemed as two pioneering methods. However, they have shortcomings in generating
unsmooth sentences and changing the original contents, respectively. In this
paper, we investigate if we can combine these two methods effectively. We
propose four ways of interactions, that are pipeline framework with tuned
orders; knowledge distillation from LLMs to attention masking model; in-context
learning with constructed parallel examples. We empirically show these
multi-way interactions can improve the baselines in certain perspective of
style strength, content preservation and text fluency. Experiments also
demonstrate that simply conducting prompting followed by attention
masking-based revision can consistently surpass the other systems, including
supervised text style transfer systems. On Yelp-clean and Amazon-clean
datasets, it improves the previously best mean metric by 0.5 and 3.0 absolute
percentages respectively, and achieves new SOTA results.

中文翻译:
无监督文本风格迁移（UTST）已成为自然语言处理（NLP）领域的关键任务，其目标是在保持句子语义、句法及其他属性不变的前提下，将文本的某一风格特征转换为另一种风格。由于缺乏平行文本对，该任务具有显著挑战性。现有UTST方法中，注意力掩码技术和大型语言模型（LLMs）被视为两大开创性方案，但前者存在生成语句不流畅的问题，后者则易改变原文内容。本文探究了这两种方法的有效结合路径，提出四种交互方式：带顺序调优的流水线框架、LLM到注意力掩码模型的知识蒸馏、基于构建平行样本的上下文学习。实验证明，这种多维度交互能在风格强度、内容保留和文本流畅性等层面提升基线效果。研究还发现，仅采用"提示生成+基于注意力掩码的修正"这一简单流程，其表现即可持续超越包括有监督文本风格迁移系统在内的其他方案。在Yelp-clean和Amazon-clean数据集上，该方法将原最优平均指标分别绝对提升0.5%和3.0%，创造了新的SOTA记录。

（翻译说明：
1. 专业术语处理：UTST、NLP、LLMs等首现时保留英文缩写并添加中文全称，符合学术规范
2. 长句拆分："aiming to..."等英文长句按中文表达习惯拆分为多个短句
3. 被动语态转换："are deemed as"等被动结构转换为"被视为"的主动表述
4. 概念显化："parallel text pairings"译为"平行文本对"而非字面直译，确保专业准确性
5. 数据呈现：百分比数据保留原始精确度，0.5/3.0等数字格式与原文严格对应
6. 术语统一：全篇保持"注意力掩码"（attention masking）等核心概念译名一致性
7. 文化适配："SOTA"采用学界通用缩略语形式而非全称翻译）
