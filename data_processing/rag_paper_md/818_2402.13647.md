# Unsupervised Text Style Transfer via LLMs and Attention Masking with Multi-way Interactions

链接: http://arxiv.org/abs/2402.13647v1

原文摘要:
Unsupervised Text Style Transfer (UTST) has emerged as a critical task within
the domain of Natural Language Processing (NLP), aiming to transfer one
stylistic aspect of a sentence into another style without changing its
semantics, syntax, or other attributes. This task is especially challenging
given the intrinsic lack of parallel text pairings. Among existing methods for
UTST tasks, attention masking approach and Large Language Models (LLMs) are
deemed as two pioneering methods. However, they have shortcomings in generating
unsmooth sentences and changing the original contents, respectively. In this
paper, we investigate if we can combine these two methods effectively. We
propose four ways of interactions, that are pipeline framework with tuned
orders; knowledge distillation from LLMs to attention masking model; in-context
learning with constructed parallel examples. We empirically show these
multi-way interactions can improve the baselines in certain perspective of
style strength, content preservation and text fluency. Experiments also
demonstrate that simply conducting prompting followed by attention
masking-based revision can consistently surpass the other systems, including
supervised text style transfer systems. On Yelp-clean and Amazon-clean
datasets, it improves the previously best mean metric by 0.5 and 3.0 absolute
percentages respectively, and achieves new SOTA results.

中文翻译:
无监督文本风格转换（UTST）已成为自然语言处理（NLP）领域的关键任务，其目标是在保持句子语义、句法及其他属性不变的前提下，将文本的某一风格特征转换为另一种风格。由于缺乏平行文本对，该任务尤为具有挑战性。现有UTST方法中，注意力掩码技术和大型语言模型（LLMs）被视为两大开创性方法，但前者存在生成语句不流畅的问题，后者则易改变原文内容。本文探究了这两种方法的有效结合路径，提出了四种交互方式：带调序机制的流水线框架、从LLMs到注意力掩码模型的知识蒸馏、基于构建平行样本的上下文学习。实验证明，这些多维交互能在风格强度、内容保留和文本流畅性等维度提升基线效果。研究还发现，仅通过提示生成后接注意力掩码修正的简单流程，即可持续超越包括有监督文本风格转换系统在内的其他方案。在Yelp-clean和Amazon-clean数据集上，该方法将原最优平均指标分别提升0.5和3.0个绝对百分点，创造了新的SOTA记录。
