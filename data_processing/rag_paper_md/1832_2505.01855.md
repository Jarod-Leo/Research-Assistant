# Intra-Layer Recurrence in Transformers for Language Modeling

链接: http://arxiv.org/abs/2505.01855v1

原文摘要:
Transformer models have established new benchmarks in natural language
processing; however, their increasing depth results in substantial growth in
parameter counts. While existing recurrent transformer methods address this
issue by reprocessing layers multiple times, they often apply recurrence
indiscriminately across entire blocks of layers. In this work, we investigate
Intra-Layer Recurrence (ILR), a more targeted approach that applies recurrence
selectively to individual layers within a single forward pass. Our experiments
show that allocating more iterations to earlier layers yields optimal results.
These findings suggest that ILR offers a promising direction for optimizing
recurrent structures in transformer architectures.

中文翻译:
Transformer模型已在自然语言处理领域树立了新的性能标杆，但其不断增加的深度导致参数量急剧膨胀。现有循环Transformer方法虽通过多次重处理层来缓解这一问题，却往往不加区分地对整个层块实施循环机制。本研究提出层内循环（ILR）这一更具针对性的方法，在单次前向传播中有选择地对特定层级实施循环处理。实验表明，将更多迭代次数分配给网络早期层级能获得最优效果。这些发现证明ILR为优化Transformer架构中的循环结构提供了新的研究方向。
