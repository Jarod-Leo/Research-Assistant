# Time-LLM: Time Series Forecasting by Reprogramming Large Language Models

链接: http://arxiv.org/abs/2310.01728v1

原文摘要:
Time series forecasting holds significant importance in many real-world
dynamic systems and has been extensively studied. Unlike natural language
process (NLP) and computer vision (CV), where a single large model can tackle
multiple tasks, models for time series forecasting are often specialized,
necessitating distinct designs for different tasks and applications. While
pre-trained foundation models have made impressive strides in NLP and CV, their
development in time series domains has been constrained by data sparsity.
Recent studies have revealed that large language models (LLMs) possess robust
pattern recognition and reasoning abilities over complex sequences of tokens.
However, the challenge remains in effectively aligning the modalities of time
series data and natural language to leverage these capabilities. In this work,
we present Time-LLM, a reprogramming framework to repurpose LLMs for general
time series forecasting with the backbone language models kept intact. We begin
by reprogramming the input time series with text prototypes before feeding it
into the frozen LLM to align the two modalities. To augment the LLM's ability
to reason with time series data, we propose Prompt-as-Prefix (PaP), which
enriches the input context and directs the transformation of reprogrammed input
patches. The transformed time series patches from the LLM are finally projected
to obtain the forecasts. Our comprehensive evaluations demonstrate that
Time-LLM is a powerful time series learner that outperforms state-of-the-art,
specialized forecasting models. Moreover, Time-LLM excels in both few-shot and
zero-shot learning scenarios.

中文翻译:
时间序列预测在众多现实动态系统中具有重要地位，并已得到广泛研究。与自然语言处理（NLP）和计算机视觉（CV）领域不同——单个大型模型可应对多任务需求，时间序列预测模型通常具有专一性，需针对不同任务和应用进行专门设计。尽管预训练基础模型在NLP和CV领域取得显著进展，但其在时间序列领域的发展受限于数据稀疏性。最新研究表明，大型语言模型（LLM）对复杂标记序列具有强大的模式识别与推理能力。然而，如何有效对齐时间序列数据与自然语言的模态以利用这些能力仍存在挑战。本文提出Time-LLM框架，通过保持主干语言模型结构不变，将其重编程为通用时间序列预测工具。我们首先用文本原型对输入时间序列进行重编程，再将其输入冻结的LLM以实现模态对齐。为增强LLM处理时序数据的推理能力，我们提出"提示即前缀"（PaP）方法，通过丰富输入上下文来引导重编程输入片段的转换。最终将LLM输出的转换后时间序列片段投影获得预测结果。综合评估表明，Time-LLM作为强大的时序学习器，其性能超越当前最先进的专用预测模型，并在小样本和零样本学习场景中表现卓越。
