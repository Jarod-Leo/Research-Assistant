# Language Model Adaptation to Specialized Domains through Selective Masking based on Genre and Topical Characteristics

链接: http://arxiv.org/abs/2402.12036v1

原文摘要:
Recent advances in pre-trained language modeling have facilitated significant
progress across various natural language processing (NLP) tasks. Word masking
during model training constitutes a pivotal component of language modeling in
architectures like BERT. However, the prevalent method of word masking relies
on random selection, potentially disregarding domain-specific linguistic
attributes. In this article, we introduce an innovative masking approach
leveraging genre and topicality information to tailor language models to
specialized domains. Our method incorporates a ranking process that prioritizes
words based on their significance, subsequently guiding the masking procedure.
Experiments conducted using continual pre-training within the legal domain have
underscored the efficacy of our approach on the LegalGLUE benchmark in the
English language. Pre-trained language models and code are freely available for
use.

中文翻译:
预训练语言建模技术的最新进展显著推动了各类自然语言处理任务的性能提升。在BERT等架构中，词语掩码作为语言建模的核心组件，其传统随机掩码策略可能忽略了特定领域的语言学特征。本文提出了一种创新性掩码方法，通过融合文本体裁与主题信息来实现领域自适应建模。我们设计了一套基于词汇重要性的优先级排序机制来指导掩码过程，在法律领域的持续预训练实验中，该方法在英文LegalGLUE基准测试中展现出卓越效果。相关预训练模型及代码均已开源供研究使用。
