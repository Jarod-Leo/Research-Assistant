# Language Model Adaptation to Specialized Domains through Selective Masking based on Genre and Topical Characteristics

链接: http://arxiv.org/abs/2402.12036v1

原文摘要:
Recent advances in pre-trained language modeling have facilitated significant
progress across various natural language processing (NLP) tasks. Word masking
during model training constitutes a pivotal component of language modeling in
architectures like BERT. However, the prevalent method of word masking relies
on random selection, potentially disregarding domain-specific linguistic
attributes. In this article, we introduce an innovative masking approach
leveraging genre and topicality information to tailor language models to
specialized domains. Our method incorporates a ranking process that prioritizes
words based on their significance, subsequently guiding the masking procedure.
Experiments conducted using continual pre-training within the legal domain have
underscored the efficacy of our approach on the LegalGLUE benchmark in the
English language. Pre-trained language models and code are freely available for
use.

中文翻译:
以下是符合要求的学术论文摘要中文翻译：

预训练语言模型的最新进展推动了各类自然语言处理（NLP）任务的显著突破。在BERT等架构中，模型训练过程中的词语掩码构成语言建模的关键环节。然而，当前主流的随机词语掩码方法可能忽略领域特定的语言特征。本文提出一种创新性掩码策略，通过融合文本体裁与主题信息来实现面向专业领域的语言模型定制。该方法采用基于词汇重要性的排序机制来指导掩码过程。在法律领域持续预训练的实验结果表明，我们的方法在英文LegalGLUE基准测试中成效显著。预训练模型及相关代码均已开源提供。

（翻译说明：
1. 专业术语处理："pre-trained language modeling"译为"预训练语言模型"，"word masking"译为"词语掩码"，"LegalGLUE"保留原名
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句，如将"leveraging genre..."处理为"通过融合...来实现"
3. 学术表达："underscored the efficacy"译为"成效显著"而非字面直译
4. 被动语态转换："have been made available"主动化为"均已开源提供"
5. 概念显化："ranking process"具体化为"排序机制"以增强可读性）
