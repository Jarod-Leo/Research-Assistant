# Benchmarking Large Language Models on CFLUE -- A Chinese Financial Language Understanding Evaluation Dataset

链接: http://arxiv.org/abs/2405.10542v1

原文摘要:
In light of recent breakthroughs in large language models (LLMs) that have
revolutionized natural language processing (NLP), there is an urgent need for
new benchmarks to keep pace with the fast development of LLMs. In this paper,
we propose CFLUE, the Chinese Financial Language Understanding Evaluation
benchmark, designed to assess the capability of LLMs across various dimensions.
Specifically, CFLUE provides datasets tailored for both knowledge assessment
and application assessment. In knowledge assessment, it consists of 38K+
multiple-choice questions with associated solution explanations. These
questions serve dual purposes: answer prediction and question reasoning. In
application assessment, CFLUE features 16K+ test instances across distinct
groups of NLP tasks such as text classification, machine translation, relation
extraction, reading comprehension, and text generation. Upon CFLUE, we conduct
a thorough evaluation of representative LLMs. The results reveal that only
GPT-4 and GPT-4-turbo achieve an accuracy exceeding 60\% in answer prediction
for knowledge assessment, suggesting that there is still substantial room for
improvement in current LLMs. In application assessment, although GPT-4 and
GPT-4-turbo are the top two performers, their considerable advantage over
lightweight LLMs is noticeably diminished. The datasets and scripts associated
with CFLUE are openly accessible at https://github.com/aliyun/cflue.

中文翻译:
鉴于大语言模型（LLMs）近期取得的突破性进展彻底改变了自然语言处理（NLP）领域，亟需建立新的基准测试以适应LLMs的快速发展。本文提出中文金融语言理解评估基准CFLUE，旨在从多维度评估LLMs的能力。具体而言，CFLUE提供了分别针对知识评估和应用评估定制的数据集。在知识评估部分，该基准包含38,000余道附带解析说明的单项选择题，这些题目兼具答案预测与问题推理双重功能；在应用评估部分，CFLUE涵盖文本分类、机器翻译、关系抽取、阅读理解及文本生成等五大类NLP任务的16,000余个测试实例。基于CFLUE，我们对代表性LLMs进行了全面评估。结果显示：在知识评估的答案预测任务中，仅GPT-4和GPT-4-turbo的准确率超过60%，表明当前LLMs仍有显著提升空间；在应用评估中，尽管GPT-4和GPT-4-turbo表现最佳，但其对轻量级LLMs的领先优势明显缩小。CFLUE相关数据集与脚本已在https://github.com/aliyun/cflue开源。
