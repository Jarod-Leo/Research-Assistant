# Large Language Models Understand Layouts

链接: http://arxiv.org/abs/2407.05750v1

原文摘要:
Large language models (LLMs) demonstrate extraordinary abilities in a wide
range of natural language processing (NLP) tasks. In this paper, we show that,
beyond text understanding capability, LLMs are capable of processing text
layouts that are denoted by spatial markers. They are able to answer questions
that require explicit spatial perceiving and reasoning, while a drastic
performance drop is observed when the spatial markers from the original data
are excluded. We perform a series of experiments with the GPT-3.5, Baichuan2,
Llama2 and ChatGLM3 models on various types of layout-sensitive datasets for
further analysis. The experimental results reveal that the layout understanding
ability of LLMs is mainly introduced by the coding data for pretraining, which
is further enhanced at the instruction-tuning stage. In addition, layout
understanding can be enhanced by integrating low-cost, auto-generated data
approached by a novel text game. Finally, we show that layout understanding
ability is beneficial for building efficient visual question-answering (VQA)
systems.

中文翻译:
大型语言模型（LLMs）在各类自然语言处理（NLP）任务中展现出卓越能力。本文研究发现，LLMs不仅具备文本理解能力，还能处理由空间标记符构成的文本布局。它们能够解答需要显式空间感知与推理的问题，而原始数据中若移除空间标记符则会导致性能急剧下降。我们基于GPT-3.5、Baichuan2、Llama2和ChatGLM3模型在多种布局敏感数据集上展开实验分析，结果表明：LLMs的布局理解能力主要源自预训练阶段接触的编程数据，并在指令微调阶段得到强化；通过新型文本游戏方法自动生成低成本数据可进一步提升该能力；最后证实布局理解能力有助于构建高效的视觉问答（VQA）系统。
