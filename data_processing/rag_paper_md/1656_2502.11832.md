# HAAN: A Holistic Approach for Accelerating Normalization Operations in Large Language Models

链接: http://arxiv.org/abs/2502.11832v1

原文摘要:
Large language models (LLMs) have revolutionized natural language processing
(NLP) tasks by achieving state-of-the-art performance across a range of
benchmarks. Central to the success of these models is the integration of
sophisticated architectural components aimed at improving training stability,
convergence speed, and generalization capabilities. Among these components,
normalization operation, such as layer normalization (LayerNorm), emerges as a
pivotal technique, offering substantial benefits to the overall model
performance. However, previous studies have indicated that normalization
operations can substantially elevate processing latency and energy usage. In
this work, we adopt the principles of algorithm and hardware co-design,
introducing a holistic normalization accelerating method named HAAN. The
evaluation results demonstrate that HAAN can achieve significantly better
hardware performance compared to state-of-the-art solutions.

中文翻译:
大语言模型（LLMs）通过在一系列基准测试中实现最先进的性能，彻底改变了自然语言处理（NLP）任务。这些模型成功的关键在于整合了旨在提升训练稳定性、收敛速度和泛化能力的复杂架构组件。其中，诸如层归一化（LayerNorm）等归一化操作作为核心技术脱颖而出，为模型整体性能带来显著提升。然而，先前研究表明归一化操作会大幅增加处理延迟和能耗。本研究采用算法与硬件协同设计理念，提出了一种名为HAAN的全新归一化加速方案。评估结果表明，与现有最优解决方案相比，HAAN能实现显著更优的硬件性能。

（翻译说明：采用学术论文摘要的简洁风格，通过以下处理确保专业性：
1. 术语统一："state-of-the-art"译为"最先进的"，"normalization"统一为"归一化"
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句，如将"Central to..."从句转换为独立陈述句
3. 被动语态转化："are integrated"译为主动态的"整合了"
4. 概念准确："co-design"译为行业通用术语"协同设计"，"hardware performance"译为"硬件性能"
5. 保持技术严谨性：保留"LayerNorm"等技术术语的英文原名加中文注释的规范译法）
