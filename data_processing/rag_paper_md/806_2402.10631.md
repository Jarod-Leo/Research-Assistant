# BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation

链接: http://arxiv.org/abs/2402.10631v1

原文摘要:
The upscaling of Large Language Models (LLMs) has yielded impressive advances
in natural language processing, yet it also poses significant deployment
challenges. Weight quantization has emerged as a widely embraced solution to
reduce memory and computational demands. This paper introduces BitDistiller, a
framework that synergizes Quantization-Aware Training (QAT) with Knowledge
Distillation (KD) to boost the performance of LLMs at ultra-low precisions
(sub-4-bit). Specifically, BitDistiller first incorporates a tailored
asymmetric quantization and clipping technique to maximally preserve the
fidelity of quantized weights, and then proposes a novel Confidence-Aware
Kullback-Leibler Divergence (CAKLD) objective, which is employed in a
self-distillation manner to enable faster convergence and superior model
performance. Empirical evaluations demonstrate that BitDistiller significantly
surpasses existing methods in both 3-bit and 2-bit configurations on general
language understanding and complex reasoning benchmarks. Notably, BitDistiller
is shown to be more cost-effective, demanding fewer data and training
resources. The code is available at https://github.com/DD-DuDa/BitDistiller.

中文翻译:
以下是符合学术规范的中文翻译：

大型语言模型（LLMs）的规模扩展显著推进了自然语言处理的发展，但同时也带来了严峻的部署挑战。权重量化作为一种被广泛采用的解决方案，能有效降低内存与计算需求。本文提出BitDistiller框架，通过协同量化感知训练（QAT）与知识蒸馏（KD）来提升超低精度（4比特以下）LLMs的性能。具体而言，BitDistiller首先采用定制化的非对称量化与截断技术以最大限度保持量化权重的保真度，继而提出新型置信度感知KL散度（CAKLD）目标函数，通过自蒸馏方式实现更快的收敛速度与更优的模型性能。实验评估表明，在通用语言理解和复杂推理基准测试中，BitDistiller在3比特和2比特配置下均显著超越现有方法。值得注意的是，该框架具有更高的成本效益，所需训练数据和计算资源更少。代码已开源：https://github.com/DD-DuDa/BitDistiller。

（翻译说明：
1. 专业术语采用学界通用译法，如"knowledge distillation"译为"知识蒸馏"而非字面直译
2. 被动语态转换为中文主动句式，如"is shown to"译为"研究表明"
3. 长难句进行合理切分，如将原文包含两个技术要点的复合句拆分为两个独立分句
4. 保持学术文本的客观性，避免添加主观评价
5. 计量单位统一使用"比特"而非"位"以符合计算机领域惯例
6. 技术指标"sub-4-bit"译为"4比特以下"确保精确性）
