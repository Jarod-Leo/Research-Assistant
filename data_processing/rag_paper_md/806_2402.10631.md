# BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation

链接: http://arxiv.org/abs/2402.10631v1

原文摘要:
The upscaling of Large Language Models (LLMs) has yielded impressive advances
in natural language processing, yet it also poses significant deployment
challenges. Weight quantization has emerged as a widely embraced solution to
reduce memory and computational demands. This paper introduces BitDistiller, a
framework that synergizes Quantization-Aware Training (QAT) with Knowledge
Distillation (KD) to boost the performance of LLMs at ultra-low precisions
(sub-4-bit). Specifically, BitDistiller first incorporates a tailored
asymmetric quantization and clipping technique to maximally preserve the
fidelity of quantized weights, and then proposes a novel Confidence-Aware
Kullback-Leibler Divergence (CAKLD) objective, which is employed in a
self-distillation manner to enable faster convergence and superior model
performance. Empirical evaluations demonstrate that BitDistiller significantly
surpasses existing methods in both 3-bit and 2-bit configurations on general
language understanding and complex reasoning benchmarks. Notably, BitDistiller
is shown to be more cost-effective, demanding fewer data and training
resources. The code is available at https://github.com/DD-DuDa/BitDistiller.

中文翻译:
大型语言模型（LLMs）的规模扩展在自然语言处理领域取得了显著进展，但也带来了巨大的部署挑战。权重量化作为一种广受认可的解决方案，能有效降低内存与计算需求。本文提出BitDistiller框架，通过将量化感知训练（QAT）与知识蒸馏（KD）协同整合，显著提升超低精度（4比特以下）LLMs的性能。具体而言，该框架首先采用定制化的非对称量化与截断技术最大化保持量化权重的保真度，继而提出创新的置信度感知KL散度（CAKLD）目标函数，通过自蒸馏方式实现更快的收敛速度和更优的模型表现。实证评估表明，在通用语言理解和复杂推理基准测试中，BitDistiller在3比特和2比特配置下均显著超越现有方法。值得注意的是，该框架被证明具有更高的成本效益，所需训练数据和资源更少。代码已开源于https://github.com/DD-DuDa/BitDistiller。
