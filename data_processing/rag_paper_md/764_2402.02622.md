# DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging

链接: http://arxiv.org/abs/2402.02622v1

原文摘要:
The transformer architecture by Vaswani et al. (2017) is now ubiquitous
across application domains, from natural language processing to speech
processing and image understanding. We propose DenseFormer, a simple
modification to the standard architecture that improves the perplexity of the
model without increasing its size -- adding a few thousand parameters for
large-scale models in the 100B parameters range. Our approach relies on an
additional averaging step after each transformer block, which computes a
weighted average of current and past representations -- we refer to this
operation as Depth-Weighted-Average (DWA). The learned DWA weights exhibit
coherent patterns of information flow, revealing the strong and structured
reuse of activations from distant layers. Experiments demonstrate that
DenseFormer is more data efficient, reaching the same perplexity of much deeper
transformer models, and that for the same perplexity, these new models
outperform transformer baselines in terms of memory efficiency and inference
time.

中文翻译:
Vaswani等人（2017）提出的Transformer架构已广泛应用于自然语言处理、语音处理及图像理解等领域。本文提出DenseFormer——通过对标准架构进行简单改进，在不增加模型参数量的前提下（对于百亿参数规模的大模型仅增加数千参数）有效降低模型困惑度。该方法通过在每层Transformer块后引入深度加权平均（Depth-Weighted-Average, DWA）操作，动态计算当前表征与历史表征的加权平均值。学习得到的DWA权重呈现出清晰的信息流动模式，揭示了深层网络对远端层激活值的结构化重用现象。实验表明：DenseFormer具有更优的数据效率，能以更浅的模型深度达到深层Transformer的困惑度水平；在同等困惑度条件下，新模型在内存效率和推理速度方面均超越基线Transformer模型。
