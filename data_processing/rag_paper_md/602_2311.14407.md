# LLamol: A Dynamic Multi-Conditional Generative Transformer for De Novo Molecular Design

链接: http://arxiv.org/abs/2311.14407v1

原文摘要:
Generative models have demonstrated substantial promise in Natural Language
Processing (NLP) and have found application in designing molecules, as seen in
General Pretrained Transformer (GPT) models. In our efforts to develop such a
tool for exploring the organic chemical space in search of potentially
electro-active compounds, we present "LLamol", a single novel generative
transformer model based on the LLama 2 architecture, which was trained on a 13M
superset of organic compounds drawn from diverse public sources. To allow for a
maximum flexibility in usage and robustness in view of potentially incomplete
data, we introduce "Stochastic Context Learning" as a new training procedure.
We demonstrate that the resulting model adeptly handles single- and
multi-conditional organic molecule generation with up to four conditions, yet
more are possible. The model generates valid molecular structures in SMILES
notation while flexibly incorporating three numerical and/or one token sequence
into the generative process, just as requested. The generated compounds are
very satisfactory in all scenarios tested. In detail, we showcase the model's
capability to utilize token sequences for conditioning, either individually or
in combination with numerical properties, making LLamol a potent tool for de
novo molecule design, easily expandable with new properties.

中文翻译:
生成式模型在自然语言处理（NLP）领域展现出巨大潜力，并已应用于分子设计领域，通用预训练Transformer（GPT）模型便是例证。为开发一种能探索有机化学空间、寻找潜在电活性化合物的工具，我们推出了基于LLama 2架构的新型生成式Transformer模型"LLamol"。该模型训练数据涵盖来自多个公开渠道的1300万种有机化合物超集。为确保模型在使用中的最大灵活性及对潜在不完整数据的鲁棒性，我们创新性地提出了"随机上下文学习"训练方法。研究表明，该模型能出色处理最多包含四个条件（可扩展更多）的单条件与多条件有机分子生成任务，能按要求灵活整合三个数值属性和/或一个标记序列到生成过程中，并以SMILES符号准确输出有效分子结构。在所有测试场景中，生成化合物均表现优异。具体而言，我们展示了模型单独使用标记序列、或结合数值属性进行条件调控的能力，这使得LLamol成为新分子设计的强大工具，且能便捷地扩展新属性支持。

（翻译说明：
1. 专业术语处理："electro-active"译为"电活性"，"SMILES notation"保留专业缩写并补充说明"符号"
2. 长句拆分：将原文复合句按中文习惯分解为多个短句，如训练数据部分独立成句
3. 被动语态转化："was trained"译为主动式"训练数据涵盖"
4. 概念显化："Stochastic Context Learning"首译添加"创新性地"强调技术新颖性
5. 逻辑显化：通过"具体而言"等连接词明确段落递进关系
6. 文化适配："de novo"采用科研界通用拉丁语保留形式，配合中文解释"新分子设计"）
