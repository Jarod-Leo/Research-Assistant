# LLMCL-GEC: Advancing Grammatical Error Correction with LLM-Driven Curriculum Learning

链接: http://arxiv.org/abs/2412.12541v1

原文摘要:
While large-scale language models (LLMs) have demonstrated remarkable
capabilities in specific natural language processing (NLP) tasks, they may
still lack proficiency compared to specialized models in certain domains, such
as grammatical error correction (GEC). Drawing inspiration from the concept of
curriculum learning, we have delved into refining LLMs into proficient GEC
experts by devising effective curriculum learning (CL) strategies. In this
paper, we introduce a novel approach, termed LLM-based curriculum learning,
which capitalizes on the robust semantic comprehension and discriminative
prowess inherent in LLMs to gauge the complexity of GEC training data. Unlike
traditional curriculum learning techniques, our method closely mirrors human
expert-designed curriculums. Leveraging the proposed LLM-based CL method, we
sequentially select varying levels of curriculums ranging from easy to hard,
and iteratively train and refine using the pretrianed T5 and LLaMA series
models. Through rigorous testing and analysis across diverse benchmark
assessments in English GEC, including the CoNLL14 test, BEA19 test, and BEA19
development sets, our approach showcases a significant performance boost over
baseline models and conventional curriculum learning methodologies.

中文翻译:
虽然大规模语言模型（LLMs）在特定自然语言处理（NLP）任务中展现出卓越能力，但在某些专业领域（如语法错误修正GEC）仍可能逊色于专用模型。受课程学习理念启发，我们通过设计有效的课程学习（CL）策略，深入探索如何将LLMs精炼为专业级GEC专家。本文提出一种创新方法——基于LLM的课程学习，该方法利用LLMs强大的语义理解与判别能力来评估GEC训练数据的复杂度。与传统课程学习技术不同，我们的方法更贴近人类专家设计的课程体系。依托所提出的LLM-CL方法，我们依次选择由易到难的多级课程，并基于预训练的T5和LLaMA系列模型进行迭代训练优化。通过在英语GEC领域的CoNLL14测试集、BEA19测试集及BEA19开发集等多个基准评估中的严格测试与分析，本方法相较基线模型与传统课程学习方法展现出显著的性能提升。

（注：根据学术翻译规范，对部分术语进行了标准化处理：
1. "grammatical error correction" 统一译为"语法错误修正"（GEC领域标准译法）
2. "pretrianed" 修正为"预训练的"（原文拼写错误处理）
3. 保持"T5/LLaMA"等模型名称原文形式
4. 采用"课程学习"而非"课程式学习"（计算机领域常用译法）
5. 通过分句重组实现中文长句拆分（如将原文复合从句拆分为三个短句））
