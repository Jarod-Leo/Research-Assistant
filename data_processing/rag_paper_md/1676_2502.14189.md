# QUAD-LLM-MLTC: Large Language Models Ensemble Learning for Healthcare Text Multi-Label Classification

链接: http://arxiv.org/abs/2502.14189v1

原文摘要:
The escalating volume of collected healthcare textual data presents a unique
challenge for automated Multi-Label Text Classification (MLTC), which is
primarily due to the scarcity of annotated texts for training and their nuanced
nature. Traditional machine learning models often fail to fully capture the
array of expressed topics. However, Large Language Models (LLMs) have
demonstrated remarkable effectiveness across numerous Natural Language
Processing (NLP) tasks in various domains, which show impressive computational
efficiency and suitability for unsupervised learning through prompt
engineering. Consequently, these LLMs promise an effective MLTC of medical
narratives. However, when dealing with various labels, different prompts can be
relevant depending on the topic. To address these challenges, the proposed
approach, QUAD-LLM-MLTC, leverages the strengths of four LLMs: GPT-4o, BERT,
PEGASUS, and BART. QUAD-LLM-MLTC operates in a sequential pipeline in which
BERT extracts key tokens, PEGASUS augments textual data, GPT-4o classifies, and
BART provides topics' assignment probabilities, which results in four
classifications, all in a 0-shot setting. The outputs are then combined using
ensemble learning and processed through a meta-classifier to produce the final
MLTC result. The approach is evaluated using three samples of annotated texts,
which contrast it with traditional and single-model methods. The results show
significant improvements across the majority of the topics in the
classification's F1 score and consistency (F1 and Micro-F1 scores of 78.17% and
80.16% with standard deviations of 0.025 and 0.011, respectively). This
research advances MLTC using LLMs and provides an efficient and scalable
solution to rapidly categorize healthcare-related text data without further
training.

中文翻译:
以下是符合学术规范的中文翻译：

医疗文本数据量的激增为自动化多标签文本分类（MLTC）带来了独特挑战，这主要源于训练用标注文本的稀缺性及其语义的微妙性。传统机器学习模型往往难以全面捕捉文本所表达的主题。然而，大语言模型（LLMs）在跨领域自然语言处理（NLP）任务中展现出卓越性能，其通过提示工程表现出的计算效率和无监督学习适应性尤为突出，这为医疗叙事文本的多标签分类提供了新机遇。但针对不同主题标签时，需要适配差异化的提示模板。

为解决这些挑战，本研究提出QUAD-LLM-MLTC方法，整合了GPT-4o、BERT、PEGASUS和BART四种大语言模型的优势。该方法采用级联处理流程：BERT负责关键标记提取，PEGASUS进行文本数据增强，GPT-4o执行分类，BART则生成主题分配概率——所有步骤均在零样本设置下完成，最终产生四组分类结果。通过集成学习融合输出并经元分类器处理，形成最终的多标签分类结果。

研究采用三组标注文本样本进行评估，与传统方法及单模型方案进行对比。实验结果表明，该方法在大多数主题分类的F1分数和一致性上均有显著提升（F1和Micro-F1分数分别达到78.17%和80.16%，标准差为0.025和0.011）。本研究不仅推进了大语言模型在多标签分类中的应用，更为医疗文本数据的快速分类提供了无需额外训练的高效可扩展解决方案。

（翻译严格遵循以下原则：
1. 专业术语统一（如MLTC/零样本设置等）
2. 被动语态转换为主动句式
3. 长句拆分符合中文表达习惯
4. 保留技术细节的精确性
5. 学术论文摘要的正式语体
6. 关键数据完整呈现）
