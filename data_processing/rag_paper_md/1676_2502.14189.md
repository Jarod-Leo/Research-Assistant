# QUAD-LLM-MLTC: Large Language Models Ensemble Learning for Healthcare Text Multi-Label Classification

链接: http://arxiv.org/abs/2502.14189v1

原文摘要:
The escalating volume of collected healthcare textual data presents a unique
challenge for automated Multi-Label Text Classification (MLTC), which is
primarily due to the scarcity of annotated texts for training and their nuanced
nature. Traditional machine learning models often fail to fully capture the
array of expressed topics. However, Large Language Models (LLMs) have
demonstrated remarkable effectiveness across numerous Natural Language
Processing (NLP) tasks in various domains, which show impressive computational
efficiency and suitability for unsupervised learning through prompt
engineering. Consequently, these LLMs promise an effective MLTC of medical
narratives. However, when dealing with various labels, different prompts can be
relevant depending on the topic. To address these challenges, the proposed
approach, QUAD-LLM-MLTC, leverages the strengths of four LLMs: GPT-4o, BERT,
PEGASUS, and BART. QUAD-LLM-MLTC operates in a sequential pipeline in which
BERT extracts key tokens, PEGASUS augments textual data, GPT-4o classifies, and
BART provides topics' assignment probabilities, which results in four
classifications, all in a 0-shot setting. The outputs are then combined using
ensemble learning and processed through a meta-classifier to produce the final
MLTC result. The approach is evaluated using three samples of annotated texts,
which contrast it with traditional and single-model methods. The results show
significant improvements across the majority of the topics in the
classification's F1 score and consistency (F1 and Micro-F1 scores of 78.17% and
80.16% with standard deviations of 0.025 and 0.011, respectively). This
research advances MLTC using LLMs and provides an efficient and scalable
solution to rapidly categorize healthcare-related text data without further
training.

中文翻译:
医疗文本数据量的激增为自动化多标签文本分类（MLTC）带来了独特挑战，这主要源于训练用标注文本的稀缺性及其微妙性。传统机器学习模型往往难以全面捕捉所表达的主题范围。然而，大型语言模型（LLMs）在跨领域自然语言处理（NLP）任务中展现出卓越效能，通过提示工程展现出令人印象深刻的计算效率和无监督学习适应性。因此，这些LLMs为医疗叙事的多标签分类提供了有效解决方案。但面对多样化标签时，不同主题可能需要适配不同的提示策略。

为解决这些挑战，本研究提出QUAD-LLM-MLTC方法，整合了GPT-4o、BERT、PEGASUS和BART四种LLMs的优势。该方法采用序列化处理流程：BERT负责关键标记提取，PEGASUS进行文本数据增强，GPT-4o执行分类，BART则提供主题分配概率——所有步骤均在零样本设置下完成，最终产生四组分类结果。通过集成学习融合输出后，经由元分类器生成最终MLTC结果。使用三组标注文本样本进行评估，与传统方法及单模型方案对比显示：在大多数主题分类中，F1分数和一致性显著提升（F1和Micro-F1分数分别达到78.17%和80.16%，标准差为0.025和0.011）。该研究不仅推动了LLMs在多标签分类中的应用，更为医疗文本数据的快速分类提供了无需额外训练的高效可扩展方案。
