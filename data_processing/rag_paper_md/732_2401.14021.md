# Accelerating Retrieval-Augmented Language Model Serving with Speculation

链接: http://arxiv.org/abs/2401.14021v1

原文摘要:
Retrieval-augmented language models (RaLM) have demonstrated the potential to
solve knowledge-intensive natural language processing (NLP) tasks by combining
a non-parametric knowledge base with a parametric language model. Instead of
fine-tuning a fully parametric model, RaLM excels at its low-cost adaptation to
the latest data and better source attribution mechanisms. Among various RaLM
approaches, iterative RaLM delivers a better generation quality due to a more
frequent interaction between the retriever and the language model. Despite the
benefits, iterative RaLM usually encounters high overheads due to the frequent
retrieval step. To this end, we propose RaLMSpec, a speculation-inspired
framework that provides generic speed-up over iterative RaLM while preserving
the same model outputs through speculative retrieval and batched verification.
By further incorporating prefetching, optimal speculation stride scheduler, and
asynchronous verification, RaLMSpec can automatically exploit the acceleration
potential to the fullest. For naive iterative RaLM serving, extensive
evaluations over three language models on four downstream QA datasets
demonstrate that RaLMSpec can achieve a speed-up ratio of 1.75-2.39x,
1.04-1.39x, and 1.31-1.77x when the retriever is an exact dense retriever,
approximate dense retriever, and sparse retriever respectively compared with
the baseline. For KNN-LM serving, RaLMSpec can achieve a speed-up ratio up to
7.59x and 2.45x when the retriever is an exact dense retriever and approximate
dense retriever, respectively, compared with the baseline.

中文翻译:
检索增强语言模型（RaLM）通过将非参数化知识库与参数化语言模型相结合，展现出解决知识密集型自然语言处理（NLP）任务的潜力。相较于微调全参数化模型，RaLM的优势在于能以更低成本适配最新数据，并提供更优的溯源机制。在各类RaLM方法中，迭代式RaLM因检索器与语言模型更频繁的交互而具有更优的生成质量，但频繁检索步骤通常导致较高开销。为此，我们提出RaLMSpec框架，其受推测执行思想启发，通过推测式检索与批量验证在保持输出一致性的同时，为迭代式RaLM提供通用加速。该框架进一步整合预取机制、最优推测跨度调度器及异步验证技术，可充分挖掘加速潜力。针对基础迭代式RaLM服务，在四个下游QA数据集上对三种语言模型的广泛测试表明：与基线相比，当检索器分别为精确稠密检索器、近似稠密检索器和稀疏检索器时，RaLMSpec可实现1.75-2.39倍、1.04-1.39倍和1.31-1.77倍的加速比。对于KNN-LM服务场景，RaLMSpec在精确稠密检索器和近似稠密检索器下分别取得最高7.59倍和2.45倍的加速效果。
