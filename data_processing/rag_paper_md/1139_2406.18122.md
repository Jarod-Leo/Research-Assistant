# Poisoned LangChain: Jailbreak LLMs by LangChain

链接: http://arxiv.org/abs/2406.18122v1

原文摘要:
With the development of natural language processing (NLP), large language
models (LLMs) are becoming increasingly popular. LLMs are integrating more into
everyday life, raising public concerns about their security vulnerabilities.
Consequently, the security of large language models is becoming critically
important. Currently, the techniques for attacking and defending against LLMs
are continuously evolving. One significant method type of attack is the
jailbreak attack, which designed to evade model safety mechanisms and induce
the generation of inappropriate content. Existing jailbreak attacks primarily
rely on crafting inducement prompts for direct jailbreaks, which are less
effective against large models with robust filtering and high comprehension
abilities. Given the increasing demand for real-time capabilities in large
language models, real-time updates and iterations of new knowledge have become
essential. Retrieval-Augmented Generation (RAG), an advanced technique to
compensate for the model's lack of new knowledge, is gradually becoming
mainstream. As RAG enables the model to utilize external knowledge bases, it
provides a new avenue for jailbreak attacks.
  In this paper, we conduct the first work to propose the concept of indirect
jailbreak and achieve Retrieval-Augmented Generation via LangChain. Building on
this, we further design a novel method of indirect jailbreak attack, termed
Poisoned-LangChain (PLC), which leverages a poisoned external knowledge base to
interact with large language models, thereby causing the large models to
generate malicious non-compliant dialogues.We tested this method on six
different large language models across three major categories of jailbreak
issues. The experiments demonstrate that PLC successfully implemented indirect
jailbreak attacks under three different scenarios, achieving success rates of
88.56%, 79.04%, and 82.69% respectively.

中文翻译:
随着自然语言处理（NLP）技术的发展，大语言模型（LLMs）的应用日益广泛。这类模型正深度融入日常生活，其潜在安全漏洞引发了公众担忧，使得大语言模型的安全性成为关键议题。当前，针对大语言模型的攻防技术持续演进，其中越狱攻击（jailbreak attack）作为重要攻击手段，旨在绕过模型安全机制并诱导生成不当内容。现有越狱攻击主要依赖精心设计的诱导提示实现直接突破，但对具备强过滤机制和高理解能力的大模型效果有限。鉴于大语言模型对实时性需求的提升，新知识的实时更新迭代变得至关重要。检索增强生成（RAG）技术作为弥补模型新知识缺口的先进方案正逐渐成为主流，其通过引入外部知识库的能力，为越狱攻击提供了新途径。

本文首次提出"间接越狱"概念，并基于LangChain框架实现检索增强生成。在此基础上，我们进一步设计出名为Poisoned-LangChain（PLC）的新型间接越狱攻击方法：通过构建带毒外部知识库与大语言模型交互，促使大模型生成恶意违规对话。我们在三类典型越狱问题上对六种不同大语言模型进行测试，实验表明PLC在三种场景下均成功实施间接越狱攻击，成功率分别达到88.56%、79.04%和82.69%。
