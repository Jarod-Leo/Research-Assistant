# Are Compressed Language Models Less Subgroup Robust?

链接: http://arxiv.org/abs/2403.17811v1

原文摘要:
To reduce the inference cost of large language models, model compression is
increasingly used to create smaller scalable models. However, little is known
about their robustness to minority subgroups defined by the labels and
attributes of a dataset. In this paper, we investigate the effects of 18
different compression methods and settings on the subgroup robustness of BERT
language models. We show that worst-group performance does not depend on model
size alone, but also on the compression method used. Additionally, we find that
model compression does not always worsen the performance on minority subgroups.
Altogether, our analysis serves to further research into the subgroup
robustness of model compression.

中文翻译:
为降低大型语言模型的推理成本，模型压缩技术正被日益广泛地应用于创建可扩展的小型模型。然而，目前对于这些压缩模型在数据集标签和属性定义的少数子群体上的鲁棒性认知仍十分有限。本文系统研究了18种不同压缩方法及参数设置对BERT语言模型子群体鲁棒性的影响。研究表明：最差子群性能不仅取决于模型规模，更与所采用的压缩方法密切相关；同时发现模型压缩并不必然导致少数子群性能的恶化。本项分析为深化模型压缩的子群体鲁棒性研究提供了新的实证基础。
