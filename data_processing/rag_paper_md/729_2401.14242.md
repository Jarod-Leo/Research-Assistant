# Improving Natural Language Capability of Code Large Language Model

链接: http://arxiv.org/abs/2401.14242v1

原文摘要:
Code large language models (Code LLMs) have demonstrated remarkable
performance in code generation. Nonetheless, most existing works focus on
boosting code LLMs from the perspective of programming capabilities, while
their natural language capabilities receive less attention. To fill this gap,
we thus propose a novel framework, comprising two modules: AttentionExtractor,
which is responsible for extracting key phrases from the user's natural
language requirements, and AttentionCoder, which leverages these extracted
phrases to generate target code to solve the requirement. This framework
pioneers an innovative idea by seamlessly integrating code LLMs with
traditional natural language processing tools. To validate the effectiveness of
the framework, we craft a new code generation benchmark, called MultiNL-H,
covering five natural languages. Extensive experimental results demonstrate the
effectiveness of our proposed framework.

中文翻译:
代码大语言模型（Code LLMs）在代码生成方面展现出卓越性能。然而现有研究多聚焦于提升模型的编程能力维度，其自然语言处理能力尚未得到充分关注。为此，我们提出一个创新框架，包含两大核心模块：负责从用户自然语言需求中提取关键短语的AttentionExtractor，以及利用这些短语生成目标代码的AttentionCoder。该框架开创性地将代码大语言模型与传统自然语言处理工具深度融合。为验证框架有效性，我们构建了涵盖五种自然语言的MultiNL-H代码生成评测基准。大量实验结果表明，该框架具有显著性能优势。
