# Raiders of the Lost Dependency: Fixing Dependency Conflicts in Python using LLMs

链接: http://arxiv.org/abs/2501.16191v1

原文摘要:
Fixing Python dependency issues is a tedious and error-prone task for
developers, who must manually identify and resolve environment dependencies and
version constraints of third-party modules and Python interpreters. Researchers
have attempted to automate this process by relying on large knowledge graphs
and database lookup tables. However, these traditional approaches face
limitations due to the variety of dependency error types, large sets of
possible module versions, and conflicts among transitive dependencies. This
study explores the potential of using large language models (LLMs) to
automatically fix dependency issues in Python programs. We introduce PLLM
(pronounced "plum"), a novel technique that employs retrieval-augmented
generation (RAG) to help an LLM infer Python versions and required modules for
a given Python file. PLLM builds a testing environment that iteratively (1)
prompts the LLM for module combinations, (2) tests the suggested changes, and
(3) provides feedback (error messages) to the LLM to refine the fix. This
feedback cycle leverages natural language processing (NLP) to intelligently
parse and interpret build error messages. We benchmark PLLM on the Gistable
HG2.9K dataset, a collection of challenging single-file Python gists. We
compare PLLM against two state-of-the-art automatic dependency inference
approaches, namely PyEGo and ReadPyE, w.r.t. the ability to resolve dependency
issues. Our results indicate that PLLM can fix more dependency issues than the
two baselines, with +218 (+15.97%) more fixes over ReadPyE and +281 (+21.58%)
over PyEGo. Our deeper analyses suggest that PLLM is particularly beneficial
for projects with many dependencies and for specific third-party numerical and
machine-learning modules. Our findings demonstrate the potential of LLM-based
approaches to iteratively resolve Python dependency issues.

中文翻译:
修复Python依赖问题对开发者而言是一项繁琐且易出错的任务，他们需要手动识别并解决第三方模块与Python解释器的环境依赖及版本约束。研究人员曾尝试通过大型知识图谱和数据库查询表来自动化这一过程，但由于依赖错误类型多样、可选模块版本数量庞大以及传递性依赖间的冲突，这些传统方法存在明显局限。本研究探索了利用大语言模型（LLM）自动修复Python程序依赖问题的潜力。我们提出PLLM（发音同"plum"）这一创新技术，它采用检索增强生成（RAG）机制辅助LLM推断给定Python文件所需的解释器版本和模块。PLLM构建了一个迭代测试环境，循环执行：(1)向LLM请求模块组合建议，(2)测试修改方案，(3)将构建错误信息反馈给LLM以优化修复方案。该反馈循环利用自然语言处理（NLP）技术智能解析构建错误信息。我们在HG2.9K数据集（包含具有挑战性的单文件Python代码片段集合）上对PLLM进行基准测试，将其与PyEGo和ReadPyE两种前沿自动依赖推断方法进行修复能力对比。结果表明，PLLM比两种基线方法多修复218个（+15.97%）和281个（+21.58%）依赖问题。深入分析显示，PLLM尤其适用于具有大量依赖项的项目，以及特定第三方数值计算和机器学习模块的场景。我们的发现验证了基于LLM的方法通过迭代解决Python依赖问题的巨大潜力。
