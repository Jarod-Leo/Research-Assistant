# AcademicGPT: Empowering Academic Research

链接: http://arxiv.org/abs/2311.12315v1

原文摘要:
Large Language Models (LLMs) have demonstrated exceptional capabilities
across various natural language processing tasks. Yet, many of these advanced
LLMs are tailored for broad, general-purpose applications. In this technical
report, we introduce AcademicGPT, designed specifically to empower academic
research. AcademicGPT is a continual training model derived from LLaMA2-70B.
Our training corpus mainly consists of academic papers, thesis, content from
some academic domain, high-quality Chinese data and others. While it may not be
extensive in data scale, AcademicGPT marks our initial venture into a
domain-specific GPT tailored for research area. We evaluate AcademicGPT on
several established public benchmarks such as MMLU and CEval, as well as on
some specialized academic benchmarks like PubMedQA, SCIEval, and our
newly-created ComputerScienceQA, to demonstrate its ability from general
knowledge ability, to Chinese ability, and to academic ability. Building upon
AcademicGPT's foundation model, we also developed several applications catered
to the academic area, including General Academic Question Answering,
AI-assisted Paper Reading, Paper Review, and AI-assisted Title and Abstract
Generation.

中文翻译:
大型语言模型（LLMs）已在各类自然语言处理任务中展现出卓越能力，但当前多数先进模型主要面向通用场景。本技术报告介绍了专为学术研究赋能的AcademicGPT——该模型基于LLaMA2-70B进行持续训练，训练语料以学术论文、学位论文、学科领域内容及优质中文数据为核心。尽管数据规模有限，AcademicGPT标志着我们在研究领域专用GPT的首次探索。我们在MMLU、CEval等主流基准测试，以及PubMedQA、SCIEval和新构建的ComputerScienceQA等学术专项评估中验证了其通用知识、中文处理及学术研究能力。基于AcademicGPT基础模型，我们进一步开发了学术问答、论文阅读辅助、审稿辅助及标题摘要生成等系列学术场景应用。
