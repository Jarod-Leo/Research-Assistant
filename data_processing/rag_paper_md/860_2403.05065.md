# Can we obtain significant success in RST discourse parsing by using Large Language Models?

链接: http://arxiv.org/abs/2403.05065v1

原文摘要:
Recently, decoder-only pre-trained large language models (LLMs), with several
tens of billion parameters, have significantly impacted a wide range of natural
language processing (NLP) tasks. While encoder-only or encoder-decoder
pre-trained language models have already proved to be effective in discourse
parsing, the extent to which LLMs can perform this task remains an open
research question. Therefore, this paper explores how beneficial such LLMs are
for Rhetorical Structure Theory (RST) discourse parsing. Here, the parsing
process for both fundamental top-down and bottom-up strategies is converted
into prompts, which LLMs can work with. We employ Llama 2 and fine-tune it with
QLoRA, which has fewer parameters that can be tuned. Experimental results on
three benchmark datasets, RST-DT, Instr-DT, and the GUM corpus, demonstrate
that Llama 2 with 70 billion parameters in the bottom-up strategy obtained
state-of-the-art (SOTA) results with significant differences. Furthermore, our
parsers demonstrated generalizability when evaluated on RST-DT, showing that,
in spite of being trained with the GUM corpus, it obtained similar performances
to those of existing parsers trained with RST-DT.

中文翻译:
近年来，仅解码器架构的预训练大语言模型（LLMs）凭借数百亿参数规模，对自然语言处理（NLP）各领域产生了深远影响。尽管仅编码器或编码器-解码器架构的预训练语言模型已在篇章解析任务中展现出卓越性能，但LLMs在此任务中的潜力仍是一个待解的研究问题。为此，本文探究了LLMs在修辞结构理论（RST）篇章解析中的适用性。我们将自顶向下和自底向上两种基础解析策略转化为LLMs可处理的提示模板，并采用参数可调性更优的QLoRA方法对700亿参数的Llama 2模型进行微调。在RST-DT、Instr-DT和GUM三个基准数据集上的实验表明：采用自底向上策略的Llama 2模型以显著优势取得了最先进（SOTA）性能。值得注意的是，当在RST-DT上进行跨域评估时，仅基于GUM语料训练的解析器表现出与RST-DT专用训练模型相当的性能，这验证了其优异的泛化能力。
