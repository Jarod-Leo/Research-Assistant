# SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator

链接: http://arxiv.org/abs/2412.12094v1

原文摘要:
Large Language Models (LLMs) have exhibited exceptional performance across a
spectrum of natural language processing tasks. However, their substantial sizes
pose considerable challenges, particularly in computational demands and
inference speed, due to their quadratic complexity. In this work, we have
identified a key pattern: certain seemingly meaningless separator tokens (i.e.,
punctuations) contribute disproportionately to attention scores compared to
semantically meaningful tokens. This observation suggests that information of
the segments between these separator tokens can be effectively condensed into
the separator tokens themselves without significant information loss. Guided by
this insight, we introduce SepLLM, a plug-and-play framework that accelerates
inference by compressing these segments and eliminating redundant tokens.
Additionally, we implement efficient kernels for training acceleration.
Experimental results across training-free, training-from-scratch, and
post-training settings demonstrate SepLLM's effectiveness. Notably, using the
Llama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the
GSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in
streaming settings, SepLLM effectively processes sequences of up to 4 million
tokens or more while maintaining consistent language modeling capabilities.

中文翻译:
以下是符合您要求的中文翻译：

大型语言模型（LLMs）在自然语言处理任务中展现出卓越性能，但其庞大的规模带来了显著挑战——由于二次方计算复杂度，模型在计算需求和推理速度方面存在严重瓶颈。本研究揭示了一个关键现象：某些看似无意义的分隔符（如标点符号）对注意力得分的贡献度远超语义相关词汇。这一发现表明，分隔符之间的文本段信息可被有效压缩至分隔符本身而不造成显著信息损失。基于此，我们提出SepLLM——一个即插即用的推理加速框架，通过压缩文本段与消除冗余标记来提升效率，同时开发了专用加速内核以优化训练过程。在免训练、从头训练和训练后三种场景下的实验均验证了SepLLM的有效性。以Llama-3-8B为基座时，SepLLM在GSM8K-CoT基准测试中保持性能相当的同时，成功将KV缓存降低50%以上。此外在流式处理场景中，该框架能稳定处理长达400万甚至更多标记的序列，同时保持持续的语言建模能力。

翻译说明：
1. 专业术语处理：LLMs统一译为"大型语言模型"，"KV cache"保留英文缩写但增加"缓存"解释
2. 长句拆分：将原文复合句按中文习惯分解为多个短句，如将"due to..."状语从句转为独立分句
3. 被动语态转换："can be condensed"译为主动式"可被压缩"
4. 概念显化："plug-and-play"译为"即插即用"并添加破折号突出
5. 数据呈现：精确保持"50%""4 million"等数字表述
6. 技术表述统一："attention scores"统一译为"注意力得分"，"streaming settings"译为"流式处理场景"
7. 逻辑连接词优化：使用"——"替代冒号实现更自然的中文转折
8. 学术风格保持：使用"揭示""基于此""验证"等学术用语
