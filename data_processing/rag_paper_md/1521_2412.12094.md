# SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator

链接: http://arxiv.org/abs/2412.12094v1

原文摘要:
Large Language Models (LLMs) have exhibited exceptional performance across a
spectrum of natural language processing tasks. However, their substantial sizes
pose considerable challenges, particularly in computational demands and
inference speed, due to their quadratic complexity. In this work, we have
identified a key pattern: certain seemingly meaningless separator tokens (i.e.,
punctuations) contribute disproportionately to attention scores compared to
semantically meaningful tokens. This observation suggests that information of
the segments between these separator tokens can be effectively condensed into
the separator tokens themselves without significant information loss. Guided by
this insight, we introduce SepLLM, a plug-and-play framework that accelerates
inference by compressing these segments and eliminating redundant tokens.
Additionally, we implement efficient kernels for training acceleration.
Experimental results across training-free, training-from-scratch, and
post-training settings demonstrate SepLLM's effectiveness. Notably, using the
Llama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the
GSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in
streaming settings, SepLLM effectively processes sequences of up to 4 million
tokens or more while maintaining consistent language modeling capabilities.

中文翻译:
大型语言模型（LLMs）在自然语言处理任务中展现出卓越性能，但其庞大的规模带来了显著挑战，尤其是因其二次复杂度导致的计算需求和推理速度问题。本研究揭示了一个关键现象：某些看似无意义的分隔符（如标点符号）对注意力得分的贡献远超语义丰富的词汇。这一发现表明，分隔符之间的段落信息可被高效压缩至分隔符本身而不造成显著信息损失。基于此，我们提出SepLLM——一个即插即用的推理加速框架，通过压缩段落并剔除冗余标记来提升效率，同时开发了专用加速内核优化训练过程。在免训练、从头训练及训练后三种场景下的实验均验证了SepLLM的有效性。以Llama-3-8B为基座时，该框架在GSM8K-CoT基准测试中保持性能相当的同时，键值缓存降低超50%。在流式处理场景中，SepLLM能稳定处理长达400万及以上标记的序列，且语言建模能力保持稳定。
