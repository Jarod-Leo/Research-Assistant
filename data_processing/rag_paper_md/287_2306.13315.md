# Abstractive Text Summarization for Resumes With Cutting Edge NLP Transformers and LSTM

链接: http://arxiv.org/abs/2306.13315v1

原文摘要:
Text summarization is a fundamental task in natural language processing that
aims to condense large amounts of textual information into concise and coherent
summaries. With the exponential growth of content and the need to extract key
information efficiently, text summarization has gained significant attention in
recent years. In this study, LSTM and pre-trained T5, Pegasus, BART and
BART-Large model performances were evaluated on the open source dataset (Xsum,
CNN/Daily Mail, Amazon Fine Food Review and News Summary) and the prepared
resume dataset. This resume dataset consists of many information such as
language, education, experience, personal information, skills, and this data
includes 75 resumes. The primary objective of this research was to classify
resume text. Various techniques such as LSTM, pre-trained models, and
fine-tuned models were assessed using a dataset of resumes. The BART-Large
model fine-tuned with the resume dataset gave the best performance.

中文翻译:
文本摘要作为自然语言处理的一项基础任务，旨在将海量文本信息压缩为简洁连贯的概要。随着内容呈指数级增长及高效提取关键信息的需求日益迫切，该技术近年来备受关注。本研究在开源数据集（Xsum、CNN/Daily Mail、亚马逊美食评论和新闻摘要）与自建简历数据集上评估了LSTM及预训练模型T5、Pegasus、BART和BART-Large的性能。该简历数据集涵盖语言、教育背景、工作经历、个人信息、技能等多维度信息，共包含75份简历。本研究核心目标是对简历文本进行分类，通过采用LSTM、预训练模型及微调模型等多种技术对简历数据集进行评估。实验结果表明，基于简历数据集微调的BART-Large模型展现出最优性能。
