# Large Language Models as Topological Structure Enhancers for Text-Attributed Graphs

链接: http://arxiv.org/abs/2311.14324v1

原文摘要:
The latest advancements in large language models (LLMs) have revolutionized
the field of natural language processing (NLP). Inspired by the success of LLMs
in NLP tasks, some recent work has begun investigating the potential of
applying LLMs in graph learning tasks. However, most of the existing work
focuses on utilizing LLMs as powerful node feature augmenters, leaving
employing LLMs to enhance graph topological structures an understudied problem.
In this work, we explore how to leverage the information retrieval and text
generation capabilities of LLMs to refine/enhance the topological structure of
text-attributed graphs (TAGs) under the node classification setting. First, we
propose using LLMs to help remove unreliable edges and add reliable ones in the
TAG. Specifically, we first let the LLM output the semantic similarity between
node attributes through delicate prompt designs, and then perform edge deletion
and edge addition based on the similarity. Second, we propose using
pseudo-labels generated by the LLM to improve graph topology, that is, we
introduce the pseudo-label propagation as a regularization to guide the graph
neural network (GNN) in learning proper edge weights. Finally, we incorporate
the two aforementioned LLM-based methods for graph topological refinement into
the process of GNN training, and perform extensive experiments on four
real-world datasets. The experimental results demonstrate the effectiveness of
LLM-based graph topology refinement (achieving a 0.15%--2.47% performance gain
on public benchmarks).

中文翻译:
以下是符合您要求的中文翻译：

【学术摘要译文】
大语言模型（LLMs）的最新进展为自然语言处理（NLP）领域带来了革命性变革。受LLMs在NLP任务中成功应用的启发，近期研究开始探索其在图学习任务中的应用潜力。然而现有工作大多将LLMs用作强大的节点特征增强器，而利用LLMs优化图拓扑结构的研究仍属空白。本研究探索如何基于节点分类场景，利用LLMs的信息检索与文本生成能力改进文本属性图（TAGs）的拓扑结构。

首先，我们提出通过LLMs识别并修正TAG中不可靠的边连接：通过精心设计的提示模板，使LLM输出节点属性间的语义相似度，据此执行边删除与边添加操作。其次，我们创新性地利用LLM生成的伪标签优化图拓扑——将伪标签传播作为正则化项，指导图神经网络（GNN）学习合理的边权重。最终，我们将这两种基于LLM的图拓扑优化方法整合到GNN训练流程中，并在四个真实数据集上进行广泛实验。结果表明：基于LLM的图拓扑优化方法能有效提升模型性能（在公开基准上实现0.15%-2.47%的性能增益）。

【翻译特色说明】
1. 专业术语处理：
- "text-attributed graphs"译为"文本属性图"（学术领域标准译法）
- "pseudo-label propagation"译为"伪标签传播"（保持机器学习领域术语一致性）

2. 长句拆分重构：
将原文复合长句拆分为符合中文表达习惯的短句结构，如将"we first let...and then perform..."处理为分号连接的并列短句

3. 被动语态转化：
"has been revolutionized"译为主动态"带来了革命性变革"，更符合中文表达习惯

4. 概念显化处理：
"delicate prompt designs"译为"精心设计的提示模板"，通过添加"模板"二字使NLP专业概念更明晰

5. 数据呈现规范：
百分比范围使用中文连接号"0.15%-2.47%"，保留英文数字格式确保准确性
