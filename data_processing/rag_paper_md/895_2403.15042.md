# LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement

链接: http://arxiv.org/abs/2403.15042v1

原文摘要:
Pretrained large language models (LLMs) are currently state-of-the-art for
solving the vast majority of natural language processing tasks. While many
real-world applications still require fine-tuning to reach satisfactory levels
of performance, many of them are in the low-data regime, making fine-tuning
challenging. To address this, we propose LLM2LLM, a targeted and iterative data
augmentation strategy that uses a teacher LLM to enhance a small seed dataset
by augmenting additional data that can be used for fine-tuning on a specific
task. LLM2LLM (1) fine-tunes a baseline student LLM on the initial seed data,
(2) evaluates and extracts data points that the model gets wrong, and (3) uses
a teacher LLM to generate synthetic data based on these incorrect data points,
which are then added back into the training data. This approach amplifies the
signal from incorrectly predicted data points by the LLM during training and
reintegrates them into the dataset to focus on more challenging examples for
the LLM. Our results show that LLM2LLM significantly enhances the performance
of LLMs in the low-data regime, outperforming both traditional fine-tuning and
other data augmentation baselines. LLM2LLM reduces the dependence on
labor-intensive data curation and paves the way for more scalable and
performant LLM solutions, allowing us to tackle data-constrained domains and
tasks. We achieve improvements up to 24.2% on the GSM8K dataset, 32.6% on
CaseHOLD, 32.0% on SNIPS, 52.6% on TREC and 39.8% on SST-2 over regular
fine-tuning in the low-data regime using a Llama-2-7B student model. Our code
is available at https://github.com/SqueezeAILab/LLM2LLM .

中文翻译:
预训练大语言模型（LLM）目前是解决绝大多数自然语言处理任务的最先进技术。尽管许多实际应用仍需通过微调以达到满意性能，但其中大量场景处于低数据状态，这使得微调面临挑战。为此，我们提出LLM2LLM——一种针对性迭代数据增强策略，该方法利用教师LLM对小规模种子数据集进行增强，生成可用于特定任务微调的补充数据。该技术通过三个核心步骤实现：(1) 基于初始种子数据对基准学生LLM进行微调，(2) 评估并提取模型预测错误的数据点，(3) 使用教师LLM根据这些错误数据点生成合成数据并重新注入训练集。该方案在训练过程中放大LLM错误预测数据点的信号，通过聚焦模型处理困难的样本实现性能提升。实验结果表明，在低数据条件下，LLM2LLM显著优于传统微调及其他数据增强基线方法。采用Llama-2-7B学生模型时，我们在GSM8K数据集上实现最高24.2%的提升，CaseHOLD达32.6%，SNIPS为32.0%，TREC提高52.6%，SST-2增强39.8%。该技术降低了对人工密集型数据标注的依赖，为构建更具扩展性和高性能的LLM解决方案开辟了新途径，使得数据受限领域和任务的处理成为可能。代码已开源：https://github.com/SqueezeAILab/LLM2LLM。
