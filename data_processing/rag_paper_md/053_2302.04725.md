# Lightweight Transformers for Clinical Natural Language Processing

链接: http://arxiv.org/abs/2302.04725v1

原文摘要:
Specialised pre-trained language models are becoming more frequent in NLP
since they can potentially outperform models trained on generic texts. BioBERT
and BioClinicalBERT are two examples of such models that have shown promise in
medical NLP tasks. Many of these models are overparametrised and
resource-intensive, but thanks to techniques like Knowledge Distillation (KD),
it is possible to create smaller versions that perform almost as well as their
larger counterparts. In this work, we specifically focus on development of
compact language models for processing clinical texts (i.e. progress notes,
discharge summaries etc). We developed a number of efficient lightweight
clinical transformers using knowledge distillation and continual learning, with
the number of parameters ranging from 15 million to 65 million. These models
performed comparably to larger models such as BioBERT and ClinicalBioBERT and
significantly outperformed other compact models trained on general or
biomedical data. Our extensive evaluation was done across several standard
datasets and covered a wide range of clinical text-mining tasks, including
Natural Language Inference, Relation Extraction, Named Entity Recognition, and
Sequence Classification. To our knowledge, this is the first comprehensive
study specifically focused on creating efficient and compact transformers for
clinical NLP tasks. The models and code used in this study can be found on our
Huggingface profile at https://huggingface.co/nlpie and Github page at
https://github.com/nlpie-research/Lightweight-Clinical-Transformers,
respectively, promoting reproducibility of our results.

中文翻译:
以下是符合要求的学术中文翻译：

专业预训练语言模型在自然语言处理领域日益普及，因其在特定领域的表现往往优于通用文本训练的模型。BioBERT和BioClinicalBERT就是这类模型的典型代表，它们在医疗文本处理任务中展现出显著优势。尽管这些模型通常存在参数过量、计算资源消耗大的问题，但通过知识蒸馏等技术，可以构建性能接近原版但体积更小的精简模型。本研究专注于开发适用于临床文本（如病程记录、出院小结等）处理的紧凑型语言模型。我们采用知识蒸馏与持续学习方法，开发了参数量在1500万至6500万之间的多种高效轻量级临床Transformer模型。这些模型在性能上可比肩BioBERT和ClinicalBioBERT等大型模型，并显著优于基于通用或生物医学数据训练的其他紧凑模型。我们在多个标准数据集上进行了全面评估，涵盖自然语言推理、关系抽取、命名实体识别和序列分类等临床文本挖掘任务。据我们所知，这是首个专门针对临床自然语言处理任务构建高效紧凑Transformer模型的系统性研究。为促进研究可复现性，本研究所用模型及代码已发布于Huggingface平台（https://huggingface.co/nlpie）和GitHub仓库（https://github.com/nlpie-research/Lightweight-Clinical-Transformers）。

注：翻译严格遵循以下学术规范：
1. 专业术语统一（如Transformer不译，Knowledge Distillation译为"知识蒸馏"）
2. 被动语态转换为中文主动表述（如"are becoming"译为"日益普及"）
3. 长句拆分重组（如原文最后两句按中文习惯分述）
4. 保留所有技术指标精确性（参数量、URL等）
5. 学术用语规范化（"comprehensive study"译为"系统性研究"）
6. 机构名称不翻译（Huggingface/GitHub保持原名）
