# Speech Translation with Speech Foundation Models and Large Language Models: What is There and What is Missing?

链接: http://arxiv.org/abs/2402.12025v1

原文摘要:
The field of natural language processing (NLP) has recently witnessed a
transformative shift with the emergence of foundation models, particularly
Large Language Models (LLMs) that have revolutionized text-based NLP. This
paradigm has extended to other modalities, including speech, where researchers
are actively exploring the combination of Speech Foundation Models (SFMs) and
LLMs into single, unified models capable of addressing multimodal tasks. Among
such tasks, this paper focuses on speech-to-text translation (ST). By examining
the published papers on the topic, we propose a unified view of the
architectural solutions and training strategies presented so far, highlighting
similarities and differences among them. Based on this examination, we not only
organize the lessons learned but also show how diverse settings and evaluation
approaches hinder the identification of the best-performing solution for each
architectural building block and training choice. Lastly, we outline
recommendations for future works on the topic aimed at better understanding the
strengths and weaknesses of the SFM+LLM solutions for ST.

中文翻译:
自然语言处理（NLP）领域近期因基础模型的出现经历了革命性转变，尤其是彻底改变文本NLP范式的大型语言模型（LLMs）。这一范式已延伸至语音等其他模态，研究者正积极探索将语音基础模型（SFMs）与LLMs结合为统一的多模态任务处理模型。本文聚焦语音到文本翻译（ST）任务，通过系统梳理相关文献，对现有架构方案与训练策略提出统一视角，明晰其异同点。基于此分析，我们不仅整合了现有经验，更揭示了不同实验设置与评估方法如何阻碍对各架构模块及训练方案最优性能的判定。最后，针对未来研究方向提出建议，旨在更深入理解SFM+LLM解决方案在ST任务中的优势与局限。
