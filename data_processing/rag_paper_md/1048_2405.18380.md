# OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection for Memory-Efficient LLM Fine-tuning

链接: http://arxiv.org/abs/2405.18380v1

原文摘要:
The rapid advancements in Large Language Models (LLMs) have revolutionized
various natural language processing tasks. However, the substantial size of
LLMs presents significant challenges in training or fine-tuning. While
parameter-efficient approaches such as low-rank adaptation (LoRA) have gained
popularity, they often compromise performance compared to full-rank
fine-tuning. In this paper, we propose Outlier-weighed Layerwise Sampled
Low-Rank Projection (OwLore), a new memory-efficient fine-tuning approach,
inspired by the layerwise outlier distribution of LLMs. Unlike LoRA, which adds
extra adapters to all layers, OwLore strategically assigns higher sampling
probabilities to layers with more outliers, selectively sampling only a few
layers and fine-tuning their pre-trained weights. To further increase the
number of fine-tuned layers without a proportional rise in memory costs, we
incorporate gradient low-rank projection, further boosting the approach's
performance. Our extensive experiments across various architectures, including
LLaMa2, LLaMa3, and Mistral, demonstrate that OwLore consistently outperforms
baseline approaches, including full fine-tuning. Specifically, it achieves up
to a 1.1% average accuracy gain on the Commonsense Reasoning benchmark, a 3.0%
improvement on MMLU, and a notable 10% boost on MT-Bench, while being more
memory efficient. OwLore allows us to fine-tune LLaMa2-7B with only 21GB of
memory. Code is available at https://github.com/pixeli99/OwLore.

中文翻译:
大型语言模型（LLM）的快速发展彻底改变了各类自然语言处理任务。然而，LLM庞大的参数量给训练或微调带来了巨大挑战。虽然低秩自适应（LoRA）等参数高效方法广受欢迎，但其性能往往逊色于全秩微调。本文提出一种新型内存高效微调方法——基于异常值加权的分层采样低秩投影（OwLore），其设计灵感源自LLM的层级异常值分布特征。与LoRA在所有层添加适配器的策略不同，OwLore创新性地为异常值较多的层级分配更高采样概率，仅选择性采样部分层级并微调其预训练权重。为进一步增加可微调层数而不显著增加内存开销，我们引入梯度低秩投影技术，从而显著提升方法性能。通过对LLaMa2、LLaMa3和Mistral等架构的广泛实验表明，OwLore在多项基准测试中均优于基线方法（包括全参数微调）：在常识推理任务上平均准确率提升1.1%，MMLU基准提升3.0%，MT-Bench测试显著提高10%，同时保持更高内存效率。OwLore仅需21GB内存即可完成LLaMa2-7B的微调。代码已开源于https://github.com/pixeli99/OwLore。
