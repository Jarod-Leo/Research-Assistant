# Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models

链接: http://arxiv.org/abs/2402.04614v1

原文摘要:
Large Language Models (LLMs) are deployed as powerful tools for several
natural language processing (NLP) applications. Recent works show that modern
LLMs can generate self-explanations (SEs), which elicit their intermediate
reasoning steps for explaining their behavior. Self-explanations have seen
widespread adoption owing to their conversational and plausible nature.
However, there is little to no understanding of their faithfulness. In this
work, we discuss the dichotomy between faithfulness and plausibility in SEs
generated by LLMs. We argue that while LLMs are adept at generating plausible
explanations -- seemingly logical and coherent to human users -- these
explanations do not necessarily align with the reasoning processes of the LLMs,
raising concerns about their faithfulness. We highlight that the current trend
towards increasing the plausibility of explanations, primarily driven by the
demand for user-friendly interfaces, may come at the cost of diminishing their
faithfulness. We assert that the faithfulness of explanations is critical in
LLMs employed for high-stakes decision-making. Moreover, we emphasize the need
for a systematic characterization of faithfulness-plausibility requirements of
different real-world applications and ensure explanations meet those needs.
While there are several approaches to improving plausibility, improving
faithfulness is an open challenge. We call upon the community to develop novel
methods to enhance the faithfulness of self explanations thereby enabling
transparent deployment of LLMs in diverse high-stakes settings.

中文翻译:
大型语言模型（LLM）作为强大工具被应用于多种自然语言处理（NLP）任务中。近期研究表明，现代LLM能够生成自我解释（SE），通过展示其推理中间步骤来阐明行为机制。这类解释因其对话式表达与表面合理性而广受采纳，但其可信性却鲜有研究。本文探讨了LLM生成自我解释中可信度与表面合理性的二元对立：尽管模型擅长生成符合人类逻辑认知的合理解释，但这些解释往往与模型实际推理过程存在偏差，引发对解释真实性的质疑。我们指出当前以提高用户友好性为导向的解释优化趋势，可能以牺牲解释可信度为代价。强调在涉及高风险决策的LLM应用中，解释的可信度至关重要，并呼吁建立系统化标准来界定不同现实场景对可信度与合理性的具体要求。当前提升合理性的方法众多，但增强可信度仍是待解难题。我们倡议学界开发新方法提升自我解释的可信度，从而推动LLM在高风险领域的透明化部署。
