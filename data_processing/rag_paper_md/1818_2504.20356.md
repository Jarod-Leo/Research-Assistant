# What Causes Knowledge Loss in Multilingual Language Models?

链接: http://arxiv.org/abs/2504.20356v1

原文摘要:
Cross-lingual transfer in natural language processing (NLP) models enhances
multilingual performance by leveraging shared linguistic knowledge. However,
traditional methods that process all data simultaneously often fail to mimic
real-world scenarios, leading to challenges like catastrophic forgetting, where
fine-tuning on new tasks degrades performance on previously learned ones. Our
study explores this issue in multilingual contexts, focusing on linguistic
differences affecting representational learning rather than just model
parameters. We experiment with 52 languages using LoRA adapters of varying
ranks to evaluate non-shared, partially shared, and fully shared parameters.
Our aim is to see if parameter sharing through adapters can mitigate forgetting
while preserving prior knowledge. We find that languages using non-Latin
scripts are more susceptible to catastrophic forgetting, whereas those written
in Latin script facilitate more effective cross-lingual transfer.

中文翻译:
自然语言处理（NLP）模型中的跨语言迁移通过共享语言学知识来提升多语言性能。然而，传统方法同步处理所有数据往往难以模拟真实场景，导致诸如灾难性遗忘等挑战——即在新任务上微调会削弱模型对已学习任务的性能。本研究聚焦多语言环境下的这一问题，重点关注影响表征学习的语言差异而非仅模型参数。我们采用不同秩的LoRA适配器对52种语言进行实验，评估非共享、部分共享和完全共享参数的效果，旨在探究适配器参数共享能否在保留先验知识的同时缓解遗忘现象。研究发现：使用非拉丁文字的语言更易遭受灾难性遗忘，而拉丁文字书写的语言则能促进更有效的跨语言迁移。
