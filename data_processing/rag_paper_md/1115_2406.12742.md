# Benchmarking Multi-Image Understanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning

链接: http://arxiv.org/abs/2406.12742v1

原文摘要:
The advancement of large language models (LLMs) has significantly broadened
the scope of applications in natural language processing, with multi-modal LLMs
extending these capabilities to integrate and interpret visual data. However,
existing benchmarks for visual language models (VLMs) predominantly focus on
single-image inputs, neglecting the crucial aspect of multi-image
understanding. In this paper, we introduce a Multi-Image Relational Benchmark
MIRB, designed to evaluate VLMs' ability to compare, analyze, and reason across
multiple images. Our benchmark encompasses four categories: perception, visual
world knowledge, reasoning, and multi-hop reasoning. Through a comprehensive
evaluation of a wide range of open-source and closed-source models, we
demonstrate that while open-source VLMs were shown to approach the performance
of GPT-4V in single-image tasks, a significant performance gap remains in
multi-image reasoning tasks. Our findings also reveal that even the
state-of-the-art GPT-4V model struggles with our benchmark, underscoring the
need for further research and development in this area. We believe our
contribution of MIRB could serve as a testbed for developing the
next-generation multi-modal models.

中文翻译:
大语言模型（LLM）的进步显著拓宽了自然语言处理的应用范围，而多模态大语言模型进一步将这些能力扩展到视觉数据的整合与解读领域。然而，现有视觉语言模型（VLM）的评测基准主要针对单图像输入，忽视了多图像理解这一关键维度。本文提出多图像关系评测基准MIRB，旨在评估VLM在跨图像比较、分析和推理方面的能力。该基准涵盖感知、视觉世界知识、推理和多跳推理四大类别。通过对各类开源与闭源模型的系统评估，我们发现：尽管开源VLM在单图像任务中已接近GPT-4V的表现，但在多图像推理任务中仍存在显著差距。研究同时表明，即使是当前最先进的GPT-4V模型，在我们的基准测试中也面临挑战，这凸显了该领域进一步研发的必要性。我们相信MIRB的提出能为下一代多模态模型的开发提供重要测试平台。
