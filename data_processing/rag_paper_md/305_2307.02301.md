# Sumformer: Universal Approximation for Efficient Transformers

链接: http://arxiv.org/abs/2307.02301v1

原文摘要:
Natural language processing (NLP) made an impressive jump with the
introduction of Transformers. ChatGPT is one of the most famous examples,
changing the perception of the possibilities of AI even outside the research
community. However, besides the impressive performance, the quadratic time and
space complexity of Transformers with respect to sequence length pose
significant limitations for handling long sequences. While efficient
Transformer architectures like Linformer and Performer with linear complexity
have emerged as promising solutions, their theoretical understanding remains
limited. In this paper, we introduce Sumformer, a novel and simple architecture
capable of universally approximating equivariant sequence-to-sequence
functions. We use Sumformer to give the first universal approximation results
for Linformer and Performer. Moreover, we derive a new proof for Transformers,
showing that just one attention layer is sufficient for universal
approximation.

中文翻译:
随着Transformer模型的引入，自然语言处理（NLP）实现了令人瞩目的飞跃。ChatGPT作为最著名的范例之一，不仅改变了研究界对人工智能潜力的认知，更在社会层面引发了广泛关注。然而，尽管Transformer性能卓越，其随序列长度呈二次方增长的时间和空间复杂度，严重限制了长序列处理能力。虽然Linformer和Performer等具有线性复杂度的高效Transformer架构被视为有前景的解决方案，但相关理论研究仍显不足。本文提出Sumformer——一种新颖且简洁的通用架构，能够对等变序列到序列函数进行普适逼近。我们运用Sumformer首次为Linformer和Performer提供了普适逼近的理论证明。此外，通过新方法推导了Transformer的普适逼近性，证明仅需单层注意力机制即可实现这一目标。
