# Enhancing Embedding Performance through Large Language Model-based Text Enrichment and Rewriting

链接: http://arxiv.org/abs/2404.12283v1

原文摘要:
Embedding models are crucial for various natural language processing tasks
but can be limited by factors such as limited vocabulary, lack of context, and
grammatical errors. This paper proposes a novel approach to improve embedding
performance by leveraging large language models (LLMs) to enrich and rewrite
input text before the embedding process. By utilizing ChatGPT 3.5 to provide
additional context, correct inaccuracies, and incorporate metadata, the
proposed method aims to enhance the utility and accuracy of embedding models.
The effectiveness of this approach is evaluated on three datasets:
Banking77Classification, TwitterSemEval 2015, and Amazon Counter-factual
Classification. Results demonstrate significant improvements over the baseline
model on the TwitterSemEval 2015 dataset, with the best-performing prompt
achieving a score of 85.34 compared to the previous best of 81.52 on the
Massive Text Embedding Benchmark (MTEB) Leaderboard. However, performance on
the other two datasets was less impressive, highlighting the importance of
considering domain-specific characteristics. The findings suggest that
LLM-based text enrichment has shown promising results to improve embedding
performance, particularly in certain domains. Hence, numerous limitations in
the process of embedding can be avoided.

中文翻译:
嵌入模型在各类自然语言处理任务中至关重要，但其性能常受限于词汇量有限、语境缺失及语法错误等因素。本文提出一种创新方法，通过利用大语言模型（LLMs）在嵌入过程前对输入文本进行语义增强与重写来提升嵌入效果。该方法借助ChatGPT 3.5提供补充语境、修正错误信息并整合元数据，旨在增强嵌入模型的实用性与准确性。研究在Banking77Classification、TwitterSemEval 2015和Amazon反事实分类三个数据集上验证了该方法的有效性。结果显示，在TwitterSemEval 2015数据集上取得显著提升，最佳提示策略获得85.34分（较MTEB排行榜原最优成绩81.52分有显著突破）。然而另两个数据集的表现未达预期，凸显了领域特异性考量的重要性。研究表明基于LLM的文本增强技术能有效提升嵌入性能（尤其在特定领域），从而规避传统嵌入过程中的诸多局限性。
