# Boosting classification reliability of NLP transformer models in the long run

链接: http://arxiv.org/abs/2302.10016v1

原文摘要:
Transformer-based machine learning models have become an essential tool for
many natural language processing (NLP) tasks since the introduction of the
method. A common objective of these projects is to classify text data.
Classification models are often extended to a different topic and/or time
period. In these situations, deciding how long a classification is suitable for
and when it is worth re-training our model is difficult. This paper compares
different approaches to fine-tune a BERT model for a long-running
classification task. We use data from different periods to fine-tune our
original BERT model, and we also measure how a second round of annotation could
boost the classification quality. Our corpus contains over 8 million comments
on COVID-19 vaccination in Hungary posted between September 2020 and December
2021. Our results show that the best solution is using all available unlabeled
comments to fine-tune a model. It is not advisable to focus only on comments
containing words that our model has not encountered before; a more efficient
solution is randomly sample comments from the new period. Fine-tuning does not
prevent the model from losing performance but merely slows it down. In a
rapidly changing linguistic environment, it is not possible to maintain model
performance without regularly annotating new text.

中文翻译:
自Transformer方法问世以来，基于该架构的机器学习模型已成为众多自然语言处理（NLP）任务的核心工具。这类项目通常以文本数据分类为核心目标，分类模型常被迁移至不同主题或时间周期使用。在此类场景下，如何判定分类模型的适用周期及何时需要重新训练模型成为难题。本文比较了针对长期分类任务微调BERT模型的不同策略，利用不同时期的数据对原始BERT模型进行微调，并评估二次标注对分类质量的提升效果。我们的语料库包含2020年9月至2021年12月期间匈牙利社交媒体上超过800万条关于COVID-19疫苗接种的评论。研究结果表明：最佳解决方案是利用所有未标注评论进行模型微调；仅聚焦于模型未见过词汇的评论并非明智之选，更高效的做法是从新周期中随机抽样评论。微调虽能延缓模型性能衰减，但无法彻底阻止其退化。在语言环境快速变迁的情况下，唯有持续标注新文本才能维持模型性能。
