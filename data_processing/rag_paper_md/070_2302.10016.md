# Boosting classification reliability of NLP transformer models in the long run

链接: http://arxiv.org/abs/2302.10016v1

原文摘要:
Transformer-based machine learning models have become an essential tool for
many natural language processing (NLP) tasks since the introduction of the
method. A common objective of these projects is to classify text data.
Classification models are often extended to a different topic and/or time
period. In these situations, deciding how long a classification is suitable for
and when it is worth re-training our model is difficult. This paper compares
different approaches to fine-tune a BERT model for a long-running
classification task. We use data from different periods to fine-tune our
original BERT model, and we also measure how a second round of annotation could
boost the classification quality. Our corpus contains over 8 million comments
on COVID-19 vaccination in Hungary posted between September 2020 and December
2021. Our results show that the best solution is using all available unlabeled
comments to fine-tune a model. It is not advisable to focus only on comments
containing words that our model has not encountered before; a more efficient
solution is randomly sample comments from the new period. Fine-tuning does not
prevent the model from losing performance but merely slows it down. In a
rapidly changing linguistic environment, it is not possible to maintain model
performance without regularly annotating new text.

中文翻译:
基于Transformer架构的机器学习模型自问世以来，已成为自然语言处理（NLP）领域诸多任务的核心工具。这类项目通常以文本数据分类为主要目标。当分类模型需要迁移至不同主题或时间周期时，如何判定分类结果的时效性及模型再训练的恰当时机成为关键难题。本研究针对长期文本分类任务，系统比较了多种BERT模型微调策略。我们采用不同时段的语料对原始BERT模型进行微调，并评估二次标注对分类质量的提升效果。实验数据包含2020年9月至2021年12月期间匈牙利社交媒体上超过800万条新冠疫苗接种相关评论。

研究结果表明：最佳解决方案是利用所有未标注评论进行模型微调。仅聚焦于包含模型未登录词汇的评论并非明智之选，更高效的策略是从新时段随机抽样评论。微调虽能延缓模型性能衰减，但无法彻底阻止其退化。在语言环境快速演变的场景下，唯有持续进行新文本标注才能维持模型性能。
