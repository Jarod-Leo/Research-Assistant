# AHP-Powered LLM Reasoning for Multi-Criteria Evaluation of Open-Ended Responses

链接: http://arxiv.org/abs/2410.01246v1

原文摘要:
Question answering (QA) tasks have been extensively studied in the field of
natural language processing (NLP). Answers to open-ended questions are highly
diverse and difficult to quantify, and cannot be simply evaluated as correct or
incorrect, unlike close-ended questions with definitive answers. While large
language models (LLMs) have demonstrated strong capabilities across various
tasks, they exhibit relatively weaker performance in evaluating answers to
open-ended questions. In this study, we propose a method that leverages LLMs
and the analytic hierarchy process (AHP) to assess answers to open-ended
questions. We utilized LLMs to generate multiple evaluation criteria for a
question. Subsequently, answers were subjected to pairwise comparisons under
each criterion with LLMs, and scores for each answer were calculated in the
AHP. We conducted experiments on four datasets using both ChatGPT-3.5-turbo and
GPT-4. Our results indicate that our approach more closely aligns with human
judgment compared to the four baselines. Additionally, we explored the impact
of the number of criteria, variations in models, and differences in datasets on
the results.

中文翻译:
问答（QA）任务在自然语言处理（NLP）领域已得到广泛研究。开放式问题的答案具有高度多样性且难以量化，无法像有明确答案的封闭式问题那样简单地判断对错。尽管大语言模型（LLM）在各种任务中展现出强大能力，但其在评估开放式问题答案时表现相对较弱。本研究提出一种结合LLM和层次分析法（AHP）的开放式问题答案评估方法：首先利用LLM为问题生成多项评估标准，随后让答案在各标准下通过LLM进行两两比较，最终通过AHP计算各答案得分。我们使用ChatGPT-3.5-turbo和GPT-4在四个数据集上进行了实验，结果表明相较于四个基线方法，本方法更贴近人类评判结果。此外，我们还探究了标准数量、模型差异及数据集变化对结果的影响。
