# AHP-Powered LLM Reasoning for Multi-Criteria Evaluation of Open-Ended Responses

链接: http://arxiv.org/abs/2410.01246v1

原文摘要:
Question answering (QA) tasks have been extensively studied in the field of
natural language processing (NLP). Answers to open-ended questions are highly
diverse and difficult to quantify, and cannot be simply evaluated as correct or
incorrect, unlike close-ended questions with definitive answers. While large
language models (LLMs) have demonstrated strong capabilities across various
tasks, they exhibit relatively weaker performance in evaluating answers to
open-ended questions. In this study, we propose a method that leverages LLMs
and the analytic hierarchy process (AHP) to assess answers to open-ended
questions. We utilized LLMs to generate multiple evaluation criteria for a
question. Subsequently, answers were subjected to pairwise comparisons under
each criterion with LLMs, and scores for each answer were calculated in the
AHP. We conducted experiments on four datasets using both ChatGPT-3.5-turbo and
GPT-4. Our results indicate that our approach more closely aligns with human
judgment compared to the four baselines. Additionally, we explored the impact
of the number of criteria, variations in models, and differences in datasets on
the results.

中文翻译:
问题回答（QA）任务在自然语言处理（NLP）领域已得到广泛研究。与具有确定答案的封闭式问题不同，开放式问题的答案具有高度多样性且难以量化，无法简单地以正确或错误进行评判。尽管大语言模型（LLMs）在各种任务中展现出强大能力，但其在评估开放式问题答案时表现相对较弱。本研究提出一种结合LLMs和层次分析法（AHP）的开放式问题答案评估方法：首先利用LLMs为问题生成多项评估标准，随后让答案在各标准下通过LLMs进行两两比较，最终通过AHP计算各答案得分。我们使用ChatGPT-3.5-turbo和GPT-4在四个数据集上开展实验，结果表明相较于四个基线模型，本方法更接近人类评判结果。此外，我们还探究了标准数量、模型版本及数据集差异对结果的影响。

（注：翻译严格遵循以下技术规范：
1. 专业术语统一处理："LLMs"统一译为"大语言模型"，"AHP"首次出现标注全称"层次分析法"
2. 被动语态转化："answers were subjected to"译为主动式"让答案...进行"
3. 长句拆分：将原文复合句拆分为符合中文表达习惯的短句结构
4. 学术用语规范："baselines"译为"基线模型"，"pairwise comparisons"译为"两两比较"
5. 逻辑显化：通过冒号和分段明确方法步骤的递进关系
6. 数据呈现：模型名称保留英文代号"ChatGPT-3.5-turbo"，符合计算机领域惯例）
