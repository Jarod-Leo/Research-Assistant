# Advancing Reasoning in Large Language Models: Promising Methods and Approaches

链接: http://arxiv.org/abs/2502.03671v1

原文摘要:
Large Language Models (LLMs) have succeeded remarkably in various natural
language processing (NLP) tasks, yet their reasoning capabilities remain a
fundamental challenge. While LLMs exhibit impressive fluency and factual
recall, their ability to perform complex reasoning-spanning logical deduction,
mathematical problem-solving, commonsense inference, and multi-step
reasoning-often falls short of human expectations. This survey provides a
comprehensive review of emerging techniques enhancing reasoning in LLMs. We
categorize existing methods into key approaches, including prompting strategies
(e.g., Chain-of-Thought reasoning, Self-Consistency, and Tree-of-Thought
reasoning), architectural innovations (e.g., retrieval-augmented models,
modular reasoning networks, and neuro-symbolic integration), and learning
paradigms (e.g., fine-tuning with reasoning-specific datasets, reinforcement
learning, and self-supervised reasoning objectives). Additionally, we explore
evaluation frameworks used to assess reasoning in LLMs and highlight open
challenges, such as hallucinations, robustness, and reasoning generalization
across diverse tasks. By synthesizing recent advancements, this survey aims to
provide insights into promising directions for future research and practical
applications of reasoning-augmented LLMs.

中文翻译:
大型语言模型（LLMs）在各种自然语言处理（NLP）任务中取得了显著成功，但其推理能力仍面临根本性挑战。尽管LLMs展现出令人印象深刻的流畅性和事实记忆能力，但在复杂推理——包括逻辑演绎、数学问题求解、常识推断以及多步推理——方面的表现往往难以达到人类预期。本文系统综述了提升LLMs推理能力的新兴技术，将现有方法归纳为三大类：提示策略（如思维链推理、自洽性验证和思维树推理）、架构创新（如检索增强模型、模块化推理网络及神经符号集成）以及学习范式（如针对推理任务的数据微调、强化学习和自监督推理目标）。此外，我们探讨了评估LLMs推理能力的框架，并指出开放性挑战，包括幻觉问题、鲁棒性以及跨任务推理泛化能力。通过整合最新研究进展，本综述旨在为增强推理能力的LLMs未来研究方向与实际应用提供前瞻性见解。
