# RoBERTweet: A BERT Language Model for Romanian Tweets

链接: http://arxiv.org/abs/2306.06598v1

原文摘要:
Developing natural language processing (NLP) systems for social media
analysis remains an important topic in artificial intelligence research. This
article introduces RoBERTweet, the first Transformer architecture trained on
Romanian tweets. Our RoBERTweet comes in two versions, following the base and
large architectures of BERT. The corpus used for pre-training the models
represents a novelty for the Romanian NLP community and consists of all tweets
collected from 2008 to 2022. Experiments show that RoBERTweet models outperform
the previous general-domain Romanian and multilingual language models on three
NLP tasks with tweet inputs: emotion detection, sexist language identification,
and named entity recognition. We make our models and the newly created corpus
of Romanian tweets freely available.

中文翻译:
开发用于社交媒体分析的自然语言处理（NLP）系统仍是人工智能研究的重要课题。本文介绍了首个基于罗马尼亚语推文训练的Transformer架构——RoBERTweet。该模型遵循BERT的基础版与大型架构，推出两种版本。用于预训练的语料库作为罗马尼亚NLP领域的新资源，包含2008至2022年间采集的所有推文。实验表明，在涉及推文输入的三个NLP任务（情绪检测、性别歧视语言识别和命名实体识别）中，RoBERTweet模型性能优于先前罗马尼亚通用领域及多语言模型。我们公开了模型及新构建的罗马尼亚推文语料库。
