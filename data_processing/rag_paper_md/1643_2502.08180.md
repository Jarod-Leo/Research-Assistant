# Enhancing LLM Character-Level Manipulation via Divide and Conquer

链接: http://arxiv.org/abs/2502.08180v1

原文摘要:
Large Language Models (LLMs) have demonstrated strong generalization
capabilities across a wide range of natural language processing (NLP) tasks.
However, they exhibit notable weaknesses in character-level string
manipulation, struggling with fundamental operations such as character
deletion, insertion, and substitution. These challenges stem primarily from
tokenization constraints, despite the critical role of such operations in data
preprocessing and code generation. Through systematic analysis, we derive two
key insights: (1) LLMs face significant difficulties in leveraging intrinsic
token knowledge for character-level reasoning, and (2) atomized word structures
can substantially enhance LLMs' ability to process token-level structural
information. Building on these insights, we propose Character-Level
Manipulation via Divide and Conquer, a novel approach designed to bridge the
gap between token-level processing and character-level manipulation. Our method
decomposes complex operations into explicit character-level subtasks coupled
with controlled token reconstruction phases, leading to significant
improvements in accuracy. Without additional training, our method significantly
improves accuracies on the $\texttt{Deletion}$, $\texttt{Insertion}$, and
$\texttt{Substitution}$ tasks. To support further research, we open-source our
implementation and benchmarks.

中文翻译:
大型语言模型（LLMs）在各类自然语言处理（NLP）任务中展现出强大的泛化能力，但在字符级字符串操作方面存在显著缺陷——难以完成字符删除、插入和替换等基础操作。尽管这些操作在数据预处理和代码生成中至关重要，其执行障碍主要源于分词机制的局限性。通过系统分析，我们获得两项关键发现：（1）LLMs难以利用内在分词知识进行字符级推理；（2）原子化的词汇结构能显著增强模型处理分词层级结构信息的能力。基于这些发现，我们提出"分治式字符级操作"新方法，旨在弥合分词处理与字符操作之间的鸿沟。该方法将复杂操作分解为显式的字符级子任务，并配合受控的分词重构阶段，从而显著提升操作精度。在无需额外训练的情况下，我们的方法使$\texttt{删除}$、$\texttt{插入}$和$\texttt{替换}$任务的准确率获得大幅提升。为促进后续研究，我们公开了实现代码与基准测试集。

（翻译说明：采用学术论文的简洁风格，保留专业术语如"tokenization"译为"分词机制"；处理长句时通过破折号和分号维持原文逻辑层次；技术术语$\texttt{Deletion}$等保留代码格式；"Divide and Conquer"译为计算机领域通用译法"分治"；通过"鸿沟""显著提升"等措辞保持原文论证力度；最后声明采用国内学术圈惯用的"公开"表述）
