# Enhancing LLM Character-Level Manipulation via Divide and Conquer

链接: http://arxiv.org/abs/2502.08180v1

原文摘要:
Large Language Models (LLMs) have demonstrated strong generalization
capabilities across a wide range of natural language processing (NLP) tasks.
However, they exhibit notable weaknesses in character-level string
manipulation, struggling with fundamental operations such as character
deletion, insertion, and substitution. These challenges stem primarily from
tokenization constraints, despite the critical role of such operations in data
preprocessing and code generation. Through systematic analysis, we derive two
key insights: (1) LLMs face significant difficulties in leveraging intrinsic
token knowledge for character-level reasoning, and (2) atomized word structures
can substantially enhance LLMs' ability to process token-level structural
information. Building on these insights, we propose Character-Level
Manipulation via Divide and Conquer, a novel approach designed to bridge the
gap between token-level processing and character-level manipulation. Our method
decomposes complex operations into explicit character-level subtasks coupled
with controlled token reconstruction phases, leading to significant
improvements in accuracy. Without additional training, our method significantly
improves accuracies on the $\texttt{Deletion}$, $\texttt{Insertion}$, and
$\texttt{Substitution}$ tasks. To support further research, we open-source our
implementation and benchmarks.

中文翻译:
大型语言模型（LLMs）在各类自然语言处理（NLP）任务中展现出强大的泛化能力，但在字符级字符串操作方面存在明显缺陷，难以完成字符删除、插入和替换等基础操作。尽管这些操作在数据预处理和代码生成中至关重要，其挑战主要源于分词机制的限制。通过系统分析，我们得出两个关键发现：（1）LLMs难以利用内在分词知识进行字符级推理；（2）原子化的词汇结构能显著增强模型处理分词层级结构信息的能力。基于此，我们提出"分治式字符级操作"方法，通过将复杂操作分解为显式的字符级子任务并结合可控的分词重构阶段，有效弥合分词处理与字符操作间的鸿沟。该方法无需额外训练即可在$\texttt{删除}$、$\texttt{插入}$和$\texttt{替换}$任务上实现准确率的大幅提升。为促进相关研究，我们已开源实现代码与基准测试集。
