# Landmark Attention: Random-Access Infinite Context Length for Transformers

链接: http://arxiv.org/abs/2305.16300v1

原文摘要:
While Transformers have shown remarkable success in natural language
processing, their attention mechanism's large memory requirements have limited
their ability to handle longer contexts. Prior approaches, such as recurrent
memory or retrieval-based augmentation, have either compromised the
random-access flexibility of attention (i.e., the capability to select any
token in the entire context) or relied on separate mechanisms for relevant
context retrieval, which may not be compatible with the model's attention. In
this paper, we present a novel approach that allows access to the complete
context while retaining random-access flexibility, closely resembling running
attention on the entire context. Our method uses a landmark token to represent
each block of the input and trains the attention to use it for selecting
relevant blocks, enabling retrieval of blocks directly through the attention
mechanism instead of by relying on a separate mechanism. Our approach
seamlessly integrates with specialized data structures and the system's memory
hierarchy, enabling processing of arbitrarily long context lengths. We
demonstrate that our method can obtain comparable performance with
Transformer-XL while significantly reducing the number of retrieved tokens in
each step. Finally, we show that fine-tuning LLaMA 7B with our method
successfully extends its context length capacity to over 32k tokens, allowing
for inference at the context lengths of GPT-4. We release the implementation of
landmark attention and the code to reproduce our experiments at
https://github.com/epfml/landmark-attention/.

中文翻译:
尽管Transformer模型在自然语言处理领域取得了显著成功，但其注意力机制对内存的高需求限制了处理长上下文的能力。现有方法如循环记忆或基于检索的增强技术，要么牺牲了注意力机制的随机访问灵活性（即无法自由选择上下文中的任意标记），要么依赖与模型注意力不兼容的独立检索机制。本文提出一种创新方案，既能保留完整的上下文访问能力，又维持了随机访问的灵活性，其效果近似于在整个上下文上运行注意力机制。

我们的方法通过引入地标标记（landmark token）来表征输入文本块，训练注意力机制利用这些标记选择相关文本块，从而直接通过注意力机制而非独立机制实现块检索。该方案能够无缝集成专用数据结构和系统内存层级，支持任意长度上下文的处理。实验表明，在保持与Transformer-XL相当性能的同时，我们的方法显著减少了每一步骤需要检索的标记数量。最终，我们通过对LLaMA 7B模型进行微调，成功将其上下文处理能力扩展至超过32k标记，达到与GPT-4相当的推理上下文长度。

相关实现代码与实验复现资源已发布于https://github.com/epfml/landmark-attention/。
