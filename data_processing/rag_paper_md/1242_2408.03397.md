# HeTraX: Energy Efficient 3D Heterogeneous Manycore Architecture for Transformer Acceleration

链接: http://arxiv.org/abs/2408.03397v1

原文摘要:
Transformers have revolutionized deep learning and generative modeling to
enable unprecedented advancements in natural language processing tasks and
beyond. However, designing hardware accelerators for executing transformer
models is challenging due to the wide variety of computing kernels involved in
the transformer architecture. Existing accelerators are either inadequate to
accelerate end-to-end transformer models or suffer notable thermal limitations.
In this paper, we propose the design of a three-dimensional heterogeneous
architecture referred to as HeTraX specifically optimized to accelerate
end-to-end transformer models. HeTraX employs hardware resources aligned with
the computational kernels of transformers and optimizes both performance and
energy. Experimental results show that HeTraX outperforms existing
state-of-the-art by up to 5.6x in speedup and improves EDP by 14.5x while
ensuring thermally feasibility.

中文翻译:
Transformer模型已彻底革新深度学习与生成式建模，在自然语言处理等任务中实现了前所未有的突破。然而，由于Transformer架构涉及多样化的计算核心，为其设计硬件加速器面临巨大挑战。现有加速器要么难以支持端到端Transformer模型的加速需求，要么存在显著的热限制问题。本文提出一种名为HeTraX的三维异构架构设计方案，专门针对端到端Transformer模型进行优化加速。该架构通过配置与Transformer计算核心相匹配的硬件资源，在性能与能效方面实现双重优化。实验结果表明，HeTraX相较现有最优方案最高可获得5.6倍加速比，能量延迟积（EDP）提升达14.5倍，同时确保热可行性。
