# Simple Techniques for Enhancing Sentence Embeddings in Generative Language Models

链接: http://arxiv.org/abs/2404.03921v1

原文摘要:
Sentence Embedding stands as a fundamental task within the realm of Natural
Language Processing, finding extensive application in search engines, expert
systems, and question-and-answer platforms. With the continuous evolution of
large language models such as LLaMA and Mistral, research on sentence embedding
has recently achieved notable breakthroughs. However, these advancements mainly
pertain to fine-tuning scenarios, leaving explorations into computationally
efficient direct inference methods for sentence representation in a nascent
stage. This paper endeavors to bridge this research gap. Through comprehensive
experimentation, we challenge the widely held belief in the necessity of an
Explicit One-word Limitation for deriving sentence embeddings from Pre-trained
Language Models (PLMs). We demonstrate that this approach, while beneficial for
generative models under direct inference scenario, is not imperative for
discriminative models or the fine-tuning of generative PLMs. This discovery
sheds new light on the design of manual templates in future studies. Building
upon this insight, we propose two innovative prompt engineering techniques
capable of further enhancing the expressive power of PLMs' raw embeddings:
Pretended Chain of Thought and Knowledge Enhancement. We confirm their
effectiveness across various PLM types and provide a detailed exploration of
the underlying factors contributing to their success.

中文翻译:
句子嵌入作为自然语言处理领域的一项基础任务，在搜索引擎、专家系统和问答平台中具有广泛应用。随着LLaMA、Mistral等大语言模型的持续演进，句子嵌入研究近期取得了显著突破。然而这些进展主要集中于微调场景，对于计算高效的直接推理式句子表征方法探索仍处于初级阶段。本文致力于填补这一研究空白。通过全面实验，我们挑战了从预训练语言模型获取句子嵌入必须采用"显式单字限制"的普遍认知，证明该方法虽有利于直接推理场景下的生成模型，但对判别式模型或生成式预训练模型的微调并非必要。这一发现为未来研究中人工模板的设计提供了新思路。基于此，我们提出两种能进一步提升预训练模型原始嵌入表现力的创新提示工程技术：伪装思维链与知识增强。我们验证了其在各类预训练模型中的有效性，并深入探究了其成功背后的关键因素。
