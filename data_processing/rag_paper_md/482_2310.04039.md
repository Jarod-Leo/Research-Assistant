# Analysis of the Reasoning with Redundant Information Provided Ability of Large Language Models

链接: http://arxiv.org/abs/2310.04039v1

原文摘要:
Recent advancements in Large Language Models (LLMs) have demonstrated
impressive capabilities across a range of natural language processing tasks,
especially in reasoning, a cornerstone for achieving Artificial General
Intelligence (AGI). However, commonly used benchmarks may not fully encapsulate
the inferential abilities of these models in real-world scenarios. To address
this gap, a new form of Question-Answering (QA) task, termed Reasoning with
Redundant Information Provided (RRIP), is introduced. The study designed a
modified version of the grade school math 8K (GSM-8K) dataset which has several
variants focusing on different attributes of redundant information. This
investigation evaluates two popular LLMs, LlaMA2-13B-chat and generative
pre-trained transformer 3.5 (GPT-3.5), contrasting their performance on
traditional QA tasks against the RRIP tasks. Findings indicate that while these
models achieved moderate success on standard QA benchmarks, their performance
notably declines when assessed on RRIP tasks. The study not only highlights the
limitations of current LLMs in handling redundant information but also suggests
that future training of these models should focus on incorporating redundant
information into the training data to increase the performance on RRIP tasks.

中文翻译:
近期，大型语言模型（LLMs）在自然语言处理任务中展现出卓越能力，尤其在作为实现通用人工智能（AGI）核心要素的推理领域表现突出。然而，现有基准测试可能无法全面反映这些模型在真实场景中的推理能力。为此，本研究提出了一种新型问答任务范式——冗余信息推理（RRIP），并基于GSM-8K小学数学数据集设计出多个聚焦冗余信息不同属性的变体版本。通过对比评估LlaMA2-13B-chat和生成式预训练变换模型3.5（GPT-3.5）在传统问答任务与RRIP任务中的表现，研究发现：尽管两类模型在标准问答基准上取得中等成功率，但其RRIP任务表现均出现显著下降。该成果不仅揭示了当前LLMs处理冗余信息的局限性，更指出未来模型训练应注重将冗余信息纳入训练数据以提升RRIP任务性能。
