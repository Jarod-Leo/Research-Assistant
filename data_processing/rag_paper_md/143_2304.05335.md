# Toxicity in ChatGPT: Analyzing Persona-assigned Language Models

链接: http://arxiv.org/abs/2304.05335v1

原文摘要:
Large language models (LLMs) have shown incredible capabilities and
transcended the natural language processing (NLP) community, with adoption
throughout many services like healthcare, therapy, education, and customer
service. Since users include people with critical information needs like
students or patients engaging with chatbots, the safety of these systems is of
prime importance. Therefore, a clear understanding of the capabilities and
limitations of LLMs is necessary. To this end, we systematically evaluate
toxicity in over half a million generations of ChatGPT, a popular
dialogue-based LLM. We find that setting the system parameter of ChatGPT by
assigning it a persona, say that of the boxer Muhammad Ali, significantly
increases the toxicity of generations. Depending on the persona assigned to
ChatGPT, its toxicity can increase up to 6x, with outputs engaging in incorrect
stereotypes, harmful dialogue, and hurtful opinions. This may be potentially
defamatory to the persona and harmful to an unsuspecting user. Furthermore, we
find concerning patterns where specific entities (e.g., certain races) are
targeted more than others (3x more) irrespective of the assigned persona, that
reflect inherent discriminatory biases in the model. We hope that our findings
inspire the broader AI community to rethink the efficacy of current safety
guardrails and develop better techniques that lead to robust, safe, and
trustworthy AI systems.

中文翻译:
大型语言模型（LLMs）展现出非凡的能力，其影响力已超越自然语言处理（NLP）领域，广泛应用于医疗保健、心理治疗、教育及客服等多个服务场景。由于用户群体包含学生、患者等对信息准确性要求极高的聊天机器人使用者，这类系统的安全性至关重要。因此，必须清晰认识LLMs的能力边界与局限。为此，我们系统评估了基于对话的热门LLM——ChatGPT超过50万条生成内容中的毒性现象。研究发现，通过赋予ChatGPT特定人设（如拳王穆罕默德·阿里）来设置系统参数，会显著增加生成内容的毒性。根据所分配人设的不同，毒性水平最高可激增6倍，表现为输出内容包含错误刻板印象、有害对话及伤害性观点，既可能对设定人设构成诽谤，也对毫无戒备的用户造成伤害。更值得警惕的是，无论分配何种人设，模型始终存在针对特定群体（如某些种族）的歧视性偏见——其被攻击频率可达其他群体的3倍，这反映出模型固有的偏见问题。我们期待这些发现能促使更广泛的AI社群反思现有安全防护机制的有效性，进而开发出更健壮、安全、可信的AI系统技术。
