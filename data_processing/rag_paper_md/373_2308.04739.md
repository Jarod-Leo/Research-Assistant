# Optimizing a Transformer-based network for a deep learning seismic processing workflow

链接: http://arxiv.org/abs/2308.04739v1

原文摘要:
StorSeismic is a recently introduced model based on the Transformer to adapt
to various seismic processing tasks through its pretraining and fine-tuning
training strategy. In the original implementation, StorSeismic utilized a
sinusoidal positional encoding and a conventional self-attention mechanism,
both borrowed from the natural language processing (NLP) applications. For
seismic processing they admitted good results, but also hinted to limitations
in efficiency and expressiveness. We propose modifications to these two key
components, by utilizing relative positional encoding and low-rank attention
matrices as replacements to the vanilla ones. The proposed changes are tested
on processing tasks applied to a realistic Marmousi and offshore field data as
a sequential strategy, starting from denoising, direct arrival removal,
multiple attenuation, and finally root-mean-squared velocity ($V_{RMS}$)
prediction for normal moveout (NMO) correction. We observe faster pretraining
and competitive results on the fine-tuning tasks and, additionally, fewer
parameters to train compared to the vanilla model.

中文翻译:
StorSeismic是一种基于Transformer架构的新型模型，其通过预训练与微调相结合的策略适应多种地震数据处理任务。在原始实现中，该模型沿用了自然语言处理领域的正弦位置编码和常规自注意力机制，虽在地震数据处理中表现良好，但存在效率与表达能力方面的局限性。本研究针对这两个核心组件提出改进方案：采用相对位置编码替代原始位置编码，使用低秩注意力矩阵替代标准注意力机制。改进后的模型在Marmousi理论模型和海上实际资料的处理任务链中进行了系统验证，依次完成去噪、直达波压制、多次波衰减及均方根速度（$V_{RMS}$）预测（用于动校正）等任务。实验结果表明：相较于原版模型，改进方案在保持微调任务性能竞争力的同时，实现了更快的预训练速度，且所需训练参数更少。
