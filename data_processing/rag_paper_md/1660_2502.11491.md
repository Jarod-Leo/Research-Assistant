# Ontology-Guided Reverse Thinking Makes Large Language Models Stronger on Knowledge Graph Question Answering

链接: http://arxiv.org/abs/2502.11491v1

原文摘要:
Large language models (LLMs) have shown remarkable capabilities in natural
language processing. However, in knowledge graph question answering tasks
(KGQA), there remains the issue of answering questions that require multi-hop
reasoning. Existing methods rely on entity vector matching, but the purpose of
the question is abstract and difficult to match with specific entities. As a
result, it is difficult to establish reasoning paths to the purpose, which
leads to information loss and redundancy. To address this issue, inspired by
human reverse thinking, we propose Ontology-Guided Reverse Thinking (ORT), a
novel framework that constructs reasoning paths from purposes back to
conditions. ORT operates in three key phases: (1) using LLM to extract purpose
labels and condition labels, (2) constructing label reasoning paths based on
the KG ontology, and (3) using the label reasoning paths to guide knowledge
retrieval. Experiments on the WebQSP and CWQ datasets show that ORT achieves
state-of-the-art performance and significantly enhances the capability of LLMs
for KGQA.

中文翻译:
大语言模型（LLM）在自然语言处理领域展现出卓越能力，但在知识图谱问答任务（KGQA）中，针对需要多跳推理的问题仍存在解答困难。现有方法依赖实体向量匹配，但问题目的具有抽象性，难以与具体实体匹配，导致无法建立通往目的的推理路径，从而引发信息丢失与冗余。受人类逆向思维启发，我们提出本体引导逆向推理框架（ORT），通过从目的回溯条件的创新方法构建推理路径。ORT包含三个关键阶段：（1）利用LLM提取目的标签与条件标签；（2）基于知识图谱本体构建标签推理路径；（3）以标签路径指导知识检索。在WebQSP和CWQ数据集上的实验表明，ORT实现了最先进性能，显著提升了LLM在KGQA任务中的表现。
