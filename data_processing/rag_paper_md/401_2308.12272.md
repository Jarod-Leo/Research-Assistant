# Simple is Better and Large is Not Enough: Towards Ensembling of Foundational Language Models

链接: http://arxiv.org/abs/2308.12272v1

原文摘要:
Foundational Language Models (FLMs) have advanced natural language processing
(NLP) research. Current researchers are developing larger FLMs (e.g., XLNet,
T5) to enable contextualized language representation, classification, and
generation. While developing larger FLMs has been of significant advantage, it
is also a liability concerning hallucination and predictive uncertainty.
Fundamentally, larger FLMs are built on the same foundations as smaller FLMs
(e.g., BERT); hence, one must recognize the potential of smaller FLMs which can
be realized through an ensemble. In the current research, we perform a reality
check on FLMs and their ensemble on benchmark and real-world datasets. We
hypothesize that the ensembling of FLMs can influence the individualistic
attention of FLMs and unravel the strength of coordination and cooperation of
different FLMs. We utilize BERT and define three other ensemble techniques:
{Shallow, Semi, and Deep}, wherein the Deep-Ensemble introduces a
knowledge-guided reinforcement learning approach. We discovered that the
suggested Deep-Ensemble BERT outperforms its large variation i.e. BERTlarge, by
a factor of many times using datasets that show the usefulness of NLP in
sensitive fields, such as mental health.

中文翻译:
基础语言模型（FLMs）推动了自然语言处理（NLP）研究的进步。当前研究者正开发更大型的FLMs（如XLNet、T5），以实现情境化语言表征、分类与生成。尽管扩大模型规模具有显著优势，但也存在幻觉和预测不确定性等弊端。本质上，大型FLMs与小型FLMs（如BERT）建立在相同基础之上，因此必须认识到通过集成方法可实现小型FLMs的潜力。本研究对基准数据集和真实场景数据集中的FLMs及其集成效果进行了现实检验。我们提出假设：FLMs的集成能改变其个体注意力机制，并释放不同FLMs协同合作的潜力。基于BERT模型，我们定义了三种集成技术——浅层集成、半深度集成和深度集成，其中深度集成引入了知识引导的强化学习方法。研究发现，在心理健康等敏感领域体现NLP实用价值的数据集上，所提出的深度集成BERT模型性能超越其大型变体BERTlarge数倍之多。
