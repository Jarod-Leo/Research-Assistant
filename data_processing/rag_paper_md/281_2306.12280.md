# SIFTER: A Task-specific Alignment Strategy for Enhancing Sentence Embeddings

链接: http://arxiv.org/abs/2306.12280v1

原文摘要:
The paradigm of pre-training followed by fine-tuning on downstream tasks has
become the mainstream method in natural language processing tasks. Although
pre-trained models have the advantage of generalization, their performance may
still vary significantly across different domain tasks. This is because the
data distribution in different domains varies. For example, the different parts
of the sentence 'He married Smt. Dipali Ghosh in 1947 and led a very happy
married life' may have different impact for downstream tasks. For similarity
calculations, words such as 'led' and 'life' are more important. On the other
hand, for sentiment analysis, the word 'happy' is crucial. This indicates that
different downstream tasks have different levels of sensitivity to sentence
components. Our starting point is to scale information of the model and data
according to the specifics of downstream tasks, enhancing domain information of
relevant parts for these tasks and reducing irrelevant elements for different
domain tasks, called SIFTER. In the experimental part, we use the SIFTER to
improve SimCSE by constructing positive sample pairs based on enhancing the
sentence stem and reducing the unimportant components in the sentence, and
maximize the similarity between three sentences. Similarly, SIFTER can improve
the gate mechanism of the LSTM model by short-circuiting the input gate of
important words so that the LSTM model remembers the important parts of the
sentence. Our experiments demonstrate that SIFTER outperforms the SimCSE and
LSTM baselines.

中文翻译:
预训练后在下游任务微调的模式已成为自然语言处理任务的主流方法。尽管预训练模型具备泛化优势，但其在不同领域任务中的表现仍可能存在显著差异。这是因为不同领域的数据分布存在差异。例如，在句子"He married Smt. Dipali Ghosh in 1947 and led a very happy married life"中，不同成分对下游任务的影响各异：对于相似度计算，"led"和"life"等词汇更为关键；而在情感分析任务中，"happy"一词则具有决定性作用。这表明不同下游任务对句子成分的敏感度存在差异。

本研究提出SIFTER方法，其核心思想是根据下游任务特性对模型和数据信息进行动态调整：增强任务相关部分的领域信息，弱化不同领域任务中的无关要素。在实验部分，我们通过SIFTER改进SimCSE模型——基于强化句子主干和弱化非重要成分构建正样本对，并最大化三个句子间的相似度。同样地，SIFTER可通过短路重要词汇的输入门来优化LSTM模型的门控机制，使模型更好地记忆句子关键部分。实验结果表明，SIFTER在SimCSE和LSTM基线模型上均取得了更优的性能表现。
