# Beyond the Answers: Reviewing the Rationality of Multiple Choice Question Answering for the Evaluation of Large Language Models

链接: http://arxiv.org/abs/2402.01349v1

原文摘要:
In the field of NLP, Large Language Models (LLMs) have markedly enhanced
performance across a variety of tasks. However, the comprehensive evaluation of
LLMs remains an inevitable challenge for the community. Recently, the adoption
of Multiple Choice Question Answering (MCQA) as a benchmark for assessing LLMs
has gained considerable traction. However, concerns regarding the robustness of
this evaluative method persist. Building upon previous discussions on the issue
of \textit{variability}, we reveal an additional dimension of concern: LLMs may
perform MCQA by selecting the least incorrect option rather than distinctly
correct. This observation suggests that LLMs might regard multiple options as
correct, which could undermine the reliability of MCQA as a metric for
evaluating LLMs. To address this challenge, we introduce an enhanced dataset
augmentation method for MCQA, termed MCQA+, to provide a more accurate
reflection of the model performance, thereby highlighting the necessity for
more sophisticated evaluation mechanisms in the assessment of LLM capabilities.

中文翻译:
在自然语言处理（NLP）领域，大语言模型（LLMs）已显著提升了各类任务的性能表现。然而，如何全面评估LLMs仍是学界不可避免的挑战。近年来，采用多项选择题问答（MCQA）作为LLMs评估基准的方法获得广泛关注，但该评估体系的稳健性仍存争议。基于先前关于"选项可变性"问题的讨论，本研究揭示了更深层的隐忧：LLMs可能通过选择"错误程度最低"而非"明确正确"的选项来完成MCQA任务。这一现象表明LLMs可能将多个选项视为正确答案，进而动摇MCQA作为评估指标的可靠性。为应对这一挑战，我们提出MCQA+——一种改进的MCQA数据集增强方法，以期更精准反映模型性能，由此揭示在LLM能力评估中构建更精密评估机制的必要性。

（翻译说明：
1. 专业术语采用学界通用译法，如"NLP"译作"自然语言处理"、"LLMs"保留英文缩写并首次出现时标注全称
2. 关键概念如"variability"根据上下文意译为"选项可变性"，并通过引号强调
3. 被动语态转换为中文主动句式（如"concerns persist"处理为"仍存争议"）
4. 长难句进行合理切分，如原文最后复合句拆分为三个中文短句
5. 技术术语"MCQA+"保留英文形式并添加破折号说明
6. 学术表述风格统一，如"reveal"译为"揭示"而非"发现"以保持严谨性）
