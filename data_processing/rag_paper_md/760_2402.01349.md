# Beyond the Answers: Reviewing the Rationality of Multiple Choice Question Answering for the Evaluation of Large Language Models

链接: http://arxiv.org/abs/2402.01349v1

原文摘要:
In the field of NLP, Large Language Models (LLMs) have markedly enhanced
performance across a variety of tasks. However, the comprehensive evaluation of
LLMs remains an inevitable challenge for the community. Recently, the adoption
of Multiple Choice Question Answering (MCQA) as a benchmark for assessing LLMs
has gained considerable traction. However, concerns regarding the robustness of
this evaluative method persist. Building upon previous discussions on the issue
of \textit{variability}, we reveal an additional dimension of concern: LLMs may
perform MCQA by selecting the least incorrect option rather than distinctly
correct. This observation suggests that LLMs might regard multiple options as
correct, which could undermine the reliability of MCQA as a metric for
evaluating LLMs. To address this challenge, we introduce an enhanced dataset
augmentation method for MCQA, termed MCQA+, to provide a more accurate
reflection of the model performance, thereby highlighting the necessity for
more sophisticated evaluation mechanisms in the assessment of LLM capabilities.

中文翻译:
在自然语言处理（NLP）领域，大语言模型（LLMs）显著提升了各类任务的性能表现。然而，如何全面评估这些模型仍是学界无法回避的挑战。近年来，采用多项选择题问答（MCQA）作为LLMs评估基准的做法获得了广泛关注，但该评测方法的可靠性仍存疑虑。基于先前关于"选项可变性"问题的讨论，本研究揭示了更深层的隐忧：LLMs可能通过选择错误程度最低的选项而非明确正确的答案来完成MCQA任务。这一现象表明模型可能将多个选项视为正确答案，从而动摇MCQA作为LLMs评估指标的可靠性。为应对这一挑战，我们提出MCQA+这一增强型数据集扩增方法，以期更精准地反映模型性能，进而证明在LLM能力评估中建立更精密评测机制的必要性。
