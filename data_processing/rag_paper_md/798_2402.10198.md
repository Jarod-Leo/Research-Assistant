# Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention

链接: http://arxiv.org/abs/2402.10198v1

原文摘要:
Transformer-based architectures achieved breakthrough performance in natural
language processing and computer vision, yet they remain inferior to simpler
linear baselines in multivariate long-term forecasting. To better understand
this phenomenon, we start by studying a toy linear forecasting problem for
which we show that transformers are incapable of converging to their true
solution despite their high expressive power. We further identify the attention
of transformers as being responsible for this low generalization capacity.
Building upon this insight, we propose a shallow lightweight transformer model
that successfully escapes bad local minima when optimized with sharpness-aware
optimization. We empirically demonstrate that this result extends to all
commonly used real-world multivariate time series datasets. In particular,
SAMformer surpasses current state-of-the-art methods and is on par with the
biggest foundation model MOIRAI while having significantly fewer parameters.
The code is available at https://github.com/romilbert/samformer.

中文翻译:
基于Transformer的架构在自然语言处理和计算机视觉领域取得了突破性性能，但在多元长期预测任务中仍逊色于更简单的线性基线模型。为深入理解这一现象，我们首先研究了一个玩具线性预测问题，结果表明尽管Transformer具备强大的表达能力，却无法收敛到真实解。我们进一步发现其注意力机制是导致泛化能力低下的关键原因。基于这一发现，我们提出了一种轻量级浅层Transformer模型——SAMformer，该模型在采用锐度感知优化训练时能成功逃离不良局部极小值。实证研究表明，这一优势可推广至所有常用现实世界多元时间序列数据集。特别值得注意的是，SAMformer不仅超越了当前最先进方法，其性能更可与参数量庞大的基础模型MOIRAI相媲美，而所需参数数量却显著减少。代码已开源：https://github.com/romilbert/samformer。

（翻译说明：采用学术论文摘要的标准表述方式，通过以下处理确保专业性和可读性：
1. 专业术语准确对应："sharpness-aware optimization"译为"锐度感知优化"
2. 长句拆分重组：将原文复合句按中文习惯分解为多个短句
3. 被动语态转化："it is shown that"转为主动式"结果表明"
4. 概念显化处理："toy problem"译为"玩具问题"并增补"模型"使指代明确
5. 逻辑连接强化：使用"基于这一发现"等短语保持论证链条清晰
6. 数据呈现规范：完整保留技术术语、模型名称和URL信息）
