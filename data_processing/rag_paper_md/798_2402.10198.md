# Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention

链接: http://arxiv.org/abs/2402.10198v1

原文摘要:
Transformer-based architectures achieved breakthrough performance in natural
language processing and computer vision, yet they remain inferior to simpler
linear baselines in multivariate long-term forecasting. To better understand
this phenomenon, we start by studying a toy linear forecasting problem for
which we show that transformers are incapable of converging to their true
solution despite their high expressive power. We further identify the attention
of transformers as being responsible for this low generalization capacity.
Building upon this insight, we propose a shallow lightweight transformer model
that successfully escapes bad local minima when optimized with sharpness-aware
optimization. We empirically demonstrate that this result extends to all
commonly used real-world multivariate time series datasets. In particular,
SAMformer surpasses current state-of-the-art methods and is on par with the
biggest foundation model MOIRAI while having significantly fewer parameters.
The code is available at https://github.com/romilbert/samformer.

中文翻译:
基于Transformer的架构在自然语言处理和计算机视觉领域取得了突破性表现，但在多元长期预测任务中仍逊色于更简单的线性基线模型。为深入理解这一现象，我们首先研究了一个玩具线性预测问题，结果表明尽管Transformer具备强大的表达能力，却无法收敛到真实解。我们进一步发现其注意力机制是导致泛化能力低下的关键原因。基于这一发现，我们提出了一种浅层轻量级Transformer模型，该模型通过锐度感知优化成功避开了不良局部极小值。实证研究表明，这一成果可推广至所有常用真实世界多元时间序列数据集。特别值得注意的是，SAMformer不仅超越了当前最先进方法，其性能更可与参数量庞大的基础模型MOIRAI相媲美，同时保持了显著精简的参数量级。代码已开源在https://github.com/romilbert/samformer。
