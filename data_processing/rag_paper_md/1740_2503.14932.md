# Prada: Black-Box LLM Adaptation with Private Data on Resource-Constrained Devices

链接: http://arxiv.org/abs/2503.14932v1

原文摘要:
In recent years, Large Language Models (LLMs) have demonstrated remarkable
abilities in various natural language processing tasks. However, adapting these
models to specialized domains using private datasets stored on
resource-constrained edge devices, such as smartphones and personal computers,
remains challenging due to significant privacy concerns and limited
computational resources. Existing model adaptation methods either compromise
data privacy by requiring data transmission or jeopardize model privacy by
exposing proprietary LLM parameters. To address these challenges, we propose
Prada, a novel privacy-preserving and efficient black-box LLM adaptation system
using private on-device datasets. Prada employs a lightweight proxy model
fine-tuned with Low-Rank Adaptation (LoRA) locally on user devices. During
inference, Prada leverages the logits offset, i.e., difference in outputs
between the base and adapted proxy models, to iteratively refine outputs from a
remote black-box LLM. This offset-based adaptation approach preserves both data
privacy and model privacy, as there is no need to share sensitive data or
proprietary model parameters. Furthermore, we incorporate speculative decoding
to further speed up the inference process of Prada, making the system
practically deployable on bandwidth-constrained edge devices, enabling a more
practical deployment of Prada. Extensive experiments on various downstream
tasks demonstrate that Prada achieves performance comparable to centralized
fine-tuning methods while significantly reducing computational overhead by up
to 60% and communication costs by up to 80%.

中文翻译:
近年来，大语言模型（LLMs）在各种自然语言处理任务中展现出卓越能力。然而，利用智能手机和个人电脑等资源受限边缘设备上的私有数据集，将这些模型适配到专业领域仍面临重大挑战，主要源于严峻的隐私顾虑和有限的计算资源。现有模型适配方法要么因需传输数据而危及数据隐私，要么因暴露专有大语言模型参数而损害模型隐私。为解决这些问题，我们提出Prada——一种基于私有设备端数据的新型隐私保护高效黑盒大语言模型适配系统。Prada通过在用户设备本地采用低秩适配（LoRA）微调的轻量级代理模型，在推理过程中利用基础模型与适配后代理模型输出间的对数偏移量，对远程黑盒大语言模型的输出进行迭代优化。这种基于偏移量的适配方法无需共享敏感数据或专有模型参数，同时保护了数据隐私和模型隐私。此外，我们引入推测式解码技术进一步加速Prada的推理过程，使系统能在带宽受限的边缘设备上实际部署。多组下游任务实验表明，Prada在实现与集中式微调方法相当性能的同时，显著降低了最高60%的计算开销和80%的通信成本。
