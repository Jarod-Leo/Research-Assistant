# Evaluating Named Entity Recognition: Comparative Analysis of Mono- and Multilingual Transformer Models on Brazilian Corporate Earnings Call Transcriptions

链接: http://arxiv.org/abs/2403.12212v1

原文摘要:
Since 2018, when the Transformer architecture was introduced, Natural
Language Processing has gained significant momentum with pre-trained
Transformer-based models that can be fine-tuned for various tasks. Most models
are pre-trained on large English corpora, making them less applicable to other
languages, such as Brazilian Portuguese. In our research, we identified two
models pre-trained in Brazilian Portuguese (BERTimbau and PTT5) and two
multilingual models (mBERT and mT5). BERTimbau and mBERT use only the Encoder
module, while PTT5 and mT5 use both the Encoder and Decoder. Our study aimed to
evaluate their performance on a financial Named Entity Recognition (NER) task
and determine the computational requirements for fine-tuning and inference. To
this end, we developed the Brazilian Financial NER (BraFiNER) dataset,
comprising sentences from Brazilian banks' earnings calls transcripts annotated
using a weakly supervised approach. Additionally, we introduced a novel
approach that reframes the token classification task as a text generation
problem. After fine-tuning the models, we evaluated them using performance and
error metrics. Our findings reveal that BERT-based models consistently
outperform T5-based models. While the multilingual models exhibit comparable
macro F1-scores, BERTimbau demonstrates superior performance over PTT5. In
terms of error metrics, BERTimbau outperforms the other models. We also
observed that PTT5 and mT5 generated sentences with changes in monetary and
percentage values, highlighting the importance of accuracy and consistency in
the financial domain. Our findings provide insights into the differing
performance of BERT- and T5-based models for the NER task.

中文翻译:
自2018年Transformer架构问世以来，基于预训练Transformer模型（可针对各类任务进行微调）的自然语言处理技术获得了显著发展。当前大多数模型基于大型英语语料库进行预训练，这导致其在巴西葡萄牙语等语言中的应用受限。本研究聚焦于两种巴西葡萄牙语预训练模型（BERTimbau和PTT5）及两种多语言模型（mBERT和mT5）。其中BERTimbau与mBERT仅采用编码器模块，而PTT5和mT5则同时使用编码器与解码器。研究旨在评估这些模型在金融领域命名实体识别（NER）任务中的表现，并测算其微调与推理阶段的算力需求。

为此，我们构建了巴西金融NER数据集（BraFiNER），该数据集包含采用弱监督方法标注的巴西银行财报电话会议记录文本。同时，我们创新性地将标记分类任务重构为文本生成问题。模型微调后，通过性能指标与误差指标进行综合评估。研究发现：基于BERT的模型表现始终优于T5系列模型；多语言模型的宏观F1分数相近，但BERTimbau显著优于PTT5；在误差指标方面，BERTimbau同样展现最佳性能。值得注意的是，PTT5和mT5在生成文本时会出现货币与百分比数值的篡改现象，这凸显了金融领域对准确性与一致性的严苛要求。本研究为理解不同架构模型在NER任务中的性能差异提供了重要洞见。
