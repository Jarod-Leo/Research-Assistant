# Empirical Study of Zero-Shot NER with ChatGPT

链接: http://arxiv.org/abs/2310.10035v1

原文摘要:
Large language models (LLMs) exhibited powerful capability in various natural
language processing tasks. This work focuses on exploring LLM performance on
zero-shot information extraction, with a focus on the ChatGPT and named entity
recognition (NER) task. Inspired by the remarkable reasoning capability of LLM
on symbolic and arithmetic reasoning, we adapt the prevalent reasoning methods
to NER and propose reasoning strategies tailored for NER. First, we explore a
decomposed question-answering paradigm by breaking down the NER task into
simpler subproblems by labels. Second, we propose syntactic augmentation to
stimulate the model's intermediate thinking in two ways: syntactic prompting,
which encourages the model to analyze the syntactic structure itself, and tool
augmentation, which provides the model with the syntactic information generated
by a parsing tool. Besides, we adapt self-consistency to NER by proposing a
two-stage majority voting strategy, which first votes for the most consistent
mentions, then the most consistent types. The proposed methods achieve
remarkable improvements for zero-shot NER across seven benchmarks, including
Chinese and English datasets, and on both domain-specific and general-domain
scenarios. In addition, we present a comprehensive analysis of the error types
with suggestions for optimization directions. We also verify the effectiveness
of the proposed methods on the few-shot setting and other LLMs.

中文翻译:
大型语言模型（LLMs）在各类自然语言处理任务中展现出强大能力。本研究聚焦于探索LLM在零样本信息抽取任务中的表现，特别是ChatGPT在命名实体识别（NER）任务上的应用。受启发于LLM在符号与算术推理方面的卓越表现，我们将主流推理方法适配至NER任务，并提出了针对性的推理策略。首先，通过将NER任务按标签拆解为更简单的子问题，探索了分步问答范式。其次，提出两种句法增强方法激发模型的中间推理：句法提示法引导模型自主分析句法结构，工具增强法则为模型提供解析工具生成的句法信息。此外，我们设计了两阶段多数投票策略将自洽性应用于NER，先投票选出最一致的实体提及，再确定最一致的实体类型。所提方法在涵盖中英文、领域专用与通用场景的七个基准测试中显著提升了零样本NER性能。研究还通过错误类型全面分析提出了优化方向建议，并在小样本设置及其他LLM上验证了方法的有效性。
