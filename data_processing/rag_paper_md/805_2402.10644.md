# Linear Transformers with Learnable Kernel Functions are Better In-Context Models

链接: http://arxiv.org/abs/2402.10644v1

原文摘要:
Advancing the frontier of subquadratic architectures for Language Models
(LMs) is crucial in the rapidly evolving field of natural language processing.
Current innovations, including State Space Models, were initially celebrated
for surpassing Transformer performance on language modeling tasks. However,
these models have revealed deficiencies in essential In-Context Learning
capabilities - a domain where the Transformer traditionally shines. The Based
model emerged as a hybrid solution, blending a Linear Transformer with a kernel
inspired by the Taylor expansion of exponential functions, augmented by
convolutional networks. Mirroring the Transformer's in-context adeptness, it
became a strong contender in the field. In our work, we present a singular,
elegant alteration to the Based kernel that amplifies its In-Context Learning
abilities evaluated with the Multi-Query Associative Recall task and overall
language modeling process, as demonstrated on the Pile dataset.

中文翻译:
在自然语言处理这一快速发展的领域中，推动语言模型（LMs）的次二次方架构前沿至关重要。当前包括状态空间模型在内的创新成果，最初因在语言建模任务上超越Transformer性能而备受赞誉。然而，这些模型在关键的上下文学习能力方面存在明显不足——而这正是Transformer的传统优势领域。Based模型作为一种混合解决方案应运而生，它将线性Transformer与受指数函数泰勒展开启发的核函数相结合，并通过卷积网络进行增强。该模型完美复现了Transformer的上下文处理能力，成为该领域的强劲竞争者。我们的工作对Based核函数进行了单一而精妙的改进，通过多查询关联召回任务的评估以及在Pile数据集上的整体语言建模表现，显著提升了其上下文学习能力。
