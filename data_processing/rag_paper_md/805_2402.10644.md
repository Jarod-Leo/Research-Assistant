# Linear Transformers with Learnable Kernel Functions are Better In-Context Models

链接: http://arxiv.org/abs/2402.10644v1

原文摘要:
Advancing the frontier of subquadratic architectures for Language Models
(LMs) is crucial in the rapidly evolving field of natural language processing.
Current innovations, including State Space Models, were initially celebrated
for surpassing Transformer performance on language modeling tasks. However,
these models have revealed deficiencies in essential In-Context Learning
capabilities - a domain where the Transformer traditionally shines. The Based
model emerged as a hybrid solution, blending a Linear Transformer with a kernel
inspired by the Taylor expansion of exponential functions, augmented by
convolutional networks. Mirroring the Transformer's in-context adeptness, it
became a strong contender in the field. In our work, we present a singular,
elegant alteration to the Based kernel that amplifies its In-Context Learning
abilities evaluated with the Multi-Query Associative Recall task and overall
language modeling process, as demonstrated on the Pile dataset.

中文翻译:
以下是符合要求的学术论文摘要中文翻译：

【译文】
在自然语言处理快速发展的领域中，推进语言模型（LM）的次二次复杂度架构前沿至关重要。以状态空间模型为代表的当前创新技术，最初因在语言建模任务上超越Transformer性能而备受赞誉。然而，这些模型在关键的情境学习能力方面存在明显缺陷——而这正是Transformer的传统优势领域。Based模型作为一种混合解决方案应运而生，它融合了线性Transformer与受指数函数泰勒展开启发的核函数，并通过卷积网络增强。该模型复现了Transformer的情境学习优势，成为该领域强有力的竞争者。本研究工作中，我们对Based核函数提出了一个简洁而独特的改进方案，通过多查询关联召回任务的评估以及在Pile数据集上的整体语言建模表现，证实了该改进能显著增强模型的情境学习能力。

【翻译要点说明】
1. 术语处理：
- "subquadratic architectures"译为"次二次复杂度架构"，准确表达计算复杂度概念
- "In-Context Learning"统一译为"情境学习"，符合NLP领域术语规范
- "Multi-Query Associative Recall"保留专业特征译为"多查询关联召回任务"

2. 句式重构：
- 将英语长句拆分为符合中文表达习惯的短句（如第一句的拆分）
- 被动语态转换（如"were initially celebrated"译为主动式"备受赞誉"）
- 学术修辞处理（如"singular, elegant alteration"译为"简洁而独特的改进方案"）

3. 学术风格保持：
- 使用"应运而生""证实了"等学术用语
- 专业符号保留（如Pile数据集不加翻译）
- 逻辑连接词使用（如"然而""通过"）保持论证严谨性

4. 技术概念准确传达：
- "Taylor expansion"译为"泰勒展开"确保数学准确性
- "convolutional networks"译为"卷积网络"符合计算机科学术语
- "strong contender"译为"强有力的竞争者"体现学术比较语境
