# Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large Language Models for Dynamic Inference Using Sorted Fine-Tuning (SoFT)

链接: http://arxiv.org/abs/2309.08968v1

原文摘要:
Large language models (LLMs) have revolutionized natural language processing
(NLP) by excelling at understanding and generating human-like text. However,
their widespread deployment can be prohibitively expensive. SortedNet is a
recent training technique for enabling dynamic inference by leveraging the
modularity in networks and sorting sub-models based on computation/accuracy in
a nested manner. We extend SortedNet to generative NLP tasks, making large
language models dynamic without any Pre-Training and by only replacing Standard
Fine-Tuning (SFT) with Sorted Fine-Tuning (SoFT). Our approach boosts model
efficiency, eliminating the need for multiple models for various scenarios
during inference. We show that this approach can unlock the power of
intermediate layers of transformers in generating the target output. Our
sub-models remain integral components of the original model, minimizing storage
requirements and transition costs between different computational/latency
budgets. The efficacy of our proposed method was demonstrated by applying it to
tune LLaMA 2 13B on the Stanford Alpaca dataset for instruction following and
TriviaQA for closed-book question answering. Our results show the superior
performance of sub-models in comparison to Standard Fine-Tuning and SFT+ICT
(Early-Exit), all achieved with efficient tuning and without additional memory
usage during inference.

中文翻译:
以下是符合要求的专业学术翻译：

大型语言模型（LLMs）通过卓越的文本理解与类人生成能力革新了自然语言处理（NLP）领域，但其大规模部署成本往往过高。SortedNet作为一种新兴训练技术，通过利用网络模块化特性并以计算量/准确率为指标进行嵌套式排序，实现了动态推理能力。本研究将SortedNet拓展至生成式NLP任务，仅需将标准微调（SFT）替换为排序微调（SoFT）而无需任何预训练过程，即可使大型语言模型具备动态适应性。该方法显著提升模型效率，消除了推理阶段为不同场景维护多个模型的必要性。我们证明该方法能有效释放Transformer中间层在目标输出生成中的潜力，所有子模型始终作为原模型的有机组成部分，最大限度降低了存储需求及不同计算/延迟预算间的切换成本。通过在LLaMA 2 13B模型上的实验验证（使用斯坦福Alpaca数据集进行指令跟随训练，TriviaQA数据集进行闭卷问答测试），结果表明：相较于标准微调与SFT+ICT（早期退出）方法，我们的子模型展现出更优性能，且所有优势均在高效调参和零额外推理内存开销的条件下实现。

注：翻译严格遵循以下学术规范：
1. 专业术语统一（如LLMs统一译为"大型语言模型"）
2. 被动语态转换为中文主动表述（如"was demonstrated"转为"实验验证"）
3. 长难句拆分重组（如将嵌套定语从句分解为多个短句）
4. 重要概念首次出现标注英文缩写
5. 保持定量描述精确性（如"13B"不作单位转换）
6. 技术方法名称保留原文（如SortedNet/SFT等）
