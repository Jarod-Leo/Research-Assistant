# Neuro-LIFT: A Neuromorphic, LLM-based Interactive Framework for Autonomous Drone FlighT at the Edge

链接: http://arxiv.org/abs/2501.19259v1

原文摘要:
The integration of human-intuitive interactions into autonomous systems has
been limited. Traditional Natural Language Processing (NLP) systems struggle
with context and intent understanding, severely restricting human-robot
interaction. Recent advancements in Large Language Models (LLMs) have
transformed this dynamic, allowing for intuitive and high-level communication
through speech and text, and bridging the gap between human commands and
robotic actions. Additionally, autonomous navigation has emerged as a central
focus in robotics research, with artificial intelligence (AI) increasingly
being leveraged to enhance these systems. However, existing AI-based navigation
algorithms face significant challenges in latency-critical tasks where rapid
decision-making is critical. Traditional frame-based vision systems, while
effective for high-level decision-making, suffer from high energy consumption
and latency, limiting their applicability in real-time scenarios. Neuromorphic
vision systems, combining event-based cameras and spiking neural networks
(SNNs), offer a promising alternative by enabling energy-efficient, low-latency
navigation. Despite their potential, real-world implementations of these
systems, particularly on physical platforms such as drones, remain scarce. In
this work, we present Neuro-LIFT, a real-time neuromorphic navigation framework
implemented on a Parrot Bebop2 quadrotor. Leveraging an LLM for natural
language processing, Neuro-LIFT translates human speech into high-level
planning commands which are then autonomously executed using event-based
neuromorphic vision and physics-driven planning. Our framework demonstrates its
capabilities in navigating in a dynamic environment, avoiding obstacles, and
adapting to human instructions in real-time.

中文翻译:
将人类直觉式交互融入自主系统的尝试一直较为有限。传统自然语言处理（NLP）系统在上下文和意图理解方面存在明显不足，严重制约了人机交互体验。随着大语言模型（LLM）的突破性进展，这一局面得以改变——通过语音和文本实现直观高效的高阶通信，成功弥合了人类指令与机器人行动之间的鸿沟。与此同时，自主导航已成为机器人研究的核心方向，人工智能（AI）技术正被广泛用于增强此类系统。然而现有基于AI的导航算法在需要快速决策的延迟敏感任务中面临严峻挑战：传统帧式视觉系统虽适用于高层决策，却存在能耗高、延迟大的缺陷，难以满足实时场景需求。神经形态视觉系统通过事件相机与脉冲神经网络（SNN）的结合，为高能效、低延迟导航提供了创新解决方案。尽管潜力巨大，这类系统（尤其是无人机等实体平台）的实际应用仍属空白。本研究提出的Neuro-LIFT框架在Parrot Bebop2四旋翼上实现了实时神经形态导航：利用LLM处理自然语言，将人类语音转换为高层规划指令，再通过基于事件的神经形态视觉与物理驱动规划自主执行。实验证明，该框架能在动态环境中实时避障、导航并响应人类指令。
