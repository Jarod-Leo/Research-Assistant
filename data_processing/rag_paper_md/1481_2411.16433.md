# Finding Structure in Language Models

链接: http://arxiv.org/abs/2411.16433v1

原文摘要:
When we speak, write or listen, we continuously make predictions based on our
knowledge of a language's grammar. Remarkably, children acquire this
grammatical knowledge within just a few years, enabling them to understand and
generalise to novel constructions that have never been uttered before. Language
models are powerful tools that create representations of language by
incrementally predicting the next word in a sentence, and they have had a
tremendous societal impact in recent years. The central research question of
this thesis is whether these models possess a deep understanding of grammatical
structure similar to that of humans. This question lies at the intersection of
natural language processing, linguistics, and interpretability. To address it,
we will develop novel interpretability techniques that enhance our
understanding of the complex nature of large-scale language models. We approach
our research question from three directions. First, we explore the presence of
abstract linguistic information through structural priming, a key paradigm in
psycholinguistics for uncovering grammatical structure in human language
processing. Next, we examine various linguistic phenomena, such as adjective
order and negative polarity items, and connect a model's comprehension of these
phenomena to the data distribution on which it was trained. Finally, we
introduce a controlled testbed for studying hierarchical structure in language
models using various synthetic languages of increasing complexity and examine
the role of feature interactions in modelling this structure. Our findings
offer a detailed account of the grammatical knowledge embedded in language
model representations and provide several directions for investigating
fundamental linguistic questions using computational methods.

中文翻译:
在我们说话、书写或聆听时，会持续基于对语言语法规则的认知进行预测。值得注意的是，儿童仅需数年便能习得这种语法知识，使其能够理解并泛化至从未听过的新颖句式。语言模型作为通过逐步预测句中下一个词来构建语言表征的强大工具，近年来产生了深远的社会影响。本论文的核心研究问题是：这些模型是否具备与人类相似的深层语法结构理解能力？该问题处于自然语言处理、语言学和可解释性研究的交叉领域。为此，我们将开发新型可解释性技术，以深化对大规模语言模型复杂本质的理解。

我们从三个维度展开研究：首先通过心理语言学揭示人类语言加工中语法结构的关键范式——结构启动效应，探究抽象语言信息的存在性；其次系统考察形容词排序、否定极性项等语言现象，将模型对这些现象的理解与其训练数据分布建立关联；最后构建受控实验环境，采用复杂度递增的多种合成语言研究语言模型中的层级结构特征，并分析特征交互在建模此类结构中的作用。研究发现不仅详尽阐释了语言模型表征中蕴含的语法知识体系，更为运用计算方法探究基础语言学问题提供了多维研究路径。
