# Rethinking Transformer-based Multi-document Summarization: An Empirical Investigation

链接: http://arxiv.org/abs/2407.11948v1

原文摘要:
The utilization of Transformer-based models prospers the growth of
multi-document summarization (MDS). Given the huge impact and widespread
adoption of Transformer-based models in various natural language processing
tasks, investigating their performance and behaviors in the context of MDS
becomes crucial for advancing the field and enhancing the quality of summary.
To thoroughly examine the behaviours of Transformer-based MDS models, this
paper presents five empirical studies on (1) measuring the impact of document
boundary separators quantitatively; (2) exploring the effectiveness of
different mainstream Transformer structures; (3) examining the sensitivity of
the encoder and decoder; (4) discussing different training strategies; and (5)
discovering the repetition in a summary generation. The experimental results on
prevalent MDS datasets and eleven evaluation metrics show the influence of
document boundary separators, the granularity of different level features and
different model training strategies. The results also reveal that the decoder
exhibits greater sensitivity to noises compared to the encoder. This
underscores the important role played by the decoder, suggesting a potential
direction for future research in MDS. Furthermore, the experimental results
indicate that the repetition problem in the generated summaries has
correlations with the high uncertainty scores.

中文翻译:
基于Transformer的模型应用推动了多文档摘要（MDS）领域的发展。鉴于该类模型在自然语言处理任务中的巨大影响力和广泛采用，深入研究其在MDS任务中的表现与行为特性，对推动领域发展和提升摘要质量至关重要。本文通过五项系统性实验全面考察基于Transformer的MDS模型：(1)量化评估文档边界分隔符的影响；(2)探究不同主流Transformer架构的有效性；(3)检验编码器与解码器的敏感性差异；(4)讨论不同训练策略；(5)揭示摘要生成中的重复现象。在主流MDS数据集和11项评估指标上的实验结果表明：文档边界分隔符的作用机制、多粒度特征的重要性以及不同训练策略的差异性。研究发现解码器对噪声的敏感度显著高于编码器，这凸显了解码器的关键作用，为未来MDS研究提供了潜在方向。实验结果还表明生成摘要中的重复问题与高不确定性评分存在相关性。
