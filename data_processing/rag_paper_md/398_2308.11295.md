# Uncertainty Estimation of Transformers' Predictions via Topological Analysis of the Attention Matrices

链接: http://arxiv.org/abs/2308.11295v1

原文摘要:
Transformer-based language models have set new benchmarks across a wide range
of NLP tasks, yet reliably estimating the uncertainty of their predictions
remains a significant challenge. Existing uncertainty estimation (UE)
techniques often fall short in classification tasks, either offering minimal
improvements over basic heuristics or relying on costly ensemble models.
Moreover, attempts to leverage common embeddings for UE in linear probing
scenarios have yielded only modest gains, indicating that alternative model
components should be explored.
  We tackle these limitations by harnessing the geometry of attention maps
across multiple heads and layers to assess model confidence. Our approach
extracts topological features from attention matrices, providing a
low-dimensional, interpretable representation of the model's internal dynamics.
Additionally, we introduce topological features to compare attention patterns
across heads and layers. Our method significantly outperforms existing UE
techniques on benchmarks for acceptability judgments and artificial text
detection, offering a more efficient and interpretable solution for uncertainty
estimation in large-scale language models.

中文翻译:
基于Transformer的语言模型已在各类自然语言处理任务中树立了新的性能标杆，但如何可靠地评估其预测结果的不确定性仍是一项重大挑战。现有不确定性估计（UE）技术在分类任务中往往表现欠佳：要么相较基础启发式方法改进有限，要么依赖计算成本高昂的集成模型。此外，在线性探测场景中尝试利用通用嵌入进行不确定性估计的实践收效甚微，这表明需要探索其他模型组件的潜力。

我们通过利用多头多层注意力映射的几何特性来评估模型置信度，从而突破这些局限。该方法从注意力矩阵中提取拓扑特征，以低维、可解释的形式呈现模型的内部动态机制。我们创新性地引入拓扑特征来比较不同注意力头与层级的注意力模式。在语法可接受性判断和人工文本检测的基准测试中，本方法显著优于现有不确定性估计技术，为大规模语言模型的不确定性评估提供了更高效且可解释的解决方案。
