# Uncertainty Estimation of Transformers' Predictions via Topological Analysis of the Attention Matrices

链接: http://arxiv.org/abs/2308.11295v1

原文摘要:
Transformer-based language models have set new benchmarks across a wide range
of NLP tasks, yet reliably estimating the uncertainty of their predictions
remains a significant challenge. Existing uncertainty estimation (UE)
techniques often fall short in classification tasks, either offering minimal
improvements over basic heuristics or relying on costly ensemble models.
Moreover, attempts to leverage common embeddings for UE in linear probing
scenarios have yielded only modest gains, indicating that alternative model
components should be explored.
  We tackle these limitations by harnessing the geometry of attention maps
across multiple heads and layers to assess model confidence. Our approach
extracts topological features from attention matrices, providing a
low-dimensional, interpretable representation of the model's internal dynamics.
Additionally, we introduce topological features to compare attention patterns
across heads and layers. Our method significantly outperforms existing UE
techniques on benchmarks for acceptability judgments and artificial text
detection, offering a more efficient and interpretable solution for uncertainty
estimation in large-scale language models.

中文翻译:
基于Transformer的语言模型已在众多自然语言处理任务中树立了新标杆，但其预测结果的不确定性估计仍面临重大挑战。现有不确定性估计技术在分类任务中表现欠佳：要么相较基础启发式方法提升有限，要么依赖计算成本高昂的集成模型。在线性探测场景下，利用通用嵌入进行不确定性估计的尝试也仅获得有限改进，这表明需要探索其他模型组件。

本研究通过分析多头多层级注意力映射的几何特性来评估模型置信度，突破现有局限。我们提出从注意力矩阵中提取拓扑特征，构建低维且可解释的模型内部动态表征，并创新性地引入拓扑特征来比较不同注意力头与层级的模式。在语法可接受性判断和人工文本检测基准测试中，本方法显著优于现有不确定性估计技术，为大规模语言模型提供了更高效、可解释的不确定性估计解决方案。
