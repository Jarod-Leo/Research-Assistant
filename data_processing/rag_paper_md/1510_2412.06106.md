# Enhanced Computationally Efficient Long LoRA Inspired Perceiver Architectures for Auto-Regressive Language Modeling

链接: http://arxiv.org/abs/2412.06106v1

原文摘要:
The Transformer architecture has revolutionized the Natural Language
Processing field and is the backbone of Large Language Models (LLMs). The
Transformer uses the attention mechanism that computes the pair-wise similarity
between its input tokens to produce latent vectors that are able to understand
the semantic meaning of the input text. One of the challenges in the
Transformer architecture is the quadratic complexity of the attention mechanism
that prohibits the efficient processing of long sequence lengths. While many
recent research works have attempted to provide a reduction from $O(n^2)$ time
complexity of attention to semi-linear complexity, it remains an unsolved
problem in the sense of maintaining a high performance when such complexity is
reduced. One of the important works in this respect is the Perceiver class of
architectures that have demonstrated excellent performance while reducing the
computation complexity. In this paper, we use the PerceiverAR that was proposed
for Auto-Regressive modeling as a baseline, and provide three different
architectural enhancements to it with varying computation overhead tradeoffs.
Inspired by the recently proposed efficient attention computation approach of
Long-LoRA, we then present an equally efficient Perceiver-based architecture
(termed as Long LoRA Pereceiver - LLP) that can be used as the base
architecture in LLMs instead of just a fine-tuning add-on. Our results on
different benchmarks indicate impressive improvements compared to recent
Transformer based models.

中文翻译:
Transformer架构彻底改变了自然语言处理领域，现已成为大语言模型（LLMs）的核心支柱。该架构采用注意力机制，通过计算输入词元间的两两相似度来生成能够理解文本语义的潜在向量。当前Transformer架构面临的主要挑战在于注意力机制具有二次方复杂度，导致其难以高效处理长序列。尽管近期许多研究试图将注意力机制的$O(n^2)$时间复杂度降至半线性复杂度，但在降低复杂度的同时保持高性能仍是一个悬而未决的难题。Perceiver系列架构是该领域的重要成果，其在降低计算复杂度的同时展现出卓越性能。本文以自回归建模提出的PerceiverAR为基线模型，通过三种不同计算开销的架构增强方案，结合近期Long-LoRA提出的高效注意力计算方法，提出了一种同等高效的Perceiver架构（称为Long LoRA Perceiver - LLP）。该架构可作为大语言模型的基础架构，而非仅作为微调附加组件。在不同基准测试中，我们的模型相较当前主流Transformer模型展现出显著性能提升。
