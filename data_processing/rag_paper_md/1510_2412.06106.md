# Enhanced Computationally Efficient Long LoRA Inspired Perceiver Architectures for Auto-Regressive Language Modeling

链接: http://arxiv.org/abs/2412.06106v1

原文摘要:
The Transformer architecture has revolutionized the Natural Language
Processing field and is the backbone of Large Language Models (LLMs). The
Transformer uses the attention mechanism that computes the pair-wise similarity
between its input tokens to produce latent vectors that are able to understand
the semantic meaning of the input text. One of the challenges in the
Transformer architecture is the quadratic complexity of the attention mechanism
that prohibits the efficient processing of long sequence lengths. While many
recent research works have attempted to provide a reduction from $O(n^2)$ time
complexity of attention to semi-linear complexity, it remains an unsolved
problem in the sense of maintaining a high performance when such complexity is
reduced. One of the important works in this respect is the Perceiver class of
architectures that have demonstrated excellent performance while reducing the
computation complexity. In this paper, we use the PerceiverAR that was proposed
for Auto-Regressive modeling as a baseline, and provide three different
architectural enhancements to it with varying computation overhead tradeoffs.
Inspired by the recently proposed efficient attention computation approach of
Long-LoRA, we then present an equally efficient Perceiver-based architecture
(termed as Long LoRA Pereceiver - LLP) that can be used as the base
architecture in LLMs instead of just a fine-tuning add-on. Our results on
different benchmarks indicate impressive improvements compared to recent
Transformer based models.

中文翻译:
Transformer架构彻底改变了自然语言处理领域，成为大语言模型（LLMs）的核心支柱。该架构采用注意力机制，通过计算输入词元间的两两相似度来生成能够理解文本语义的潜在向量。然而，Transformer面临的关键挑战在于注意力机制的二次方复杂度，这导致其难以高效处理长序列输入。尽管近期众多研究尝试将注意力机制的时间复杂度从$O(n^2)$降至半线性复杂度，但在保持高性能的前提下实现这一目标仍是未解难题。其中，Perceiver架构系列通过降低计算复杂度仍能保持卓越性能，成为该领域的重要突破。

本文以自回归建模的PerceiverAR架构为基线模型，提出三种具有不同计算开销权衡的架构增强方案。受近期Long-LoRA高效注意力计算方法的启发，我们进一步设计出同等高效的Perceiver衍生架构（称为Long LoRA Perceiver - LLP）。该架构不仅可作为微调附加模块，更能作为LLMs的基础架构替代方案。在不同基准测试中，我们的模型相较当前基于Transformer的模型展现出显著性能提升。
