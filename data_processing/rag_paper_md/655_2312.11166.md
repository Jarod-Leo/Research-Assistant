# Structure-Preserving Transformers for Learning Parametrized Hamiltonian Systems

链接: http://arxiv.org/abs/2312.11166v1

原文摘要:
Two of the many trends in neural network research of the past few years have
been (i) the learning of dynamical systems, especially with recurrent neural
networks such as long short-term memory networks (LSTMs) and (ii) the
introduction of transformer neural networks for natural language processing
(NLP) tasks.
  While some work has been performed on the intersection of these two trends,
those efforts were largely limited to using the vanilla transformer directly
without adjusting its architecture for the setting of a physical system.
  In this work we develop a transformer-inspired neural network and use it to
learn a dynamical system. We (for the first time) change the activation
function of the attention layer to imbue the transformer with
structure-preserving properties to improve long-term stability. This is shown
to be of great advantage when applying the neural network to learning the
trajectory of a rigid body.

中文翻译:
过去几年神经网络研究中的两大趋势为：(i) 动态系统学习（尤其通过长短期记忆网络等循环神经网络实现）与(ii) 面向自然语言处理任务的Transformer神经网络架构的兴起。尽管已有研究尝试融合这两大方向，但现有工作多直接沿用标准Transformer架构，未针对物理系统特性进行结构调整。本研究创新性地构建了受Transformer启发的神经网络用于动态系统学习，首次通过改变注意力层激活函数使网络具备结构保持特性，显著提升了长期预测稳定性。在刚体运动轨迹学习任务中，这一改进展现出显著优势。
