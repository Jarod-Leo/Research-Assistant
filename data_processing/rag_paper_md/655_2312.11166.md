# Structure-Preserving Transformers for Learning Parametrized Hamiltonian Systems

链接: http://arxiv.org/abs/2312.11166v1

原文摘要:
Two of the many trends in neural network research of the past few years have
been (i) the learning of dynamical systems, especially with recurrent neural
networks such as long short-term memory networks (LSTMs) and (ii) the
introduction of transformer neural networks for natural language processing
(NLP) tasks.
  While some work has been performed on the intersection of these two trends,
those efforts were largely limited to using the vanilla transformer directly
without adjusting its architecture for the setting of a physical system.
  In this work we develop a transformer-inspired neural network and use it to
learn a dynamical system. We (for the first time) change the activation
function of the attention layer to imbue the transformer with
structure-preserving properties to improve long-term stability. This is shown
to be of great advantage when applying the neural network to learning the
trajectory of a rigid body.

中文翻译:
过去几年神经网络研究中的两大趋势是：(1) 动态系统学习（尤其通过长短期记忆网络等循环神经网络实现）；(2) 为自然语言处理任务引入Transformer神经网络。尽管已有研究尝试融合这两大趋势，但这些工作大多直接使用原始Transformer架构，未针对物理系统特性进行调整。本研究开发了一种受Transformer启发的神经网络用于动态系统学习，首次通过改变注意力层的激活函数，使Transformer具备结构保持特性以增强长期稳定性。在刚体轨迹学习任务中，该方法展现出显著优势。

（翻译说明：
1. 采用学术论文摘要的简洁风格，将原文两个段落整合为符合中文表达习惯的连贯段落
2. 专业术语规范处理："LSTMs"译为通用译名"长短期记忆网络"，"Transformer"保留英文原名
3. 被动语态转化："were largely limited to"译为主动式"大多直接使用"
4. 关键创新点强调："for the first time"译为"首次"并前置
5. 物理概念准确传达："structure-preserving properties"译为专业术语"结构保持特性"
6. 句式重组：将原文最后两句因果关系整合为"该方法展现出显著优势"的总结句式）
