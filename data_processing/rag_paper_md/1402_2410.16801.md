# Controlled Low-Rank Adaptation with Subspace Regularization for Continued Training on Large Language Models

链接: http://arxiv.org/abs/2410.16801v1

原文摘要:
Large language models (LLMs) exhibit remarkable capabilities in natural
language processing but face catastrophic forgetting when learning new tasks,
where adaptation to a new domain leads to a substantial decline in performance
on previous tasks. In this paper, we propose Controlled LoRA (CLoRA), a
sub-space regularization method on LoRA structure. Aiming to reduce the scale
of output change while introduce minimal constraint on model capacity, CLoRA
imposes constraint on the direction of updating matrix's null space.
Experimental results on one-stage LLM finetuning tasks and continual learning
settings highlight the superority of CLoRA as a effective parameter efficient
finetuning method with catastrophic forgetting mitigating.Further investigation
for model parameters indicates that CLoRA effectively balances the trade-off
between model capacity and degree of forgetting.

中文翻译:
大语言模型（LLMs）在自然语言处理领域展现出卓越能力，但在学习新任务时面临灾难性遗忘问题——适应新领域会导致先前任务性能急剧下降。本文提出受控低秩自适应（CLoRA），一种基于LoRA结构的子空间正则化方法。该方法通过在更新矩阵的零空间方向上施加约束，旨在以最小化模型能力限制为代价，有效控制输出变化的幅度。单阶段LLM微调任务和持续学习场景下的实验结果表明，CLoRA作为一种参数高效的微调方法，在缓解灾难性遗忘方面具有显著优势。对模型参数的进一步分析表明，CLoRA能有效平衡模型容量与遗忘程度之间的权衡关系。
