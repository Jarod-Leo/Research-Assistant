# Open or Closed LLM for Lesser-Resourced Languages? Lessons from Greek

链接: http://arxiv.org/abs/2501.12826v1

原文摘要:
Natural Language Processing (NLP) for lesser-resourced languages faces
persistent challenges, including limited datasets, inherited biases from
high-resource languages, and the need for domain-specific solutions. This study
addresses these gaps for Modern Greek through three key contributions. First,
we evaluate the performance of open-source (Llama-70b) and closed-source
(GPT-4o mini) large language models (LLMs) on seven core NLP tasks with dataset
availability, revealing task-specific strengths, weaknesses, and parity in
their performance. Second, we expand the scope of Greek NLP by reframing
Authorship Attribution as a tool to assess potential data usage by LLMs in
pre-training, with high 0-shot accuracy suggesting ethical implications for
data provenance. Third, we showcase a legal NLP case study, where a Summarize,
Translate, and Embed (STE) methodology outperforms the traditional TF-IDF
approach for clustering \emph{long} legal texts. Together, these contributions
provide a roadmap to advance NLP in lesser-resourced languages, bridging gaps
in model evaluation, task innovation, and real-world impact.

中文翻译:
针对资源匮乏语言的自然语言处理（NLP）长期面临数据集有限、继承自高资源语言的偏见以及需要领域专用解决方案等挑战。本研究通过三项关键贡献为现代希腊语填补了这些空白：首先，我们在七个已有数据集的NLP核心任务上评估开源模型（Llama-70b）与闭源模型（GPT-4o mini）的表现，揭示了任务特异性优势、弱点及性能均衡性；其次，通过将作者归属任务重构为检测大语言模型预训练数据使用情况的工具，以高达零样本准确率的结果揭示了数据来源的伦理问题，拓展了希腊语NLP的研究边界；最后，我们呈现了一个法律NLP案例研究，其中"摘要-翻译-嵌入"（STE）方法在长法律文本聚类任务中显著优于传统TF-IDF方法。这些成果共同为推进资源匮乏语言的NLP发展提供了路线图，在模型评估、任务创新与实际应用层面架设了桥梁。
