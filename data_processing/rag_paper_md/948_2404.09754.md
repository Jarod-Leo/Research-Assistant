# Resilience of Large Language Models for Noisy Instructions

链接: http://arxiv.org/abs/2404.09754v1

原文摘要:
As the rapidly advancing domain of natural language processing (NLP), large
language models (LLMs) have emerged as powerful tools for interpreting human
commands and generating text across various tasks. Nonetheless, the resilience
of LLMs to handle text containing inherent errors, stemming from human
interactions and collaborative systems, has not been thoroughly explored. Our
study investigates the resilience of LLMs against five common types of
disruptions including 1) ASR (Automatic Speech Recognition) errors, 2) OCR
(Optical Character Recognition) errors, 3) grammatical mistakes, 4)
typographical errors, and 5) distractive content. We aim to investigate how
these models react by deliberately embedding these errors into instructions.
Our findings reveal that while some LLMs show a degree of resistance to certain
types of noise, their overall performance significantly suffers. This
emphasizes the importance of further investigation into enhancing model
resilience. In response to the observed decline in performance, our study also
evaluates a "re-pass" strategy, designed to purify the instructions of noise
before the LLMs process them. Our analysis indicates that correcting noisy
instructions, particularly for open-source LLMs, presents significant
challenges.

中文翻译:
作为自然语言处理（NLP）快速发展的前沿领域，大语言模型（LLM）已成为解读人类指令并执行多样化文本生成任务的强大工具。然而，这些模型对于处理源自人机交互与协作系统固有错误的文本时表现出的鲁棒性，尚未得到充分研究。本研究系统评估了LLM对五类常见干扰的抵抗能力，包括：1）语音识别（ASR）错误、2）光学字符识别（OCR）错误、3）语法错误、4）拼写错误及5）干扰性内容。我们通过刻意在指令中植入这些错误，旨在探究模型的具体反应。

研究发现，虽然部分LLM对特定类型的噪声表现出一定抵抗力，但其整体性能仍显著受损。这一现象凸显了增强模型鲁棒性研究的迫切需求。针对观察到的性能下降问题，本研究还评估了"二次处理"策略——即在LLM处理前对含噪指令进行净化。分析表明，对开源LLM而言，噪声指令的校正工作尤其面临重大挑战。
