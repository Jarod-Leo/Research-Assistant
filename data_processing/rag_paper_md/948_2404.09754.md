# Resilience of Large Language Models for Noisy Instructions

链接: http://arxiv.org/abs/2404.09754v1

原文摘要:
As the rapidly advancing domain of natural language processing (NLP), large
language models (LLMs) have emerged as powerful tools for interpreting human
commands and generating text across various tasks. Nonetheless, the resilience
of LLMs to handle text containing inherent errors, stemming from human
interactions and collaborative systems, has not been thoroughly explored. Our
study investigates the resilience of LLMs against five common types of
disruptions including 1) ASR (Automatic Speech Recognition) errors, 2) OCR
(Optical Character Recognition) errors, 3) grammatical mistakes, 4)
typographical errors, and 5) distractive content. We aim to investigate how
these models react by deliberately embedding these errors into instructions.
Our findings reveal that while some LLMs show a degree of resistance to certain
types of noise, their overall performance significantly suffers. This
emphasizes the importance of further investigation into enhancing model
resilience. In response to the observed decline in performance, our study also
evaluates a "re-pass" strategy, designed to purify the instructions of noise
before the LLMs process them. Our analysis indicates that correcting noisy
instructions, particularly for open-source LLMs, presents significant
challenges.

中文翻译:
作为自然语言处理（NLP）领域快速发展的前沿技术，大语言模型（LLMs）已成为解读人类指令并生成多样化文本任务的强大工具。然而，这些模型对于处理源自人机交互与协作系统中固有文本错误的鲁棒性尚未得到充分探索。本研究系统评估了LLMs对五类常见干扰的抵抗能力，包括：1）自动语音识别（ASR）错误、2）光学字符识别（OCR）错误、3）语法错误、4）拼写错误及5）干扰性内容。我们通过刻意在指令中植入这些错误，旨在探究模型的具体反应。实验结果表明，虽然部分LLMs对特定类型的噪声表现出一定抵抗力，但其整体性能仍显著受损，这凸显了增强模型鲁棒性研究的迫切性。针对观察到的性能下降现象，本研究还评估了"二次处理"策略——即在LLMs处理前对含噪指令进行净化。分析显示，对开源LLMs而言，修正含噪指令仍存在重大挑战。
