# Native vs Non-Native Language Prompting: A Comparative Analysis

链接: http://arxiv.org/abs/2409.07054v1

原文摘要:
Large language models (LLMs) have shown remarkable abilities in different
fields, including standard Natural Language Processing (NLP) tasks. To elicit
knowledge from LLMs, prompts play a key role, consisting of natural language
instructions. Most open and closed source LLMs are trained on available labeled
and unlabeled resources--digital content such as text, images, audio, and
videos. Hence, these models have better knowledge for high-resourced languages
but struggle with low-resourced languages. Since prompts play a crucial role in
understanding their capabilities, the language used for prompts remains an
important research question. Although there has been significant research in
this area, it is still limited, and less has been explored for medium to
low-resourced languages. In this study, we investigate different prompting
strategies (native vs. non-native) on 11 different NLP tasks associated with 12
different Arabic datasets (9.7K data points). In total, we conducted 197
experiments involving 3 LLMs, 12 datasets, and 3 prompting strategies. Our
findings suggest that, on average, the non-native prompt performs the best,
followed by mixed and native prompts.

中文翻译:
大型语言模型（LLM）在包括标准自然语言处理（NLP）任务在内的多个领域展现出卓越能力。为激发LLM的知识潜能，提示（prompt）作为由自然语言指令构成的关键要素发挥着核心作用。当前多数开源与闭源LLM均基于现有标注及未标注资源（如文本、图像、音频、视频等数字内容）进行训练，这使得模型对高资源语言表现优异，却难以应对低资源语言。鉴于提示语对理解模型能力至关重要，其语言选择仍是一个重要研究课题。尽管该领域已有显著研究进展，但相关探索仍显不足，尤其针对中低资源语言的研究更为有限。本研究通过11类NLP任务（涉及12个阿拉伯语数据集，共9.7K数据点），系统考察了三种提示策略（母语提示、非母语提示及混合提示）的效果。总计197组实验覆盖3种LLM、12个数据集和3种提示策略，结果表明：非母语提示整体表现最优，混合提示次之，母语提示居末。
