# Typhoon: Towards an Effective Task-Specific Masking Strategy for Pre-trained Language Models

链接: http://arxiv.org/abs/2303.15619v1

原文摘要:
Through exploiting a high level of parallelism enabled by graphics processing
units, transformer architectures have enabled tremendous strides forward in the
field of natural language processing. In a traditional masked language model,
special MASK tokens are used to prompt our model to gather contextual
information from surrounding words to restore originally hidden information. In
this paper, we explore a task-specific masking framework for pre-trained large
language models that enables superior performance on particular downstream
tasks on the datasets in the GLUE benchmark. We develop our own masking
algorithm, Typhoon, based on token input gradients, and compare this with other
standard baselines. We find that Typhoon offers performance competitive with
whole-word masking on the MRPC dataset. Our implementation can be found in a
public Github Repository.

中文翻译:
通过充分利用图形处理器所提供的高度并行计算能力，变压器架构在自然语言处理领域实现了重大突破。传统掩码语言模型中，特殊MASK标记被用于引导模型从上下文词汇中收集信息以还原被遮蔽内容。本文研究了一种针对预训练大语言模型的任务特定掩码框架，该框架在GLUE基准测试数据集的下游任务中展现出卓越性能。我们基于词元输入梯度开发了新型掩码算法Typhoon，并与多种标准基线方法进行对比实验。研究发现，在MRPC数据集上，Typhoon算法性能与全词掩码技术相当。本研究的完整实现已发布于公开的GitHub代码库。
