# Backdoor Removal for Generative Large Language Models

链接: http://arxiv.org/abs/2405.07667v1

原文摘要:
With rapid advances, generative large language models (LLMs) dominate various
Natural Language Processing (NLP) tasks from understanding to reasoning. Yet,
language models' inherent vulnerabilities may be exacerbated due to increased
accessibility and unrestricted model training on massive data. A malicious
adversary may publish poisoned data online and conduct backdoor attacks on the
victim LLMs pre-trained on the poisoned data. Backdoored LLMs behave
innocuously for normal queries and generate harmful responses when the backdoor
trigger is activated. Despite significant efforts paid to LLMs' safety issues,
LLMs are still struggling against backdoor attacks. As Anthropic recently
revealed, existing safety training strategies, including supervised fine-tuning
(SFT) and Reinforcement Learning from Human Feedback (RLHF), fail to revoke the
backdoors once the LLM is backdoored during the pre-training stage. In this
paper, we present Simulate and Eliminate (SANDE) to erase the undesired
backdoored mappings for generative LLMs. We initially propose Overwrite
Supervised Fine-tuning (OSFT) for effective backdoor removal when the trigger
is known. Then, to handle scenarios where trigger patterns are unknown, we
integrate OSFT into our two-stage framework, SANDE. Unlike other works that
assume access to cleanly trained models, our safety-enhanced LLMs are able to
revoke backdoors without any reference. Consequently, our safety-enhanced LLMs
no longer produce targeted responses when the backdoor triggers are activated.
We conduct comprehensive experiments to show that our proposed SANDE is
effective against backdoor attacks while bringing minimal harm to LLMs'
powerful capability.

中文翻译:
随着生成式大语言模型（LLM）的快速发展，其在自然语言处理（NLP）领域已全面主导从理解到推理的各项任务。然而，由于模型可获取性提升及海量数据无限制训练，语言模型固有的脆弱性可能被进一步放大。恶意攻击者可通过发布投毒数据，对基于该数据预训练的受害LLM实施后门攻击。被植入后门的LLM在正常查询时表现无害，但当后门触发器激活时会产生有害响应。尽管学界已投入大量精力解决LLM安全问题，后门攻击仍是重大威胁。正如Anthropic最新研究表明，现有安全训练策略（包括监督微调SFT和人类反馈强化学习RLHF）一旦LLM在预训练阶段被植入后门，均无法有效消除后门。本文提出"模拟消除"（SANDE）方法，用于消除生成式LLM中的非预期后门映射。我们首先提出"覆盖式监督微调"（OSFT）方案，可在已知触发器时有效移除后门；针对触发器模式未知的场景，进一步将OSFT整合至两阶段SANDE框架。与依赖干净训练模型的现有方案不同，我们的安全增强型LLM无需任何参考模型即可消除后门，从而在触发器激活时不再生成目标响应。全面实验表明，SANDE能有效防御后门攻击，同时对LLM的核心能力影响极小。
