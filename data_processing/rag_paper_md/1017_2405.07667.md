# Backdoor Removal for Generative Large Language Models

链接: http://arxiv.org/abs/2405.07667v1

原文摘要:
With rapid advances, generative large language models (LLMs) dominate various
Natural Language Processing (NLP) tasks from understanding to reasoning. Yet,
language models' inherent vulnerabilities may be exacerbated due to increased
accessibility and unrestricted model training on massive data. A malicious
adversary may publish poisoned data online and conduct backdoor attacks on the
victim LLMs pre-trained on the poisoned data. Backdoored LLMs behave
innocuously for normal queries and generate harmful responses when the backdoor
trigger is activated. Despite significant efforts paid to LLMs' safety issues,
LLMs are still struggling against backdoor attacks. As Anthropic recently
revealed, existing safety training strategies, including supervised fine-tuning
(SFT) and Reinforcement Learning from Human Feedback (RLHF), fail to revoke the
backdoors once the LLM is backdoored during the pre-training stage. In this
paper, we present Simulate and Eliminate (SANDE) to erase the undesired
backdoored mappings for generative LLMs. We initially propose Overwrite
Supervised Fine-tuning (OSFT) for effective backdoor removal when the trigger
is known. Then, to handle scenarios where trigger patterns are unknown, we
integrate OSFT into our two-stage framework, SANDE. Unlike other works that
assume access to cleanly trained models, our safety-enhanced LLMs are able to
revoke backdoors without any reference. Consequently, our safety-enhanced LLMs
no longer produce targeted responses when the backdoor triggers are activated.
We conduct comprehensive experiments to show that our proposed SANDE is
effective against backdoor attacks while bringing minimal harm to LLMs'
powerful capability.

中文翻译:
随着技术的飞速发展，生成式大语言模型（LLMs）已在从理解到推理的各类自然语言处理（NLP）任务中占据主导地位。然而，由于模型可访问性提升及海量数据无限制训练，语言模型固有的脆弱性可能被进一步放大。恶意攻击者可通过在线发布投毒数据，对基于此类数据预训练的受害LLMs实施后门攻击。被植入后门的LLMs在正常查询时表现无害，一旦后门触发器激活则生成有害响应。尽管学界对LLMs安全问题投入大量努力，其防御后门攻击的能力仍显不足。如Anthropic最新研究表明，现有安全训练策略（包括监督微调SFT和人类反馈强化学习RLHF）一旦模型在预训练阶段被植入后门，均无法有效消除后门。本文提出"模拟消除"（SANDE）方法，用于消除生成式LLMs中的非预期后门映射。我们首先提出覆盖式监督微调（OSFT）方案，在已知触发器时实现高效后门清除；进而针对触发器模式未知场景，将OSFT融入两阶段框架SANDE。与依赖干净模型参照的其他方案不同，我们的安全增强型LLMs无需任何参考即可消除后门，确保触发器激活时不再生成目标响应。通过全面实验验证，SANDE在有效抵御后门攻击的同时，对LLMs强大能力的损害微乎其微。
