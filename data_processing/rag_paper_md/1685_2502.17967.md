# LLM Knows Geometry Better than Algebra: Numerical Understanding of LLM-Based Agents in A Trading Arena

链接: http://arxiv.org/abs/2502.17967v1

原文摘要:
Recent advancements in large language models (LLMs) have significantly
improved performance in natural language processing tasks. However, their
ability to generalize to dynamic, unseen tasks, particularly in numerical
reasoning, remains a challenge. Existing benchmarks mainly evaluate LLMs on
problems with predefined optimal solutions, which may not align with real-world
scenarios where clear answers are absent. To bridge this gap, we design the
Agent Trading Arena, a virtual numerical game simulating complex economic
systems through zero-sum games, where agents invest in stock portfolios. Our
experiments reveal that LLMs, including GPT-4o, struggle with algebraic
reasoning when dealing with plain-text stock data, often focusing on local
details rather than global trends. In contrast, LLMs perform significantly
better with geometric reasoning when presented with visual data, such as
scatter plots or K-line charts, suggesting that visual representations enhance
numerical reasoning. This capability is further improved by incorporating the
reflection module, which aids in the analysis and interpretation of complex
data. We validate our findings on NASDAQ Stock dataset, where LLMs demonstrate
stronger reasoning with visual data compared to text. Our code and data are
publicly available at https://github.com/wekjsdvnm/Agent-Trading-Arena.git.

中文翻译:
近期，大型语言模型（LLMs）在自然语言处理任务中的性能显著提升。然而，其在动态、未见任务（尤其是数值推理）上的泛化能力仍面临挑战。现有基准测试主要评估LLMs在具有预定义最优解问题上的表现，这与现实世界中缺乏明确答案的场景并不相符。为弥合这一差距，我们设计了"智能体交易竞技场"——一个通过零和博弈模拟复杂经济系统的虚拟数值游戏，其中智能体对股票组合进行投资。实验表明，包括GPT-4o在内的LLMs在处理纯文本股票数据时存在代数推理困难，往往关注局部细节而非整体趋势。相比之下，当呈现散点图或K线图等视觉数据时，LLMs的几何推理能力显著提升，这表明视觉表征能增强数值推理。通过引入反思模块（用于辅助复杂数据的分析与解读），该能力得到进一步改善。我们在纳斯达克股票数据集上验证了这一发现：相较于文本数据，LLMs在视觉数据上展现出更强的推理能力。相关代码与数据已开源：https://github.com/wekjsdvnm/Agent-Trading-Arena.git。
