# Temporal Blind Spots in Large Language Models

链接: http://arxiv.org/abs/2401.12078v1

原文摘要:
Large language models (LLMs) have recently gained significant attention due
to their unparalleled ability to perform various natural language processing
tasks. These models, benefiting from their advanced natural language
understanding capabilities, have demonstrated impressive zero-shot performance.
However, the pre-training data utilized in LLMs is often confined to a specific
corpus, resulting in inherent freshness and temporal scope limitations.
Consequently, this raises concerns regarding the effectiveness of LLMs for
tasks involving temporal intents. In this study, we aim to investigate the
underlying limitations of general-purpose LLMs when deployed for tasks that
require a temporal understanding. We pay particular attention to handling
factual temporal knowledge through three popular temporal QA datasets.
Specifically, we observe low performance on detailed questions about the past
and, surprisingly, for rather new information. In manual and automatic testing,
we find multiple temporal errors and characterize the conditions under which QA
performance deteriorates. Our analysis contributes to understanding LLM
limitations and offers valuable insights into developing future models that can
better cater to the demands of temporally-oriented tasks. The code is
available\footnote{https://github.com/jwallat/temporalblindspots}.

中文翻译:
大型语言模型（LLMs）近期因其在执行多样化自然语言处理任务中的卓越能力而备受关注。得益于先进的自然语言理解能力，这些模型展现出令人印象深刻的零样本性能。然而，LLMs所使用的预训练数据通常局限于特定语料库，导致其存在固有的时效性与时间范围局限性。这一缺陷引发了人们对其在处理具有时间意图任务时有效性的担忧。本研究旨在探究通用型LLMs在部署至需要时间理解能力的任务时所暴露的根本性局限。我们重点通过三个主流时序问答数据集，对事实性时间知识的处理机制进行剖析。研究发现：模型对历史细节问题的回答准确率偏低，而令人意外的是，其对较新信息的处理同样表现不佳。通过人工与自动化测试，我们识别出多种时间性错误，并系统归纳了导致问答性能下降的具体情境。本分析不仅有助于理解LLMs的局限性，更为开发能更好满足时序任务需求的未来模型提供了重要启示。相关代码已开源\footnote{https://github.com/jwallat/temporalblindspots}。
