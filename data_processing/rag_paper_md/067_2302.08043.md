# GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks

链接: http://arxiv.org/abs/2302.08043v1

原文摘要:
Graphs can model complex relationships between objects, enabling a myriad of
Web applications such as online page/article classification and social
recommendation. While graph neural networks(GNNs) have emerged as a powerful
tool for graph representation learning, in an end-to-end supervised setting,
their performance heavily rely on a large amount of task-specific supervision.
To reduce labeling requirement, the "pre-train, fine-tune" and "pre-train,
prompt" paradigms have become increasingly common. In particular, prompting is
a popular alternative to fine-tuning in natural language processing, which is
designed to narrow the gap between pre-training and downstream objectives in a
task-specific manner. However, existing study of prompting on graphs is still
limited, lacking a universal treatment to appeal to different downstream tasks.
In this paper, we propose GraphPrompt, a novel pre-training and prompting
framework on graphs. GraphPrompt not only unifies pre-training and downstream
tasks into a common task template, but also employs a learnable prompt to
assist a downstream task in locating the most relevant knowledge from the
pre-train model in a task-specific manner. Finally, we conduct extensive
experiments on five public datasets to evaluate and analyze GraphPrompt.

中文翻译:
图能够建模对象间复杂的关联关系，支撑着网页/文章分类、社交推荐等众多网络应用。尽管图神经网络已成为图表示学习的强大工具，但在端到端的监督学习场景下，其性能高度依赖大量任务特定的标注数据。为降低标注需求，"预训练-微调"与"预训练-提示"范式日益普及。其中提示学习作为自然语言处理领域替代微调的流行方案，通过任务定制化方式有效弥合了预训练目标与下游任务间的差异。然而当前图提示学习研究仍处于探索阶段，缺乏适用于多样化下游任务的通用框架。

本文提出GraphPrompt——一种创新的图预训练与提示学习框架。该框架不仅通过统一的任务模板实现了预训练与下游任务的范式对齐，还引入可学习的提示机制，使下游任务能自适应地从预训练模型中定位最相关的知识。最终，我们在五个公开数据集上进行了全面的实验评估与深入分析。
