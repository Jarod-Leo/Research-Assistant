# PLeak: Prompt Leaking Attacks against Large Language Model Applications

链接: http://arxiv.org/abs/2405.06823v1

原文摘要:
Large Language Models (LLMs) enable a new ecosystem with many downstream
applications, called LLM applications, with different natural language
processing tasks. The functionality and performance of an LLM application
highly depend on its system prompt, which instructs the backend LLM on what
task to perform. Therefore, an LLM application developer often keeps a system
prompt confidential to protect its intellectual property. As a result, a
natural attack, called prompt leaking, is to steal the system prompt from an
LLM application, which compromises the developer's intellectual property.
Existing prompt leaking attacks primarily rely on manually crafted queries, and
thus achieve limited effectiveness.
  In this paper, we design a novel, closed-box prompt leaking attack framework,
called PLeak, to optimize an adversarial query such that when the attacker
sends it to a target LLM application, its response reveals its own system
prompt. We formulate finding such an adversarial query as an optimization
problem and solve it with a gradient-based method approximately. Our key idea
is to break down the optimization goal by optimizing adversary queries for
system prompts incrementally, i.e., starting from the first few tokens of each
system prompt step by step until the entire length of the system prompt.
  We evaluate PLeak in both offline settings and for real-world LLM
applications, e.g., those on Poe, a popular platform hosting such applications.
Our results show that PLeak can effectively leak system prompts and
significantly outperforms not only baselines that manually curate queries but
also baselines with optimized queries that are modified and adapted from
existing jailbreaking attacks. We responsibly reported the issues to Poe and
are still waiting for their response. Our implementation is available at this
repository: https://github.com/BHui97/PLeak.

中文翻译:
大型语言模型（LLMs）催生了一个包含众多下游应用的新生态系统，这些应用被称为LLM应用，涉及不同的自然语言处理任务。LLM应用的功能与性能高度依赖于其系统提示（system prompt），该提示用于指导后端LLM执行特定任务。因此，开发者通常会对系统提示保密以保护知识产权。由此衍生出一种名为"提示泄露"（prompt leaking）的攻击方式，即窃取LLM应用的系统提示，从而损害开发者的知识产权。现有攻击主要依赖人工构造的查询语句，效果有限。

本文提出了一种新型黑盒提示泄露攻击框架PLeak，通过优化对抗性查询语句，使得目标LLM应用的响应能暴露其系统提示。我们将该问题建模为优化问题，并采用基于梯度的方法进行近似求解。核心创新在于采用增量式优化策略：从系统提示的首个标记开始逐步优化对抗查询，直至获取完整系统提示。

我们在离线环境和真实LLM应用（如热门平台Poe上的应用）中评估PLeak。实验表明，PLeak不仅能有效泄露系统提示，其效果显著优于人工构造查询的基线方法，也优于基于现有越狱攻击修改的优化查询基线。我们已向Poe平台负责任地披露了该问题，目前仍在等待其回应。代码实现已开源：https://github.com/BHui97/PLeak。
