# Continuum Attention for Neural Operators

链接: http://arxiv.org/abs/2406.06486v1

原文摘要:
Transformers, and the attention mechanism in particular, have become
ubiquitous in machine learning. Their success in modeling nonlocal, long-range
correlations has led to their widespread adoption in natural language
processing, computer vision, and time-series problems. Neural operators, which
map spaces of functions into spaces of functions, are necessarily both
nonlinear and nonlocal if they are universal; it is thus natural to ask whether
the attention mechanism can be used in the design of neural operators.
Motivated by this, we study transformers in the function space setting. We
formulate attention as a map between infinite dimensional function spaces and
prove that the attention mechanism as implemented in practice is a Monte Carlo
or finite difference approximation of this operator. The function space
formulation allows for the design of transformer neural operators, a class of
architectures designed to learn mappings between function spaces, for which we
prove a universal approximation result. The prohibitive cost of applying the
attention operator to functions defined on multi-dimensional domains leads to
the need for more efficient attention-based architectures. For this reason we
also introduce a function space generalization of the patching strategy from
computer vision, and introduce a class of associated neural operators.
Numerical results, on an array of operator learning problems, demonstrate the
promise of our approaches to function space formulations of attention and their
use in neural operators.

中文翻译:
Transformer模型，尤其是其中的注意力机制，已在机器学习领域无处不在。其在建模非局部、长程相关性方面的成功，使其被广泛应用于自然语言处理、计算机视觉和时间序列问题中。神经算子作为将函数空间映射到函数空间的工具，若想具备普适性，则必然兼具非线性和非局部特性；因此很自然地会思考：能否利用注意力机制来设计神经算子？基于这一动机，我们在函数空间框架下研究Transformer模型。

我们将注意力机制表述为无限维函数空间之间的映射，并证明实际应用中实现的注意力机制是该算子的蒙特卡洛或有限差分近似。这种函数空间表述方式为设计Transformer神经算子——一类专门学习函数空间映射的架构——提供了理论基础，我们为此类架构证明了其通用逼近性质。由于将注意力算子应用于多维域定义函数时存在极高的计算成本，这促使我们需要开发更高效的基于注意力的架构。为此，我们引入了计算机视觉中分块策略的函数空间推广，并提出了一类相关神经算子。

在多个算子学习问题上的数值实验表明，我们所提出的函数空间注意力表述方法及其在神经算子中的应用具有显著优势。
