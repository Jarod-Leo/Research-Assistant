# Large Language Models as Evaluators for Recommendation Explanations

链接: http://arxiv.org/abs/2406.03248v1

原文摘要:
The explainability of recommender systems has attracted significant attention
in academia and industry. Many efforts have been made for explainable
recommendations, yet evaluating the quality of the explanations remains a
challenging and unresolved issue. In recent years, leveraging LLMs as
evaluators presents a promising avenue in Natural Language Processing tasks
(e.g., sentiment classification, information extraction), as they perform
strong capabilities in instruction following and common-sense reasoning.
However, evaluating recommendation explanatory texts is different from these
NLG tasks, as its criteria are related to human perceptions and are usually
subjective. In this paper, we investigate whether LLMs can serve as evaluators
of recommendation explanations. To answer the question, we utilize real user
feedback on explanations given from previous work and additionally collect
third-party annotations and LLM evaluations. We design and apply a 3-level meta
evaluation strategy to measure the correlation between evaluator labels and the
ground truth provided by users. Our experiments reveal that LLMs, such as GPT4,
can provide comparable evaluations with appropriate prompts and settings. We
also provide further insights into combining human labels with the LLM
evaluation process and utilizing ensembles of multiple heterogeneous LLM
evaluators to enhance the accuracy and stability of evaluations. Our study
verifies that utilizing LLMs as evaluators can be an accurate, reproducible and
cost-effective solution for evaluating recommendation explanation texts. Our
code is available at https://github.com/Xiaoyu-SZ/LLMasEvaluator.

中文翻译:
推荐系统的可解释性在学术界和工业界引起了广泛关注。尽管研究者们为实现可解释推荐付出了诸多努力，但解释质量的评估仍是一个具有挑战性且尚未解决的难题。近年来，利用大语言模型（LLM）作为评估器在自然语言处理任务（如情感分类、信息抽取）中展现出巨大潜力，因其在指令遵循和常识推理方面表现卓越。然而，推荐解释文本的评估不同于这些自然语言生成任务，其标准涉及人类主观感知且具有高度主观性。本文探究大语言模型能否作为推荐解释的评估器。为此，我们基于前人研究的真实用户反馈数据，额外收集了第三方标注和LLM评估结果，设计并应用三级元评估策略来衡量评估者标签与用户提供基准真相之间的相关性。实验表明，如GPT-4等大语言模型在适当提示和设置下能提供与人类相当的评估质量。我们还深入探讨了如何将人工标注融入LLM评估流程，以及通过集成多个异构LLM评估器来提升评估的准确性和稳定性。本研究验证了使用LLM作为评估器能为推荐解释文本提供准确、可复现且高性价比的评估方案。代码已开源：https://github.com/Xiaoyu-SZ/LLMasEvaluator。

（翻译说明：采用学术论文的规范表达，保留专业术语如"meta evaluation"译为"元评估"；将长句合理切分为符合中文表达习惯的短句；"ground truth"译为"基准真相"符合计算机领域惯例；通过"潜力/卓越/深入探讨"等词汇保持原文严谨性；URL链接按规范完整保留）
