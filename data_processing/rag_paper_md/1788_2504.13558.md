# Transformers Can Overcome the Curse of Dimensionality: A Theoretical Study from an Approximation Perspective

链接: http://arxiv.org/abs/2504.13558v1

原文摘要:
The Transformer model is widely used in various application areas of machine
learning, such as natural language processing. This paper investigates the
approximation of the H\"older continuous function class
$\mathcal{H}_{Q}^{\beta}\left([0,1]^{d\times n},\mathbb{R}^{d\times n}\right)$
by Transformers and constructs several Transformers that can overcome the curse
of dimensionality. These Transformers consist of one self-attention layer with
one head and the softmax function as the activation function, along with
several feedforward layers. For example, to achieve an approximation accuracy
of $\epsilon$, if the activation functions of the feedforward layers in the
Transformer are ReLU and floor, only
$\mathcal{O}\left(\log\frac{1}{\epsilon}\right)$ layers of feedforward layers
are needed, with widths of these layers not exceeding
$\mathcal{O}\left(\frac{1}{\epsilon^{2/\beta}}\log\frac{1}{\epsilon}\right)$.
If other activation functions are allowed in the feedforward layers, the width
of the feedforward layers can be further reduced to a constant. These results
demonstrate that Transformers have a strong expressive capability. The
construction in this paper is based on the Kolmogorov-Arnold Representation
Theorem and does not require the concept of contextual mapping, hence our proof
is more intuitively clear compared to previous Transformer approximation works.
Additionally, the translation technique proposed in this paper helps to apply
the previous approximation results of feedforward neural networks to
Transformer research.

中文翻译:
Transformer模型在机器学习的各个应用领域，如自然语言处理中，被广泛使用。本文研究了Transformer对H\"older连续函数类$\mathcal{H}_{Q}^{\beta}\left([0,1]^{d\times n},\mathbb{R}^{d\times n}\right)$的逼近，并构造了若干能克服维度诅咒的Transformer。这些Transformer由一个使用softmax函数作为激活函数的单头自注意力层和若干前馈层组成。例如，要达到$\epsilon$的逼近精度，若Transformer中前馈层的激活函数为ReLU和floor，则仅需$\mathcal{O}\left(\log\frac{1}{\epsilon}\right)$层前馈层，且这些层的宽度不超过$\mathcal{O}\left(\frac{1}{\epsilon^{2/\beta}}\log\frac{1}{\epsilon}\right)$；若允许前馈层使用其他激活函数，则前馈层的宽度可进一步缩减为常数。这些结果表明Transformer具有很强的表达能力。本文的构造基于Kolmogorov-Arnold表示定理，且不需要上下文映射的概念，因此我们的证明相较于之前的Transformer逼近工作更加直观清晰。此外，本文提出的平移技术有助于将前馈神经网络已有的逼近结果应用到Transformer研究中。
