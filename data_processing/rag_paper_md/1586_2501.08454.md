# Tag&Tab: Pretraining Data Detection in Large Language Models Using Keyword-Based Membership Inference Attack

链接: http://arxiv.org/abs/2501.08454v1

原文摘要:
Large language models (LLMs) have become essential digital task assistance
tools. Their training relies heavily on the collection of vast amounts of data,
which may include copyright-protected or sensitive information. Recent studies
on the detection of pretraining data in LLMs have primarily focused on
sentence-level or paragraph-level membership inference attacks (MIAs), usually
involving probability analysis of the target model prediction tokens. However,
the proposed methods often demonstrate poor performance, specifically in terms
of accuracy, failing to account for the semantic importance of textual content
and word significance. To address these shortcomings, we propose Tag&Tab, a
novel approach for detecting data that has been used as part of the LLM
pretraining. Our method leverages advanced natural language processing (NLP)
techniques to tag keywords in the input text - a process we term Tagging. Then,
the LLM is used to obtain the probabilities of these keywords and calculate
their average log-likelihood to determine input text membership, a process we
refer to as Tabbing. Our experiments on three benchmark datasets (BookMIA,
MIMIR, and the Pile) and several open-source LLMs of varying sizes demonstrate
an average increase in the AUC scores ranging from 4.1% to 12.1% over
state-of-the-art methods. Tag&Tab not only sets a new standard for data leakage
detection in LLMs, but its outstanding performance is a testament to the
importance of words in MIAs on LLMs.

中文翻译:
大型语言模型（LLMs）已成为不可或缺的数字任务辅助工具。其训练高度依赖海量数据收集，这些数据可能包含受版权保护或敏感信息。当前针对LLM预训练数据检测的研究主要集中于句子级或段落级的成员推理攻击（MIAs），通常涉及对目标模型预测标记的概率分析。然而，现有方法普遍表现欠佳，尤其在准确性方面，未能充分考虑文本内容的语义重要性及词汇显著性。为弥补这些不足，我们提出Tag&Tab——一种创新的LLM预训练数据检测方法。该方法运用先进自然语言处理（NLP）技术对输入文本进行关键词标注（即"标记"阶段），随后利用LLM获取这些关键词的概率并计算其平均对数似然以判定文本成员资格（即"制表"阶段）。在三个基准数据集（BookMIA、MIMIR和The Pile）及多个不同规模开源LLM上的实验表明，相较于最先进方法，该方法使AUC分数平均提升4.1%至12.1%。Tag&Tab不仅为LLM数据泄露检测树立了新标杆，其卓越性能更印证了词汇在LLM成员推理攻击中的关键作用。
