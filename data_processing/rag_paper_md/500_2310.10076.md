# Verbosity Bias in Preference Labeling by Large Language Models

链接: http://arxiv.org/abs/2310.10076v1

原文摘要:
In recent years, Large Language Models (LLMs) have witnessed a remarkable
surge in prevalence, altering the landscape of natural language processing and
machine learning. One key factor in improving the performance of LLMs is
alignment with humans achieved with Reinforcement Learning from Human Feedback
(RLHF), as for many LLMs such as GPT-4, Bard, etc. In addition, recent studies
are investigating the replacement of human feedback with feedback from other
LLMs named Reinforcement Learning from AI Feedback (RLAIF). We examine the
biases that come along with evaluating LLMs with other LLMs and take a closer
look into verbosity bias -- a bias where LLMs sometimes prefer more verbose
answers even if they have similar qualities. We see that in our problem
setting, GPT-4 prefers longer answers more than humans. We also propose a
metric to measure this bias.

中文翻译:
近年来，大型语言模型（LLMs）的普及度显著提升，重塑了自然语言处理与机器学习的格局。提升LLMs性能的关键因素之一在于通过人类反馈强化学习（RLHF）实现与人类的对齐，这一点在诸如GPT-4、Bard等众多模型中得以体现。此外，近期研究正探索用其他LLMs的反馈替代人类反馈，即AI反馈强化学习（RLAIF）。本文深入分析了使用LLMs评估其他LLMs时伴随的偏见，尤其聚焦于冗长偏好——即LLMs有时更倾向于选择表述更冗长的答案，即使其质量相近。研究发现，在我们的实验设定下，GPT-4相比人类表现出更强的长答案偏好，并据此提出了一种量化该偏见的度量方法。
