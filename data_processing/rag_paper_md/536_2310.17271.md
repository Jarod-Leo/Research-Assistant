# Understanding the Role of Input Token Characters in Language Models: How Does Information Loss Affect Performance?

链接: http://arxiv.org/abs/2310.17271v1

原文摘要:
Understanding how and what pre-trained language models (PLMs) learn about
language is an open challenge in natural language processing. Previous work has
focused on identifying whether they capture semantic and syntactic information,
and how the data or the pre-training objective affects their performance.
However, to the best of our knowledge, no previous work has specifically
examined how information loss in input token characters affects the performance
of PLMs. In this study, we address this gap by pre-training language models
using small subsets of characters from individual tokens. Surprisingly, we find
that pre-training even under extreme settings, i.e. using only one character of
each token, the performance retention in standard NLU benchmarks and probing
tasks compared to full-token models is high. For instance, a model pre-trained
only on single first characters from tokens achieves performance retention of
approximately $90$\% and $77$\% of the full-token model in SuperGLUE and GLUE
tasks, respectively.

中文翻译:
理解预训练语言模型（PLMs）如何以及学习语言的哪些方面，是自然语言处理领域一个开放的挑战。先前的研究主要集中在识别模型是否捕捉了语义和句法信息，以及数据或预训练目标如何影响其性能。然而，据我们所知，尚无研究专门探讨输入词符字符信息丢失对PLMs性能的影响。本研究通过仅使用词符中极小子集字符进行预训练，填补了这一空白。令人惊讶的是，即使在极端设置下（例如仅使用每个词符的一个字符）进行预训练，模型在标准自然语言理解基准测试和探测任务中，相比完整词符模型仍能保持较高性能表现。例如，仅基于词符首字符预训练的模型，在SuperGLUE和GLUE任务中分别能达到完整词符模型约90%和77%的性能保留率。
