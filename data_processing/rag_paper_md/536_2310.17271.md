# Understanding the Role of Input Token Characters in Language Models: How Does Information Loss Affect Performance?

链接: http://arxiv.org/abs/2310.17271v1

原文摘要:
Understanding how and what pre-trained language models (PLMs) learn about
language is an open challenge in natural language processing. Previous work has
focused on identifying whether they capture semantic and syntactic information,
and how the data or the pre-training objective affects their performance.
However, to the best of our knowledge, no previous work has specifically
examined how information loss in input token characters affects the performance
of PLMs. In this study, we address this gap by pre-training language models
using small subsets of characters from individual tokens. Surprisingly, we find
that pre-training even under extreme settings, i.e. using only one character of
each token, the performance retention in standard NLU benchmarks and probing
tasks compared to full-token models is high. For instance, a model pre-trained
only on single first characters from tokens achieves performance retention of
approximately $90$\% and $77$\% of the full-token model in SuperGLUE and GLUE
tasks, respectively.

中文翻译:
以下是符合要求的学术中文翻译：

理解预训练语言模型（PLMs）如何学习语言以及学习哪些语言知识，是自然语言处理领域一个尚未解决的挑战。现有研究主要集中于分析模型是否捕获语义与句法信息，以及训练数据或预训练目标如何影响其性能。然而据我们所知，此前尚未有研究专门考察输入词符的字符信息缺失对PLMs性能的影响。本研究通过仅使用词符中有限字符子集进行模型预训练来填补这一空白。令人惊讶的是，即使在极端设定下（例如仅使用每个词符的单个字符）进行预训练，模型在标准自然语言理解基准测试和探测任务中仍能保持与完整词符模型相近的性能表现。具体而言，仅基于词符首字符预训练的模型在SuperGLUE和GLUE任务中分别能达到完整词符模型约90%和77%的性能保留率。

（翻译说明：
1. 专业术语统一处理："pre-trained language models"规范译为"预训练语言模型"并首次出现标注英文缩写PLMs
2. 被动语态转化："has been focused on"转换为主动式"主要集中于"
3. 长句拆分：将原文复合长句拆分为符合中文表达习惯的短句结构
4. 数据呈现规范：保留原始研究数据格式（90%和77%）及基准测试名称（SuperGLUE/GLUE）的原貌
5. 学术表达："performance retention"译为"性能保留率"符合计算机领域术语
6. 逻辑连接：通过"例如""具体而言"等连接词保持论证逻辑的连贯性）
