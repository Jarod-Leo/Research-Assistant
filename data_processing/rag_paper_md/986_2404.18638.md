# Reinforcement Learning Problem Solving with Large Language Models

链接: http://arxiv.org/abs/2404.18638v1

原文摘要:
Large Language Models (LLMs) encapsulate an extensive amount of world
knowledge, and this has enabled their application in various domains to improve
the performance of a variety of Natural Language Processing (NLP) tasks. This
has also facilitated a more accessible paradigm of conversation-based
interactions between humans and AI systems to solve intended problems. However,
one interesting avenue that shows untapped potential is the use of LLMs as
Reinforcement Learning (RL) agents to enable conversational RL problem solving.
Therefore, in this study, we explore the concept of formulating Markov Decision
Process-based RL problems as LLM prompting tasks. We demonstrate how LLMs can
be iteratively prompted to learn and optimize policies for specific RL tasks.
In addition, we leverage the introduced prompting technique for episode
simulation and Q-Learning, facilitated by LLMs. We then show the practicality
of our approach through two detailed case studies for "Research Scientist" and
"Legal Matter Intake" workflows.

中文翻译:
大型语言模型（LLMs）蕴含了丰富的世界知识，这使其能够应用于多个领域以提升各类自然语言处理（NLP）任务的性能。同时，该特性也促成了更便捷的人机对话交互范式，用于解决目标问题。然而，一个尚未充分挖掘的有趣方向是将LLMs作为强化学习（RL）智能体，实现基于对话的RL问题求解。为此，本研究探索了将基于马尔可夫决策过程的RL问题转化为LLM提示任务的方法。我们展示了如何通过迭代式提示让LLMs学习并优化特定RL任务的策略，并利用所提出的提示技术实现由LLMs驱动的场景模拟与Q学习。最后，通过"科研人员工作流"和"法律事务受理"两个详细案例研究，验证了该方法的实用性。
