# Knowledge Graph Guided Semantic Evaluation of Language Models For User Trust

链接: http://arxiv.org/abs/2305.04989v1

原文摘要:
A fundamental question in natural language processing is - what kind of
language structure and semantics is the language model capturing? Graph formats
such as knowledge graphs are easy to evaluate as they explicitly express
language semantics and structure. This study evaluates the semantics encoded in
the self-attention transformers by leveraging explicit knowledge graph
structures. We propose novel metrics to measure the reconstruction error when
providing graph path sequences from a knowledge graph and trying to
reproduce/reconstruct the same from the outputs of the self-attention
transformer models. The opacity of language models has an immense bearing on
societal issues of trust and explainable decision outcomes. Our findings
suggest that language models are models of stochastic control processes for
plausible language pattern generation. However, they do not ascribe object and
concept-level meaning and semantics to the learned stochastic patterns such as
those described in knowledge graphs. Furthermore, to enable robust evaluation
of concept understanding by language models, we construct and make public an
augmented language understanding benchmark built on the General Language
Understanding Evaluation (GLUE) benchmark. This has significant
application-level user trust implications as stochastic patterns without a
strong sense of meaning cannot be trusted in high-stakes applications.

中文翻译:
自然语言处理中的一个核心问题是：语言模型究竟捕捉了何种语言结构与语义？知识图谱等图结构因其明确表达了语言语义和结构而易于评估。本研究通过利用显式的知识图谱结构，评估了自注意力Transformer模型所编码的语义信息。我们提出创新性指标，用于衡量当输入知识图谱路径序列时，模型输出结果与原始图谱路径的重构误差。语言模型的不透明性深刻影响着社会层面的信任问题与决策结果的可解释性。研究发现表明，语言模型本质上是为生成合理语言模式而建立的随机控制过程模型，但并未像知识图谱那样为习得的随机模式赋予对象及概念层级的含义与语义。此外，为强化对语言模型概念理解能力的稳健评估，我们在通用语言理解评估基准（GLUE）基础上构建并公开了一个增强版语言理解基准。这一发现具有重要的应用级用户信任意义——缺乏明确意义感知的随机模式无法在高风险应用场景中获得信任。
