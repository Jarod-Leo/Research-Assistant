# Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks

链接: http://arxiv.org/abs/2403.09832v1

原文摘要:
Large Language Models (LLMs) are increasingly becoming the preferred
foundation platforms for many Natural Language Processing tasks such as Machine
Translation, owing to their quality often comparable to or better than
task-specific models, and the simplicity of specifying the task through natural
language instructions or in-context examples. Their generality, however, opens
them up to subversion by end users who may embed into their requests
instructions that cause the model to behave in unauthorized and possibly unsafe
ways. In this work we study these Prompt Injection Attacks (PIAs) on multiple
families of LLMs on a Machine Translation task, focusing on the effects of
model size on the attack success rates. We introduce a new benchmark data set
and we discover that on multiple language pairs and injected prompts written in
English, larger models under certain conditions may become more susceptible to
successful attacks, an instance of the Inverse Scaling phenomenon (McKenzie et
al., 2023). To our knowledge, this is the first work to study non-trivial LLM
scaling behaviour in a multi-lingual setting.

中文翻译:
大型语言模型（LLMs）正日益成为众多自然语言处理任务（如机器翻译）的首选基础平台，这得益于其质量常与专用模型相当甚至更优，且通过自然语言指令或上下文示例即可简洁地定义任务。然而，其通用性也使其面临终端用户的潜在颠覆——用户可能在请求中嵌入特定指令，导致模型以未经授权甚至不安全的方式运作。本研究针对机器翻译任务，探究了不同系列LLMs面临的提示注入攻击（PIAs），重点关注模型规模对攻击成功率的影响。我们引入了一个新的基准数据集，并在多语言对及英文编写的注入提示实验中发现：特定条件下，规模更大的模型反而更容易遭受成功攻击，这一现象印证了逆向缩放效应（McKenzie等，2023）。据我们所知，这是首个在多语言环境中系统研究LLMs非平凡缩放行为的工作。
