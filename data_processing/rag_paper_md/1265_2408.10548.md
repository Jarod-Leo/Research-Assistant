# Language Modeling on Tabular Data: A Survey of Foundations, Techniques and Evolution

链接: http://arxiv.org/abs/2408.10548v1

原文摘要:
Tabular data, a prevalent data type across various domains, presents unique
challenges due to its heterogeneous nature and complex structural
relationships. Achieving high predictive performance and robustness in tabular
data analysis holds significant promise for numerous applications. Influenced
by recent advancements in natural language processing, particularly transformer
architectures, new methods for tabular data modeling have emerged. Early
techniques concentrated on pre-training transformers from scratch, often
encountering scalability issues. Subsequently, methods leveraging pre-trained
language models like BERT have been developed, which require less data and
yield enhanced performance. The recent advent of large language models, such as
GPT and LLaMA, has further revolutionized the field, facilitating more advanced
and diverse applications with minimal fine-tuning. Despite the growing
interest, a comprehensive survey of language modeling techniques for tabular
data remains absent. This paper fills this gap by providing a systematic review
of the development of language modeling for tabular data, encompassing: (1) a
categorization of different tabular data structures and data types; (2) a
review of key datasets used in model training and tasks used for evaluation;
(3) a summary of modeling techniques including widely-adopted data processing
methods, popular architectures, and training objectives; (4) the evolution from
adapting traditional Pre-training/Pre-trained language models to the
utilization of large language models; (5) an identification of persistent
challenges and potential future research directions in language modeling for
tabular data analysis. GitHub page associated with this survey is available at:
https://github.com/lanxiang1017/Language-Modeling-on-Tabular-Data-Survey.git.

中文翻译:
表格数据作为一种跨领域普遍存在的数据类型，因其异构性和复杂的结构关系而带来独特挑战。在表格数据分析中实现高预测性能与鲁棒性，对众多应用场景具有重大意义。受自然语言处理领域（尤其是Transformer架构）近期突破的启发，表格数据建模新方法不断涌现。早期技术主要聚焦于从头预训练Transformer模型，但常面临可扩展性问题；随后发展出基于BERT等预训练语言模型的方法，这些方法数据需求更少且性能更优；而GPT、LLaMA等大语言模型的出现进一步推动领域革新，通过少量微调即可实现更先进多样的应用。尽管研究热度持续攀升，目前仍缺乏对表格数据语言建模技术的系统性综述。本文通过梳理表格数据语言建模的发展脉络填补这一空白，内容包括：（1）不同表格数据结构与数据类型的分类体系；（2）模型训练关键数据集与评估任务的全面盘点；（3）涵盖主流数据处理方法、常用架构和训练目标的建模技术总结；（4）从传统预训练/预训练语言模型适配到大语言模型应用的演进路径；（5）表格数据分析语言建模中持续存在的挑战与未来潜在研究方向。本综述关联GitHub页面详见：https://github.com/lanxiang1017/Language-Modeling-on-Tabular-Data-Survey.git。
