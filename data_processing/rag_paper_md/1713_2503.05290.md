# MatrixFlow: System-Accelerator co-design for high-performance transformer applications

链接: http://arxiv.org/abs/2503.05290v1

原文摘要:
Transformers are central to advances in artificial intelligence (AI),
excelling in fields ranging from computer vision to natural language
processing. Despite their success, their large parameter count and
computational demands challenge efficient acceleration. To address these
limitations, this paper proposes MatrixFlow, a novel co-designed
system-accelerator architecture based on a loosely coupled systolic array
including a new software mapping approach for efficient transformer code
execution. MatrixFlow is co-optimized via a novel dataflow-based matrix
multiplication technique that reduces memory overhead. These innovations
significantly improve data throughput, which is critical for handling the
extensive computations required by transformers. We validate our approach
through full system simulation using gem5 across various BERT and ViT
Transformer models featuring different data types, demonstrating significant
application-wide speed-ups. Our method achieves up to a 22x improvement
compared to a many-core CPU system, and outperforms the closest
state-of-the-art loosely-coupled and tightly-coupled accelerators by over 5x
and 8x, respectively.

中文翻译:
Transformer模型是推动人工智能进步的核心技术，在计算机视觉到自然语言处理等多个领域表现卓越。然而，其庞大的参数量与计算需求对高效加速提出了挑战。为突破这些限制，本文提出MatrixFlow——一种基于松散耦合脉动阵列的协同设计系统-加速器架构，包含创新的软件映射方法以实现高效Transformer代码执行。该系统通过基于数据流的矩阵乘法协同优化技术降低内存开销，显著提升数据处理吞吐量，这对处理Transformer所需的大规模计算至关重要。我们采用gem5全系统仿真平台，在不同数据类型的BERT和ViT Transformer模型上进行验证，结果表明该方法能实现显著的全局加速效果：相比众核CPU系统最高可提升22倍性能，较最先进的松散耦合与紧密耦合加速器分别实现超过5倍和8倍的性能优势。
