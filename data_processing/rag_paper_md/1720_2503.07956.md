# EFPC: Towards Efficient and Flexible Prompt Compression

链接: http://arxiv.org/abs/2503.07956v1

原文摘要:
The emergence of large language models (LLMs) like GPT-4 has revolutionized
natural language processing (NLP), enabling diverse, complex tasks. However,
extensive token counts lead to high computational and financial burdens. To
address this, we propose Efficient and Flexible Prompt Compression (EFPC), a
novel method unifying task-aware and task-agnostic compression for a favorable
accuracy-efficiency trade-off. EFPC uses GPT-4 to generate compressed prompts
and integrates them with original prompts for training. During training and
inference, we selectively prepend user instructions and compress prompts based
on predicted probabilities. EFPC is highly data-efficient, achieving
significant performance with minimal data. Compared to the state-of-the-art
method LLMLingua-2, EFPC achieves a 4.8% relative improvement in F1-score with
1% additional data at a 4x compression rate, and an 11.4% gain with 10%
additional data on the LongBench single-doc QA benchmark. EFPC's unified
framework supports broad applicability and enhances performance across various
models, tasks, and domains, offering a practical advancement in NLP.

中文翻译:
以GPT-4为代表的大语言模型（LLMs）的出现彻底改变了自然语言处理（NLP）领域，使其能够执行多样化复杂任务。然而，庞大的token数量导致高昂的计算与财务成本。为此，我们提出高效灵活提示压缩方法（EFPC），通过统一任务感知与任务无关的压缩策略，实现精度与效率的优化平衡。该方法利用GPT-4生成压缩提示，并将其与原始提示融合训练。在训练和推理阶段，我们基于预测概率动态选择前置用户指令并进行提示压缩。EFPC具有卓越的数据效率，仅需少量数据即可显著提升性能。相比当前最先进的LLMLingua-2方法，在LongBench单文档问答基准测试中，EFPC在4倍压缩率下仅增加1%数据量就实现F1值4.8%的相对提升，当数据量增加10%时性能增益达11.4%。这种统一框架具备广泛适用性，能有效提升跨模型、跨任务、跨领域的表现，为NLP领域提供了切实可行的技术突破。
