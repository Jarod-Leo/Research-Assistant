# CaTs and DAGs: Integrating Directed Acyclic Graphs with Transformers and Fully-Connected Neural Networks for Causally Constrained Predictions

链接: http://arxiv.org/abs/2410.14485v1

原文摘要:
Artificial Neural Networks (ANNs), including fully-connected networks and
transformers, are highly flexible and powerful function approximators, widely
applied in fields like computer vision and natural language processing.
However, their inability to inherently respect causal structures can limit
their robustness, making them vulnerable to covariate shift and difficult to
interpret/explain. This poses significant challenges for their reliability in
real-world applications. In this paper, we introduce Causal Fully-Connected
Neural Networks (CFCNs) and Causal Transformers (CaTs), two general model
families designed to operate under predefined causal constraints, as specified
by a Directed Acyclic Graph (DAG). These models retain the powerful function
approximation abilities of traditional neural networks while adhering to the
underlying structural constraints, improving robustness, reliability, and
interpretability at inference time. This approach opens new avenues for
deploying neural networks in more demanding, real-world scenarios where
robustness and explainability is critical.

中文翻译:
人工神经网络（ANNs），包括全连接网络和Transformer模型，因其高度灵活且强大的函数逼近能力，被广泛应用于计算机视觉与自然语言处理等领域。然而，这类模型本质上无法遵循因果结构，这会削弱其鲁棒性——不仅对协变量偏移敏感，也难以进行解释/归因分析，从而严重制约了实际应用中的可靠性。本文提出因果全连接神经网络（CFCNs）和因果Transformer（CaTs）两类通用模型框架，它们能够在有向无环图（DAG）定义的因果约束条件下运行。这些模型既保留了传统神经网络强大的函数逼近能力，又遵循底层结构约束，在推理时显著提升了鲁棒性、可靠性和可解释性。该方法为神经网络在要求严苛的现实场景（如需要强健性和可解释性的关键领域）中的应用开辟了新途径。
