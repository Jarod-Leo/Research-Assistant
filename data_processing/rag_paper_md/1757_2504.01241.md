# Catastrophic Forgetting in LLMs: A Comparative Analysis Across Language Tasks

链接: http://arxiv.org/abs/2504.01241v1

原文摘要:
Large Language Models (LLMs) have significantly advanced Natural Language
Processing (NLP), particularly in Natural Language Understanding (NLU) tasks.
As we progress toward an agentic world where LLM-based agents autonomously
handle specialized tasks, it becomes crucial for these models to adapt to new
tasks without forgetting previously learned information - a challenge known as
catastrophic forgetting. This study evaluates the continual fine-tuning of
various open-source LLMs with different parameter sizes (specifically models
under 10 billion parameters) on key NLU tasks from the GLUE benchmark,
including SST-2, MRPC, CoLA, and MNLI. By employing prompt engineering and
task-specific adjustments, we assess and compare the models' abilities to
retain prior knowledge while learning new tasks. Our results indicate that
models such as Phi-3.5-mini exhibit minimal forgetting while maintaining strong
learning capabilities, making them well-suited for continual learning
environments. Additionally, models like Orca-2-7b and Qwen2.5-7B demonstrate
impressive learning abilities and overall performance after fine-tuning. This
work contributes to understanding catastrophic forgetting in LLMs and
highlights prompting engineering to optimize model performance for continual
learning scenarios.

中文翻译:
大型语言模型（LLM）显著推动了自然语言处理（NLP）的发展，尤其在自然语言理解（NLU）任务中表现突出。随着我们迈向由LLM驱动的智能体自主处理专业任务的"代理世界"，这些模型必须能在适应新任务的同时不遗忘已学知识——这一挑战被称为灾难性遗忘。本研究评估了多种开源LLM（参数规模均在100亿以下）在GLUE基准关键NLU任务（包括SST-2、MRPC、CoLA和MNLI）上的持续微调表现。通过采用提示工程和任务特定调整，我们系统评估比较了模型在学习新任务时保留原有知识的能力。结果表明，Phi-3.5-mini等模型在保持强大学习能力的同时仅出现极少量遗忘，非常适合持续学习环境；而Orca-2-7b和Qwen2.5-7B等模型在微调后展现出卓越的学习能力和综合性能。该研究不仅深化了对LLM灾难性遗忘现象的理解，更通过提示工程为持续学习场景下的模型性能优化提供了实践路径。
