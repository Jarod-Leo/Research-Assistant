# Investigating Task Arithmetic for Zero-Shot Information Retrieval

链接: http://arxiv.org/abs/2505.00649v1

原文摘要:
Large Language Models (LLMs) have shown impressive zero-shot performance
across a variety of Natural Language Processing tasks, including document
re-ranking. However, their effectiveness degrades on unseen tasks and domains,
largely due to shifts in vocabulary and word distributions. In this paper, we
investigate Task Arithmetic, a technique that combines the weights of LLMs
pre-trained on different tasks or domains via simple mathematical operations,
such as addition or subtraction, to adapt retrieval models without requiring
additional fine-tuning. Our method is able to synthesize diverse tasks and
domain knowledge into a single model, enabling effective zero-shot adaptation
in different retrieval contexts. Extensive experiments on publicly available
scientific, biomedical, and multilingual datasets show that our method improves
state-of-the-art re-ranking performance by up to 18% in NDCG@10 and 15% in
P@10. In addition to these empirical gains, our analysis provides insights into
the strengths and limitations of Task Arithmetic as a practical strategy for
zero-shot learning and model adaptation. We make our code publicly available at
https://github.com/DetectiveMB/Task-Arithmetic-for-ZS-IR.

中文翻译:
大型语言模型（LLMs）在包括文档重排序在内的多种自然语言处理任务中展现出卓越的零样本性能。然而，面对未知任务和领域时其效能会下降，这主要源于词汇和词分布的变化。本文研究了任务算术技术——通过对不同任务或领域预训练的LLM权重进行简单数学运算（如加减法）来融合模型，从而无需额外微调即可适配检索任务。该方法能将多样化的任务与领域知识整合至单一模型，实现不同检索场景下的高效零样本适配。在公开的科学、生物医学及多语言数据集上的大量实验表明，我们的方法将NDCG@10和P@10指标下的最先进重排序性能分别提升了18%和15%。除实证效果外，分析还揭示了任务算术作为零样本学习与模型适配实用策略的优势与局限。代码已开源于https://github.com/DetectiveMB/Task-Arithmetic-for-ZS-IR。
