# Investigating Task Arithmetic for Zero-Shot Information Retrieval

链接: http://arxiv.org/abs/2505.00649v1

原文摘要:
Large Language Models (LLMs) have shown impressive zero-shot performance
across a variety of Natural Language Processing tasks, including document
re-ranking. However, their effectiveness degrades on unseen tasks and domains,
largely due to shifts in vocabulary and word distributions. In this paper, we
investigate Task Arithmetic, a technique that combines the weights of LLMs
pre-trained on different tasks or domains via simple mathematical operations,
such as addition or subtraction, to adapt retrieval models without requiring
additional fine-tuning. Our method is able to synthesize diverse tasks and
domain knowledge into a single model, enabling effective zero-shot adaptation
in different retrieval contexts. Extensive experiments on publicly available
scientific, biomedical, and multilingual datasets show that our method improves
state-of-the-art re-ranking performance by up to 18% in NDCG@10 and 15% in
P@10. In addition to these empirical gains, our analysis provides insights into
the strengths and limitations of Task Arithmetic as a practical strategy for
zero-shot learning and model adaptation. We make our code publicly available at
https://github.com/DetectiveMB/Task-Arithmetic-for-ZS-IR.

中文翻译:
以下是符合要求的学术摘要中文翻译：

大型语言模型（LLMs）在包括文档重排序在内的多种自然语言处理任务中展现出卓越的零样本性能。然而，当面对未知任务和领域时，其效能会显著下降，这主要源于词汇和词分布的变化。本文研究"任务算术"技术——通过简单的数学运算（如加减法）整合不同任务或领域预训练LLMs的权重，从而无需额外微调即可实现检索模型的自适应。我们的方法能够将多样化任务与领域知识融合至单一模型，实现不同检索场景下的有效零样本适应。基于公开科学文献、生物医学及多语言数据集的广泛实验表明，该方法在NDCG@10和P@10指标上分别将当前最优重排序性能提升达18%和15%。除实证效果外，我们的分析还揭示了任务算术作为零样本学习与模型适应策略的优势与局限。代码已开源：https://github.com/DetectiveMB/Task-Arithmetic-for-ZS-IR。

（翻译严格遵循以下原则：
1. 专业术语准确统一（如zero-shot/零样本、NDCG@10等）
2. 被动语态转换为中文主动表述（如"have shown"→"展现出"）
3. 长句合理切分（如将原文复合句拆分为多个短句）
4. 学术用语规范化（"empirical gains"→"实证效果"）
5. 保留技术概念核心含义（"Task Arithmetic"→"任务算术"并添加引号强调）
6. 数字及专有名词完整保留
7. 流畅性与准确性平衡（如"shifts in vocabulary and word distributions"意译为"词汇和词分布的变化"））
