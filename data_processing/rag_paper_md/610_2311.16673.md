# Large Language Models Meet Computer Vision: A Brief Survey

链接: http://arxiv.org/abs/2311.16673v1

原文摘要:
Recently, the intersection of Large Language Models (LLMs) and Computer
Vision (CV) has emerged as a pivotal area of research, driving significant
advancements in the field of Artificial Intelligence (AI). As transformers have
become the backbone of many state-of-the-art models in both Natural Language
Processing (NLP) and CV, understanding their evolution and potential
enhancements is crucial. This survey paper delves into the latest progressions
in the domain of transformers and their subsequent successors, emphasizing
their potential to revolutionize Vision Transformers (ViTs) and LLMs. This
survey also presents a comparative analysis, juxtaposing the performance
metrics of several leading paid and open-source LLMs, shedding light on their
strengths and areas of improvement as well as a literature review on how LLMs
are being used to tackle vision related tasks. Furthermore, the survey presents
a comprehensive collection of datasets employed to train LLMs, offering
insights into the diverse data available to achieve high performance in various
pre-training and downstream tasks of LLMs. The survey is concluded by
highlighting open directions in the field, suggesting potential venues for
future research and development. This survey aims to underscores the profound
intersection of LLMs on CV, leading to a new era of integrated and advanced AI
models.

中文翻译:
近年来，大型语言模型（LLMs）与计算机视觉（CV）的交叉领域已成为关键研究方向，推动着人工智能（AI）领域的重大进展。随着Transformer架构逐渐成为自然语言处理（NLP）和CV领域众多前沿模型的基石，理解其演进路径与优化潜力至关重要。本综述论文深入探讨了Transformer及其后续衍生模型的最新进展，重点分析其对视觉Transformer（ViTs）和LLMs的变革性影响。通过对比分析多款主流付费与开源LLMs的性能指标，本研究揭示了其优势所在与改进空间，并对LLMs处理视觉相关任务的文献进行了系统性回顾。此外，本文全面整理了用于训练LLMs的数据集资源，为如何利用多样化数据提升LLMs在预训练及下游任务中的性能提供了见解。最后，本研究指明了该领域的开放性问题，为未来研发方向提出潜在路径。本综述旨在强调LLMs与CV的深度融合，预示着集成化先进AI模型新时代的到来。
