# Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the Ugly

链接: http://arxiv.org/abs/2310.03150v1

原文摘要:
Large Language Models (LLM) and foundation models are popular as they offer
new opportunities for individuals and businesses to improve natural language
processing, interact with data, and retrieve information faster. However,
training or fine-tuning LLMs requires a vast amount of data, which can be
challenging to access due to legal or technical restrictions and may require
private computing resources. Federated Learning (FL) is a solution designed to
overcome these challenges and expand data access for deep learning
applications.
  This paper takes a hardware-centric approach to explore how LLMs can be
brought to modern edge computing systems. Our study fine-tunes the FLAN-T5
model family, ranging from 80M to 3B parameters, using FL for a text
summarization task. We provide a micro-level hardware benchmark, compare the
model FLOP utilization to a state-of-the-art data center GPU, and study the
network utilization in realistic conditions. Our contribution is twofold:
First, we evaluate the current capabilities of edge computing systems and their
potential for LLM FL workloads. Second, by comparing these systems with a
data-center GPU, we demonstrate the potential for improvement and the next
steps toward achieving greater computational efficiency at the edge.

中文翻译:
大型语言模型（LLM）与基础模型因其为个人与企业提升自然语言处理能力、优化数据交互及加速信息检索提供了新机遇而广受欢迎。然而，训练或微调LLM需要海量数据，这些数据可能因法律或技术限制难以获取，且往往依赖私有计算资源。联邦学习（FL）正是为解决这些挑战、扩展深度学习应用数据访问而设计的解决方案。

本文采用以硬件为核心的研究方法，探讨如何将LLM部署至现代边缘计算系统。我们通过联邦学习对参数规模从8000万到30亿不等的FLAN-T5模型家族进行文本摘要任务的微调，提供微观层面硬件基准测试，将模型浮点运算利用率与尖端数据中心GPU进行对比，并研究实际场景中的网络利用率。研究贡献体现在两方面：首先，评估边缘计算系统当前处理LLM联邦学习工作负载的能力与潜力；其次，通过与数据中心GPU的对比，揭示边缘设备提升计算效能的改进空间及未来技术路径。
