# Reusing Softmax Hardware Unit for GELU Computation in Transformers

链接: http://arxiv.org/abs/2402.10118v1

原文摘要:
Transformers have improved drastically the performance of natural language
processing (NLP) and computer vision applications. The computation of
transformers involves matrix multiplications and non-linear activation
functions such as softmax and GELU (Gaussion Error Linear Unit) that are
accelerated directly in hardware. Currently, function evaluation is done
separately for each function and rarely allows for hardware reuse. To mitigate
this problem, in this work, we map the computation of GELU to a softmax
operator. In this way, the efficient hardware units designed already for
softmax can be reused for computing GELU as well. Computation of GELU can enjoy
the inherent vectorized nature of softmax and produce in parallel multiple GELU
outcomes. Experimental results show that computing GELU via a pre-existing and
incrementally modified softmax hardware unit (a) does not reduce the accuracy
of representative NLP applications and (b) allows the reduction of the overall
hardware area and power by 6.1% and 11.9%, respectively, on average.

中文翻译:
以下是符合要求的学术摘要中文翻译：

Transformer模型显著提升了自然语言处理（NLP）与计算机视觉应用的性能。其计算过程涉及矩阵乘法及softmax、高斯误差线性单元（GELU）等非线性激活函数，这些运算通常由硬件直接加速。现有方案中，各函数的计算单元独立工作，鲜少实现硬件复用。为解决该问题，本研究将GELU运算映射至softmax算子，从而复用现有高效softmax硬件单元来执行GELU计算。该方法可继承softmax的固有向量化特性，实现多GELU结果的并行输出。实验表明：（1）通过增量修改现有softmax硬件单元计算GELU不会降低典型NLP应用的精度；（2）平均可减少6.1%的硬件面积与11.9%的功耗。

（翻译严格遵循以下原则：
1. 专业术语规范统一（如Transformer/GELU不译，softmax视语境处理）
2. 被动语态转换为中文主动句式（如"are accelerated"→"由...加速"）
3. 长难句合理切分（如将实验结果的并列关系用分号结构化呈现）
4. 保留学术文本的严谨性（如"respectively"译为"分别"的隐式处理）
5. 符合中文科技论文摘要的简洁特征（全文无冗余修饰，信息密度高））
