# Reusing Softmax Hardware Unit for GELU Computation in Transformers

链接: http://arxiv.org/abs/2402.10118v1

原文摘要:
Transformers have improved drastically the performance of natural language
processing (NLP) and computer vision applications. The computation of
transformers involves matrix multiplications and non-linear activation
functions such as softmax and GELU (Gaussion Error Linear Unit) that are
accelerated directly in hardware. Currently, function evaluation is done
separately for each function and rarely allows for hardware reuse. To mitigate
this problem, in this work, we map the computation of GELU to a softmax
operator. In this way, the efficient hardware units designed already for
softmax can be reused for computing GELU as well. Computation of GELU can enjoy
the inherent vectorized nature of softmax and produce in parallel multiple GELU
outcomes. Experimental results show that computing GELU via a pre-existing and
incrementally modified softmax hardware unit (a) does not reduce the accuracy
of representative NLP applications and (b) allows the reduction of the overall
hardware area and power by 6.1% and 11.9%, respectively, on average.

中文翻译:
Transformer模型显著提升了自然语言处理(NLP)与计算机视觉应用的性能。其计算过程涉及矩阵乘法及softmax、高斯误差线性单元(GELU)等非线性激活函数，这些运算通常直接在硬件中加速实现。当前各类函数的硬件评估单元各自独立设计，极少实现硬件资源共享。为解决这一问题，本研究将GELU的计算映射至softmax算子，使得已为softmax优化的硬件单元可复用于GELU计算。通过这种方式，GELU计算既能继承softmax固有的向量化特性，又可并行生成多个计算结果。实验表明：基于现有软硬件单元进行增量式修改的GELU计算方案(a)在典型NLP应用中未导致精度下降，(b)平均可减少6.1%的硬件面积和11.9%的功耗。
