# A Trip Towards Fairness: Bias and De-Biasing in Large Language Models

链接: http://arxiv.org/abs/2305.13862v1

原文摘要:
Cheap-to-Build Very Large-Language Models (CtB-LLMs) with affordable training
are emerging as the next big revolution in natural language processing and
understanding. These CtB-LLMs are democratizing access to trainable Very
Large-Language Models (VLLMs) and, thus, may represent the building blocks of
many NLP systems solving downstream tasks. Hence, a little or a large bias in
CtB-LLMs may cause huge harm. In this paper, we performed a large investigation
of the bias of three families of CtB-LLMs, and we showed that debiasing
techniques are effective and usable. Indeed, according to current tests, the
LLaMA and the OPT families have an important bias in gender, race, religion,
and profession. In contrast to the analysis for other LLMs, we discovered that
bias depends not on the number of parameters but on the perplexity. Finally,
the debiasing of OPT using LoRA reduces bias up to 4.12 points in the
normalized stereotype score.

中文翻译:
低成本构建且训练费用可承受的超大规模语言模型（CtB-LLMs）正成为自然语言处理与理解领域的下一场重大革命。这类CtB-LLM降低了超大规模可训练语言模型（VLLMs）的使用门槛，有望成为解决下游任务的众多NLP系统基础组件。然而，CtB-LLMs中无论微小或显著的偏见都可能造成严重危害。本文针对三个CtB-LLM家族的偏见展开大规模研究，证实了去偏见技术的有效性与实用性。当前测试表明，LLaMA和OPT系列模型在性别、种族、宗教及职业维度存在显著偏见。与其他大语言模型的分析不同，我们发现其偏见程度不取决于参数量，而是与模型困惑度相关。最终通过LoRA技术对OPT模型进行去偏处理后，其标准化刻板印象评分最高可降低4.12分。
