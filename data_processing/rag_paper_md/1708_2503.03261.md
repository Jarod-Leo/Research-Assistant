# Can Frontier LLMs Replace Annotators in Biomedical Text Mining? Analyzing Challenges and Exploring Solutions

链接: http://arxiv.org/abs/2503.03261v1

原文摘要:
Large language models (LLMs) can perform various natural language processing
(NLP) tasks through in-context learning without relying on supervised data.
However, multiple previous studies have reported suboptimal performance of LLMs
in biological text mining. By analyzing failure patterns in these evaluations,
we identified three primary challenges for LLMs in biomedical corpora: (1) LLMs
fail to learn implicit dataset-specific nuances from supervised data, (2) The
common formatting requirements of discriminative tasks limit the reasoning
capabilities of LLMs particularly for LLMs that lack test-time compute, and (3)
LLMs struggle to adhere to annotation guidelines and match exact schemas, which
hinders their ability to understand detailed annotation requirements which is
essential in biomedical annotation workflow. To address these challenges, we
experimented with prompt engineering techniques targeted to the above issues,
and developed a pipeline that dynamically extracts instructions from annotation
guidelines. Our findings show that frontier LLMs can approach or surpass the
performance of state-of-the-art (SOTA) BERT-based models with minimal reliance
on manually annotated data and without fine-tuning. Furthermore, we performed
model distillation on a closed-source LLM, demonstrating that a BERT model
trained exclusively on synthetic data annotated by LLMs can also achieve a
practical performance. Based on these results, we explored the feasibility of
partially replacing manual annotation with LLMs in production scenarios for
biomedical text mining.

中文翻译:
大型语言模型（LLMs）能够通过上下文学习执行各类自然语言处理任务，而无需依赖监督数据。然而，多项前期研究表明，LLMs在生物医学文本挖掘中的表现不尽如人意。通过分析这些评估中的失败模式，我们识别出LLMs在生物医学语料库中面临的三大核心挑战：（1）无法从监督数据中学习数据集特有的隐含特征；（2）判别性任务常见的格式化要求限制了LLMs（尤其是缺乏测试时计算资源的模型）的推理能力；（3）难以严格遵循标注指南和匹配精确模式，这阻碍了其对生物医学标注工作流中关键细节要求的理解。

针对这些挑战，我们实验了针对上述问题的提示工程技术，并开发了一个动态从标注指南中提取指令的流程。研究结果表明，前沿LLMs在极少依赖人工标注数据且未经微调的情况下，其性能可接近或超越基于BERT的最先进模型。此外，我们对闭源LLM进行了模型蒸馏实验，证明仅使用LLMs标注的合成数据训练的BERT模型也能达到实用性能。基于这些发现，我们探讨了在生物医学文本挖掘生产场景中，用LLMs部分替代人工标注的可行性。
