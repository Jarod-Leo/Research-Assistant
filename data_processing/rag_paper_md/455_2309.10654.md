# CFGPT: Chinese Financial Assistant with Large Language Model

链接: http://arxiv.org/abs/2309.10654v1

原文摘要:
Large language models (LLMs) have demonstrated great potential in natural
language processing tasks within the financial domain. In this work, we present
a Chinese Financial Generative Pre-trained Transformer framework, named CFGPT,
which includes a dataset~(CFData) for pre-training and supervised fine-tuning,
a financial LLM~(CFLLM) to adeptly manage financial texts, and a deployment
framework~(CFAPP) designed to navigate real-world financial applications. The
CFData comprising both a pre-training dataset and a supervised fine-tuning
dataset, where the pre-training dataset collates Chinese financial data and
analytics, alongside a smaller subset of general-purpose text with 584M
documents and 141B tokens in total, and the supervised fine-tuning dataset is
tailored for six distinct financial tasks, embodying various facets of
financial analysis and decision-making with 1.5M instruction pairs and 1.5B
tokens in total. The CFLLM, which is based on InternLM-7B to balance the model
capability and size, is trained on CFData in two stage, continued pre-training
and supervised fine-tuning. The CFAPP is centered on large language models
(LLMs) and augmented with additional modules to ensure multifaceted
functionality in real-world application. Our codes are released at
https://github.com/TongjiFinLab/CFGPT.

中文翻译:
大型语言模型（LLMs）在金融领域的自然语言处理任务中展现出巨大潜力。本研究提出名为CFGPT的中文金融生成预训练框架，包含三部分：用于预训练与监督微调的数据集CFData、擅长处理金融文本的金融大模型CFLLM，以及面向实际金融应用的部署框架CFAPP。CFData由预训练数据集和监督微调数据集构成——预训练数据集整合了中文金融数据与分析文本，辅以少量通用语料，总计5.84亿文档、1410亿token；监督微调数据集则针对六类金融任务定制，涵盖金融分析与决策的多个维度，包含150万条指令对、15亿token。CFLLM基于InternLM-7B模型以平衡能力与规模，通过CFData进行两阶段训练（持续预训练与监督微调）。CFAPP以LLM为核心，通过功能模块扩展确保实际应用的多维需求。代码已开源于https://github.com/TongjiFinLab/CFGPT。
