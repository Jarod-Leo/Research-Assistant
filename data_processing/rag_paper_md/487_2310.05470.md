# Generative Judge for Evaluating Alignment

链接: http://arxiv.org/abs/2310.05470v1

原文摘要:
The rapid development of Large Language Models (LLMs) has substantially
expanded the range of tasks they can address. In the field of Natural Language
Processing (NLP), researchers have shifted their focus from conventional NLP
tasks (e.g., sequence tagging and parsing) towards tasks that revolve around
aligning with human needs (e.g., brainstorming and email writing). This shift
in task distribution imposes new requirements on evaluating these aligned
models regarding generality (i.e., assessing performance across diverse
scenarios), flexibility (i.e., examining under different protocols), and
interpretability (i.e., scrutinizing models with explanations). In this paper,
we propose a generative judge with 13B parameters, Auto-J, designed to address
these challenges. Our model is trained on user queries and LLM-generated
responses under massive real-world scenarios and accommodates diverse
evaluation protocols (e.g., pairwise response comparison and single-response
evaluation) with well-structured natural language critiques. To demonstrate the
efficacy of our approach, we construct a new testbed covering 58 different
scenarios. Experimentally, Auto-J outperforms a series of strong competitors,
including both open-source and closed-source models, by a large margin. We also
provide detailed analysis and case studies to further reveal the potential of
our method and make a variety of resources public at
https://github.com/GAIR-NLP/auto-j.

中文翻译:
大型语言模型（LLM）的快速发展极大地拓展了其可处理的任务范围。在自然语言处理（NLP）领域，研究者的关注点已从传统NLP任务（如序列标注和句法分析）转向以人类需求为核心的任务（如头脑风暴和邮件撰写）。这种任务分布的变化对评估这些对齐模型提出了新要求，包括通用性（即评估模型在多样化场景下的表现）、灵活性（即支持不同评估协议）和可解释性（即通过自然语言解释来审视模型）。本文提出一个130亿参数生成式评判模型Auto-J，旨在应对这些挑战。该模型基于海量真实场景中的用户查询与LLM生成响应进行训练，能适配多种评估协议（如双响应对比和单响应评估），并提供结构清晰的批评性分析。为验证方法有效性，我们构建了覆盖58个场景的新测试平台。实验表明，Auto-J显著优于包括开源与闭源模型在内的一系列强基线模型。通过详细分析与案例研究，我们进一步揭示了该方法的潜力，并将相关资源公开于https://github.com/GAIR-NLP/auto-j。
