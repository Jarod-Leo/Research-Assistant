# SPADE: Self-supervised Pretraining for Acoustic DisEntanglement

链接: http://arxiv.org/abs/2302.01483v1

原文摘要:
Self-supervised representation learning approaches have grown in popularity
due to the ability to train models on large amounts of unlabeled data and have
demonstrated success in diverse fields such as natural language processing,
computer vision, and speech. Previous self-supervised work in the speech domain
has disentangled multiple attributes of speech such as linguistic content,
speaker identity, and rhythm. In this work, we introduce a self-supervised
approach to disentangle room acoustics from speech and use the acoustic
representation on the downstream task of device arbitration. Our results
demonstrate that our proposed approach significantly improves performance over
a baseline when labeled training data is scarce, indicating that our
pretraining scheme learns to encode room acoustic information while remaining
invariant to other attributes of the speech signal.

中文翻译:
自监督表示学习方法因其能在海量无标注数据上训练模型而日益受到青睐，并在自然语言处理、计算机视觉及语音等多个领域展现出卓越成效。既往语音领域的自监督研究已成功分离出语言内容、说话人身份和节奏等多重特征。本研究提出一种自监督框架，旨在从语音信号中解耦房间混响特性，并将所得声学表征应用于设备仲裁的下游任务。实验结果表明：在标注训练数据稀缺时，相较于基线模型，本方法能显著提升任务性能，这证实了我们的预训练方案能有效编码房间声学信息，同时保持对语音信号其他属性的不变性。
