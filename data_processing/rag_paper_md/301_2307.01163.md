# Improving Language Plasticity via Pretraining with Active Forgetting

链接: http://arxiv.org/abs/2307.01163v1

原文摘要:
Pretrained language models (PLMs) are today the primary model for natural
language processing. Despite their impressive downstream performance, it can be
difficult to apply PLMs to new languages, a barrier to making their
capabilities universally accessible. While prior work has shown it possible to
address this issue by learning a new embedding layer for the new language,
doing so is both data and compute inefficient. We propose to use an active
forgetting mechanism during pretraining, as a simple way of creating PLMs that
can quickly adapt to new languages. Concretely, by resetting the embedding
layer every K updates during pretraining, we encourage the PLM to improve its
ability of learning new embeddings within a limited number of updates, similar
to a meta-learning effect. Experiments with RoBERTa show that models pretrained
with our forgetting mechanism not only demonstrate faster convergence during
language adaptation but also outperform standard ones in a low-data regime,
particularly for languages that are distant from English.

中文翻译:
预训练语言模型（PLMs）已成为当前自然语言处理的核心技术。尽管其在下游任务中表现卓越，但将PLMs应用于新语言时仍面临挑战，这阻碍了其能力的全球普及。已有研究提出通过学习新语言的嵌入层来解决该问题，但这种方法在数据和计算效率上均存在不足。我们提出在预训练阶段引入主动遗忘机制，作为创建能快速适应新语言的PLMs的简易途径。具体而言，通过在预训练期间每K次更新重置嵌入层，我们促使PLM在有限更新次数内提升学习新嵌入的能力，类似于元学习效应。基于RoBERTa的实验表明，采用遗忘机制预训练的模型不仅在语言适应阶段收敛更快，在低数据场景下（尤其是与英语差异较大的语言）其性能也优于标准模型。
