# Controllable Text Generation for Large Language Models: A Survey

链接: http://arxiv.org/abs/2408.12599v1

原文摘要:
In Natural Language Processing (NLP), Large Language Models (LLMs) have
demonstrated high text generation quality. However, in real-world applications,
LLMs must meet increasingly complex requirements. Beyond avoiding misleading or
inappropriate content, LLMs are also expected to cater to specific user needs,
such as imitating particular writing styles or generating text with poetic
richness. These varied demands have driven the development of Controllable Text
Generation (CTG) techniques, which ensure that outputs adhere to predefined
control conditions--such as safety, sentiment, thematic consistency, and
linguistic style--while maintaining high standards of helpfulness, fluency, and
diversity.
  This paper systematically reviews the latest advancements in CTG for LLMs,
offering a comprehensive definition of its core concepts and clarifying the
requirements for control conditions and text quality. We categorize CTG tasks
into two primary types: content control and attribute control. The key methods
are discussed, including model retraining, fine-tuning, reinforcement learning,
prompt engineering, latent space manipulation, and decoding-time intervention.
We analyze each method's characteristics, advantages, and limitations,
providing nuanced insights for achieving generation control. Additionally, we
review CTG evaluation methods, summarize its applications across domains, and
address key challenges in current research, including reduced fluency and
practicality. We also propose several appeals, such as placing greater emphasis
on real-world applications in future research. This paper aims to offer
valuable guidance to researchers and developers in the field. Our reference
list and Chinese version are open-sourced at
https://github.com/IAAR-Shanghai/CTGSurvey.

中文翻译:
在自然语言处理（NLP）领域，大语言模型（LLMs）已展现出卓越的文本生成能力。然而实际应用中，LLMs需要满足日益复杂的生成需求：除避免产生误导性或不当内容外，还需实现模仿特定文风、生成富有诗意的文本等个性化要求。这些多样化需求推动了可控文本生成（CTG）技术的发展，其核心在于确保输出文本既符合预设的控制条件（如安全性、情感倾向、主题一致性和语言风格），又能保持高水平的实用性、流畅性和多样性。

本文系统梳理了LLMs可控文本生成的最新研究进展，首先明确定义了CTG的核心概念，厘清了控制条件与文本质量要求。我们将CTG任务划分为内容控制与属性控制两大类型，重点探讨了模型重训练、微调、强化学习、提示工程、潜空间操控和解码干预等六类主流方法，详细分析各类方法的特性、优势与局限，为生成控制提供多维度实现思路。此外，本文综述了CTG评估体系，总结了各领域应用现状，指出现有研究在流畅性下降、实用性不足等方面的关键挑战，并提出加强实际场景应用研究等多项倡议，旨在为领域研究者与开发者提供有价值的参考。参考文献列表及中文版开源发布于https://github.com/IAAR-Shanghai/CTGSurvey。
