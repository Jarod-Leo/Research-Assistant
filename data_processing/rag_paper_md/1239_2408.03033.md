# L3iTC at the FinLLM Challenge Task: Quantization for Financial Text Classification & Summarization

链接: http://arxiv.org/abs/2408.03033v1

原文摘要:
This article details our participation (L3iTC) in the FinLLM Challenge Task
2024, focusing on two key areas: Task 1, financial text classification, and
Task 2, financial text summarization. To address these challenges, we
fine-tuned several large language models (LLMs) to optimize performance for
each task. Specifically, we used 4-bit quantization and LoRA to determine which
layers of the LLMs should be trained at a lower precision. This approach not
only accelerated the fine-tuning process on the training data provided by the
organizers but also enabled us to run the models on low GPU memory. Our
fine-tuned models achieved third place for the financial classification task
with an F1-score of 0.7543 and secured sixth place in the financial
summarization task on the official test datasets.

中文翻译:
本文详细介绍了我们团队（L3iTC）参与2024年FinLLM挑战赛的情况，重点聚焦两大核心任务：任务1的金融文本分类与任务2的金融文本摘要生成。为应对这些挑战，我们通过微调多种大语言模型（LLMs）来优化各任务表现。具体而言，我们采用4位量化和LoRA技术来确定LLMs中哪些层级适合以较低精度进行训练。该方法不仅显著加速了在主办方提供训练数据上的微调过程，还使得模型能够在低GPU显存环境下运行。经官方测试集验证，我们微调后的模型在金融分类任务中以0.7543的F1分数获得第三名，同时在金融摘要生成任务中位列第六。

（注：根据学术论文摘要的翻译规范，在保持专业术语准确性的前提下，对原文进行了符合中文科技论文表达的句式重组。关键术语处理说明：
1. "4-bit quantization"译为"4位量化"符合计算机领域术语标准
2. "LoRA"保留英文缩写（因其在中文文献中普遍采用原名）
3. "F1-score"采用"F1分数"的通用译法
4. 将英语长句合理切分为符合中文阅读习惯的短句，如将原文最后一句拆分为两个独立成果表述）
