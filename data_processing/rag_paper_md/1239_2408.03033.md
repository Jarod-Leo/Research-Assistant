# L3iTC at the FinLLM Challenge Task: Quantization for Financial Text Classification & Summarization

链接: http://arxiv.org/abs/2408.03033v1

原文摘要:
This article details our participation (L3iTC) in the FinLLM Challenge Task
2024, focusing on two key areas: Task 1, financial text classification, and
Task 2, financial text summarization. To address these challenges, we
fine-tuned several large language models (LLMs) to optimize performance for
each task. Specifically, we used 4-bit quantization and LoRA to determine which
layers of the LLMs should be trained at a lower precision. This approach not
only accelerated the fine-tuning process on the training data provided by the
organizers but also enabled us to run the models on low GPU memory. Our
fine-tuned models achieved third place for the financial classification task
with an F1-score of 0.7543 and secured sixth place in the financial
summarization task on the official test datasets.

中文翻译:
本文详细介绍了我们团队（L3iTC）参与FinLLM挑战赛2024的情况，重点聚焦于两大核心任务：任务1的金融文本分类与任务2的金融文本摘要生成。针对这些挑战，我们通过微调多种大语言模型（LLMs）来优化各任务表现。具体而言，采用4位量化技术和LoRA方法确定模型各层应采用的训练精度级别，这一策略不仅加速了组委会提供训练数据的微调过程，还实现了在低显存GPU上的模型运行。最终我们的微调模型在官方测试集上取得金融分类任务F1值0.7543的季军成绩，并在金融摘要生成任务中位列第六。
