# SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models

链接: http://arxiv.org/abs/2503.07605v1

原文摘要:
Large Language Models have achieved remarkable success across various natural
language processing tasks, yet their high computational cost during inference
remains a major bottleneck. This paper introduces Sparse Expert Activation
Pruning (SEAP), a training-free pruning method that selectively retains
task-relevant parameters to reduce inference overhead. Inspired by the
clustering patterns of hidden states and activations in LLMs, SEAP identifies
task-specific expert activation patterns and prunes the model while preserving
task performance and enhancing computational efficiency. Experimental results
demonstrate that SEAP significantly reduces computational overhead while
maintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both
WandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2%
performance drop compared to the dense model. These findings highlight SEAP's
scalability and effectiveness, making it a promising approach for optimizing
large-scale LLMs.

中文翻译:
大型语言模型在各类自然语言处理任务中取得了显著成就，但其推理阶段的高计算成本仍是主要瓶颈。本文提出无需训练的稀疏专家激活剪枝方法（SEAP），通过选择性保留任务相关参数来降低推理开销。受大模型中隐藏状态与激活聚类特性的启发，SEAP识别任务特定的专家激活模式，在维持任务性能的同时提升计算效率。实验表明，SEAP能显著减少计算开销且保持竞争力：50%剪枝率时性能超越WandA和FLAP方法20%以上，20%剪枝率时相较稠密模型仅产生2.2%的性能下降。这些发现证明了SEAP的可扩展性与有效性，为大规模语言模型优化提供了新思路。
