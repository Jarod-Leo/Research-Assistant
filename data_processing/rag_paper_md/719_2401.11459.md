# AttentionLego: An Open-Source Building Block For Spatially-Scalable Large Language Model Accelerator With Processing-In-Memory Technology

链接: http://arxiv.org/abs/2401.11459v1

原文摘要:
Large language models (LLMs) with Transformer architectures have become
phenomenal in natural language processing, multimodal generative artificial
intelligence, and agent-oriented artificial intelligence. The self-attention
module is the most dominating sub-structure inside Transformer-based LLMs.
Computation using general-purpose graphics processing units (GPUs) inflicts
reckless demand for I/O bandwidth for transferring intermediate calculation
results between memories and processing units. To tackle this challenge, this
work develops a fully customized vanilla self-attention accelerator,
AttentionLego, as the basic building block for constructing spatially
expandable LLM processors. AttentionLego provides basic implementation with
fully-customized digital logic incorporating Processing-In-Memory (PIM)
technology. It is based on PIM-based matrix-vector multiplication and look-up
table-based Softmax design. The open-source code is available online:
https://bonany.cc/attentionleg.

中文翻译:
基于Transformer架构的大语言模型（LLM）在自然语言处理、多模态生成式人工智能以及面向代理的人工智能领域展现出非凡能力。自注意力模块作为Transformer类LLM中最核心的子结构，其通用图形处理器（GPU）计算方式因需在存储单元与处理单元间频繁传输中间结果，导致对I/O带宽的极端需求。为应对这一挑战，本研究开发了一款全定制化基础自注意力加速器AttentionLego，作为构建可空间扩展LLM处理器的基本单元。该加速器采用存内计算（PIM）技术实现全定制数字逻辑，核心设计包括基于PIM的矩阵向量乘法器和查表式Softmax模块。项目开源代码详见：https://bonany.cc/attentionleg。
