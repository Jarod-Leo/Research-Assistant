# LEGO: Language Model Building Blocks

链接: http://arxiv.org/abs/2410.18287v1

原文摘要:
Large language models (LLMs) are essential in natural language processing
(NLP) but are costly in data collection, pre-training, fine-tuning, and
inference. Task-specific small language models (SLMs) offer a cheaper
alternative but lack robustness and generalization. This paper proposes LEGO, a
novel technique to extract SLMs from an LLM and recombine them. Using
state-of-the-art LLM pruning strategies, we can create task- and user-specific
SLM building blocks that are efficient for fine-tuning and inference while also
preserving user data privacy. LEGO utilizes Federated Learning and a novel
aggregation scheme for the LLM reconstruction, maintaining robustness without
high costs and preserving user data privacy. We experimentally demonstrate the
versatility of LEGO, showing its ability to enable model heterogeneity and
mitigate the effects of data heterogeneity while maintaining LLM robustness.

中文翻译:
大型语言模型（LLM）在自然语言处理（NLP）中占据核心地位，但其数据收集、预训练、微调及推理过程成本高昂。针对特定任务的小型语言模型（SLM）虽成本较低，却存在鲁棒性与泛化能力不足的问题。本文提出LEGO技术，通过从LLM中提取并重组SLM模块，结合前沿的LLM剪枝策略，构建出高效且保护用户数据隐私的任务定制化SLM组件。该技术采用联邦学习框架与创新的聚合方案进行LLM重构，在避免高成本的同时保持模型鲁棒性。实验证明LEGO具备多重优势：支持模型异构性、缓解数据异构性影响，并始终维持LLM级别的稳健表现。
