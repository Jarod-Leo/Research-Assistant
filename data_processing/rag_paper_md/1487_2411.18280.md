# Neutralizing Backdoors through Information Conflicts for Large Language Models

链接: http://arxiv.org/abs/2411.18280v1

原文摘要:
Large language models (LLMs) have seen significant advancements, achieving
superior performance in various Natural Language Processing (NLP) tasks, from
understanding to reasoning. However, they remain vulnerable to backdoor
attacks, where models behave normally for standard queries but generate harmful
responses or unintended output when specific triggers are activated. Existing
backdoor defenses often suffer from drawbacks that they either focus on
detection without removal, rely on rigid assumptions about trigger properties,
or prove to be ineffective against advanced attacks like multi-trigger
backdoors. In this paper, we present a novel method to eliminate backdoor
behaviors from LLMs through the construction of information conflicts using
both internal and external mechanisms. Internally, we leverage a lightweight
dataset to train a conflict model, which is then merged with the backdoored
model to neutralize malicious behaviors by embedding contradictory information
within the model's parametric memory. Externally, we incorporate convincing
contradictory evidence into the prompt to challenge the model's internal
backdoor knowledge. Experimental results on classification and conversational
tasks across 4 widely used LLMs demonstrate that our method outperforms 8
state-of-the-art backdoor defense baselines. We can reduce the attack success
rate of advanced backdoor attacks by up to 98% while maintaining over 90% clean
data accuracy. Furthermore, our method has proven to be robust against adaptive
backdoor attacks. The code will be open-sourced upon publication.

中文翻译:
大型语言模型（LLMs）已取得显著进展，在从理解到推理的各类自然语言处理（NLP）任务中展现出卓越性能。然而，它们仍易受后门攻击威胁——模型对常规查询表现正常，却在特定触发器激活时生成有害响应或非预期输出。现有后门防御方案普遍存在局限：要么仅聚焦检测而无法清除后门，要么依赖对触发器特性的僵化假设，或对多触发器后门等高级攻击束手无策。本文提出一种创新方法，通过内部与外部机制构建信息冲突来消除LLMs的后门行为。内部机制方面，我们利用轻量级数据集训练冲突模型，将其与受感染模型融合，通过在参数记忆中嵌入矛盾信息来中和恶意行为；外部机制方面，我们在输入提示中加入具有说服力的矛盾证据，以挑战模型内部的后门知识。在4种主流LLMs上进行的分类和对话任务实验表明，本方法优于8种最先进的后门防御基线方案，能将高级后门攻击成功率降低达98%，同时保持90%以上的干净数据准确率。此外，该方法对自适应后门攻击展现出强鲁棒性。代码将在论文发表后开源。
