# NoWag: A Unified Framework for Shape Preserving Compression of Large Language Models

链接: http://arxiv.org/abs/2504.14569v1

原文摘要:
Large language models (LLMs) exhibit remarkable performance across various
natural language processing tasks but suffer from immense computational and
memory demands, limiting their deployment in resource-constrained environments.
To address this challenge, we propose NoWag: (Normalized Weight and Activation
Guided Compression), a unified framework for zero-shot shape preserving
compression algorithms. We compressed Llama-2 7B/13B/70B and Llama-3 8/70BB
models, using two popular forms of shape-preserving compression, vector
quantization NoWag-VQ (NoWag for Vector Quantization), and
unstructured/semi-structured pruning NoWag-P (NoWag for Pruning). We found that
NoWag-VQ significantly outperforms state-of-the-art zero shot VQ, and that
NoWag-P performs competitively against state-of-the-art methods. These results
suggest commonalities between these compression paradigms that could inspire
future work. Our code is available at https://github.com/LawrenceRLiu/NoWag

中文翻译:
大型语言模型（LLMs）在各类自然语言处理任务中展现出卓越性能，但其庞大的计算与内存需求制约了在资源受限环境中的部署。为应对这一挑战，我们提出NoWag框架（标准化权重与激活引导的压缩技术），这是一种零样本形状保持压缩算法的统一框架。我们采用两种主流形状保持压缩形式——向量量化NoWag-VQ（面向向量量化的NoWag）和非结构化/半结构化剪枝NoWag-P（面向剪枝的NoWag），对Llama-2 7B/13B/70B及Llama-3 8B/70B模型进行压缩。实验表明，NoWag-VQ显著优于当前最先进的零样本向量量化方法，而NoWag-P与前沿方法相比也具竞争力。这些发现揭示了不同压缩范式间的共性特征，可为未来研究提供启示。项目代码已开源于https://github.com/LawrenceRLiu/NoWag。
