# Partial Tensorized Transformers for Natural Language Processing

链接: http://arxiv.org/abs/2310.20077v1

原文摘要:
The transformer architecture has revolutionized Natural Language Processing
(NLP) and other machine-learning tasks, due to its unprecedented accuracy.
However, their extensive memory and parameter requirements often hinder their
practical applications. In this work, we study the effect of tensor-train
decomposition to improve the accuracy and compress transformer vision-language
neural networks, namely BERT and ViT. We focus both on embedding-layer
compression and partial tensorization of neural networks (PTNN) through an
algorithmic approach. Our novel PTNN approach significantly improves the
accuracy of existing models by up to 5%, all without the need for post-training
adjustments, breaking new ground in the field of tensor decomposition.

中文翻译:
变压器架构凭借其前所未有的准确性，彻底改变了自然语言处理（NLP）及其他机器学习任务。然而，其庞大的内存与参数需求常阻碍实际应用。本研究探讨了张量链分解对提升视觉语言神经网络（BERT与ViT）精度及压缩效果的贡献，重点通过算法路径实现嵌入层压缩与神经网络部分张量化（PTNN）。我们提出的新型PTNN方法将现有模型精度最高提升5%，且无需训练后调整，在张量分解领域实现了突破性进展。
