# Massively Multilingual Shallow Fusion with Large Language Models

链接: http://arxiv.org/abs/2302.08917v1

原文摘要:
While large language models (LLM) have made impressive progress in natural
language processing, it remains unclear how to utilize them in improving
automatic speech recognition (ASR). In this work, we propose to train a single
multilingual language model (LM) for shallow fusion in multiple languages. We
push the limits of the multilingual LM to cover up to 84 languages by scaling
up using a mixture-of-experts LLM, i.e., generalist language model (GLaM). When
the number of experts increases, GLaM dynamically selects only two at each
decoding step to keep the inference computation roughly constant. We then apply
GLaM to a multilingual shallow fusion task based on a state-of-the-art
end-to-end model. Compared to a dense LM of similar computation during
inference, GLaM reduces the WER of an English long-tail test set by 4.4%
relative. In a multilingual shallow fusion task, GLaM improves 41 out of 50
languages with an average relative WER reduction of 3.85%, and a maximum
reduction of 10%. Compared to the baseline model, GLaM achieves an average WER
reduction of 5.53% over 43 languages.

中文翻译:
尽管大语言模型（LLM）在自然语言处理领域取得了显著进展，但其在提升自动语音识别（ASR）性能方面的应用路径仍不明确。本研究提出训练单一多语言语言模型（LM），用于支持多语言的浅层融合技术。通过采用专家混合架构的大语言模型——通用语言模型（GLaM），我们将多语言LM的覆盖能力突破至84种语言。随着专家数量增加，GLaM在每一步解码时动态选择仅两个专家参与计算，从而保持推理过程的计算量基本恒定。基于最先进的端到端模型框架，我们将GLaM应用于多语言浅层融合任务。相较于计算量相近的密集型LM，GLaM在英语长尾测试集上相对降低了4.4%的词错误率（WER）。在多语言浅层融合任务中，GLaM在50种语言中的41种上实现性能提升，平均相对WER降低3.85%，最大降幅达10%。与基线模型相比，GLaM在43种语言上平均WER降低5.53%。
