# MCSD: An Efficient Language Model with Diverse Fusion

链接: http://arxiv.org/abs/2406.12230v1

原文摘要:
Transformers excel in Natural Language Processing (NLP) due to their prowess
in capturing long-term dependencies but suffer from exponential resource
consumption with increasing sequence lengths. To address these challenges, we
propose MCSD model, an efficient language model with linear scaling and fast
inference speed. MCSD model leverages diverse feature fusion, primarily through
the multi-channel slope and decay (MCSD) block, to robustly represent features.
This block comprises slope and decay sections that extract features across
diverse temporal receptive fields, facilitating capture of both local and
global information. In addition, MCSD block conducts element-wise fusion of
diverse features to further enhance the delicate feature extraction capability.
For inference, we formulate the inference process into a recurrent
representation, slashing space complexity to $O(1)$ and time complexity to
$O(N)$ respectively. Our experiments show that MCSD attains higher throughput
and lower GPU memory consumption compared to Transformers, while maintaining
comparable performance to larger-scale language learning models on benchmark
tests. These attributes position MCSD as a promising base for edge deployment
and embodied intelligence.

中文翻译:
Transformer凭借其捕捉长程依赖关系的卓越能力在自然语言处理(NLP)领域表现出色，但随着序列长度增加会面临计算资源指数级消耗的问题。为应对这些挑战，我们提出具有线性复杂度与快速推理特性的高效语言模型MCSD。该模型通过多通道斜率衰减(MCSD)模块实现多样化特征融合，构建鲁棒的特征表征体系。该模块由斜率与衰减双分支构成，能跨多尺度时序感受野提取特征，有效捕获局部与全局信息；同时采用逐元素特征融合策略，进一步提升精细化特征提取能力。在推理阶段，我们将计算过程重构为循环表示形式，使空间复杂度降至O(1)，时间复杂度保持O(N)线性增长。实验表明，MCSD在保持与大规模语言模型相当性能的同时，相比Transformer实现了更高吞吐量与更低GPU内存占用。这些特性使MCSD成为边缘计算部署与具身智能应用的理想基础架构。
