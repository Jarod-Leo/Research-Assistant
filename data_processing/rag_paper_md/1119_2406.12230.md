# MCSD: An Efficient Language Model with Diverse Fusion

链接: http://arxiv.org/abs/2406.12230v1

原文摘要:
Transformers excel in Natural Language Processing (NLP) due to their prowess
in capturing long-term dependencies but suffer from exponential resource
consumption with increasing sequence lengths. To address these challenges, we
propose MCSD model, an efficient language model with linear scaling and fast
inference speed. MCSD model leverages diverse feature fusion, primarily through
the multi-channel slope and decay (MCSD) block, to robustly represent features.
This block comprises slope and decay sections that extract features across
diverse temporal receptive fields, facilitating capture of both local and
global information. In addition, MCSD block conducts element-wise fusion of
diverse features to further enhance the delicate feature extraction capability.
For inference, we formulate the inference process into a recurrent
representation, slashing space complexity to $O(1)$ and time complexity to
$O(N)$ respectively. Our experiments show that MCSD attains higher throughput
and lower GPU memory consumption compared to Transformers, while maintaining
comparable performance to larger-scale language learning models on benchmark
tests. These attributes position MCSD as a promising base for edge deployment
and embodied intelligence.

中文翻译:
以下是符合学术规范的中文翻译：

Transformer模型因其卓越的长距离依赖捕捉能力在自然语言处理（NLP）领域表现突出，但随着序列长度增加会面临计算资源消耗呈指数级增长的问题。为应对这些挑战，我们提出具有线性复杂度与快速推理特性的高效语言模型MCSD。该模型通过多通道斜率衰减（MCSD）模块实现多样化特征融合，构建鲁棒的特征表征体系。该模块包含斜率与衰减双通路结构，能在不同时间感受野范围内提取特征，有效捕捉局部与全局信息。此外，MCSD模块通过元素级特征融合机制进一步增强精细化特征提取能力。在推理阶段，我们将计算过程重构为循环表示形式，使空间复杂度降至O(1)，时间复杂度保持O(N)线性增长。实验表明：在基准测试中，MCSD在保持与大规模语言学习模型相当性能的同时，相比Transformer实现了更高吞吐量与更低GPU显存消耗。这些特性使MCSD成为边缘计算部署与具身智能应用的理想基础架构。

（翻译严格遵循以下原则：
1. 专业术语统一（如"receptive fields"译为"感受野"）
2. 被动语态转换（英文被动句转为中文主动表述）
3. 长句拆分重组（如将复合从句分解为符合中文表达习惯的短句）
4. 概念准确传达（如"embodied intelligence"译为具身智能）
5. 保留数学符号规范（O(1)/O(N)等复杂度表示）
6. 学术用语规范化（"throughput"译为"吞吐量"而非"产量"））
