# Benchmarking Large Language Model Capabilities for Conditional Generation

链接: http://arxiv.org/abs/2306.16793v1

原文摘要:
Pre-trained large language models (PLMs) underlie most new developments in
natural language processing. They have shifted the field from
application-specific model pipelines to a single model that is adapted to a
wide range of tasks. Autoregressive PLMs like GPT-3 or PaLM, alongside
techniques like few-shot learning, have additionally shifted the output
modality to generation instead of classification or regression. Despite their
ubiquitous use, the generation quality of language models is rarely evaluated
when these models are introduced. Additionally, it is unclear how existing
generation tasks--while they can be used to compare systems at a high
level--relate to the real world use cases for which people have been adopting
them. In this work, we discuss how to adapt existing application-specific
generation benchmarks to PLMs and provide an in-depth, empirical study of the
limitations and capabilities of PLMs in natural language generation tasks along
dimensions such as scale, architecture, input and output language. Our results
show that PLMs differ in their applicability to different data regimes and
their generalization to multiple languages and inform which PLMs to use for a
given generation task setup. We share best practices to be taken into
consideration when benchmarking generation capabilities during the development
of upcoming PLMs.

中文翻译:
预训练大语言模型（PLMs）已成为自然语言处理领域大多数新进展的核心技术。它们推动该领域从针对特定应用的模型流水线转向单一模型适配多样化任务。诸如GPT-3或PaLM这类自回归PLMs，结合小样本学习等技术，进一步将输出模式转向生成式任务而非分类或回归。尽管应用广泛，这些语言模型在发布时其生成质量却鲜少被系统评估。此外，现有生成任务虽可用于高层次系统对比，但其与人们实际采用的应用场景之间的关联仍不明确。本研究探讨了如何将面向特定应用的生成基准适配至PLMs，并通过规模、架构、输入输出语言等多维度，对PLMs在自然语言生成任务中的能力与局限展开深度实证分析。实验结果表明，不同PLMs在数据适应性、多语言泛化能力方面存在显著差异，这为特定生成任务场景下的模型选择提供了依据。我们总结了在开发新一代PLMs时评估生成能力需考虑的最佳实践准则。
