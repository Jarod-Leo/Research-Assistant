# Crafting Interpretable Embeddings by Asking LLMs Questions

链接: http://arxiv.org/abs/2405.16714v1

原文摘要:
Large language models (LLMs) have rapidly improved text embeddings for a
growing array of natural-language processing tasks. However, their opaqueness
and proliferation into scientific domains such as neuroscience have created a
growing need for interpretability. Here, we ask whether we can obtain
interpretable embeddings through LLM prompting. We introduce question-answering
embeddings (QA-Emb), embeddings where each feature represents an answer to a
yes/no question asked to an LLM. Training QA-Emb reduces to selecting a set of
underlying questions rather than learning model weights.
  We use QA-Emb to flexibly generate interpretable models for predicting fMRI
voxel responses to language stimuli. QA-Emb significantly outperforms an
established interpretable baseline, and does so while requiring very few
questions. This paves the way towards building flexible feature spaces that can
concretize and evaluate our understanding of semantic brain representations. We
additionally find that QA-Emb can be effectively approximated with an efficient
model, and we explore broader applications in simple NLP tasks.

中文翻译:
大型语言模型（LLMs）的快速发展正推动文本嵌入技术在日益丰富的自然语言处理任务中不断进步。然而，其不透明性及向神经科学等领域的渗透，催生了对于可解释性日益增长的需求。本文探讨了能否通过LLM提示获得可解释的嵌入表示。我们提出问答嵌入（QA-Emb）方法，其每个特征维度对应LLM对一个是非问题的回答。训练QA-Emb本质上转化为选择一组基础问题，而非学习模型参数。

我们将QA-Emb灵活应用于预测fMRI体素对语言刺激响应的可解释模型构建。该方法显著优于现有可解释基线模型，且仅需极少数量的问题即可实现。这为构建灵活的特征空间铺平了道路，该空间能具体化并验证我们对大脑语义表征的理解。研究还发现QA-Emb可通过高效模型有效近似，并探索了其在简单NLP任务中的广泛应用潜力。
