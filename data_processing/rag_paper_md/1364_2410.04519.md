# RevMUX: Data Multiplexing with Reversible Adapters for Efficient LLM Batch Inference

链接: http://arxiv.org/abs/2410.04519v1

原文摘要:
Large language models (LLMs) have brought a great breakthrough to the natural
language processing (NLP) community, while leading the challenge of handling
concurrent customer queries due to their high throughput demands. Data
multiplexing addresses this by merging multiple inputs into a single composite
input, allowing more efficient inference through a shared forward pass.
However, as distinguishing individuals from a composite input is challenging,
conventional methods typically require training the entire backbone, yet still
suffer from performance degradation. In this paper, we introduce RevMUX, a
parameter-efficient data multiplexing framework that incorporates a reversible
design in the multiplexer, which can be reused by the demultiplexer to perform
reverse operations and restore individual samples for classification. Extensive
experiments on four datasets and three types of LLM backbones demonstrate the
effectiveness of RevMUX for enhancing LLM inference efficiency while retaining
a satisfactory classification performance.

中文翻译:
大型语言模型（LLMs）为自然语言处理（NLP）领域带来了重大突破，但也因其高吞吐量需求而面临处理并发用户查询的挑战。数据复用技术通过将多个输入合并为单一复合输入，借助共享前向传播实现更高效的推理。然而，由于从复合输入中区分个体样本具有挑战性，传统方法通常需要训练整个骨干网络，却仍存在性能下降的问题。本文提出RevMUX框架，采用参数高效的数据复用设计，在复用器中融入可逆结构，使得解复用器能复用该结构执行逆向操作以还原个体样本进行分类。基于四种数据集和三类LLM骨干网络的广泛实验表明，RevMUX在保持满意分类性能的同时，显著提升了LLM的推理效率。
