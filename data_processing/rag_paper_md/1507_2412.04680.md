# Superpixel Tokenization for Vision Transformers: Preserving Semantic Integrity in Visual Tokens

链接: http://arxiv.org/abs/2412.04680v1

原文摘要:
Transformers, a groundbreaking architecture proposed for Natural Language
Processing (NLP), have also achieved remarkable success in Computer Vision. A
cornerstone of their success lies in the attention mechanism, which models
relationships among tokens. While the tokenization process in NLP inherently
ensures that a single token does not contain multiple semantics, the
tokenization of Vision Transformer (ViT) utilizes tokens from uniformly
partitioned square image patches, which may result in an arbitrary mixing of
visual concepts in a token. In this work, we propose to substitute the
grid-based tokenization in ViT with superpixel tokenization, which employs
superpixels to generate a token that encapsulates a sole visual concept.
Unfortunately, the diverse shapes, sizes, and locations of superpixels make
integrating superpixels into ViT tokenization rather challenging. Our
tokenization pipeline, comprised of pre-aggregate extraction and
superpixel-aware aggregation, overcomes the challenges that arise in superpixel
tokenization. Extensive experiments demonstrate that our approach, which
exhibits strong compatibility with existing frameworks, enhances the accuracy
and robustness of ViT on various downstream tasks.

中文翻译:
Transformer作为一种革命性的自然语言处理（NLP）架构，在计算机视觉领域同样取得了显著成功。其核心优势在于能建模语义单元间关系的注意力机制。NLP中的分词过程天然保证了单个token不包含多重语义，而视觉Transformer（ViT）采用的均匀划分图像块生成token的方式，可能导致单个token混杂多个视觉概念。本研究提出用超像素分词替代ViT的网格分词，通过超像素生成仅包含单一视觉概念的token。然而超像素形态、尺寸与空间位置的多样性，为其融入ViT分词带来了巨大挑战。我们设计的包含预聚合特征提取和超像素感知聚合的分词流程，有效解决了超像素分词中的技术难题。大量实验表明，该方法与现有框架具有强兼容性，能显著提升ViT在各类下游任务中的准确性与鲁棒性。
