# From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models

链接: http://arxiv.org/abs/2406.16838v1

原文摘要:
One of the most striking findings in modern research on large language models
(LLMs) is that scaling up compute during training leads to better results.
However, less attention has been given to the benefits of scaling compute
during inference. This survey focuses on these inference-time approaches. We
explore three areas under a unified mathematical formalism: token-level
generation algorithms, meta-generation algorithms, and efficient generation.
Token-level generation algorithms, often called decoding algorithms, operate by
sampling a single token at a time or constructing a token-level search space
and then selecting an output. These methods typically assume access to a
language model's logits, next-token distributions, or probability scores.
Meta-generation algorithms work on partial or full sequences, incorporating
domain knowledge, enabling backtracking, and integrating external information.
Efficient generation methods aim to reduce token costs and improve the speed of
generation. Our survey unifies perspectives from three research communities:
traditional natural language processing, modern LLMs, and machine learning
systems.

中文翻译:
现代大型语言模型（LLM）研究中最引人注目的发现之一是：训练阶段增加计算量能提升模型性能。然而，推理阶段的计算扩展效益却较少受到关注。本综述聚焦于这些推理时优化方法，通过统一的数学框架探讨三大领域：令牌级生成算法、元生成算法以及高效生成技术。

令牌级生成算法（常称为解码算法）通过逐令牌采样或构建令牌级搜索空间后选择输出来运作，这类方法通常需要访问语言模型的逻辑值、下一令牌分布或概率分数。元生成算法则作用于部分或完整序列，可融合领域知识、支持回溯机制并整合外部信息。高效生成技术致力于降低令牌成本并提升生成速度。

本综述整合了来自传统自然语言处理、现代大型语言模型和机器学习系统三大研究领域的视角，为推理时计算扩展提供了系统性分析框架。
