# LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs

链接: http://arxiv.org/abs/2308.08469v1

原文摘要:
Multivariate time-series forecasting is vital in various domains, e.g.,
economic planning and weather prediction. Deep train-from-scratch models have
exhibited effective performance yet require large amounts of data, which limits
real-world applicability. Recently, researchers have leveraged the
representation learning transferability of pre-trained Large Language Models
(LLMs) to handle limited non-linguistic datasets effectively. However,
incorporating LLMs with time-series data presents challenges of limited
adaptation due to different compositions between time-series and linguistic
data, and the inability to process multi-scale temporal information. To tackle
these challenges, we propose LLM4TS, a framework for time-series forecasting
with pre-trained LLMs. LLM4TS consists of a two-stage fine-tuning strategy: the
time-series alignment stage to align LLMs with the nuances of time-series data,
and the forecasting fine-tuning stage for downstream time-series forecasting
tasks. Furthermore, our framework features a novel two-level aggregation method
that integrates multi-scale temporal data within pre-trained LLMs, enhancing
their ability to interpret time-specific information. In experiments across 7
time-series forecasting datasets, LLM4TS is superior to existing
state-of-the-art methods compared with trained-from-scratch models in full-shot
scenarios, and also achieves the highest rank in few-shot scenarios. In
addition, evaluations compared with different unsupervised representation
learning approaches highlight LLM4TS's effectiveness with representation
learning in forecasting tasks. Ablation studies further validate each
component's contribution to LLM4TS and underscore the essential role of
utilizing LLM's pre-trained weights for optimal performance. The code is
available at https://github.com/blacksnail789521/LLM4TS.

中文翻译:
多元时间序列预测在经济规划、气象预报等诸多领域至关重要。传统深度模型虽展现出优异性能，但其"从零训练"的特性需要海量数据支撑，这限制了实际应用场景。近期研究尝试利用预训练大语言模型（LLMs）的表示学习迁移能力来处理小规模非语言数据集，但面临两大挑战：时间序列数据与语言数据的结构差异导致模型适配受限，以及现有方法难以处理多尺度时序信息。为此，我们提出LLM4TS框架，通过两阶段微调策略实现LLMs在时序预测中的高效迁移：第一阶段时序对齐微调使模型适应时序数据特性，第二阶段预测任务微调专注于下游预测性能。该框架创新性地采用双层聚合机制，在预训练LLMs内部整合多尺度时序信息，显著提升模型对时序特征的解析能力。在7个基准数据集上的实验表明，LLM4TS在全样本场景下优于现有"从零训练"的顶尖方法，在少样本场景中同样保持最高排名。与无监督表示学习方法的对比验证了该框架在特征学习方面的优越性，消融实验则证实了各组件贡献及预训练权值的关键作用。代码已开源于https://github.com/blacksnail789521/LLM4TS。
