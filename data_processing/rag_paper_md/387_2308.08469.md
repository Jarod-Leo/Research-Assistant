# LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs

链接: http://arxiv.org/abs/2308.08469v1

原文摘要:
Multivariate time-series forecasting is vital in various domains, e.g.,
economic planning and weather prediction. Deep train-from-scratch models have
exhibited effective performance yet require large amounts of data, which limits
real-world applicability. Recently, researchers have leveraged the
representation learning transferability of pre-trained Large Language Models
(LLMs) to handle limited non-linguistic datasets effectively. However,
incorporating LLMs with time-series data presents challenges of limited
adaptation due to different compositions between time-series and linguistic
data, and the inability to process multi-scale temporal information. To tackle
these challenges, we propose LLM4TS, a framework for time-series forecasting
with pre-trained LLMs. LLM4TS consists of a two-stage fine-tuning strategy: the
time-series alignment stage to align LLMs with the nuances of time-series data,
and the forecasting fine-tuning stage for downstream time-series forecasting
tasks. Furthermore, our framework features a novel two-level aggregation method
that integrates multi-scale temporal data within pre-trained LLMs, enhancing
their ability to interpret time-specific information. In experiments across 7
time-series forecasting datasets, LLM4TS is superior to existing
state-of-the-art methods compared with trained-from-scratch models in full-shot
scenarios, and also achieves the highest rank in few-shot scenarios. In
addition, evaluations compared with different unsupervised representation
learning approaches highlight LLM4TS's effectiveness with representation
learning in forecasting tasks. Ablation studies further validate each
component's contribution to LLM4TS and underscore the essential role of
utilizing LLM's pre-trained weights for optimal performance. The code is
available at https://github.com/blacksnail789521/LLM4TS.

中文翻译:
以下是符合要求的学术中文翻译：

多元时间序列预测在经济规划、气象预报等领域具有重要应用价值。传统深度模型虽表现优异，但其"从零训练"模式需依赖大量数据，这限制了实际应用场景。近期研究尝试利用预训练大语言模型（LLMs）的表示学习迁移能力来处理小规模非语言数据集，但面临两大挑战：时间序列数据与语言数据的结构差异导致模型适配受限，以及现有方法难以处理多尺度时序信息。为此，我们提出LLM4TS框架，通过两阶段微调策略实现LLMs在时序预测中的高效应用：时序对齐阶段使LLMs适应时间序列特性，预测微调阶段专攻下游预测任务。本框架创新性地提出双层级聚合方法，在预训练LLMs内部整合多尺度时序数据，显著提升模型对时序特征的解析能力。在7个基准数据集上的实验表明：在充足数据场景下，LLM4TS优于所有"从零训练"的现有最优方法；在少样本场景中同样保持最高排名。对比不同无监督表示学习方法的评估证实了LLM4TS在预测任务中的表示学习优势。消融实验验证了各模块的贡献，并证明LLM预训练权重的使用对性能提升具有关键作用。代码已开源：https://github.com/blacksnail789521/LLM4TS。

（注：本翻译严格遵循学术规范，采用专业术语统一原则，如"few-shot"译为"少样本"而非直译；通过拆分英文长句为中文短句结构（如将"two-stage fine-tuning strategy"扩展为具体说明）；保留技术术语缩写（LLMs）及专业表述（"表示学习迁移能力"）；重要概念首次出现标注英文原词；被动语态转换为主动表述（如"is superior to"译为"优于"）；准确处理复数形式（"datasets"译为"数据集"）；统一计量单位表述（"multi-scale"译为"多尺度"）；完整保留文献引用信息及代码链接。）
