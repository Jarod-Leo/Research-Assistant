# LLM Tree Search

链接: http://arxiv.org/abs/2410.19117v1

原文摘要:
This project aims to investigate a novel sequence generation method inspired
by the AlphaGo paradigm, adapting it for use with large language models (LLMs).
The proposed approach involves creating search trees of different possible
completions and evaluating these completions based on model confidence. By
considering various paths in the search tree and scoring them according to the
model's confidence in each completion, we can generate diverse and high-quality
sequences. This research explores the implementation of this paradigm by using
confidence as a proxy for response quality akin to beam search
\citep{vijayakumar2016diverse}. The primary goal of this paper is to outline
the paradigm and demonstrate its potential, rather than focusing on achieving
perfect results. The paper will outline the reasons why we believe this
paradigm has the potential to improve LLMs in the following manners: 1)
increase output quality, 2) decrease errors, 3) eliminate or reduce the
compound error problems, 4) generate diverse and creative completions, 5) allow
for iterative problem-solving, and 6) self-training. We expect this approach to
yield a set of diverse and coherent sequences, offering insights into balancing
exploration and exploitation in sequence generation. Potential applications
include creative text generation tasks, such as storytelling and content
creation, as well as other natural language processing domains, like machine
translation and automated summarization. The goal is that the model will be far
more effective as it will be able to consider many possible variations allowing
it to find the ideal completion. This research aims to contribute to the
understanding of effective search strategies in sequence generation and their
impact on generating high-quality, varied textual outputs.

中文翻译:
本项目旨在探索一种受AlphaGo范式启发的新型序列生成方法，并将其适配于大型语言模型（LLMs）。该方案通过构建不同可能补全结果的搜索树，并基于模型置信度对这些补全进行评估。通过考量搜索树中的多种路径，并根据模型对每个补全的置信度进行评分，我们能够生成多样化且高质量的序列。本研究尝试以置信度作为响应质量的代理指标（类似于集束搜索\citep{vijayakumar2016diverse}）来实现这一范式。本文的核心目标在于阐明该范式的框架并论证其潜力，而非追求完美结果。我们将从以下维度阐述该范式可能提升LLMs的六大方向：1）提升输出质量；2）减少错误；3）消除或缓解复合错误问题；4）生成多样化创意补全；5）支持迭代式问题求解；6）实现自我训练。我们预期该方法能产出一组兼具多样性与连贯性的序列，为平衡序列生成中的探索与利用提供新思路。潜在应用场景包括故事创作、内容生成等创意文本任务，以及机器翻译、自动摘要等自然语言处理领域。该模型通过考量多种可能变体来寻找最优补全，其效能有望显著提升。本研究致力于深化对序列生成中有效搜索策略的理解，揭示其对生成高质量、多样化文本输出的影响机制。
