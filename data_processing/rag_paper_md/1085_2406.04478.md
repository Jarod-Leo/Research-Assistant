# PromptFix: Few-shot Backdoor Removal via Adversarial Prompt Tuning

链接: http://arxiv.org/abs/2406.04478v1

原文摘要:
Pre-trained language models (PLMs) have attracted enormous attention over the
past few years with their unparalleled performances. Meanwhile, the soaring
cost to train PLMs as well as their amazing generalizability have jointly
contributed to few-shot fine-tuning and prompting as the most popular training
paradigms for natural language processing (NLP) models. Nevertheless, existing
studies have shown that these NLP models can be backdoored such that model
behavior is manipulated when trigger tokens are presented. In this paper, we
propose PromptFix, a novel backdoor mitigation strategy for NLP models via
adversarial prompt-tuning in few-shot settings. Unlike existing NLP backdoor
removal methods, which rely on accurate trigger inversion and subsequent model
fine-tuning, PromptFix keeps the model parameters intact and only utilizes two
extra sets of soft tokens which approximate the trigger and counteract it
respectively. The use of soft tokens and adversarial optimization eliminates
the need to enumerate possible backdoor configurations and enables an adaptive
balance between trigger finding and preservation of performance. Experiments
with various backdoor attacks validate the effectiveness of the proposed method
and the performances when domain shift is present further shows PromptFix's
applicability to models pretrained on unknown data source which is the common
case in prompt tuning scenarios.

中文翻译:
预训练语言模型（PLMs）凭借其卓越性能在过去几年中引发了广泛关注。与此同时，训练PLMs的飙升成本与其惊人的泛化能力共同推动了小样本微调与提示学习成为自然语言处理（NLP）模型最主流的训练范式。然而现有研究表明，这些NLP模型可能被植入后门，导致模型在触发词出现时行为被操控。本文提出PromptFix——一种通过小样本场景下对抗性提示调优的新型NLP模型后门防御策略。与现有依赖精确触发词反演及后续模型微调的NLP后门消除方法不同，PromptFix保持模型参数不变，仅使用两组额外软标记分别逼近触发词并实施抵消。通过软标记与对抗优化，该方法无需枚举潜在后门配置，并能自适应平衡触发词定位与性能保持。多组后门攻击实验验证了该方法的有效性，而存在领域偏移时的表现进一步证明PromptFix适用于预训练数据源未知的模型（这正是提示调优场景的常态）。
