# Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training

链接: http://arxiv.org/abs/2310.16484v1

原文摘要:
Representational spaces learned via language modeling are fundamental to
Natural Language Processing (NLP), however there has been limited understanding
regarding how and when during training various types of linguistic information
emerge and interact. Leveraging a novel information theoretic probing suite,
which enables direct comparisons of not just task performance, but their
representational subspaces, we analyze nine tasks covering syntax, semantics
and reasoning, across 2M pre-training steps and five seeds. We identify
critical learning phases across tasks and time, during which subspaces emerge,
share information, and later disentangle to specialize. Across these phases,
syntactic knowledge is acquired rapidly after 0.5% of full training. Continued
performance improvements primarily stem from the acquisition of open-domain
knowledge, while semantics and reasoning tasks benefit from later boosts to
long-range contextualization and higher specialization. Measuring cross-task
similarity further reveals that linguistically related tasks share information
throughout training, and do so more during the critical phase of learning than
before or after. Our findings have implications for model interpretability,
multi-task learning, and learning from limited data.

中文翻译:
通过语言建模习得的表征空间是自然语言处理（NLP）的基础，然而关于训练过程中各类语言信息何时以及如何涌现与交互，现有理解仍较为有限。本研究采用新颖的信息论探测工具集（支持直接比较任务表现及其表征子空间），对涵盖句法、语义和推理的九项任务展开分析，追踪200万次预训练步骤及五次随机种子实验。我们发现跨任务的关键学习阶段：子空间先形成并共享信息，随后解耦以实现专门化。在这些阶段中，句法知识在完成0.5%训练量后即快速习得，后续性能提升主要源于开放领域知识的获取，而语义与推理任务则受益于后期长距离上下文建模能力的增强和更高程度的专门化。跨任务相似性测量进一步揭示：语言关联任务在整个训练过程中持续共享信息，且这种共享在关键学习阶段比前后期更为显著。这些发现对模型可解释性、多任务学习及小样本学习具有重要启示。
