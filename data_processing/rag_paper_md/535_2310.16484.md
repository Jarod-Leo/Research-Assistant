# Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training

链接: http://arxiv.org/abs/2310.16484v1

原文摘要:
Representational spaces learned via language modeling are fundamental to
Natural Language Processing (NLP), however there has been limited understanding
regarding how and when during training various types of linguistic information
emerge and interact. Leveraging a novel information theoretic probing suite,
which enables direct comparisons of not just task performance, but their
representational subspaces, we analyze nine tasks covering syntax, semantics
and reasoning, across 2M pre-training steps and five seeds. We identify
critical learning phases across tasks and time, during which subspaces emerge,
share information, and later disentangle to specialize. Across these phases,
syntactic knowledge is acquired rapidly after 0.5% of full training. Continued
performance improvements primarily stem from the acquisition of open-domain
knowledge, while semantics and reasoning tasks benefit from later boosts to
long-range contextualization and higher specialization. Measuring cross-task
similarity further reveals that linguistically related tasks share information
throughout training, and do so more during the critical phase of learning than
before or after. Our findings have implications for model interpretability,
multi-task learning, and learning from limited data.

中文翻译:
通过语言建模学习到的表征空间是自然语言处理（NLP）的基础，然而关于训练过程中各类语言信息何时以及如何涌现与交互，学界认知仍存在空白。本研究采用新颖的信息论探测工具套件（支持直接比较任务表现及其表征子空间），系统分析了涵盖句法、语义和推理的九项任务在200万次预训练步骤和五个随机种子下的演化规律。我们发现不同任务和时间维度上存在关键学习阶段：子空间先涌现并共享信息，随后解耦以实现专业化。在这些阶段中，句法知识在完成0.5%训练量后即快速习得，而持续的性能提升主要源于开放领域知识的获取；语义与推理任务则受益于后期长距离上下文建模能力的增强及更高程度的专业化。跨任务相似性测量进一步表明，语言学关联任务在整个训练过程中持续共享信息，且在关键学习阶段的共享程度显著高于前后时期。本研究对模型可解释性、多任务学习及小样本学习具有重要启示意义。

（翻译说明：
1. 专业术语处理："representational spaces"译为"表征空间"，"information theoretic probing suite"译为"信息论探测工具套件"以保持学术严谨性
2. 长句拆分：将原文复合句按中文表达习惯分解为多个短句，如将"during which..."独立成句
3. 逻辑显化：添加"研究发现"等引导词明确研究结论的呈现层次
4. 数据呈现：将"2M"转化为中文习惯的"200万次"，"0.5%"保留数字形式但添加"训练量"补充说明
5. 概念对应："disentangle to specialize"译为"解耦以实现专业化"准确传达机器学习领域的特定含义
6. 学术风格：使用"学界认知""具有重要启示意义"等符合中文论文摘要的规范表达）
