# Explanation sensitivity to the randomness of large language models: the case of journalistic text classification

链接: http://arxiv.org/abs/2410.05085v1

原文摘要:
Large language models (LLMs) perform very well in several natural language
processing tasks but raise explainability challenges. In this paper, we examine
the effect of random elements in the training of LLMs on the explainability of
their predictions. We do so on a task of opinionated journalistic text
classification in French. Using a fine-tuned CamemBERT model and an explanation
method based on relevance propagation, we find that training with different
random seeds produces models with similar accuracy but variable explanations.
We therefore claim that characterizing the explanations' statistical
distribution is needed for the explainability of LLMs. We then explore a
simpler model based on textual features which offers stable explanations but is
less accurate. Hence, this simpler model corresponds to a different tradeoff
between accuracy and explainability. We show that it can be improved by
inserting features derived from CamemBERT's explanations. We finally discuss
new research directions suggested by our results, in particular regarding the
origin of the sensitivity observed in the training randomness.

中文翻译:
大型语言模型（LLMs）在多项自然语言处理任务中表现卓越，但也面临可解释性挑战。本文通过法语观点性新闻文本分类任务，研究了LLMs训练过程中随机因素对其预测可解释性的影响。基于微调的CamemBERT模型和相关性传播解释方法，我们发现不同随机种子训练产生的模型虽准确率相近，但解释结果存在显著差异。因此，我们主张需要通过统计分布特征来描述解释结果，以实现LLMs的可解释性。随后探索了基于文本特征的简化模型，该模型虽解释稳定性更高但准确率较低，呈现出准确性与可解释性的不同权衡关系。研究表明，通过融入CamemBERT解释衍生的特征可提升该简化模型性能。最后讨论了研究成果启示的新研究方向，特别是关于训练随机性敏感度根源的探讨。
