# Knowledge-Infused Self Attention Transformers

链接: http://arxiv.org/abs/2306.13501v1

原文摘要:
Transformer-based language models have achieved impressive success in various
natural language processing tasks due to their ability to capture complex
dependencies and contextual information using self-attention mechanisms.
However, they are not without limitations. These limitations include
hallucinations, where they produce incorrect outputs with high confidence, and
alignment issues, where they generate unhelpful and unsafe outputs for human
users. These limitations stem from the absence of implicit and missing context
in the data alone. To address this, researchers have explored augmenting these
models with external knowledge from knowledge graphs to provide the necessary
additional context. However, the ad-hoc nature of existing methods makes it
difficult to properly analyze the effects of knowledge infusion on the many
moving parts or components of a transformer. This paper introduces a systematic
method for infusing knowledge into different components of a transformer-based
model. A modular framework is proposed to identify specific components within
the transformer architecture, such as the self-attention mechanism, encoder
layers, or the input embedding layer, where knowledge infusion can be applied.
Additionally, extensive experiments are conducted on the General Language
Understanding Evaluation (GLUE) benchmark tasks, and the findings are reported.
This systematic approach aims to facilitate more principled approaches to
incorporating knowledge into language model architectures.

中文翻译:
基于Transformer的语言模型因其能够利用自注意力机制捕捉复杂依赖关系和上下文信息，在各种自然语言处理任务中取得了显著成功。然而，这些模型仍存在局限性，包括会产生高置信度错误输出的"幻觉"问题，以及生成对人类用户无益甚至有害内容的"对齐偏差"问题。这些缺陷源于模型仅依赖数据中隐含且不完整的上下文信息。为解决这一问题，研究者尝试通过引入知识图谱的外部知识来提供必要的补充语境。但现有方法多为临时性方案，难以系统分析知识注入对Transformer各动态组件的影响。

本文提出了一种将知识系统化注入Transformer模型不同组件的创新方法。我们设计了一个模块化框架，用于定位模型架构中适合知识注入的特定组件（如自注意力机制、编码器层或输入嵌入层）。通过在通用语言理解评估（GLUE）基准任务上进行大量实验，我们验证了该方法的有效性。这种系统化知识注入方案旨在为语言模型架构的知识融合提供更规范化的研究路径。

（注：根据学术翻译规范，对原文进行了以下优化处理：
1. 将专业术语"hallucinations"译为业界通用译法"幻觉"
2. "alignment issues"译为"对齐偏差"以准确反映技术内涵
3. 长难句拆分重组，如将"the ad-hoc nature..."整句转化为符合中文表达习惯的转折结构
4. 补充"我们"作为主语使译文更符合中文论文表述习惯
5. 重要概念首次出现时保留英文缩写（GLUE）并标注全称
6. 保持被动语态与主动语态的合理转换，如"extensive experiments are conducted"译为主动式）
