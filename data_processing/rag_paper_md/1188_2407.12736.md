# CHOSEN: Compilation to Hardware Optimization Stack for Efficient Vision Transformer Inference

链接: http://arxiv.org/abs/2407.12736v1

原文摘要:
Vision Transformers (ViTs) represent a groundbreaking shift in machine
learning approaches to computer vision. Unlike traditional approaches, ViTs
employ the self-attention mechanism, which has been widely used in natural
language processing, to analyze image patches. Despite their advantages in
modeling visual tasks, deploying ViTs on hardware platforms, notably
Field-Programmable Gate Arrays (FPGAs), introduces considerable challenges.
These challenges stem primarily from the non-linear calculations and high
computational and memory demands of ViTs. This paper introduces CHOSEN, a
software-hardware co-design framework to address these challenges and offer an
automated framework for ViT deployment on the FPGAs in order to maximize
performance. Our framework is built upon three fundamental contributions:
multi-kernel design to maximize the bandwidth, mainly targeting benefits of
multi DDR memory banks, approximate non-linear functions that exhibit minimal
accuracy degradation, and efficient use of available logic blocks on the FPGA,
and efficient compiler to maximize the performance and memory-efficiency of the
computing kernels by presenting a novel algorithm for design space exploration
to find optimal hardware configuration that achieves optimal throughput and
latency. Compared to the state-of-the-art ViT accelerators, CHOSEN achieves a
1.5x and 1.42x improvement in the throughput on the DeiT-S and DeiT-B models.

中文翻译:
视觉变换器（ViTs）在计算机视觉的机器学习方法中实现了革命性突破。不同于传统方法，ViTs采用自然语言处理领域广泛使用的自注意力机制来分析图像块。尽管在视觉任务建模方面具有优势，但将ViTs部署至硬件平台（尤其是现场可编程门阵列FPGAs）时仍面临重大挑战，这些挑战主要源于ViTs的非线性计算特性及其对算力和内存的高需求。本文提出CHOSEN——一种软硬件协同设计框架，通过三大核心创新应对上述挑战：采用多内核设计以最大化带宽（重点利用多DDR内存组优势）、构建精度损失极小的近似非线性函数模块、高效利用FPGA逻辑资源；同时开发了智能编译器，通过创新的设计空间探索算法寻找最优硬件配置，实现计算内核的性能与内存效率最大化。实验表明，在DeiT-S和DeiT-B模型上，CHOSEN的吞吐量分别达到现有最优ViT加速器的1.5倍和1.42倍。
