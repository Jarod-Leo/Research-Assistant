# Large Language Models in Computer Science Education: A Systematic Literature Review

链接: http://arxiv.org/abs/2410.16349v1

原文摘要:
Large language models (LLMs) are becoming increasingly better at a wide range
of Natural Language Processing tasks (NLP), such as text generation and
understanding. Recently, these models have extended their capabilities to
coding tasks, bridging the gap between natural languages (NL) and programming
languages (PL). Foundational models such as the Generative Pre-trained
Transformer (GPT) and LLaMA series have set strong baseline performances in
various NL and PL tasks. Additionally, several models have been fine-tuned
specifically for code generation, showing significant improvements in
code-related applications. Both foundational and fine-tuned models are
increasingly used in education, helping students write, debug, and understand
code. We present a comprehensive systematic literature review to examine the
impact of LLMs in computer science and computer engineering education. We
analyze their effectiveness in enhancing the learning experience, supporting
personalized education, and aiding educators in curriculum development. We
address five research questions to uncover insights into how LLMs contribute to
educational outcomes, identify challenges, and suggest directions for future
research.

中文翻译:
大型语言模型（LLM）在文本生成与理解等自然语言处理（NLP）任务中表现日益卓越。近期，这些模型将其能力扩展至编程任务，弥合了自然语言（NL）与编程语言（PL）之间的鸿沟。以生成式预训练变换器（GPT）和LLaMA系列为代表的基础模型，已在各类NL与PL任务中建立了强劲的基准性能。此外，多个专门针对代码生成进行微调的模型在代码相关应用中展现出显著提升。基础模型与微调模型正日益广泛应用于教育领域，助力学生编写、调试及理解代码。本文通过系统性文献综述，全面考察LLM对计算机科学与计算机工程教育的影响，分析其在优化学习体验、支持个性化教育、辅助教师课程开发等方面的实效性。我们围绕五个研究问题展开探讨，揭示LLM如何促进教育成果，识别现存挑战，并为未来研究方向提出建议。
