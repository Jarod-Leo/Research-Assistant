# E-InMeMo: Enhanced Prompting for Visual In-Context Learning

链接: http://arxiv.org/abs/2504.18158v1

原文摘要:
Large-scale models trained on extensive datasets have become the standard due
to their strong generalizability across diverse tasks. In-context learning
(ICL), widely used in natural language processing, leverages these models by
providing task-specific prompts without modifying their parameters. This
paradigm is increasingly being adapted for computer vision, where models
receive an input-output image pair, known as an in-context pair, alongside a
query image to illustrate the desired output. However, the success of visual
ICL largely hinges on the quality of these prompts. To address this, we propose
Enhanced Instruct Me More (E-InMeMo), a novel approach that incorporates
learnable perturbations into in-context pairs to optimize prompting. Through
extensive experiments on standard vision tasks, E-InMeMo demonstrates superior
performance over existing state-of-the-art methods. Notably, it improves mIoU
scores by 7.99 for foreground segmentation and by 17.04 for single object
detection when compared to the baseline without learnable prompts. These
results highlight E-InMeMo as a lightweight yet effective strategy for
enhancing visual ICL. Code is publicly available at:
https://github.com/Jackieam/E-InMeMo

中文翻译:
基于海量数据训练的大规模模型因其在多样化任务中展现出的强大泛化能力，已成为当前技术标准。自然语言处理领域广泛采用的上下文学习（ICL）机制，通过提供任务特定提示而不修改模型参数的方式充分利用了这些模型。这一范式正逐渐被引入计算机视觉领域，模型通过接收输入-输出图像对（即上下文配对样本）与查询图像来演示预期输出。然而视觉ICL的成功很大程度上依赖于提示质量。为此，我们提出增强型指导优化框架E-InMeMo，该方法通过将可学习扰动融入上下文配对样本来实现提示优化。在标准视觉任务上的大量实验表明，E-InMeMo显著优于现有最先进方法：相较于无学习提示的基线模型，其在前景分割任务中mIoU指标提升7.99，在单目标检测任务中提升17.04。这些成果确立了E-InMeMo作为一种轻量级但高效的视觉ICL增强策略。代码已开源于：https://github.com/Jackieam/E-InMeMo
