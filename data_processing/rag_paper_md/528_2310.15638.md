# CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation

链接: http://arxiv.org/abs/2310.15638v1

原文摘要:
Annotated data plays a critical role in Natural Language Processing (NLP) in
training models and evaluating their performance. Given recent developments in
Large Language Models (LLMs), models such as ChatGPT demonstrate zero-shot
capability on many text-annotation tasks, comparable with or even exceeding
human annotators. Such LLMs can serve as alternatives for manual annotation,
due to lower costs and higher scalability. However, limited work has leveraged
LLMs as complementary annotators, nor explored how annotation work is best
allocated among humans and LLMs to achieve both quality and cost objectives. We
propose CoAnnotating, a novel paradigm for Human-LLM co-annotation of
unstructured texts at scale. Under this framework, we utilize uncertainty to
estimate LLMs' annotation capability. Our empirical study shows CoAnnotating to
be an effective means to allocate work from results on different datasets, with
up to 21% performance improvement over random baseline. For code
implementation, see https://github.com/SALT-NLP/CoAnnotating.

中文翻译:
在自然语言处理（NLP）领域，标注数据对模型训练与性能评估具有关键作用。随着大语言模型（LLMs）的最新进展，诸如ChatGPT等模型在众多文本标注任务中展现出零样本能力，其表现媲美甚至超越人工标注者。这类LLMs凭借更低成本与更高扩展性，可作为人工标注的替代方案。然而，目前较少研究将LLMs作为补充性标注工具，也未深入探索如何最优分配人力与LLMs的标注任务以实现质量与成本的双重目标。我们提出协同标注（CoAnnotating）这一创新范式，用于大规模非结构化文本的人机协同标注。该框架通过不确定性度量来评估LLMs的标注能力，实证研究表明：在不同数据集上，协同标注策略相比随机基线最高可提升21%的标注效能，成为任务分配的有效方法。代码实现详见https://github.com/SALT-NLP/CoAnnotating。
