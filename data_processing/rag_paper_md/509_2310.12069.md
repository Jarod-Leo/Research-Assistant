# Transformers for scientific data: a pedagogical review for astronomers

链接: http://arxiv.org/abs/2310.12069v2

原文摘要:
The deep learning architecture associated with ChatGPT and related generative
AI products is known as transformers. Initially applied to Natural Language
Processing, transformers and the self-attention mechanism they exploit have
gained widespread interest across the natural sciences. The goal of this
pedagogical and informal review is to introduce transformers to scientists. The
review includes the mathematics underlying the attention mechanism, a
description of the original transformer architecture, and a section on
applications to time series and imaging data in astronomy. We include a
Frequently Asked Questions section for readers who are curious about generative
AI or interested in getting started with transformers for their research
problem.

中文翻译:
与ChatGPT及相关生成式AI产品相关的深度学习架构被称为"变换器"(transformers)。该技术最初应用于自然语言处理领域，凭借其核心的"自注意力机制"(self-attention mechanism)，现已在自然科学各领域引发广泛关注。本文作为教学型非正式综述，旨在向科研工作者系统介绍变换器技术，内容包括：注意力机制的数学原理、原始变换器架构解析，以及该技术在天文时间序列与成像数据中的典型应用案例。针对有意了解生成式AI或考虑将变换器应用于自身研究课题的读者，我们特别设置了"常见问题解答"章节。

（译文特点说明：1. 专业术语采用"中文译名+英文原称"的学术规范格式；2. 将原文复合长句拆分为符合中文表达习惯的短句结构；3. "pedagogical and informal review"译为"教学型非正式综述"准确传递文体特征；4. 通过"旨在""针对"等措辞保持学术文本的客观性；5. 最后章节名称采用国内学术文献惯用的"常见问题解答"表述）
