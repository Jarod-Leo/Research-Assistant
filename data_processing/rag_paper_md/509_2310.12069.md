# Transformers for scientific data: a pedagogical review for astronomers

链接: http://arxiv.org/abs/2310.12069v2

原文摘要:
The deep learning architecture associated with ChatGPT and related generative
AI products is known as transformers. Initially applied to Natural Language
Processing, transformers and the self-attention mechanism they exploit have
gained widespread interest across the natural sciences. The goal of this
pedagogical and informal review is to introduce transformers to scientists. The
review includes the mathematics underlying the attention mechanism, a
description of the original transformer architecture, and a section on
applications to time series and imaging data in astronomy. We include a
Frequently Asked Questions section for readers who are curious about generative
AI or interested in getting started with transformers for their research
problem.

中文翻译:
与ChatGPT及相关生成式AI产品相关的深度学习架构被称为变换器(transformers)。这一架构最初应用于自然语言处理领域，其核心的自注意力机制如今已在自然科学各领域引发广泛关注。本教程式非正式综述旨在向科研工作者介绍变换器技术，内容包括注意力机制的数学原理、原始变换器架构的解析，以及该技术在天文学时间序列与成像数据中的应用专题。我们还为对生成式AI感兴趣或考虑将变换器应用于研究问题的读者准备了常见问题解答章节。
