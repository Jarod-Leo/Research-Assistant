# SlimFit: Memory-Efficient Fine-Tuning of Transformer-based Models Using Training Dynamics

链接: http://arxiv.org/abs/2305.18513v1

原文摘要:
Transformer-based models, such as BERT and ViT, have achieved
state-of-the-art results across different natural language processing (NLP) and
computer vision (CV) tasks. However, these models are extremely memory
intensive during their fine-tuning process, making them difficult to deploy on
GPUs with limited memory resources. To address this issue, we introduce a new
tool called SlimFit that reduces the memory requirements of these models by
dynamically analyzing their training dynamics and freezing less-contributory
layers during fine-tuning. The layers to freeze are chosen using a runtime
inter-layer scheduling algorithm. SlimFit adopts quantization and pruning for
particular layers to balance the load of dynamic activations and to minimize
the memory footprint of static activations, where static activations refer to
those that cannot be discarded regardless of freezing. This allows SlimFit to
freeze up to 95% of layers and reduce the overall on-device GPU memory usage of
transformer-based models such as ViT and BERT by an average of 2.2x, across
different NLP and CV benchmarks/datasets such as GLUE, SQuAD 2.0, CIFAR-10,
CIFAR-100 and ImageNet with an average degradation of 0.2% in accuracy. For
such NLP and CV tasks, SlimFit can reduce up to 3.1x the total on-device memory
usage with an accuracy degradation of only up to 0.4%. As a result, while
fine-tuning of ViT on ImageNet and BERT on SQuAD 2.0 with a batch size of 128
requires 3 and 2 32GB GPUs respectively, SlimFit enables their fine-tuning on a
single 32GB GPU without any significant accuracy degradation.

中文翻译:
基于Transformer的模型（如BERT和ViT）在自然语言处理（NLP）和计算机视觉（CV）任务中均取得了最先进的成果。然而，这些模型在微调过程中对内存的需求极高，导致难以在显存资源有限的GPU上部署。针对这一问题，我们开发了一款名为SlimFit的新工具，通过动态分析模型训练动态性，在微调期间冻结贡献度较低的层，从而降低内存占用。具体实现采用运行时层间调度算法选择待冻结层，并对特定层实施量化和剪枝技术，以平衡动态激活的负载并最小化静态激活（指无论是否冻结都必须保留的激活）的内存占用。实验表明，SlimFit可冻结高达95%的层，在GLUE、SQuAD 2.0、CIFAR-10、CIFAR-100和ImageNet等NLP与CV基准数据集上，平均将ViT和BERT等模型的GPU显存占用降低2.2倍，精度损失仅为0.2%。对于部分任务，最高可实现3.1倍的显存节省，精度损失不超过0.4%。以ImageNet上微调ViT（批大小128）和SQuAD 2.0上微调BERT为例，原方案分别需要3块和2块32GB GPU，而SlimFit仅需单块32GB GPU即可完成微调，且保持精度基本无损。
