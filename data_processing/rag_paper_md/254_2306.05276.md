# Extensive Evaluation of Transformer-based Architectures for Adverse Drug Events Extraction

链接: http://arxiv.org/abs/2306.05276v1

原文摘要:
Adverse Event (ADE) extraction is one of the core tasks in digital
pharmacovigilance, especially when applied to informal texts. This task has
been addressed by the Natural Language Processing community using large
pre-trained language models, such as BERT. Despite the great number of
Transformer-based architectures used in the literature, it is unclear which of
them has better performances and why. Therefore, in this paper we perform an
extensive evaluation and analysis of 19 Transformer-based models for ADE
extraction on informal texts. We compare the performance of all the considered
models on two datasets with increasing levels of informality (forums posts and
tweets). We also combine the purely Transformer-based models with two
commonly-used additional processing layers (CRF and LSTM), and analyze their
effect on the models performance. Furthermore, we use a well-established
feature importance technique (SHAP) to correlate the performance of the models
with a set of features that describe them: model category (AutoEncoding,
AutoRegressive, Text-to-Text), pretraining domain, training from scratch, and
model size in number of parameters. At the end of our analyses, we identify a
list of take-home messages that can be derived from the experimental data.

中文翻译:
不良事件（ADE）抽取是数字药物警戒中的核心任务之一，尤其在处理非正式文本时。自然语言处理领域常采用BERT等大型预训练语言模型来解决这一问题。尽管已有大量基于Transformer架构的模型被应用，但各模型性能优劣及其原因尚不明确。为此，本文对19种基于Transformer的非正式文本ADE抽取模型进行了全面评估与分析。我们在两种信息非正式程度递增的数据集（论坛帖子和推文）上对比了所有模型的性能表现，并将纯Transformer模型与两种常用附加处理层（条件随机场CRF和长短期记忆网络LSTM）结合，分析其对模型性能的影响。此外，采用成熟的SHAP特征重要性分析技术，将模型性能与四类特征相关联：模型类别（自编码型、自回归型、文本到文本型）、预训练领域、是否从头训练以及参数量级。通过系统分析，最终从实验数据中提炼出具有实践指导意义的结论清单。
