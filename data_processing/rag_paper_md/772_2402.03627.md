# Partially Recentralization Softmax Loss for Vision-Language Models Robustness

链接: http://arxiv.org/abs/2402.03627v1

原文摘要:
As Large Language Models make a breakthrough in natural language processing
tasks (NLP), multimodal technique becomes extremely popular. However, it has
been shown that multimodal NLP are vulnerable to adversarial attacks, where the
outputs of a model can be dramatically changed by a perturbation to the input.
While several defense techniques have been proposed both in computer vision and
NLP models, the multimodal robustness of models have not been fully explored.
In this paper, we study the adversarial robustness provided by modifying loss
function of pre-trained multimodal models, by restricting top K softmax
outputs. Based on the evaluation and scoring, our experiments show that after a
fine-tuning, adversarial robustness of pre-trained models can be significantly
improved, against popular attacks. Further research should be studying, such as
output diversity, generalization and the robustness-performance trade-off of
this kind of loss functions. Our code will be available after this paper is
accepted

中文翻译:
随着大语言模型在自然语言处理任务(NLP)中取得突破性进展，多模态技术变得极为流行。然而研究表明，多模态NLP系统易受对抗性攻击影响，微小的输入扰动就可能导致模型输出发生显著变化。尽管计算机视觉和NLP领域已提出多种防御技术，但模型的多模态鲁棒性仍未得到充分探索。本文通过限制前K个softmax输出的方法，研究修改预训练多模态模型损失函数所提供的对抗鲁棒性。实验评估表明，经过微调后，预训练模型针对常见攻击的对抗鲁棒性可获得显著提升。未来研究应进一步探讨此类损失函数的输出多样性、泛化能力以及鲁棒性与性能的权衡关系。本文录用后代码将予以公开。
