# Length Extrapolation of Transformers: A Survey from the Perspective of Position Encoding

链接: http://arxiv.org/abs/2312.17044v2

原文摘要:
Built upon the Transformer, large language models (LLMs) have captured
worldwide attention due to their remarkable abilities. Nevertheless, all
Transformer-based models including LLMs suffer from a preset length limit and
can hardly generalize from short training sequences to longer inference ones,
namely, they cannot perform length extrapolation to handle long sequences,
which severely hinders their application in scenarios demanding long input
sequences such as legal or scientific documents. Thus, numerous methods have
emerged to enhance the length extrapolation of Transformers. Despite the great
research efforts, a systematic survey is still lacking. To fill this gap, we
delve into these advances in a unified notation from the perspective of
positional encoding (PE), as it has been considered the primary factor on
length extrapolation. Specifically, we begin with extrapolatable PEs that have
dominated this research field. Then, we dive into extrapolation methods based
on them, covering position interpolation and randomized position methods.
Finally, several challenges and future directions in this area are highlighted.
Through this survey, we aim to enable the reader to gain a deep understanding
of existing methods and provide stimuli for future research.

中文翻译:
基于Transformer架构的大型语言模型(LLMs)凭借其卓越能力引发了全球关注。然而，包括LLMs在内的所有基于Transformer的模型都存在预设长度限制，难以从短训练序列泛化到更长推理序列——即无法通过长度外推处理长序列，这严重阻碍了其在法律文书或科学文献等需要长输入序列场景中的应用。为此，大量增强Transformer长度外推能力的方法应运而生。尽管研究投入巨大，目前仍缺乏系统性综述。为填补这一空白，我们从位置编码(Positional Encoding, PE)这一被视为影响长度外推关键因素的视角出发，以统一符号体系深入梳理相关进展。具体而言，首先剖析主导该研究领域的可外推位置编码；继而探究基于这些编码的外推方法，涵盖位置插值与随机位置方法；最后指出该领域若干挑战与未来方向。通过本综述，我们旨在帮助读者深入理解现有方法，并为未来研究提供启示。
