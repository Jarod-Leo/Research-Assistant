# Comparison of Pre-trained Language Models for Turkish Address Parsing

链接: http://arxiv.org/abs/2306.13947v1

原文摘要:
Transformer based pre-trained models such as BERT and its variants, which are
trained on large corpora, have demonstrated tremendous success for natural
language processing (NLP) tasks. Most of academic works are based on the
English language; however, the number of multilingual and language specific
studies increase steadily. Furthermore, several studies claimed that language
specific models outperform multilingual models in various tasks. Therefore, the
community tends to train or fine-tune the models for the language of their case
study, specifically. In this paper, we focus on Turkish maps data and
thoroughly evaluate both multilingual and Turkish based BERT, DistilBERT,
ELECTRA and RoBERTa. Besides, we also propose a MultiLayer Perceptron (MLP) for
fine-tuning BERT in addition to the standard approach of one-layer fine-tuning.
For the dataset, a mid-sized Address Parsing corpus taken with a relatively
high quality is constructed. Conducted experiments on this dataset indicate
that Turkish language specific models with MLP fine-tuning yields slightly
better results when compared to the multilingual fine-tuned models. Moreover,
visualization of address tokens' representations further indicates the
effectiveness of BERT variants for classifying a variety of addresses.

中文翻译:
基于Transformer架构的预训练模型（如BERT及其变体）通过大规模语料训练，在自然语言处理任务中展现出卓越性能。当前学术研究主要集中于英语领域，但针对多语言及特定语种的研究正持续增长。值得注意的是，多项研究表明特定语言模型在各类任务中的表现优于多语言模型，这促使学界更倾向于针对研究对象的语言专门训练或微调模型。本文以土耳其语地图数据为研究对象，系统评估了多语言与土耳其语专用的BERT、DistilBERT、ELECTRA和RoBERTa模型。此外，我们创新性地提出采用多层感知机（MLP）进行BERT微调，突破了传统单层微调方法的局限。实验采用自建的中等规模、高质量地址解析语料库，结果表明：经过MLP微调的土耳其语专用模型性能略优于多语言微调模型。通过可视化地址标记的向量表征，进一步验证了BERT系列模型在多样化地址分类任务中的有效性。
