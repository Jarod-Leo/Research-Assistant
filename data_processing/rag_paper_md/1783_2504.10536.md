# Federated Learning with Layer Skipping: Efficient Training of Large Language Models for Healthcare NLP

链接: http://arxiv.org/abs/2504.10536v1

原文摘要:
Federated learning (FL) enables collaborative model training across
organizations without sharing raw data, addressing crucial privacy concerns in
healthcare natural language processing (NLP). However, training large language
models (LLMs) in federated settings faces significant challenges, including
communication overhead and data heterogeneity. We propose Layer-Skipping
Federated Learning, where only selected layers of a pre-trained LLM are
fine-tuned across clients while others remain frozen. Applied to LLaMA 3.2-1B,
our approach reduces communication costs by approximately 70% while maintaining
performance within 2% of centralized training. We evaluate our method on
clinical NER and classification tasks using i2b2 and MIMIC-III datasets. Our
experiments demonstrate that Layer-Skipping FL outperforms competitive
baselines, handles non-IID clinical data distributions effectively, and shows
robustness when combined with differential privacy. This approach represents a
practical solution for privacy-preserving collaborative learning in healthcare
NLP.

中文翻译:
联邦学习（FL）实现了跨机构的协同模型训练而无需共享原始数据，有效解决了医疗自然语言处理（NLP）中的隐私核心问题。然而在联邦环境下训练大语言模型（LLM）面临重大挑战，包括通信开销和数据异构性。我们提出层跳跃联邦学习方案，仅对预训练LLM的选定层进行客户端微调，其余层保持冻结。将该方法应用于LLaMA 3.2-1B模型时，通信成本降低约70%，同时性能维持在集中式训练2%的误差范围内。基于i2b2和MIMIC-III数据集，我们在临床命名实体识别和分类任务上验证了该方法。实验表明：层跳跃联邦学习优于竞争基线方案，能有效处理非独立同分布的临床数据，与差分隐私结合时展现出强健性。这一方法为医疗NLP领域的隐私保护协同学习提供了实用解决方案。
