# Bi-LAT: Bilateral Control-Based Imitation Learning via Natural Language and Action Chunking with Transformers

链接: http://arxiv.org/abs/2504.01301v1

原文摘要:
We present Bi-LAT, a novel imitation learning framework that unifies
bilateral control with natural language processing to achieve precise force
modulation in robotic manipulation. Bi-LAT leverages joint position, velocity,
and torque data from leader-follower teleoperation while also integrating
visual and linguistic cues to dynamically adjust applied force. By encoding
human instructions such as "softly grasp the cup" or "strongly twist the
sponge" through a multimodal Transformer-based model, Bi-LAT learns to
distinguish nuanced force requirements in real-world tasks. We demonstrate
Bi-LAT's performance in (1) unimanual cup-stacking scenario where the robot
accurately modulates grasp force based on language commands, and (2) bimanual
sponge-twisting task that requires coordinated force control. Experimental
results show that Bi-LAT effectively reproduces the instructed force levels,
particularly when incorporating SigLIP among tested language encoders. Our
findings demonstrate the potential of integrating natural language cues into
imitation learning, paving the way for more intuitive and adaptive human-robot
interaction. For additional material, please visit:
https://mertcookimg.github.io/bi-lat/

中文翻译:
我们提出了一种新型模仿学习框架Bi-LAT，该框架将双边控制与自然语言处理相结合，实现机器人操作中的精确力调节。该系统通过多模态Transformer模型，利用主从遥操作中的关节位置、速度和扭矩数据，同时整合视觉与语言线索来动态调整施力强度。Bi-LAT能够编码"轻轻握住杯子"或"用力拧海绵"等人机指令，学习识别实际任务中微妙的力需求。我们在两个场景验证了其性能：(1)单手机器人根据语言指令精确调节抓握力的叠杯任务；(2)需要协调施力的双手海绵扭转任务。实验结果表明，在测试的语言编码器中，采用SigLIP的Bi-LAT能有效复现指令要求的力水平。这项研究揭示了将自然语言线索融入模仿学习的潜力，为更直观自适应的人机交互开辟了新途径。更多资料请访问：https://mertcookimg.github.io/bi-lat/
