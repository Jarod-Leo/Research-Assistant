# Concept-based explainability for an EEG transformer model

链接: http://arxiv.org/abs/2307.12745v1

原文摘要:
Deep learning models are complex due to their size, structure, and inherent
randomness in training procedures. Additional complexity arises from the
selection of datasets and inductive biases. Addressing these challenges for
explainability, Kim et al. (2018) introduced Concept Activation Vectors (CAVs),
which aim to understand deep models' internal states in terms of human-aligned
concepts. These concepts correspond to directions in latent space, identified
using linear discriminants. Although this method was first applied to image
classification, it was later adapted to other domains, including natural
language processing. In this work, we attempt to apply the method to
electroencephalogram (EEG) data for explainability in Kostas et al.'s BENDR
(2021), a large-scale transformer model. A crucial part of this endeavor
involves defining the explanatory concepts and selecting relevant datasets to
ground concepts in the latent space. Our focus is on two mechanisms for EEG
concept formation: the use of externally labeled EEG datasets, and the
application of anatomically defined concepts. The former approach is a
straightforward generalization of methods used in image classification, while
the latter is novel and specific to EEG. We present evidence that both
approaches to concept formation yield valuable insights into the
representations learned by deep EEG models.

中文翻译:
深度学习模型因其规模、结构及训练过程中固有的随机性而显得复杂。数据集的选择与归纳偏好的引入进一步加剧了这种复杂性。为应对可解释性挑战，Kim等人（2018年）提出了概念激活向量（CAVs），旨在通过人类可理解的概念来解析深度模型的内部状态。这些概念对应潜在空间中的特定方向，通过线性判别方法识别。虽然该方法最初应用于图像分类领域，但后续被拓展至自然语言处理等其他领域。本研究尝试将该方法应用于Kostas等人（2021年）开发的大规模Transformer模型BENDR所处理的脑电图（EEG）数据，以增强其可解释性。研究的关键环节在于定义解释性概念，并选择相关数据集以在潜在空间中锚定这些概念。我们重点探讨两种EEG概念形成机制：利用外部标注的EEG数据集，以及应用解剖学定义的概念。前者是图像分类方法的直接延伸，后者则是针对EEG数据提出的创新方法。实验证据表明，两种概念形成途径均能有效揭示深度EEG模型学习到的表征特征。
