# Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: A Preliminary Empirical Study

链接: http://arxiv.org/abs/2304.00723v1

原文摘要:
Evaluating the quality of generated text is a challenging task in NLP, due to
the inherent complexity and diversity of text. Recently, large language models
(LLMs) have garnered significant attention due to their impressive performance
in various tasks. Therefore, we present this paper to investigate the
effectiveness of LLMs, especially ChatGPT, and explore ways to optimize their
use in assessing text quality. We compared three kinds of reference-free
evaluation methods. The experimental results prove that ChatGPT is capable of
evaluating text quality effectively from various perspectives without reference
and demonstrates superior performance than most existing automatic metrics. In
particular, the Explicit Score, which utilizes ChatGPT to generate a numeric
score measuring text quality, is the most effective and reliable method among
the three exploited approaches. However, directly comparing the quality of two
texts may lead to suboptimal results. We believe this paper will provide
valuable insights for evaluating text quality with LLMs and have released the
used data.

中文翻译:
评估生成文本的质量是自然语言处理领域的一项挑战性任务，这源于文本固有的复杂性和多样性。近年来，大型语言模型（LLMs）凭借其在各类任务中的卓越表现而备受关注。为此，我们开展本研究以探究LLMs（特别是ChatGPT）在文本质量评估中的有效性，并探索优化其应用的方法。我们比较了三种无参考评估方法，实验结果证明：ChatGPT能够在无参考文本的情况下，从多维度有效评估文本质量，其表现优于大多数现有自动评估指标。其中"显式评分法"（利用ChatGPT生成量化文本质量的数值分数）在三种方法中最为高效可靠。但需注意的是，直接比较两篇文本质量可能导致次优结果。本研究为利用LLMs评估文本质量提供了重要见解，相关数据已公开。
