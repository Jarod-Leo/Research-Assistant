# WDMoE: Wireless Distributed Mixture of Experts for Large Language Models

链接: http://arxiv.org/abs/2411.06681v1

原文摘要:
Large Language Models (LLMs) have achieved significant success in various
natural language processing tasks, but the role of wireless networks in
supporting LLMs has not been thoroughly explored. In this paper, we propose a
wireless distributed Mixture of Experts (WDMoE) architecture to enable
collaborative deployment of LLMs across edge servers at the base station (BS)
and mobile devices in wireless networks. Specifically, we decompose the MoE
layer in LLMs by placing the gating network and the preceding neural network
layer at BS, while distributing the expert networks among the devices. This
deployment leverages the parallel inference capabilities of expert networks on
mobile devices, effectively utilizing the limited computing and caching
resources of these devices. Accordingly, we develop a performance metric for
WDMoE-based LLMs, which accounts for both model capability and latency. To
minimize the latency while maintaining accuracy, we jointly optimize expert
selection and bandwidth allocation based on the performance metric. Moreover,
we build a hardware testbed using NVIDIA Jetson kits to validate the
effectiveness of WDMoE. Both theoretical simulations and practical hardware
experiments demonstrate that the proposed method can significantly reduce the
latency without compromising LLM performance.

中文翻译:
大型语言模型（LLMs）在各类自然语言处理任务中取得了显著成功，但无线网络对其支持作用尚未得到充分探索。本文提出一种无线分布式专家混合（WDMoE）架构，实现LLMs在基站（BS）边缘服务器与移动设备间的协同部署。具体而言，我们将MoE层解构：门控网络及前序神经网络层部署于基站，专家网络则分布于终端设备。这种部署方式充分利用移动设备上专家网络的并行推理能力，有效整合终端有限的计算与缓存资源。据此，我们构建了兼顾模型能力与延迟的性能评估指标。为在保证精度的前提下最小化延迟，基于该指标联合优化专家选择与带宽分配。此外，我们采用NVIDIA Jetson套件搭建硬件测试平台验证WDMoE的有效性。理论仿真与实际硬件实验均表明，所提方法能在保持LLM性能的同时显著降低延迟。
