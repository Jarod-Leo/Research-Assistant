# MaLA-500: Massive Language Adaptation of Large Language Models

链接: http://arxiv.org/abs/2401.13303v1

原文摘要:
Large language models (LLMs) have advanced the state of the art in natural
language processing. However, their predominant design for English or a limited
set of languages creates a substantial gap in their effectiveness for
low-resource languages. To bridge this gap, we introduce MaLA-500, a novel
large language model designed to cover an extensive range of 534 languages. To
train MaLA-500, we employ vocabulary extension and continued pretraining on
LLaMA 2 with Glot500-c. Our intrinsic evaluation demonstrates that MaLA-500 is
better at predicting the given texts of low-resource languages than existing
multilingual LLMs. Moreover, the extrinsic evaluation of in-context learning
shows that MaLA-500 outperforms previous LLMs on SIB200 and Taxi1500 by a
significant margin, i.e., 11.68% and 4.82% marco-average accuracy across
languages. We release MaLA-500 at https://huggingface.co/MaLA-LM

中文翻译:
大型语言模型（LLM）在自然语言处理领域取得了显著进展，但其主要针对英语或少数语种的设计，导致对低资源语言的处理效能存在显著不足。为弥补这一差距，我们推出了MaLA-500——一个创新性的大语言模型，其设计覆盖534种广泛语言。通过词汇扩展技术并基于LLaMA 2架构持续使用Glot500-c数据集进行预训练，MaLA-500在内部评估中展现出比现有多语言LLM更优异的低资源语言文本预测能力。外部上下文学习评估进一步表明，该模型在SIB200和Taxi1500基准测试中以显著优势超越前代模型，跨语言宏平均准确率分别提升11.68%和4.82%。模型已发布于https://huggingface.co/MaLA-LM。
