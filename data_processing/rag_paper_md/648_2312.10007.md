# Faithful Persona-based Conversational Dataset Generation with Large Language Models

链接: http://arxiv.org/abs/2312.10007v1

原文摘要:
High-quality conversational datasets are essential for developing AI models
that can communicate with users. One way to foster deeper interactions between
a chatbot and its user is through personas, aspects of the user's character
that provide insights into their personality, motivations, and behaviors.
Training Natural Language Processing (NLP) models on a diverse and
comprehensive persona-based dataset can lead to conversational models that
create a deeper connection with the user, and maintain their engagement. In
this paper, we leverage the power of Large Language Models (LLMs) to create a
large, high-quality conversational dataset from a seed dataset. We propose a
Generator-Critic architecture framework to expand the initial dataset, while
improving the quality of its conversations. The Generator is an LLM prompted to
output conversations. The Critic consists of a mixture of expert LLMs that
control the quality of the generated conversations. These experts select the
best generated conversations, which we then use to improve the Generator. We
release Synthetic-Persona-Chat, consisting of 20k conversations seeded from
Persona-Chat. We evaluate the quality of Synthetic-Persona-Chat and our
generation framework on different dimensions through extensive experiments, and
observe that the losing rate of Synthetic-Persona-Chat against Persona-Chat
during Turing test decreases from 17.2% to 8.8% over three iterations.

中文翻译:
高质量对话数据集对于开发能与用户交流的AI模型至关重要。为促进聊天机器人与用户间的深度互动，人物角色（persona）成为关键切入点——这些反映用户性格特质、动机与行为模式的元素能提供深层认知。通过在多样化、全面的人物角色数据集上训练自然语言处理模型，可构建出能与用户建立深度连接并维持互动黏性的对话系统。本文利用大语言模型（LLM）的生成能力，基于初始种子数据集构建大规模高质量对话数据集。我们提出生成器-评判器（Generator-Critic）架构框架：生成器由提示工程驱动的LLM负责对话生成；评判器则由混合专家模型（Mixture of Experts）组成，通过多维度质量评估筛选最优对话，进而迭代优化生成器。基于Persona-Chat种子数据生成的Synthetic-Persona-Chat包含2万组对话，实验表明经过三轮迭代优化，该数据集在图灵测试中相对于原始数据集的负率从17.2%显著降至8.8%，验证了框架的有效性。
