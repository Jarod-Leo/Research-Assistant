# Efficient Multimodal Fusion via Interactive Prompting

链接: http://arxiv.org/abs/2304.06306v1

原文摘要:
Large-scale pre-training has brought unimodal fields such as computer vision
and natural language processing to a new era. Following this trend, the size of
multi-modal learning models constantly increases, leading to an urgent need to
reduce the massive computational cost of finetuning these models for downstream
tasks. In this paper, we propose an efficient and flexible multimodal fusion
method, namely PMF, tailored for fusing unimodally pre-trained transformers.
Specifically, we first present a modular multimodal fusion framework that
exhibits high flexibility and facilitates mutual interactions among different
modalities. In addition, we disentangle vanilla prompts into three types in
order to learn different optimizing objectives for multimodal learning. It is
also worth noting that we propose to add prompt vectors only on the deep layers
of the unimodal transformers, thus significantly reducing the training memory
usage. Experiment results show that our proposed method achieves comparable
performance to several other multimodal finetuning methods with less than 3%
trainable parameters and up to 66% saving of training memory usage.

中文翻译:
大规模预训练已将计算机视觉和自然语言处理等单模态领域推向新纪元。在此趋势下，多模态学习模型的规模持续扩大，亟需降低这些模型在下游任务微调时的巨大计算成本。本文提出一种高效灵活的多模态融合方法PMF，专为融合单模态预训练Transformer而设计。具体而言，我们首先提出模块化多模态融合框架，该框架具有高度灵活性，并能促进不同模态间的交互作用。此外，我们将传统提示向量解耦为三种类型，以学习多模态任务中不同的优化目标。值得注意的是，我们提出仅在单模态Transformer的深层添加提示向量，从而显著降低训练内存消耗。实验结果表明，在可训练参数不足3%、训练内存节省最高达66%的情况下，所提方法性能与多种多模态微调方法相当。
