# Combining Denoising Autoencoders with Contrastive Learning to fine-tune Transformer Models

链接: http://arxiv.org/abs/2405.14437v1

原文摘要:
Recently, using large pretrained Transformer models for transfer learning
tasks has evolved to the point where they have become one of the flagship
trends in the Natural Language Processing (NLP) community, giving rise to
various outlooks such as prompt-based, adapters or combinations with
unsupervised approaches, among many others. This work proposes a 3 Phase
technique to adjust a base model for a classification task. First, we adapt the
model's signal to the data distribution by performing further training with a
Denoising Autoencoder (DAE). Second, we adjust the representation space of the
output to the corresponding classes by clustering through a Contrastive
Learning (CL) method. In addition, we introduce a new data augmentation
approach for Supervised Contrastive Learning to correct the unbalanced
datasets. Third, we apply fine-tuning to delimit the predefined categories.
These different phases provide relevant and complementary knowledge to the
model to learn the final task. We supply extensive experimental results on
several datasets to demonstrate these claims. Moreover, we include an ablation
study and compare the proposed method against other ways of combining these
techniques.

中文翻译:
近年来，基于大规模预训练Transformer模型的迁移学习技术已发展成为自然语言处理（NLP）领域的核心趋势之一，催生了提示学习、适配器机制以及与无监督方法结合等多种技术路径。本研究提出了一种三阶段分类任务适配方法：首先通过去噪自编码器（DAE）进行增量训练，使模型信号适应目标数据分布；其次采用对比学习（CL）聚类方法调整输出表示空间以匹配目标类别，并创新性地提出面向监督对比学习的增强数据方法以修正数据集不平衡问题；最后通过微调明确界定预设类别边界。这三个阶段为模型习得最终任务提供了互补性知识。我们在多个数据集上进行了广泛实验验证，同时包含消融研究及与其他组合技术的对比分析，充分证明了该方法的有效性。
