# BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters for Efficient LLM Inference

链接: http://arxiv.org/abs/2410.23079v1

原文摘要:
Large language models (LLMs) are essential in natural language processing but
often struggle with inference speed and computational efficiency, limiting
real-time deployment. The key-value (KV) cache mechanism reduces computational
overhead in transformer models, but challenges in maintaining contextual
understanding remain. In this paper, we propose BUZZ, a novel KV caching
algorithm that leverages structured contextual information to minimize cache
memory usage while enhancing inference speed. BUZZ employs a beehive-structured
sparse cache, incorporating a sliding window to capture recent information and
dynamically segmenting historical tokens into chunks to prioritize important
tokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:
CNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ
(1) reduces cache memory usage by $\textbf{2.5}\times$ in LLM inference while
maintaining over 99% accuracy in long-text summarization, and (2) surpasses
state-of-the-art performance in multi-document question answering by
$\textbf{7.69%}$ under the same memory limit, where full cache methods
encounter out-of-memory issues. Additionally, BUZZ achieves significant
inference speedup with a $\log{n}$ time complexity. The code is available at
https://github.com/JunqiZhao888/buzz-llm.

中文翻译:
大语言模型（LLMs）在自然语言处理中至关重要，但其推理速度和计算效率的不足限制了实时部署。基于键值（KV）缓存的机制虽然降低了Transformer模型的计算开销，但在保持上下文理解方面仍存在挑战。本文提出BUZZ——一种创新的KV缓存算法，通过利用结构化上下文信息，在提升推理速度的同时最小化缓存内存占用。该算法采用蜂巢结构的稀疏缓存设计，结合滑动窗口捕获近期信息，并动态将历史令牌分块以优先处理局部邻域中的重要令牌。我们在四个真实数据集（CNN/Daily Mail、XSUM、Wikitext和10-QA）上验证了BUZZ的性能：实验表明，(1) 在长文本摘要任务中，BUZZ将LLM推理的缓存内存占用降低至$\textbf{2.5}$倍，同时保持99%以上的准确率；(2) 在相同内存限制下（完整缓存方法出现内存不足时），多文档问答任务性能超越现有最佳方法$\textbf{7.69\%}$。此外，BUZZ凭借$\log{n}$时间复杂度实现了显著的推理加速。代码已开源：https://github.com/JunqiZhao888/buzz-llm。
