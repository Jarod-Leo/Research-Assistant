# Detecting Language Model Attacks with Perplexity

链接: http://arxiv.org/abs/2308.14132v1

原文摘要:
A novel hack involving Large Language Models (LLMs) has emerged, exploiting
adversarial suffixes to deceive models into generating perilous responses. Such
jailbreaks can trick LLMs into providing intricate instructions to a malicious
user for creating explosives, orchestrating a bank heist, or facilitating the
creation of offensive content. By evaluating the perplexity of queries with
adversarial suffixes using an open-source LLM (GPT-2), we found that they have
exceedingly high perplexity values. As we explored a broad range of regular
(non-adversarial) prompt varieties, we concluded that false positives are a
significant challenge for plain perplexity filtering. A Light-GBM trained on
perplexity and token length resolved the false positives and correctly detected
most adversarial attacks in the test set.

中文翻译:
一种利用大型语言模型（LLM）的新型攻击手段近期出现，通过添加对抗性后缀诱导模型生成危险回复。此类越狱攻击可诱骗LLM向恶意用户提供制造爆炸物、策划银行劫案或协助制作攻击性内容的详细指导。基于开源模型GPT-2对含对抗性后缀查询的困惑度评估显示，这些查询具有异常高的困惑值。在广泛考察常规（非对抗性）提示的多样性后，我们发现误报是单纯困惑度过滤面临的主要挑战。通过训练基于困惑度和标记长度的Light-GBM模型，有效解决了误报问题，并在测试集中准确识别了绝大多数对抗攻击。
