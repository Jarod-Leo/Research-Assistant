# Detecting Language Model Attacks with Perplexity

链接: http://arxiv.org/abs/2308.14132v1

原文摘要:
A novel hack involving Large Language Models (LLMs) has emerged, exploiting
adversarial suffixes to deceive models into generating perilous responses. Such
jailbreaks can trick LLMs into providing intricate instructions to a malicious
user for creating explosives, orchestrating a bank heist, or facilitating the
creation of offensive content. By evaluating the perplexity of queries with
adversarial suffixes using an open-source LLM (GPT-2), we found that they have
exceedingly high perplexity values. As we explored a broad range of regular
(non-adversarial) prompt varieties, we concluded that false positives are a
significant challenge for plain perplexity filtering. A Light-GBM trained on
perplexity and token length resolved the false positives and correctly detected
most adversarial attacks in the test set.

中文翻译:
一种利用大型语言模型（LLMs）的新型攻击手段正在兴起——通过植入对抗性后缀诱导模型生成危险内容。这类"越狱"攻击能诱使LLMs为恶意用户提供制造爆炸物、策划银行劫案或生成攻击性内容的详细指导。基于开源模型GPT-2对含对抗性后缀查询语句的困惑度评估显示，这类语句具有极高的困惑度值。在广泛考察各类常规（非对抗性）提示模板后，我们发现单纯依赖困惑度过滤会产生大量误判。通过采用基于困惑度和标记长度的Light-GBM分类器，系统成功消除误报，并在测试集中准确识别出绝大多数对抗攻击。

（翻译说明：采用技术文本的简洁风格，将"jailbreaks"译为行业术语"越狱"，"perplexity"统一译为"困惑度"。通过拆分英文长句为中文短句结构，如将"exploiting..."处理为破折号引导的插入说明。保留"Light-GBM"等技术术语原称，使用"对抗性后缀"等符合中文NLP领域表述的译法。最后两句通过"通过..."的衔接保持逻辑连贯性。）
