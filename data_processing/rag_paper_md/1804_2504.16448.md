# EMRModel: A Large Language Model for Extracting Medical Consultation Dialogues into Structured Medical Records

链接: http://arxiv.org/abs/2504.16448v1

原文摘要:
Medical consultation dialogues contain critical clinical information, yet
their unstructured nature hinders effective utilization in diagnosis and
treatment. Traditional methods, relying on rule-based or shallow machine
learning techniques, struggle to capture deep and implicit semantics. Recently,
large pre-trained language models and Low-Rank Adaptation (LoRA), a lightweight
fine-tuning method, have shown promise for structured information extraction.
We propose EMRModel, a novel approach that integrates LoRA-based fine-tuning
with code-style prompt design, aiming to efficiently convert medical
consultation dialogues into structured electronic medical records (EMRs).
Additionally, we construct a high-quality, realistically grounded dataset of
medical consultation dialogues with detailed annotations. Furthermore, we
introduce a fine-grained evaluation benchmark for medical consultation
information extraction and provide a systematic evaluation methodology,
advancing the optimization of medical natural language processing (NLP) models.
Experimental results show EMRModel achieves an F1 score of 88.1%, improving
by49.5% over standard pre-trained models. Compared to traditional LoRA
fine-tuning methods, our model shows superior performance, highlighting its
effectiveness in structured medical record extraction tasks.

中文翻译:
医疗问诊对话蕴含关键的临床信息，但其非结构化特性阻碍了诊疗过程中的有效利用。传统方法依赖规则或浅层机器学习技术，难以捕捉深层隐含语义。近年来，大语言预训练模型与轻量化微调方法LoRA（低秩适应）为结构化信息抽取提供了新思路。我们提出EMRModel，创新性地结合基于LoRA的微调策略与代码式提示设计，旨在高效实现医疗问诊对话到结构化电子病历（EMR）的转换。同时构建了高质量、贴近真实场景的医疗问诊对话数据集，并提供精细化标注。此外，针对医疗问诊信息抽取任务提出细粒度评估基准，系统化设计评价方法，推动医疗自然语言处理模型的优化。实验表明EMRModel的F1值达到88.1%，较标准预训练模型提升49.5%。与传统LoRA微调方法相比，模型性能优势显著，在结构化病历抽取任务中展现出卓越效果。
