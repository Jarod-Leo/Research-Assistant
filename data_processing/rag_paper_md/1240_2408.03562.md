# A Comparison of LLM Finetuning Methods & Evaluation Metrics with Travel Chatbot Use Case

链接: http://arxiv.org/abs/2408.03562v1

原文摘要:
This research compares large language model (LLM) fine-tuning methods,
including Quantized Low Rank Adapter (QLoRA), Retrieval Augmented fine-tuning
(RAFT), and Reinforcement Learning from Human Feedback (RLHF), and additionally
compared LLM evaluation methods including End to End (E2E) benchmark method of
"Golden Answers", traditional natural language processing (NLP) metrics, RAG
Assessment (Ragas), OpenAI GPT-4 evaluation metrics, and human evaluation,
using the travel chatbot use case. The travel dataset was sourced from the the
Reddit API by requesting posts from travel-related subreddits to get
travel-related conversation prompts and personalized travel experiences, and
augmented for each fine-tuning method. We used two pretrained LLMs utilized for
fine-tuning research: LLaMa 2 7B, and Mistral 7B. QLoRA and RAFT are applied to
the two pretrained models. The inferences from these models are extensively
evaluated against the aforementioned metrics. The best model according to human
evaluation and some GPT-4 metrics was Mistral RAFT, so this underwent a
Reinforcement Learning from Human Feedback (RLHF) training pipeline, and
ultimately was evaluated as the best model. Our main findings are that: 1)
quantitative and Ragas metrics do not align with human evaluation, 2) Open AI
GPT-4 evaluation most aligns with human evaluation, 3) it is essential to keep
humans in the loop for evaluation because, 4) traditional NLP metrics
insufficient, 5) Mistral generally outperformed LLaMa, 6) RAFT outperforms
QLoRA, but still needs postprocessing, 7) RLHF improves model performance
significantly. Next steps include improving data quality, increasing data
quantity, exploring RAG methods, and focusing data collection on a specific
city, which would improve data quality by narrowing the focus, while creating a
useful product.

中文翻译:
本研究以旅行聊天机器人为应用场景，对比了量化低秩适配器（QLoRA）、检索增强微调（RAFT）和基于人类反馈的强化学习（RLHF）等大语言模型（LLM）微调方法，并系统评估了"黄金答案"端到端基准测试、传统自然语言处理（NLP）指标、RAG评估框架（Ragas）、OpenAI GPT-4评估指标及人工评估等评测方法。旅行数据集通过Reddit API从相关子论坛获取旅行对话提示和个性化旅行经历，并针对不同微调方法进行数据增强。研究选用LLaMa 2 7B和Mistral 7B两个预训练模型进行微调实验，应用QLoRA和RAFT方法后，采用多维度指标对模型输出进行全面评估。人工评估与部分GPT-4指标显示Mistral RAFT表现最佳，经RLHF训练流程后最终被评定为最优模型。主要发现包括：1）量化指标与Ragas评估结果与人工评价不一致；2）OpenAI GPT-4评估与人工评价契合度最高；3）必须保持人工参与评估环节；4）传统NLP指标存在不足；5）Mistral总体优于LLaMa；6）RAFT优于QLoRA但仍需后处理；7）RLHF能显著提升模型性能。后续研究将着力提升数据质量与规模，探索RAG方法，并聚焦特定城市开展数据收集以提升数据针对性，同时开发实用化产品。
