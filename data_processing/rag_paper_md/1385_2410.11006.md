# Effective Self-Mining of In-Context Examples for Unsupervised Machine Translation with LLMs

链接: http://arxiv.org/abs/2410.11006v1

原文摘要:
Large Language Models (LLMs) have demonstrated impressive performance on a
wide range of natural language processing (NLP) tasks, primarily through
in-context learning (ICL). In ICL, the LLM is provided with examples that
represent a given task such that it learns to generate answers for test inputs.
However, access to these in-context examples is not guaranteed especially for
low-resource or massively multilingual tasks. In this work, we propose an
unsupervised approach to mine in-context examples for machine translation (MT),
enabling unsupervised MT (UMT) across different languages. Our approach begins
with word-level mining to acquire word translations that are then used to
perform sentence-level mining. As the quality of mined parallel pairs may not
be optimal due to noise or mistakes, we introduce a filtering criterion to
select the optimal in-context examples from a pool of unsupervised parallel
sentences. We evaluate our approach using two multilingual LLMs on 288
directions from the FLORES-200 dataset and analyze the impact of various
linguistic features on performance. Our findings demonstrate the effectiveness
of our unsupervised approach in mining in-context examples for MT, leading to
better or comparable translation performance as translation with regular
in-context samples (extracted from human-annotated data), while also
outperforming the other state-of-the-art UMT methods by an average of $7$ BLEU
points.

中文翻译:
大型语言模型（LLMs）在广泛的自然语言处理（NLP）任务中展现出卓越性能，其核心机制在于上下文学习（ICL）。该方法通过向模型提供代表特定任务的示例，使其学会为测试输入生成答案。然而，获取这类上下文示例并非易事，尤其在低资源或大规模多语言任务中。本研究提出一种无监督方法，用于挖掘机器翻译（MT）的上下文示例，从而实现跨语言的无监督机器翻译（UMT）。  

我们的方法首先进行词级挖掘以获取单词翻译，随后利用这些翻译结果进行句级挖掘。由于挖掘的平行句对可能因噪声或错误导致质量欠佳，我们引入过滤标准，从无监督平行句库中筛选最优上下文示例。基于FLORES-200数据集的288个语言方向，我们使用两种多语言LLM评估该方法，并分析不同语言特征对性能的影响。实验结果表明：所提出的无监督方法能有效挖掘MT上下文示例，其翻译性能优于或持平于使用常规上下文样本（源自人工标注数据）的翻译效果，同时以平均7个BLEU分的优势超越其他最先进的UMT方法。
