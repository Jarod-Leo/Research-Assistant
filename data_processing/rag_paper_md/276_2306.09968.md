# ClinicalGPT: Large Language Models Finetuned with Diverse Medical Data and Comprehensive Evaluation

链接: http://arxiv.org/abs/2306.09968v1

原文摘要:
Large language models have exhibited exceptional performance on various
Natural Language Processing (NLP) tasks, leveraging techniques such as the
pre-training, and instruction fine-tuning. Despite these advances, their
effectiveness in medical applications is limited, due to challenges such as
factual inaccuracies, reasoning abilities, and lack grounding in real-world
experience. In this study, we present ClinicalGPT, a language model explicitly
designed and optimized for clinical scenarios. By incorporating extensive and
diverse real-world data, such as medical records, domain-specific knowledge,
and multi-round dialogue consultations in the training process, ClinicalGPT is
better prepared to handle multiple clinical task. Furthermore, we introduce a
comprehensive evaluation framework that includes medical knowledge
question-answering, medical exams, patient consultations, and diagnostic
analysis of medical records. Our results demonstrate that ClinicalGPT
significantly outperforms other models in these tasks, highlighting the
effectiveness of our approach in adapting large language models to the critical
domain of healthcare.

中文翻译:
大型语言模型通过预训练和指令微调等技术，在各类自然语言处理任务中展现出卓越性能。然而在医疗应用领域，由于存在事实准确性不足、推理能力有限以及缺乏真实场景经验等问题，其实际效能仍受制约。本研究提出ClinicalGPT——一个专为临床场景设计与优化的语言模型。通过整合电子病历、领域知识和多轮诊疗对话等多样化真实数据参与训练，该模型显著提升了处理复杂临床任务的能力。我们同时构建了包含医学知识问答、执业考试、患者咨询和病历诊断分析在内的综合评估体系。实验结果表明，ClinicalGPT在这些任务中全面超越现有模型，验证了大型语言模型在医疗关键领域适配方法的有效性。
