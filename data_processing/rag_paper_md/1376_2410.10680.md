# Evaluating SQL Understanding in Large Language Models

链接: http://arxiv.org/abs/2410.10680v1

原文摘要:
The rise of large language models (LLMs) has significantly impacted various
domains, including natural language processing (NLP) and image generation, by
making complex computational tasks more accessible. While LLMs demonstrate
impressive generative capabilities, there is an ongoing debate about their
level of "understanding," particularly in structured domains like SQL. In this
paper, we evaluate the extent to which LLMs "understand" SQL by testing them on
a series of key SQL tasks. These tasks, such as syntax error detection, missing
token identification, query performance prediction, query equivalence checking,
and query explanation, assess the models' proficiency in recognition, context
awareness, semantics, and coherence, which are essential skills for SQL
understanding. We generate labeled datasets from well-known workloads, and
evaluate the latest LLMs, focusing on how query complexity and syntactic
features influence performance. Our results indicate that while GPT4 excels at
tasks requiring recognition and context, all models struggle with deeper
semantic understanding and coherence, especially in query equivalence and
performance estimation, revealing the limitations of current LLMs in achieving
full SQL comprehension.

中文翻译:
大型语言模型（LLM）的崛起显著影响了包括自然语言处理（NLP）和图像生成在内的多个领域，使得复杂计算任务更易于实现。尽管LLM展现出令人印象深刻的生成能力，关于其"理解"水平的争论持续存在，尤其在SQL这类结构化领域。本文通过一系列关键SQL任务测试，评估LLM对SQL的"理解"程度。这些任务涵盖语法错误检测、缺失标记识别、查询性能预测、查询等价性检查及查询解释，旨在检验模型在识别、上下文感知、语义理解和逻辑连贯性方面的能力——这些正是SQL理解的核心技能。我们从知名工作负载生成标注数据集，评估最新LLM的表现，重点关注查询复杂度和句法特征如何影响性能。结果表明：虽然GPT4在需要识别和上下文感知的任务中表现优异，但所有模型在深层语义理解和逻辑连贯性（特别是查询等价性和性能估计方面）均存在困难，揭示了当前LLM在实现完整SQL理解方面的局限性。
