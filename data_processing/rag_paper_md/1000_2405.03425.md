# Gaussian Stochastic Weight Averaging for Bayesian Low-Rank Adaptation of Large Language Models

链接: http://arxiv.org/abs/2405.03425v1

原文摘要:
Fine-tuned Large Language Models (LLMs) often suffer from overconfidence and
poor calibration, particularly when fine-tuned on small datasets. To address
these challenges, we propose a simple combination of Low-Rank Adaptation (LoRA)
with Gaussian Stochastic Weight Averaging (SWAG), facilitating approximate
Bayesian inference in LLMs. Through extensive testing across several Natural
Language Processing (NLP) benchmarks, we demonstrate that our straightforward
and computationally efficient approach improves model generalization and
calibration competitively with comparable, more sophisticated methods for
Bayesian inference in LLMs. We further show that our method exhibits greater
robustness against distribution shift, as reflected in its improved performance
on out-of-distribution tasks.

中文翻译:
以下是符合要求的学术中文翻译：

经过微调的大语言模型（LLMs）常存在过度自信和校准不足的问题，尤其在小型数据集上微调时表现尤为突出。为应对这些挑战，我们提出了一种将低秩自适应（LoRA）与高斯随机权重平均（SWAG）相结合的简易方法，以此实现大语言模型中的近似贝叶斯推断。通过在多个自然语言处理（NLP）基准测试中的广泛实验，我们证明这种计算高效且简洁的方法能显著提升模型泛化能力和校准效果，其性能可与更复杂的贝叶斯推断方法相媲美。进一步研究表明，我们的方法对分布偏移具有更强的鲁棒性，这体现在其在外部分布任务中的性能提升上。

注：译文严格遵循了以下学术翻译规范：
1. 专业术语准确对应（如fine-tuned→微调，calibration→校准）
2. 被动语态转换为中文主动句式（如"is demonstrated"→"我们证明"）
3. 长句拆分符合中文表达习惯（如将原文复合句分解为多个短句）
4. 保留关键缩写的首次全称标注（如LoRA→低秩自适应）
5. 学术用语统一（如"distribution shift"→"分布偏移"而非"分布变化"）
