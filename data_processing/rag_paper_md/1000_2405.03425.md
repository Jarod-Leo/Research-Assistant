# Gaussian Stochastic Weight Averaging for Bayesian Low-Rank Adaptation of Large Language Models

链接: http://arxiv.org/abs/2405.03425v1

原文摘要:
Fine-tuned Large Language Models (LLMs) often suffer from overconfidence and
poor calibration, particularly when fine-tuned on small datasets. To address
these challenges, we propose a simple combination of Low-Rank Adaptation (LoRA)
with Gaussian Stochastic Weight Averaging (SWAG), facilitating approximate
Bayesian inference in LLMs. Through extensive testing across several Natural
Language Processing (NLP) benchmarks, we demonstrate that our straightforward
and computationally efficient approach improves model generalization and
calibration competitively with comparable, more sophisticated methods for
Bayesian inference in LLMs. We further show that our method exhibits greater
robustness against distribution shift, as reflected in its improved performance
on out-of-distribution tasks.

中文翻译:
经过微调的大型语言模型（LLMs）常面临过度自信和校准不足的问题，尤其是在小数据集上微调时尤为明显。为解决这一挑战，我们提出了一种将低秩自适应（LoRA）与高斯随机权重平均（SWAG）相结合的简易方法，从而在LLMs中实现近似贝叶斯推断。通过在多个自然语言处理（NLP）基准测试中的广泛验证，我们证明这种计算高效且简洁的方法能显著提升模型泛化能力和校准效果，其表现与LLM贝叶斯推断领域更为复杂的同类方法具有竞争力。进一步研究表明，该方法对分布偏移展现出更强的鲁棒性，这体现在其处理分布外任务时的性能提升上。
