# Optimal inference of a generalised Potts model by single-layer transformers with factored attention

链接: http://arxiv.org/abs/2304.07235v1

原文摘要:
Transformers are neural networks that revolutionized natural language
processing and machine learning. They process sequences of inputs, like words,
using a mechanism called self-attention, which is trained via masked language
modeling (MLM). In MLM, a word is randomly masked in an input sequence, and the
network is trained to predict the missing word. Despite the practical success
of transformers, it remains unclear what type of data distribution
self-attention can learn efficiently. Here, we show analytically that if one
decouples the treatment of word positions and embeddings, a single layer of
self-attention learns the conditionals of a generalized Potts model with
interactions between sites and Potts colors. Moreover, we show that training
this neural network is exactly equivalent to solving the inverse Potts problem
by the so-called pseudo-likelihood method, well known in statistical physics.
Using this mapping, we compute the generalization error of self-attention in a
model scenario analytically using the replica method.

中文翻译:
Transformer是一种彻底革新自然语言处理与机器学习的神经网络架构。其通过名为自注意力（self-attention）的机制处理输入序列（如单词序列），该机制采用掩码语言建模（MLM）进行训练。在MLM中，输入序列中的单词会被随机遮蔽，网络通过预测缺失单词来完成训练。尽管Transformer已取得显著实践成果，但关于自注意力机制能高效学习何种数据分布仍不明确。本研究通过解析方法证明：若将词位置与词嵌入的处理解耦，单层自注意力可学习具有位点与Potts颜色相互作用的广义Potts模型条件分布。进一步揭示该神经网络的训练过程完全等价于统计物理学中著名的伪似然法求解逆Potts问题。借助这一映射关系，我们运用复本方法在模型场景中解析计算了自注意力的泛化误差。
