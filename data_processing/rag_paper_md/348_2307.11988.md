# Sparse then Prune: Toward Efficient Vision Transformers

链接: http://arxiv.org/abs/2307.11988v1

原文摘要:
The Vision Transformer architecture is a deep learning model inspired by the
success of the Transformer model in Natural Language Processing. However, the
self-attention mechanism, large number of parameters, and the requirement for a
substantial amount of training data still make Vision Transformers
computationally burdensome. In this research, we investigate the possibility of
applying Sparse Regularization to Vision Transformers and the impact of
Pruning, either after Sparse Regularization or without it, on the trade-off
between performance and efficiency. To accomplish this, we apply Sparse
Regularization and Pruning methods to the Vision Transformer architecture for
image classification tasks on the CIFAR-10, CIFAR-100, and ImageNet-100
datasets. The training process for the Vision Transformer model consists of two
parts: pre-training and fine-tuning. Pre-training utilizes ImageNet21K data,
followed by fine-tuning for 20 epochs. The results show that when testing with
CIFAR-100 and ImageNet-100 data, models with Sparse Regularization can increase
accuracy by 0.12%. Furthermore, applying pruning to models with Sparse
Regularization yields even better results. Specifically, it increases the
average accuracy by 0.568% on CIFAR-10 data, 1.764% on CIFAR-100, and 0.256% on
ImageNet-100 data compared to pruning models without Sparse Regularization.
Code can be accesed here: https://github.com/yogiprsty/Sparse-ViT

中文翻译:
视觉Transformer架构是一种受自然语言处理中Transformer模型成功启发的深度学习模型。然而，自注意力机制、大量参数以及对海量训练数据的需求，仍使得视觉Transformer面临计算负担过重的问题。本研究探讨了稀疏正则化在视觉Transformer中的应用可能性，以及剪枝操作（无论是否结合稀疏正则化）对模型性能与效率平衡的影响。为此，我们在CIFAR-10、CIFAR-100和ImageNet-100数据集的图像分类任务中，对视觉Transformer架构实施了稀疏正则化与剪枝方法。该模型的训练过程分为预训练和微调两个阶段：预训练使用ImageNet21K数据，随后进行20个epoch的微调。实验结果表明，在CIFAR-100和ImageNet-100测试数据上，采用稀疏正则化的模型能将准确率提升0.12%。更重要的是，对经过稀疏正则化的模型实施剪枝能获得更优效果——相较于未使用稀疏正则化的剪枝模型，其在CIFAR-10数据上的平均准确率提升0.568%，CIFAR-100提升1.764%，ImageNet-100提升0.256%。代码详见：https://github.com/yogiprsty/Sparse-ViT
