# Towards a More Inclusive AI: Progress and Perspectives in Large Language Model Training for the Sámi Language

链接: http://arxiv.org/abs/2405.05777v1

原文摘要:
S\'ami, an indigenous language group comprising multiple languages, faces
digital marginalization due to the limited availability of data and
sophisticated language models designed for its linguistic intricacies. This
work focuses on increasing technological participation for the S\'ami language.
We draw the attention of the ML community towards the language modeling problem
of Ultra Low Resource (ULR) languages. ULR languages are those for which the
amount of available textual resources is very low, and the speaker count for
them is also very low. ULRLs are also not supported by mainstream Large
Language Models (LLMs) like ChatGPT, due to which gathering artificial training
data for them becomes even more challenging. Mainstream AI foundational model
development has given less attention to this category of languages. Generally,
these languages have very few speakers, making it hard to find them. However,
it is important to develop foundational models for these ULR languages to
promote inclusion and the tangible abilities and impact of LLMs. To this end,
we have compiled the available S\'ami language resources from the web to create
a clean dataset for training language models. In order to study the behavior of
modern LLM models with ULR languages (S\'ami), we have experimented with
different kinds of LLMs, mainly at the order of $\sim$ seven billion
parameters. We have also explored the effect of multilingual LLM training for
ULRLs. We found that the decoder-only models under a sequential multilingual
training scenario perform better than joint multilingual training, whereas
multilingual training with high semantic overlap, in general, performs better
than training from scratch.This is the first study on the S\'ami language for
adapting non-statistical language models that use the latest developments in
the field of natural language processing (NLP).

中文翻译:
萨米语作为一种包含多种语言的土著语系，由于缺乏针对其复杂语言特性设计的数据资源和成熟语言模型，正面临数字化边缘化问题。本研究致力于提升萨米语的技术参与度，旨在唤起机器学习社区对超低资源语言（ULR）建模问题的关注。这类语言不仅可用文本资源极其匮乏、使用人口稀少，更因ChatGPT等主流大语言模型（LLM）未予支持，导致人工训练数据获取尤为困难。当前主流AI基础模型开发对此类语言的关注明显不足——尽管这些语言使用者寥寥且难以寻获，但为其构建基础模型对促进技术包容性及展现LLM实际能力与影响力至关重要。

为此，我们系统整合了网络可获取的萨米语资源，构建了用于语言模型训练的洁净数据集。为探究现代LLM在超低资源语言（以萨米语为例）上的表现，我们以约70亿参数规模的模型为主，对不同类型LLM进行了实验验证，同时考察了多语言训练对ULR语言的影响。研究发现：在序列化多语言训练场景下，仅解码器模型表现优于联合多语言训练；而具有高语义重叠的多语言训练整体优于从零开始训练。本研究首次将自然语言处理（NLP）领域最新进展应用于萨米语的非统计语言模型适配，填补了该领域的空白。
