# DialogueLLM: Context and Emotion Knowledge-Tuned LLaMA Models for Emotion Recognition in Conversations

链接: http://arxiv.org/abs/2310.11374v1

原文摘要:
Large language models (LLMs) and their variants have shown extraordinary
efficacy across numerous downstream natural language processing (NLP) tasks,
which has presented a new vision for the development of NLP. Despite their
remarkable performance in natural language generating (NLG), LLMs lack a
distinct focus on the emotion understanding domain. As a result, using LLMs for
emotion recognition may lead to suboptimal and inadequate precision. Another
limitation of LLMs is that they are typical trained without leveraging
multi-modal information. To overcome these limitations, we propose DialogueLLM,
a context and emotion knowledge tuned LLM that is obtained by fine-tuning LLaMA
models with 13,638 multi-modal (i.e., texts and videos) emotional dialogues.
The visual information is considered as the supplementary knowledge to
construct high-quality instructions. We offer a comprehensive evaluation of our
proposed model on three benchmarking emotion recognition in conversations (ERC)
datasets and compare the results against the SOTA baselines and other SOTA
LLMs. Additionally, DialogueLLM-7B can be easily trained using LoRA on a 40GB
A100 GPU in 5 hours, facilitating reproducibility for other researchers.

中文翻译:
大型语言模型（LLM）及其变体在众多下游自然语言处理（NLP）任务中展现出卓越效能，为NLP领域的发展开辟了新视野。尽管在自然语言生成（NLG）方面表现突出，LLM对情感理解领域的针对性关注仍显不足，直接应用于情感识别可能导致效果欠佳且精度有限。另一局限性在于现有LLM通常未充分利用多模态信息进行训练。为突破这些限制，我们提出DialogueLLM——一种基于上下文与情感知识调优的LLM，通过对13,638组多模态（文本与视频）情感对话数据微调LLaMA模型获得。视觉信息被作为构建高质量指令的补充知识，我们在三个会话情感识别（ERC）基准数据集上全面评估模型性能，并与SOTA基线及其他先进LLM进行对比。此外，DialogueLLM-7B仅需在40GB A100 GPU上通过LoRA技术训练5小时即可完成，极大提升了其他研究者的可复现性。
