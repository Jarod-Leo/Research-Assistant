# Challenges and Opportunities of Using Transformer-Based Multi-Task Learning in NLP Through ML Lifecycle: A Survey

链接: http://arxiv.org/abs/2308.08234v1

原文摘要:
The increasing adoption of natural language processing (NLP) models across
industries has led to practitioners' need for machine learning systems to
handle these models efficiently, from training to serving them in production.
However, training, deploying, and updating multiple models can be complex,
costly, and time-consuming, mainly when using transformer-based pre-trained
language models. Multi-Task Learning (MTL) has emerged as a promising approach
to improve efficiency and performance through joint training, rather than
training separate models. Motivated by this, we first provide an overview of
transformer-based MTL approaches in NLP. Then, we discuss the challenges and
opportunities of using MTL approaches throughout typical ML lifecycle phases,
specifically focusing on the challenges related to data engineering, model
development, deployment, and monitoring phases. This survey focuses on
transformer-based MTL architectures and, to the best of our knowledge, is novel
in that it systematically analyses how transformer-based MTL in NLP fits into
ML lifecycle phases. Furthermore, we motivate research on the connection
between MTL and continual learning (CL), as this area remains unexplored. We
believe it would be practical to have a model that can handle both MTL and CL,
as this would make it easier to periodically re-train the model, update it due
to distribution shifts, and add new capabilities to meet real-world
requirements.

中文翻译:
随着自然语言处理（NLP）模型在各行业的广泛应用，从业者对机器学习系统提出了从训练到生产环境部署的全流程高效处理需求。然而，基于Transformer的预训练语言模型在训练、部署和更新多个模型时往往面临复杂度高、成本昂贵且耗时等问题。多任务学习（MTL）作为一种通过联合训练提升效率与性能的替代方案应运而生。基于此，本文首先系统梳理了基于Transformer的NLP多任务学习方法，继而从机器学习生命周期视角出发，重点探讨了数据工程、模型开发、部署及监控等阶段应用MTL面临的挑战与机遇。本综述的创新性在于首次系统分析了基于Transformer的MTL方法如何嵌入机器学习生命周期各环节。此外，我们提出应加强MTL与持续学习（CL）的交叉研究——这一尚未充分探索的领域。构建同时支持MTL与CL的模型具有显著实用价值，既能简化模型的周期性重训练流程，又能适应数据分布变化的需求更新，还可动态扩展模型能力以满足实际场景需求。
