# A LLM-Powered Automatic Grading Framework with Human-Level Guidelines Optimization

链接: http://arxiv.org/abs/2410.02165v1

原文摘要:
Open-ended short-answer questions (SAGs) have been widely recognized as a
powerful tool for providing deeper insights into learners' responses in the
context of learning analytics (LA). However, SAGs often present challenges in
practice due to the high grading workload and concerns about inconsistent
assessments. With recent advancements in natural language processing (NLP),
automatic short-answer grading (ASAG) offers a promising solution to these
challenges. Despite this, current ASAG algorithms are often limited in
generalizability and tend to be tailored to specific questions. In this paper,
we propose a unified multi-agent ASAG framework, GradeOpt, which leverages
large language models (LLMs) as graders for SAGs. More importantly, GradeOpt
incorporates two additional LLM-based agents - the reflector and the refiner -
into the multi-agent system. This enables GradeOpt to automatically optimize
the original grading guidelines by performing self-reflection on its errors.
Through experiments on a challenging ASAG task, namely the grading of
pedagogical content knowledge (PCK) and content knowledge (CK) questions,
GradeOpt demonstrates superior performance in grading accuracy and behavior
alignment with human graders compared to representative baselines. Finally,
comprehensive ablation studies confirm the effectiveness of the individual
components designed in GradeOpt.

中文翻译:
开放式简答题（SAGs）在学习分析领域已被广泛视为深入理解学习者反馈的重要工具。然而实践中，这类题型常因评分工作量大和评估一致性难以保障而面临挑战。随着自然语言处理技术的进步，自动简答评分系统（ASAG）为这些问题提供了创新解决方案。但现有ASAG算法普遍存在泛化性不足、需针对特定题目定制等局限。本研究提出统一的多智能体评分框架GradeOpt，其核心在于运用大语言模型作为评分主体。该框架的创新性在于引入两个基于大语言模型的辅助智能体——反思器与优化器，通过错误自检机制实现原始评分标准的自主优化。在学科教学知识（PCK）与内容知识（CK）这类高难度评分任务上的实验表明，GradeOpt在评分准确率及与人类评分者行为一致性方面均优于代表性基线模型。最终的消融实验验证了框架中各组件的有效性。
