# Rethinking Semi-supervised Learning with Language Models

链接: http://arxiv.org/abs/2305.13002v1

原文摘要:
Semi-supervised learning (SSL) is a popular setting aiming to effectively
utilize unlabelled data to improve model performance in downstream natural
language processing (NLP) tasks. Currently, there are two popular approaches to
make use of unlabelled data: Self-training (ST) and Task-adaptive pre-training
(TAPT). ST uses a teacher model to assign pseudo-labels to the unlabelled data,
while TAPT continues pre-training on the unlabelled data before fine-tuning. To
the best of our knowledge, the effectiveness of TAPT in SSL tasks has not been
systematically studied, and no previous work has directly compared TAPT and ST
in terms of their ability to utilize the pool of unlabelled data. In this
paper, we provide an extensive empirical study comparing five state-of-the-art
ST approaches and TAPT across various NLP tasks and data sizes, including in-
and out-of-domain settings. Surprisingly, we find that TAPT is a strong and
more robust SSL learner, even when using just a few hundred unlabelled samples
or in the presence of domain shifts, compared to more sophisticated ST
approaches, and tends to bring greater improvements in SSL than in
fully-supervised settings. Our further analysis demonstrates the risks of using
ST approaches when the size of labelled or unlabelled data is small or when
domain shifts exist. We offer a fresh perspective for future SSL research,
suggesting the use of unsupervised pre-training objectives over dependency on
pseudo labels.

中文翻译:
半监督学习（SSL）是一种旨在有效利用未标注数据以提升下游自然语言处理（NLP）任务模型性能的流行范式。当前，利用未标注数据的主流方法有两种：自训练（Self-training, ST）和任务自适应预训练（Task-adaptive pre-training, TAPT）。ST通过教师模型为未标注数据分配伪标签，而TAPT则是在微调前对未标注数据进行持续预训练。据我们所知，TAPT在SSL任务中的有效性尚未得到系统研究，此前也未有工作直接比较TAPT与ST在利用未标注数据方面的能力。本文通过大量实证研究，对比了五种前沿ST方法与TAPT在不同NLP任务、数据规模（包括域内和跨域场景）下的表现。令人惊讶的是，我们发现相较于复杂的ST方法，TAPT展现出更强健的SSL学习能力——即使仅使用数百个未标注样本或存在领域偏移时依然稳定，且其在SSL中的性能提升往往优于全监督场景。进一步分析表明，当标注或未标注数据量较小或存在领域偏移时，ST方法存在显著风险。本研究为未来SSL研究提供了新视角，建议优先采用无监督预训练目标而非依赖伪标签策略。
