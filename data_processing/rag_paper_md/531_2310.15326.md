# Specialist or Generalist? Instruction Tuning for Specific NLP Tasks

链接: http://arxiv.org/abs/2310.15326v1

原文摘要:
The potential of large language models (LLMs) to simultaneously perform a
wide range of natural language processing (NLP) tasks has been the subject of
extensive research. Although instruction tuning has proven to be a
data-efficient method for transforming LLMs into such generalist models, their
performance still lags behind specialist models trained exclusively for
specific tasks. In this paper, we investigate whether incorporating
broad-coverage generalist instruction tuning can contribute to building a
specialist model. We hypothesize that its efficacy depends on task specificity
and skill requirements. Our experiments assess four target tasks with distinct
coverage levels, revealing that integrating generalist instruction tuning
consistently enhances model performance when the task coverage is broad. The
effect is particularly pronounced when the amount of task-specific training
data is limited. Further investigation into three target tasks focusing on
different capabilities demonstrates that generalist instruction tuning improves
understanding and reasoning abilities. However, for tasks requiring factual
knowledge, generalist data containing hallucinatory information may negatively
affect the model's performance. Overall, our work provides a systematic guide
for developing specialist models with general instruction tuning. Our code and
other related resources can be found at
https://github.com/DavidFanzz/Generalist_or_Specialist.

中文翻译:
大型语言模型（LLM）同时执行多种自然语言处理（NLP）任务的潜力已成为广泛研究的主题。尽管指令微调被证明是将LLM转化为此类通用模型的数据高效方法，但其性能仍落后于专为特定任务训练的专业模型。本文探讨了融入广泛覆盖的通用指令微调是否有助于构建专业模型，并提出其效果取决于任务特性和技能需求。通过针对四种不同覆盖范围目标任务的实验，我们发现当任务覆盖较广时，结合通用指令微调能持续提升模型性能，尤其在任务特定训练数据有限时效果更为显著。进一步针对三种不同能力目标任务的实验表明，通用指令微调可增强理解和推理能力，但对于需要事实知识的任务，包含幻觉信息的通用数据可能对模型性能产生负面影响。本研究为基于通用指令微调开发专业模型提供了系统性指导，相关代码和资源详见https://github.com/DavidFanzz/Generalist_or_Specialist。
