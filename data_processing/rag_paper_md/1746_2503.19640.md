# An Efficient Data Reuse with Tile-Based Adaptive Stationary for Transformer Accelerators

链接: http://arxiv.org/abs/2503.19640v1

原文摘要:
Transformer-based models have become the \textit{de facto} backbone across
many fields, such as computer vision and natural language processing. However,
as these models scale in size, external memory access (EMA) for weight and
activations becomes a critical bottleneck due to its significantly higher
energy consumption compared to internal computations. While most prior work has
focused on optimizing the self-attention mechanism, little attention has been
given to optimizing data transfer during linear projections, where EMA costs
are equally important. In this paper, we propose the Tile-based Adaptive
Stationary (TAS) scheme that selects the input or weight stationary in a tile
granularity, based on the input sequence length. Our experimental results
demonstrate that TAS can significantly reduce EMA by more than 97\% compared to
traditional stationary schemes, while being compatible with various attention
optimization techniques and hardware accelerators.

中文翻译:
基于Transformer的模型已成为计算机视觉和自然语言处理等诸多领域事实上的核心架构。然而，随着模型规模扩大，权重和激活值的外部存储器访问（EMA）因其远高于内部计算的能耗，逐渐成为关键性能瓶颈。现有研究多聚焦于优化自注意力机制，却忽视了线性投影阶段数据传输的优化——该环节的EMA开销同样不容忽视。本文提出基于分块的自适应驻留（TAS）策略，根据输入序列长度以分块粒度动态选择输入驻留或权重驻留方案。实验表明，相比传统驻留方案，TAS能显著降低97%以上的EMA开销，且兼容多种注意力优化技术与硬件加速器。
