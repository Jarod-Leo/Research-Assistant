# Large Language Models Meet NLP: A Survey

链接: http://arxiv.org/abs/2405.12819v1

原文摘要:
While large language models (LLMs) like ChatGPT have shown impressive
capabilities in Natural Language Processing (NLP) tasks, a systematic
investigation of their potential in this field remains largely unexplored. This
study aims to address this gap by exploring the following questions: (1) How
are LLMs currently applied to NLP tasks in the literature? (2) Have traditional
NLP tasks already been solved with LLMs? (3) What is the future of the LLMs for
NLP? To answer these questions, we take the first step to provide a
comprehensive overview of LLMs in NLP. Specifically, we first introduce a
unified taxonomy including (1) parameter-frozen application and (2)
parameter-tuning application to offer a unified perspective for understanding
the current progress of LLMs in NLP. Furthermore, we summarize the new
frontiers and the associated challenges, aiming to inspire further
groundbreaking advancements. We hope this work offers valuable insights into
the {potential and limitations} of LLMs in NLP, while also serving as a
practical guide for building effective LLMs in NLP.

中文翻译:
尽管如ChatGPT等大型语言模型（LLM）在自然语言处理（NLP）任务中展现出卓越能力，其在该领域的系统性潜力探究仍属空白。本研究通过三个核心问题填补这一空白：（1）当前文献中LLM如何应用于NLP任务？（2）传统NLP任务是否已被LLM解决？（3）LLM在NLP中的未来前景如何？为此，我们首次对NLP领域的LLM研究进行全面梳理，提出包含（1）参数冻结应用与（2）参数调优应用的统一分类框架，为理解LLM在NLP中的进展提供整合视角。此外，我们总结了新兴前沿领域及相应挑战，旨在激发突破性进展。本工作不仅揭示了LLM在NLP中的潜力与局限，更致力于为构建高效NLP专用LLM提供实践指南。
