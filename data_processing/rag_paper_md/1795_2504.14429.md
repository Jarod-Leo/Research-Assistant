# ResNetVLLM-2: Addressing ResNetVLLM's Multi-Modal Hallucinations

链接: http://arxiv.org/abs/2504.14429v1

原文摘要:
Large Language Models (LLMs) have transformed natural language processing
(NLP) tasks, but they suffer from hallucination, generating plausible yet
factually incorrect content. This issue extends to Video-Language Models
(VideoLLMs), where textual descriptions may inaccurately represent visual
content, resulting in multi-modal hallucinations. In this paper, we address
hallucination in ResNetVLLM, a video-language model combining ResNet visual
encoders with LLMs. We introduce a two-step protocol: (1) a faithfulness
detection strategy that uses a modified Lynx model to assess semantic alignment
between generated captions and ground-truth video references, and (2) a
hallucination mitigation strategy using Retrieval-Augmented Generation (RAG)
with an ad-hoc knowledge base dynamically constructed during inference. Our
enhanced model, ResNetVLLM-2, reduces multi-modal hallucinations by
cross-verifying generated content against external knowledge, improving factual
consistency. Evaluation on the ActivityNet-QA benchmark demonstrates a
substantial accuracy increase from 54.8% to 65.3%, highlighting the
effectiveness of our hallucination detection and mitigation strategies in
enhancing video-language model reliability.

中文翻译:
大型语言模型（LLMs）革新了自然语言处理（NLP）任务，但其存在幻觉问题——生成看似合理实则事实错误的内容。这一问题同样存在于视频语言模型（VideoLLMs）中，文本描述可能无法准确反映视觉内容，导致多模态幻觉。本文针对结合ResNet视觉编码器与LLMs的视频语言模型ResNetVLLM，提出两步解决方案：（1）采用改进版Lynx模型的忠实度检测策略，评估生成字幕与真实视频参考间的语义对齐；（2）通过检索增强生成（RAG）技术，在推理时动态构建专用知识库以实现幻觉缓解。优化后的ResNetVLLM-2模型通过外部知识交叉验证生成内容，减少多模态幻觉并提升事实一致性。在ActivityNet-QA基准测试中，模型准确率从54.8%显著提升至65.3%，验证了所提幻觉检测与缓解策略对增强视频语言模型可靠性的有效性。
