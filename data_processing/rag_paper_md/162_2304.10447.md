# Domain-specific Continued Pretraining of Language Models for Capturing Long Context in Mental Health

链接: http://arxiv.org/abs/2304.10447v1

原文摘要:
Pretrained language models have been used in various natural language
processing applications. In the mental health domain, domain-specific language
models are pretrained and released, which facilitates the early detection of
mental health conditions. Social posts, e.g., on Reddit, are usually long
documents. However, there are no domain-specific pretrained models for
long-sequence modeling in the mental health domain. This paper conducts
domain-specific continued pretraining to capture the long context for mental
health. Specifically, we train and release MentalXLNet and MentalLongformer
based on XLNet and Longformer. We evaluate the mental health classification
performance and the long-range ability of these two domain-specific pretrained
models. Our models are released in HuggingFace.

中文翻译:
预训练语言模型已广泛应用于各类自然语言处理任务中。在心理健康领域，研究者已预训练并发布了多个领域专用模型，为心理健康问题的早期识别提供了技术支持。然而，社交媒体平台（如Reddit）上的用户发帖通常属于长文本序列，当前心理健康领域尚缺乏针对长序列建模的专用预训练模型。本研究通过领域适应性持续预训练，构建了能够捕捉心理健康长文本上下文特征的模型。具体而言，我们在XLNet和Longformer基础架构上，训练并发布了MentalXLNet与MentalLongformer两个模型。通过系统评估，我们验证了这两个领域专用预训练模型在心理健康分类任务中的表现及其长文本处理能力。所有模型均已开源发布于HuggingFace平台。
