# Is LLM the Silver Bullet to Low-Resource Languages Machine Translation?

链接: http://arxiv.org/abs/2503.24102v1

原文摘要:
Low-Resource Languages (LRLs) present significant challenges in natural
language processing due to their limited linguistic resources and
underrepresentation in standard datasets. While recent advancements in Large
Language Models (LLMs) and Neural Machine Translation (NMT) have substantially
improved translation capabilities for high-resource languages, performance
disparities persist for LRLs, particularly impacting privacy-sensitive and
resource-constrained scenarios. This paper systematically evaluates the
limitations of current LLMs across 200 languages using benchmarks such as
FLORES-200. We also explore alternative data sources, including news articles
and bilingual dictionaries, and demonstrate how knowledge distillation from
large pre-trained models can significantly improve smaller LRL translations.
Additionally, we investigate various fine-tuning strategies, revealing that
incremental enhancements markedly reduce performance gaps on smaller LLMs.

中文翻译:
低资源语言（LRLs）由于语言资源匮乏且在标准数据集中代表性不足，给自然语言处理带来了重大挑战。尽管大语言模型（LLMs）和神经机器翻译（NMT）的最新进展显著提升了高资源语言的翻译能力，但LRLs的性能差距依然存在，这对隐私敏感和资源受限的场景影响尤为突出。本文通过FLORES-200等基准测试，系统评估了当前LLMs在200种语言中的局限性。我们还探索了新闻文本、双语词典等替代数据源，并证明从大型预训练模型中进行知识蒸馏可显著提升小型LRL翻译模型的性能。此外，通过研究不同微调策略，我们发现增量式优化能明显缩小小型LLMs的性能差距。
