# LLM with Relation Classifier for Document-Level Relation Extraction

链接: http://arxiv.org/abs/2408.13889v1

原文摘要:
Large language models (LLMs) have created a new paradigm for natural language
processing. Despite their advancement, LLM-based methods still lag behind
traditional approaches in document-level relation extraction (DocRE), a
critical task for understanding complex entity relations within long context.
This paper investigates the causes of this performance gap, identifying the
dispersion of attention by LLMs due to entity pairs without relations as a key
factor. We then introduce a novel classifier-LLM approach to DocRE.
Particularly, the proposed approach begins with a classifier designed to select
entity pair candidates that exhibit potential relations and then feed them to
LLM for final relation classification. This method ensures that the LLM's
attention is directed at relation-expressing entity pairs instead of those
without relations during inference. Experiments on DocRE benchmarks reveal that
our method significantly outperforms recent LLM-based DocRE models and narrows
the performance gap with state-of-the-art BERT-based models.

中文翻译:
大型语言模型（LLMs）为自然语言处理开创了全新范式。尽管技术不断进步，但在文档级关系抽取（DocRE）这一理解长文本中复杂实体关系的关键任务上，基于LLM的方法仍落后于传统方法。本文通过研究发现，LLMs因处理无关系实体对时产生的注意力分散是造成性能差距的主因。为此，我们提出了一种创新的分类器-LLM协同框架：首先由分类器筛选出具有潜在关系的实体对候选集，再交由LLM进行最终关系分类。该方法确保模型在推理过程中将注意力集中于表达关系的实体对，而非无关实体组合。在DocRE基准测试中，新方法显著优于近期基于LLM的文档关系抽取模型，并大幅缩小了与最先进BERT基模型之间的性能差距。
