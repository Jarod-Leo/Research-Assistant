# Zero-shot prompt-based classification: topic labeling in times of foundation models in German Tweets

链接: http://arxiv.org/abs/2406.18239v1

原文摘要:
Filtering and annotating textual data are routine tasks in many areas, like
social media or news analytics. Automating these tasks allows to scale the
analyses wrt. speed and breadth of content covered and decreases the manual
effort required. Due to technical advancements in Natural Language Processing,
specifically the success of large foundation models, a new tool for automating
such annotation processes by using a text-to-text interface given written
guidelines without providing training samples has become available.
  In this work, we assess these advancements in-the-wild by empirically testing
them in an annotation task on German Twitter data about social and political
European crises. We compare the prompt-based results with our human annotation
and preceding classification approaches, including Naive Bayes and a BERT-based
fine-tuning/domain adaptation pipeline. Our results show that the prompt-based
approach - despite being limited by local computation resources during the
model selection - is comparable with the fine-tuned BERT but without any
annotated training data. Our findings emphasize the ongoing paradigm shift in
the NLP landscape, i.e., the unification of downstream tasks and elimination of
the need for pre-labeled training data.

中文翻译:
在社交媒体或新闻分析等诸多领域，文本数据的筛选与标注是常规工作。实现这些任务的自动化能显著提升分析效率、扩大内容覆盖范围，并减少人工投入。随着自然语言处理技术的进步——尤其是大型基础模型取得的突破性进展，如今可通过文本到文本接口直接依据书面指导（无需提供训练样本）来自动完成标注流程。

本研究以欧洲社会政治危机相关的德语推特数据为对象，通过实证测试评估了这项技术在实际应用中的表现。我们将基于提示词的方法与人工标注结果及传统分类方法（包括朴素贝叶斯和基于BERT的微调/领域适应流程）进行对比。结果显示：尽管模型选择过程受限于本地计算资源，基于提示词的方法在无需任何标注训练数据的情况下，其表现仍可与微调后的BERT模型相媲美。这一发现印证了自然语言处理领域正在发生的范式转变——即下游任务的统一化与预标注训练数据需求的消除。
