# Event Stream GPT: A Data Pre-processing and Modeling Library for Generative, Pre-trained Transformers over Continuous-time Sequences of Complex Events

链接: http://arxiv.org/abs/2306.11547v2

原文摘要:
Generative, pre-trained transformers (GPTs, a.k.a. "Foundation Models") have
reshaped natural language processing (NLP) through their versatility in diverse
downstream tasks. However, their potential extends far beyond NLP. This paper
provides a software utility to help realize this potential, extending the
applicability of GPTs to continuous-time sequences of complex events with
internal dependencies, such as medical record datasets. Despite their
potential, the adoption of foundation models in these domains has been hampered
by the lack of suitable tools for model construction and evaluation. To bridge
this gap, we introduce Event Stream GPT (ESGPT), an open-source library
designed to streamline the end-to-end process for building GPTs for
continuous-time event sequences. ESGPT allows users to (1) build flexible,
foundation-model scale input datasets by specifying only a minimal
configuration file, (2) leverage a Hugging Face compatible modeling API for
GPTs over this modality that incorporates intra-event causal dependency
structures and autoregressive generation capabilities, and (3) evaluate models
via standardized processes that can assess few and even zero-shot performance
of pre-trained models on user-specified fine-tuning tasks.

中文翻译:
生成式预训练变换器（GPT，亦称“基础模型”）凭借其在多样化下游任务中的通用性，已重塑了自然语言处理（NLP）领域。然而，其潜力远不止于NLP。本文提供了一款软件工具，旨在帮助实现这一潜力，将GPT的适用范围扩展至具有内部依赖关系的复杂事件连续时间序列，如医疗记录数据集。尽管潜力巨大，基础模型在这些领域的应用却因缺乏合适的模型构建与评估工具而受阻。为弥合这一缺口，我们推出了事件流GPT（ESGPT），这是一个开源库，专为简化构建连续时间事件序列GPT的端到端流程而设计。ESGPT使用户能够：（1）仅通过指定一个极简配置文件即可构建灵活、基础模型规模的输入数据集；（2）利用兼容Hugging Face的建模API，针对该模态的GPT模型整合事件内因果依赖结构和自回归生成能力；（3）通过标准化流程评估模型，该流程可评估预训练模型在用户指定微调任务上的少量样本甚至零样本性能。
