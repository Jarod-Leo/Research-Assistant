# Inference Optimizations for Large Language Models: Effects, Challenges, and Practical Considerations

链接: http://arxiv.org/abs/2408.03130v1

原文摘要:
Large language models are ubiquitous in natural language processing because
they can adapt to new tasks without retraining. However, their sheer scale and
complexity present unique challenges and opportunities, prompting researchers
and practitioners to explore novel model training, optimization, and deployment
methods. This literature review focuses on various techniques for reducing
resource requirements and compressing large language models, including
quantization, pruning, knowledge distillation, and architectural optimizations.
The primary objective is to explore each method in-depth and highlight its
unique challenges and practical applications. The discussed methods are
categorized into a taxonomy that presents an overview of the optimization
landscape and helps navigate it to understand the research trajectory better.

中文翻译:
大型语言模型在自然语言处理领域无处不在，因其无需重新训练即可适应新任务。然而，其庞大的规模和复杂性带来了独特的挑战与机遇，促使研究者和实践者探索新颖的模型训练、优化及部署方法。本文献综述聚焦于降低资源需求与压缩大型语言模型的各种技术，包括量化、剪枝、知识蒸馏和架构优化。主要目标是深入探讨每种方法，并重点分析其独特挑战与实际应用。所述方法被归类为一个分类体系，该体系不仅呈现了优化领域的全景概览，更有助于研究者厘清脉络以更好地把握研究轨迹。

（翻译说明：采用学术文献的严谨表述风格，通过以下处理实现专业性与可读性平衡：
1. 术语统一："quantization/pruning/knowledge distillation"分别译为行业标准译法"量化/剪枝/知识蒸馏"
2. 句式重构：将英文长句拆解为符合中文表达习惯的短句，如将"prompting researchers..."独立成句并添加"促使"衔接词
3. 概念显化："taxonomy"译为"分类体系"而非直译"分类法"，更符合计算机领域语境
4. 动态对等："help navigate it"意译为"厘清脉络"而非字面翻译，保留指导性语义
5. 学术规范：使用"所述""聚焦于"等文献常用表述，保持文体一致性）
