# Will Affective Computing Emerge from Foundation Models and General AI? A First Evaluation on ChatGPT

链接: http://arxiv.org/abs/2303.03186v1

原文摘要:
ChatGPT has shown the potential of emerging general artificial intelligence
capabilities, as it has demonstrated competent performance across many natural
language processing tasks. In this work, we evaluate the capabilities of
ChatGPT to perform text classification on three affective computing problems,
namely, big-five personality prediction, sentiment analysis, and suicide
tendency detection. We utilise three baselines, a robust language model
(RoBERTa-base), a legacy word model with pretrained embeddings (Word2Vec), and
a simple bag-of-words baseline (BoW). Results show that the RoBERTa trained for
a specific downstream task generally has a superior performance. On the other
hand, ChatGPT provides decent results, and is relatively comparable to the
Word2Vec and BoW baselines. ChatGPT further shows robustness against noisy
data, where Word2Vec models achieve worse results due to noise. Results
indicate that ChatGPT is a good generalist model that is capable of achieving
good results across various problems without any specialised training, however,
it is not as good as a specialised model for a downstream task.

中文翻译:
ChatGPT展现了新兴通用人工智能技术的潜力，其在多项自然语言处理任务中均表现出色。本研究评估了ChatGPT在三个情感计算问题上的文本分类能力：大五人格预测、情感分析和自杀倾向检测。我们采用三个基线模型进行对比：鲁棒语言模型（RoBERTa-base）、预训练词嵌入的传统词模型（Word2Vec）以及简单的词袋基线模型（BoW）。结果表明，针对特定下游任务微调的RoBERTa模型普遍具有最优性能。相比之下，ChatGPT虽能取得不错的效果，其表现与Word2Vec和BoW基线模型大体相当。值得注意的是，ChatGPT在面对噪声数据时展现出更强的鲁棒性——在相同噪声条件下Word2Vec模型性能显著下降。研究证实ChatGPT作为通用型模型，无需专门训练即可在各类问题上取得良好效果，但其性能仍逊色于针对下游任务专门优化的模型。
