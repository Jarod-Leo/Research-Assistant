# GPT-DETOX: An In-Context Learning-Based Paraphraser for Text Detoxification

链接: http://arxiv.org/abs/2404.03052v1

原文摘要:
Harmful and offensive communication or content is detrimental to social
bonding and the mental state of users on social media platforms. Text
detoxification is a crucial task in natural language processing (NLP), where
the goal is removing profanity and toxicity from text while preserving its
content. Supervised and unsupervised learning are common approaches for
designing text detoxification solutions. However, these methods necessitate
fine-tuning, leading to computational overhead. In this paper, we propose
GPT-DETOX as a framework for prompt-based in-context learning for text
detoxification using GPT-3.5 Turbo. We utilize zero-shot and few-shot prompting
techniques for detoxifying input sentences. To generate few-shot prompts, we
propose two methods: word-matching example selection (WMES) and
context-matching example selection (CMES). We additionally take into account
ensemble in-context learning (EICL) where the ensemble is shaped by base
prompts from zero-shot and all few-shot settings. We use ParaDetox and APPDIA
as benchmark detoxification datasets. Our experimental results show that the
zero-shot solution achieves promising performance, while our best few-shot
setting outperforms the state-of-the-art models on ParaDetox and shows
comparable results on APPDIA. Our EICL solutions obtain the greatest
performance, adding at least 10% improvement, against both datasets.

中文翻译:
社交媒体平台上的有害及冒犯性交流内容会损害用户间的社会联结与心理健康。文本净化作为自然语言处理领域的关键任务，旨在消除文本中的污言秽语与毒性内容，同时保持其核心信息。当前主流解决方案通常采用监督学习或无监督学习方法，但这些方法需要进行精细调参，导致计算资源消耗较大。本研究提出GPT-DETOX框架，基于GPT-3.5 Turbo模型实现提示上下文学习的文本净化方案。我们采用零样本提示与少样本提示技术对输入语句进行净化处理，并提出两种少样本提示生成方法：词汇匹配示例选择法（WMES）和上下文匹配示例选择法（CMES）。此外还引入集成式上下文学习策略（EICL），通过整合零样本与各类少样本的基础提示构建集成模型。实验采用ParaDetox和APPDIA作为基准净化数据集，结果表明：零样本方案已展现出优异性能，而最优少样本设置在ParaDetox数据集上超越现有最优模型，在APPDIA数据集上取得可比结果。集成式上下文学习方案表现最为突出，在两个数据集上均实现至少10%的性能提升。
