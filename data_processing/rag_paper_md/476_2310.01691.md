# Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across Language Models

链接: http://arxiv.org/abs/2310.01691v1

原文摘要:
Prompt tuning in natural language processing (NLP) has become an increasingly
popular method for adapting large language models to specific tasks. However,
the transferability of these prompts, especially continuous prompts, between
different models remains a challenge. In this work, we propose a zero-shot
continuous prompt transfer method, where source prompts are encoded into
relative space and the corresponding target prompts are searched for
transferring to target models. Experimental results confirm the effectiveness
of our method, showing that 'task semantics' in continuous prompts can be
generalized across various language models. Moreover, we find that combining
'task semantics' from multiple source models can further enhance the
generalizability of transfer.

中文翻译:
自然语言处理（NLP）中的提示调优已成为将大型语言模型适配至特定任务的日益流行方法。然而，这些提示（尤其是连续型提示）在不同模型间的可迁移性仍面临挑战。本研究提出一种零样本连续提示迁移方法：通过将源提示编码至相对空间，并搜索对应目标提示以实现向目标模型的迁移。实验结果验证了该方法的有效性，表明连续提示中的"任务语义"可在不同语言模型间实现泛化。此外，我们发现融合多个源模型的"任务语义"能进一步提升迁移的泛化能力。
