# EdgeShard: Efficient LLM Inference via Collaborative Edge Computing

链接: http://arxiv.org/abs/2405.14371v1

原文摘要:
Large language models (LLMs) have shown great potential in natural language
processing and content generation. However, current LLMs heavily rely on cloud
computing, leading to prolonged latency, high bandwidth cost, and privacy
concerns. Edge computing is promising to address such concerns by deploying
LLMs on edge devices, closer to data sources. Some works try to leverage model
quantization to reduce the model size to fit the resource-constraint edge
devices, but they lead to accuracy loss. Other works use cloud-edge
collaboration, suffering from unstable network connections. In this work, we
leverage collaborative edge computing to facilitate the collaboration among
edge devices and cloud servers for jointly performing efficient LLM inference.
We propose a general framework to partition the LLM model into shards and
deploy on distributed devices. To achieve efficient LLM inference, we formulate
an adaptive joint device selection and model partition problem and design an
efficient dynamic programming algorithm to optimize the inference latency and
throughput, respectively. Experiments of Llama2 serial models on a
heterogeneous physical prototype demonstrate that EdgeShard achieves up to 50%
latency reduction and 2x throughput improvement over baseline methods.

中文翻译:
大型语言模型（LLMs）在自然语言处理与内容生成领域展现出巨大潜力。然而，当前LLMs高度依赖云计算，导致延迟增加、带宽成本高昂及隐私泄露风险。边缘计算通过将LLMs部署在更接近数据源的边缘设备上，有望解决这些问题。现有研究尝试通过模型量化压缩模型尺寸以适应资源受限的边缘设备，但会引发精度损失；另一些采用云边协同的方案则受限于网络连接不稳定性。本研究提出基于协作式边缘计算的创新框架，通过边缘设备与云服务器协同实现高效LLM推理。我们设计了一种通用架构，将LLM模型分割为分片并分布式部署。为优化推理效率，构建了自适应联合设备选择与模型分割问题模型，并开发高效动态规划算法分别优化推理延迟和吞吐量。在异构物理原型上进行的Llama2系列模型实验表明，相比基线方法，EdgeShard方案可实现最高50%的延迟降低与2倍的吞吐量提升。
