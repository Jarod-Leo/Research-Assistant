# On Speeding Up Language Model Evaluation

链接: http://arxiv.org/abs/2407.06172v1

原文摘要:
Developing prompt-based methods with Large Language Models (LLMs) requires
making numerous decisions, which give rise to a combinatorial search problem
over hyper-parameters. This exhaustive evaluation can be time-consuming and
costly. In this paper, we propose an $\textit{adaptive}$ approach to explore
this space. We are exploiting the fact that often only few samples are needed
to identify clearly superior or inferior settings, and that many evaluation
tests are highly correlated. We lean on multi-armed bandits to sequentially
identify the next (method, validation sample)-pair to evaluate and utilize
low-rank matrix factorization to fill in missing evaluations. We carefully
assess the efficacy of our approach on several competitive benchmark problems
and show that it can identify the top-performing method using only 5-15% of the
typical resources -- resulting in 85-95% LLM cost savings. Our code is
available at https://github.com/kilian-group/banditeval.

中文翻译:
开发基于提示的大型语言模型（LLM）方法需要做出大量决策，这引发了超参数组合搜索问题。这种穷举式评估既耗时又昂贵。本文提出一种$\textit{自适应}$探索策略，其核心在于：通常仅需少量样本即可识别明显优劣的设置，且众多评估测试具有高度相关性。我们采用多臂老虎机算法动态选择待评估的（方法，验证样本）组合，并利用低秩矩阵分解填补缺失评估值。通过在多个竞争性基准问题上的严谨验证，本方法仅需消耗常规资源的5-15%即可锁定最优方法，实现85-95%的LLM成本节约。代码已开源于https://github.com/kilian-group/banditeval。
