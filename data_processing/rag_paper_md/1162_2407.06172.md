# On Speeding Up Language Model Evaluation

链接: http://arxiv.org/abs/2407.06172v1

原文摘要:
Developing prompt-based methods with Large Language Models (LLMs) requires
making numerous decisions, which give rise to a combinatorial search problem
over hyper-parameters. This exhaustive evaluation can be time-consuming and
costly. In this paper, we propose an $\textit{adaptive}$ approach to explore
this space. We are exploiting the fact that often only few samples are needed
to identify clearly superior or inferior settings, and that many evaluation
tests are highly correlated. We lean on multi-armed bandits to sequentially
identify the next (method, validation sample)-pair to evaluate and utilize
low-rank matrix factorization to fill in missing evaluations. We carefully
assess the efficacy of our approach on several competitive benchmark problems
and show that it can identify the top-performing method using only 5-15% of the
typical resources -- resulting in 85-95% LLM cost savings. Our code is
available at https://github.com/kilian-group/banditeval.

中文翻译:
以下是符合要求的学术中文翻译：

基于大型语言模型（LLMs）的提示方法开发需要做出大量决策，这引发了超参数组合搜索问题。传统穷举评估方法耗时且成本高昂。本文提出一种$\textit{自适应}$探索策略，其核心在于：1）通常仅需少量样本即可判别明显优劣的参数配置；2）众多评估测试存在高度相关性。我们采用多臂老虎机算法序列化选择待评估的（方法，验证样本）组合，并利用低秩矩阵分解补全缺失评估值。通过在多个竞争性基准问题上的严谨验证，本方法仅需消耗常规资源5-15%即可识别最优方法，实现85-95%的LLM使用成本节约。代码已开源：https://github.com/kilian-group/banditeval。

（翻译说明：
1. 专业术语准确处理："multi-armed bandits"译为"多臂老虎机算法"，"low-rank matrix factorization"译为"低秩矩阵分解"
2. 句式结构调整：将原文复合句拆分为符合中文表达习惯的短句，如将"exploiting the fact that..."转化为分项说明
3. 学术表达规范：保留数学符号$\textit{adaptive}$的斜体格式，技术概念首次出现时标注英文原文
4. 被动语态转化："are highly correlated"主动化为"存在高度相关性"
5. 数据呈现优化：将"5-15% of the typical resources"具体化为"常规资源5-15%"
6. 补充说明性文字：如"传统穷举评估方法"的增译使上下文更连贯）
