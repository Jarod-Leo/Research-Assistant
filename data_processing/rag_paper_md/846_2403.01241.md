# IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact

链接: http://arxiv.org/abs/2403.01241v1

原文摘要:
Large language models (LLMs) excel in natural language processing but demand
intensive computation. To mitigate this, various quantization methods have been
explored, yet they compromise LLM performance. This paper unveils a previously
overlooked type of outliers in LLMs. Such outliers are found to allocate most
of the attention scores on initial tokens of input, termed as pivot tokens,
which are crucial to the performance of quantized LLMs. Given that, we propose
IntactKV to generate the KV cache of pivot tokens losslessly from the
full-precision model. The approach is simple and easy to combine with existing
quantization solutions with no extra inference overhead. Besides, IntactKV can
be calibrated as additional LLM parameters to boost the quantized LLMs further
with minimal training costs. Mathematical analysis also proves that IntactKV
effectively reduces the upper bound of quantization error. Empirical results
show that IntactKV brings consistent improvement over various quantization
methods across different LLMs and downstream tasks, leading to the new
state-of-the-art for LLM quantization. The codes are available at
https://github.com/ruikangliu/IntactKV.

中文翻译:
大语言模型（LLMs）在自然语言处理领域表现卓越，但其计算需求极高。为缓解这一问题，研究者探索了多种量化方法，但这些方法往往以牺牲模型性能为代价。本文揭示了一类此前被忽视的LLM异常值现象：这类异常值会将大部分注意力权重集中在输入文本的起始标记（称为“枢纽标记”）上，而这些标记对量化后LLM的性能至关重要。基于此发现，我们提出IntactKV方法，通过无损生成全精度模型中枢纽标记的键值缓存（KV cache）。该方法实现简洁，无需额外推理开销即可与现有量化方案结合使用。此外，IntactKV还可作为可调参数进行校准，以极低的训练成本进一步提升量化模型性能。数学分析证明该方法能有效降低量化误差上界。实验结果表明，在不同LLM架构和下游任务中，IntactKV对各类量化方法均能带来稳定提升，创造了LLM量化技术的新标杆。代码已开源于https://github.com/ruikangliu/IntactKV。
