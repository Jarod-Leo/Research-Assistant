# Sometimes Painful but Certainly Promising: Feasibility and Trade-offs of Language Model Inference at the Edge

链接: http://arxiv.org/abs/2503.09114v1

原文摘要:
The rapid rise of Language Models (LMs) has expanded the capabilities of
natural language processing, powering applications from text generation to
complex decision-making. While state-of-the-art LMs often boast hundreds of
billions of parameters and are primarily deployed in data centers, recent
trends show a growing focus on compact models-typically under 10 billion
parameters-enabled by techniques such as quantization and other model
compression techniques. This shift paves the way for LMs on edge devices,
offering potential benefits such as enhanced privacy, reduced latency, and
improved data sovereignty. However, the inherent complexity of even these
smaller models, combined with the limited computing resources of edge hardware,
raises critical questions about the practical trade-offs in executing LM
inference outside the cloud. To address these challenges, we present a
comprehensive evaluation of generative LM inference on representative CPU-based
and GPU-accelerated edge devices. Our study measures key performance
indicators-including memory usage, inference speed, and energy
consumption-across various device configurations. Additionally, we examine
throughput-energy trade-offs, cost considerations, and usability, alongside an
assessment of qualitative model performance. While quantization helps mitigate
memory overhead, it does not fully eliminate resource bottlenecks, especially
for larger models. Our findings quantify the memory and energy constraints that
must be considered for practical real-world deployments, offering concrete
insights into the trade-offs between model size, inference performance, and
efficiency. The exploration of LMs at the edge is still in its early stages. We
hope this study provides a foundation for future research, guiding the
refinement of models, the enhancement of inference efficiency, and the
advancement of edge-centric AI systems.

中文翻译:
语言模型（LMs）的迅速崛起拓展了自然语言处理的能力边界，其应用场景已从文本生成延伸至复杂决策。尽管最先进的LMs通常拥有数千亿参数且主要部署在数据中心，但近期趋势显示，通过量化和模型压缩等技术实现的紧凑模型（通常参数规模在百亿以下）正日益受到关注。这一转变为边缘设备部署LMs铺平了道路，有望带来隐私增强、延迟降低和数据主权保障等优势。然而，即便这些小型模型仍具有固有复杂性，加之边缘硬件有限的计算资源，使得在云端之外执行LM推理时面临关键的实际权衡问题。

为应对这些挑战，我们针对基于CPU和GPU加速的代表性边缘设备开展了生成式LM推理的全面评估。研究测量了不同设备配置下的关键性能指标，包括内存占用、推理速度和能耗等。此外，我们还考察了吞吐量与能耗的权衡关系、成本因素及可用性，并对模型定性表现进行了评估。研究发现，量化技术虽能缓解内存开销，但无法完全消除资源瓶颈——尤其对于较大模型而言。我们的量化分析揭示了实际部署中必须考虑的内存与能耗限制，为模型规模、推理性能与效率之间的权衡提供了具体见解。

边缘环境下的LMs探索仍处于起步阶段。本研究旨在为未来研究奠定基础，推动模型优化、推理效率提升以及边缘中心化AI系统的发展。通过明确资源约束与性能表现的关联性，我们期望为边缘计算场景下的语言模型部署提供实践指导。
