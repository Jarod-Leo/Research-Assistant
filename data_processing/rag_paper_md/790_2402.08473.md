# Intriguing Differences Between Zero-Shot and Systematic Evaluations of Vision-Language Transformer Models

链接: http://arxiv.org/abs/2402.08473v1

原文摘要:
Transformer-based models have dominated natural language processing and other
areas in the last few years due to their superior (zero-shot) performance on
benchmark datasets. However, these models are poorly understood due to their
complexity and size. While probing-based methods are widely used to understand
specific properties, the structures of the representation space are not
systematically characterized; consequently, it is unclear how such models
generalize and overgeneralize to new inputs beyond datasets. In this paper,
based on a new gradient descent optimization method, we are able to explore the
embedding space of a commonly used vision-language model. Using the Imagenette
dataset, we show that while the model achieves over 99\% zero-shot
classification performance, it fails systematic evaluations completely. Using a
linear approximation, we provide a framework to explain the striking
differences. We have also obtained similar results using a different model to
support that our results are applicable to other transformer models with
continuous inputs. We also propose a robust way to detect the modified images.

中文翻译:
基于Transformer的模型凭借其在基准数据集上卓越的（零样本）性能，在过去几年主导了自然语言处理及其他领域。然而，由于这类模型结构复杂且规模庞大，其工作机制仍未被充分理解。尽管基于探针的方法被广泛用于理解特定属性，但表征空间的结构尚未得到系统性刻画；因此，我们无法明确这类模型在数据集之外如何对新输入实现泛化与过度泛化。本文通过一种新型梯度下降优化方法，成功探索了常用视觉语言模型的嵌入空间。基于Imagenette数据集的实验表明：虽然该模型零样本分类准确率超过99%，但在系统性评估中完全失效。通过线性近似方法，我们提出了解释这种显著差异的理论框架。使用不同模型的复现实验进一步证实，该结论同样适用于其他具有连续输入的Transformer模型。此外，我们还提出了一种鲁棒的修改图像检测方法。

（翻译说明：
1. 专业术语处理："zero-shot"译为"零样本"，"benchmark datasets"译为"基准数据集"，"embedding space"译为"嵌入空间"等保持学术规范性
2. 长句拆分：将原文复合句按中文表达习惯分解为多个短句，如将"due to..."因果从句转为独立分句
3. 被动语态转化："are poorly understood"译为主动式"仍未被充分理解"
4. 概念显化："systematic evaluations"补充译为"系统性评估"以明确指代
5. 逻辑连接强化：添加"基于""通过"等连接词保持论证连贯性
6. 学术风格保持：使用"刻画""证实""鲁棒"等符合论文摘要的正式用语）
