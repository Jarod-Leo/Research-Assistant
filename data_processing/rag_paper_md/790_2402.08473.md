# Intriguing Differences Between Zero-Shot and Systematic Evaluations of Vision-Language Transformer Models

链接: http://arxiv.org/abs/2402.08473v1

原文摘要:
Transformer-based models have dominated natural language processing and other
areas in the last few years due to their superior (zero-shot) performance on
benchmark datasets. However, these models are poorly understood due to their
complexity and size. While probing-based methods are widely used to understand
specific properties, the structures of the representation space are not
systematically characterized; consequently, it is unclear how such models
generalize and overgeneralize to new inputs beyond datasets. In this paper,
based on a new gradient descent optimization method, we are able to explore the
embedding space of a commonly used vision-language model. Using the Imagenette
dataset, we show that while the model achieves over 99\% zero-shot
classification performance, it fails systematic evaluations completely. Using a
linear approximation, we provide a framework to explain the striking
differences. We have also obtained similar results using a different model to
support that our results are applicable to other transformer models with
continuous inputs. We also propose a robust way to detect the modified images.

中文翻译:
基于Transformer的模型凭借其在基准数据集上卓越的（零样本）性能表现，近年来主导了自然语言处理及其他领域的研究。然而，由于模型结构复杂且规模庞大，学界对其内在机制仍缺乏深入理解。尽管基于探针的方法被广泛用于解析特定属性，但表征空间的结构尚未得到系统性刻画；这导致我们难以理解此类模型在数据集之外对新输入的泛化及过度泛化行为。本文通过一种新型梯度下降优化方法，成功探索了常用视觉-语言模型的嵌入空间特征。以Imagenette数据集为例，研究发现虽然该模型零样本分类准确率超过99%，但在系统性评估中却完全失效。通过线性近似方法，我们构建了一个理论框架来解释这种显著差异。为验证结论的普适性，研究还采用另一模型复现了相似结果，表明该发现适用于其他具有连续输入的Transformer模型。此外，本文提出了一种鲁棒的图像篡改检测方法。
