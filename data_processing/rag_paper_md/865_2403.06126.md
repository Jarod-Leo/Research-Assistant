# In-context Prompt Learning for Test-time Vision Recognition with Frozen Vision-language Model

链接: http://arxiv.org/abs/2403.06126v1

原文摘要:
Current pre-trained vision-language models, such as CLIP, have demonstrated
remarkable zero-shot generalization capabilities across various downstream
tasks. However, their performance significantly degrades when test inputs
exhibit different distributions. In this paper, we explore the concept of
test-time prompt tuning (TTPT), which facilitates the adaptation of the CLIP
model to novel downstream tasks through a one-step unsupervised optimization
that involves only test samples. Inspired by in-context learning in natural
language processing (NLP), we propose In-Context Prompt Learning (InCPL) for
test-time visual recognition tasks, which empowers a pre-trained
vision-language model with labeled examples as context information on
downstream task. Specifically, InCPL associates a new test sample with very few
labeled examples (sometimes just one) as context information, enabling reliable
label estimation for the test sample and facilitating model adaptation. To
achieve this, InCPL employs an efficient language-to-vision translator to
explore the textual prior information for visual prompt learning. Further, we
introduce a context-aware unsupervised loss to optimize visual prompts tailored
to test samples. Finally, we design a cyclic learning strategy for visual and
textual prompts to ensure mutual synergy across different modalities. This
enables a pre-trained, frozen CLIP model to adapt to any task using its learned
adaptive prompt. Our method demonstrates superior performance and achieves
state-of-the-art results across various downstream datasets.

中文翻译:
当前，预训练的视觉-语言模型（如CLIP）已在多种下游任务中展现出卓越的零样本泛化能力。然而，当测试输入呈现不同分布时，其性能会显著下降。本文探索了测试时提示调优（TTPT）的概念，通过仅利用测试样本的一步无监督优化，使CLIP模型能够快速适应新的下游任务。受自然语言处理（NLP）中上下文学习的启发，我们提出面向测试时视觉识别任务的上下文提示学习框架（InCPL），该框架通过将标注样本作为下游任务的上下文信息，增强预训练视觉-语言模型的适应性。具体而言，InCPL将每个新测试样本与极少量标注样本（有时仅需一个）关联为上下文信息，从而实现对测试样本的可靠标签预测并促进模型适配。为实现这一目标，InCPL采用高效的文本-视觉转换器挖掘文本先验信息以指导视觉提示学习；进一步提出上下文感知的无监督损失函数，优化针对测试样本的视觉提示；最终设计跨模态的视觉-文本提示循环学习策略，确保不同模态间的协同优化。该方法使冻结参数的预训练CLIP模型能够通过自适应提示适配任意任务，在多个下游数据集上取得最优性能，达到当前最先进水平。
