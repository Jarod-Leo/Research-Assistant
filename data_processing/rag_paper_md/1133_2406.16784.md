# The Progression of Transformers from Language to Vision to MOT: A Literature Review on Multi-Object Tracking with Transformers

链接: http://arxiv.org/abs/2406.16784v1

原文摘要:
The transformer neural network architecture allows for autoregressive
sequence-to-sequence modeling through the use of attention layers. It was
originally created with the application of machine translation but has
revolutionized natural language processing. Recently, transformers have also
been applied across a wide variety of pattern recognition tasks, particularly
in computer vision. In this literature review, we describe major advances in
computer vision utilizing transformers. We then focus specifically on
Multi-Object Tracking (MOT) and discuss how transformers are increasingly
becoming competitive in state-of-the-art MOT works, yet still lag behind
traditional deep learning methods.

中文翻译:
Transformer神经网络架构通过注意力层的运用，实现了自回归的序列到序列建模。该架构最初为机器翻译应用而设计，却彻底革新了自然语言处理领域。近年来，Transformer模型更被广泛应用于各类模式识别任务，尤其在计算机视觉领域表现突出。本文献综述系统梳理了Transformer在计算机视觉中的重大进展，并聚焦多目标跟踪（MOT）任务展开讨论：尽管Transformer在当前最先进的MOT研究中正逐渐展现出竞争力，但其性能仍落后于传统深度学习方法。
