# QPruner: Probabilistic Decision Quantization for Structured Pruning in Large Language Models

链接: http://arxiv.org/abs/2412.11629v1

原文摘要:
The rise of large language models (LLMs) has significantly advanced various
natural language processing (NLP) tasks. However, the resource demands of these
models pose substantial challenges. Structured pruning is an effective approach
to reducing model size, but it often results in significant accuracy
degradation, necessitating parameter updates to adapt. Unfortunately, such
fine-tuning requires substantial memory, which limits its applicability. To
address these challenges, we introduce quantization into the structured pruning
framework to reduce memory consumption during both fine-tuning and inference.
However, the combined errors from pruning and quantization increase the
difficulty of fine-tuning, requiring a more refined quantization scheme. To
this end, we propose QPruner, a novel framework that employs structured pruning
to reduce model size, followed by a layer-wise mixed-precision quantization
scheme. Quantization precisions are assigned to each layer based on their
importance to the target task, and Bayesian optimization is employed to refine
precision allocation strategies, ensuring a balance between model accuracy and
memory efficiency. Extensive experiments on benchmark datasets demonstrate that
QPruner significantly outperforms existing methods in memory savings while
maintaining or improving model performance.

中文翻译:
大型语言模型（LLMs）的崛起显著推动了自然语言处理（NLP）各项任务的进展，但其资源需求也带来了巨大挑战。结构化剪枝虽能有效缩减模型规模，却常导致精度显著下降，需通过参数更新进行适配。然而，此类微调过程需要消耗大量内存，限制了其应用范围。为解决这些问题，我们在结构化剪枝框架中引入量化技术，以降低微调与推理阶段的内存占用。但剪枝与量化叠加的误差增加了微调难度，需要更精细的量化方案。为此，我们提出QPruner框架：先通过结构化剪枝压缩模型规模，再采用分层混合精度量化策略。该框架根据各层对目标任务的重要性分配量化精度，并利用贝叶斯优化优化精度分配方案，确保模型精度与内存效率的平衡。在基准数据集上的大量实验表明，QPruner在保持或提升模型性能的同时，内存节省效果显著优于现有方法。
