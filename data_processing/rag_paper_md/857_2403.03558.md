# Benchmarking Hallucination in Large Language Models based on Unanswerable Math Word Problem

链接: http://arxiv.org/abs/2403.03558v1

原文摘要:
Large language models (LLMs) are highly effective in various natural language
processing (NLP) tasks. However, they are susceptible to producing unreliable
conjectures in ambiguous contexts called hallucination. This paper presents a
new method for evaluating LLM hallucination in Question Answering (QA) based on
the unanswerable math word problem (MWP). To support this approach, we
innovatively develop a dataset called Unanswerable Math Word Problem (UMWP)
which comprises 5200 questions across five categories. We developed an
evaluation methodology combining text similarity and mathematical expression
detection to determine whether LLM considers the question unanswerable. The
results of extensive experiments conducted on 31 LLMs, including GPT-3,
InstructGPT, LLaMA, and Claude, demonstrate that in-context learning and
reinforcement learning with human feedback (RLHF) training significantly
enhance the model's ability to avoid hallucination. We show that utilizing MWP
is a reliable and effective approach to assess hallucination. Our code and data
are available at https://github.com/Yuki-Asuuna/UMWP.

中文翻译:
大型语言模型（LLMs）在各种自然语言处理（NLP）任务中表现卓越，但在模糊语境下容易产生不可靠的推测，这种现象被称为幻觉。本文提出了一种基于不可解数学应用题（MWP）的新方法，用于评估问答（QA）任务中LLM的幻觉问题。为支持该方法，我们创新性地构建了包含五类5200道题目的不可解数学应用题数据集（UMWP）。通过结合文本相似度与数学表达式检测的评估方法，可判定LLM是否识别问题的不可解性。在GPT-3、InstructGPT、LLaMA和Claude等31个LLM上的大量实验表明：上下文学习与基于人类反馈的强化学习（RLHF）训练能显著提升模型规避幻觉的能力。研究证实，利用数学应用题是评估幻觉现象可靠且有效的途径。相关代码与数据已开源：https://github.com/Yuki-Asuuna/UMWP。
