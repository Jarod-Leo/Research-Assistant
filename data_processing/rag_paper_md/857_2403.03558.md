# Benchmarking Hallucination in Large Language Models based on Unanswerable Math Word Problem

链接: http://arxiv.org/abs/2403.03558v1

原文摘要:
Large language models (LLMs) are highly effective in various natural language
processing (NLP) tasks. However, they are susceptible to producing unreliable
conjectures in ambiguous contexts called hallucination. This paper presents a
new method for evaluating LLM hallucination in Question Answering (QA) based on
the unanswerable math word problem (MWP). To support this approach, we
innovatively develop a dataset called Unanswerable Math Word Problem (UMWP)
which comprises 5200 questions across five categories. We developed an
evaluation methodology combining text similarity and mathematical expression
detection to determine whether LLM considers the question unanswerable. The
results of extensive experiments conducted on 31 LLMs, including GPT-3,
InstructGPT, LLaMA, and Claude, demonstrate that in-context learning and
reinforcement learning with human feedback (RLHF) training significantly
enhance the model's ability to avoid hallucination. We show that utilizing MWP
is a reliable and effective approach to assess hallucination. Our code and data
are available at https://github.com/Yuki-Asuuna/UMWP.

中文翻译:
以下是符合要求的学术化中文翻译：

大型语言模型（LLMs）在各类自然语言处理（NLP）任务中表现卓越，但在模糊语境下容易产生不可靠的推测，这种现象被称为"幻觉"。本文提出了一种基于不可解数学应用题（MWP）的新方法，用于评估问答系统（QA）中的LLM幻觉问题。为支持该方法，我们创新性地构建了"不可解数学应用题数据集"（UMWP），包含5个类别共计5200个问题。我们开发了一套结合文本相似度与数学表达式检测的评估方案，用以判定LLM是否能够识别问题的不可解性。通过对GPT-3、InstructGPT、LLaMA和Claude等31个LLM的广泛实验表明：上下文学习（in-context learning）和基于人类反馈的强化学习（RLHF）能显著提升模型规避幻觉的能力。本研究证实，利用数学应用题是评估语言模型幻觉现象的可靠且有效的方法。相关代码与数据集已开源：https://github.com/Yuki-Asuuna/UMWP。

（翻译严格遵循以下原则：
1. 专业术语准确统一："hallucination"译为学界通用译法"幻觉"，"in-context learning"等专业表述保留英文并标注中文
2. 被动语态转化："are susceptible to"等被动结构转换为中文主动表达
3. 长句拆分：将原文复合长句按中文习惯分解为多个短句
4. 逻辑显化：通过"为支持该方法"等连接词明确原文隐含逻辑关系
5. 数据规范：数字统一使用中文计数单位"共计5200个"
6. 学术风格：采用"表明""证实"等学术用语，保持客观严谨语气）
