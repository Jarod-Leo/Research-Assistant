# Boosting Healthcare LLMs Through Retrieved Context

链接: http://arxiv.org/abs/2409.15127v1

原文摘要:
This study leverages optimized context retrieval to enhance open-source Large
Language Models (LLMs) for cost-effective, high performance healthcare AI. We
demonstrate that this approach achieves state-of-the-art accuracy on medical
question answering at a fraction of the cost of proprietary models,
significantly improving the cost-accuracy Pareto frontier on the MedQA
benchmark. Key contributions include: (1) OpenMedQA, a novel benchmark
revealing a performance gap in open-ended medical QA compared to
multiple-choice formats; (2) a practical, reproducible pipeline for context
retrieval optimization; and (3) open-source resources (Prompt Engine,
CoT/ToT/Thinking databases) to empower healthcare AI development. By advancing
retrieval techniques and QA evaluation, we enable more affordable and reliable
LLM solutions for healthcare.

中文翻译:
本研究通过优化上下文检索机制，显著提升了开源大语言模型(LLM)在医疗人工智能领域的性价比表现。我们证实该方法能以专有模型极小成本实现医学问答任务的最先进准确率，在MedQA基准测试中显著改善了成本-准确率的帕累托前沿。核心创新包括：(1)OpenMedQA新基准测试，揭示了开放式医学问答与选择题形式间的性能差距；(2)一套可复现的上下文检索优化实践流程；(3)开源资源库(Prompt Engine、思维链/思维树数据库)以推动医疗AI发展。通过改进检索技术与问答评估体系，我们为医疗领域提供了更经济可靠的LLM解决方案。
