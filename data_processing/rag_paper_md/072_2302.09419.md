# A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT

链接: http://arxiv.org/abs/2302.09419v1

原文摘要:
Pretrained Foundation Models (PFMs) are regarded as the foundation for
various downstream tasks with different data modalities. A PFM (e.g., BERT,
ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable
parameter initialization for a wide range of downstream applications. BERT
learns bidirectional encoder representations from Transformers, which are
trained on large datasets as contextual language models. Similarly, the
generative pretrained transformer (GPT) method employs Transformers as the
feature extractor and is trained using an autoregressive paradigm on large
datasets. Recently, ChatGPT shows promising success on large language models,
which applies an autoregressive language model with zero shot or few shot
prompting. The remarkable achievements of PFM have brought significant
breakthroughs to various fields of AI. Numerous studies have proposed different
methods, raising the demand for an updated survey. This study provides a
comprehensive review of recent research advancements, challenges, and
opportunities for PFMs in text, image, graph, as well as other data modalities.
The review covers the basic components and existing pretraining methods used in
natural language processing, computer vision, and graph learning. Additionally,
it explores advanced PFMs used for different data modalities and unified PFMs
that consider data quality and quantity. The review also discusses research
related to the fundamentals of PFMs, such as model efficiency and compression,
security, and privacy. Finally, the study provides key implications, future
research directions, challenges, and open problems in the field of PFMs.
Overall, this survey aims to shed light on the research of the PFMs on
scalability, security, logical reasoning ability, cross-domain learning
ability, and the user-friendly interactive ability for artificial general
intelligence.

中文翻译:
预训练基础模型（PFMs）被视为处理多模态下游任务的基石。这类模型（如BERT、ChatGPT和GPT-4）通过大规模数据训练，为广泛的下游应用提供了优质的参数初始化方案。BERT基于Transformer架构学习双向编码表征，通过海量数据集训练成为上下文感知的语言模型；GPT系列同样采用Transformer作为特征提取器，但通过自回归范式进行预训练。近期，ChatGPT通过零样本/少样本提示的自回归语言模型架构，在大型语言模型领域取得突破性进展。PFMs的卓越成就推动了人工智能各领域的重大发展，大量创新方法不断涌现，亟需系统性综述研究。

本文全面梳理了PFMs在文本、图像、图结构及其他模态中的最新进展、挑战与机遇。内容涵盖自然语言处理、计算机视觉和图学习领域的基础组件与现有预训练方法，深入分析了跨模态专用PFMs与兼顾数据质量-数量的统一PFMs。研究还探讨了PFMs基础性问题，包括模型效率优化、压缩技术、安全与隐私保护。最后，本文提炼出关键启示，指明未来研究方向与开放性问题，重点展望了PFMs在可扩展性、安全性、逻辑推理能力、跨域学习能力及人机友好交互等方面的发展路径，为通用人工智能研究提供前瞻视角。

（注：根据学术规范要求，译文在保持专业性的同时：1. 将英文长句拆分为符合中文表达习惯的短句；2. 对"Transformer"等专业术语保留原名；3. 采用"基石""范式""提炼"等符合计算机领域论文风格的措辞；4. 通过"亟需""展望"等词汇体现学术文本的严谨性；5. 对"zero shot/few shot"等技术概念采用业界通用译法）
