# A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks

链接: http://arxiv.org/abs/2306.07303v1

原文摘要:
Transformer is a deep neural network that employs a self-attention mechanism
to comprehend the contextual relationships within sequential data. Unlike
conventional neural networks or updated versions of Recurrent Neural Networks
(RNNs) such as Long Short-Term Memory (LSTM), transformer models excel in
handling long dependencies between input sequence elements and enable parallel
processing. As a result, transformer-based models have attracted substantial
interest among researchers in the field of artificial intelligence. This can be
attributed to their immense potential and remarkable achievements, not only in
Natural Language Processing (NLP) tasks but also in a wide range of domains,
including computer vision, audio and speech processing, healthcare, and the
Internet of Things (IoT). Although several survey papers have been published
highlighting the transformer's contributions in specific fields, architectural
differences, or performance evaluations, there is still a significant absence
of a comprehensive survey paper encompassing its major applications across
various domains. Therefore, we undertook the task of filling this gap by
conducting an extensive survey of proposed transformer models from 2017 to
2022. Our survey encompasses the identification of the top five application
domains for transformer-based models, namely: NLP, Computer Vision,
Multi-Modality, Audio and Speech Processing, and Signal Processing. We analyze
the impact of highly influential transformer-based models in these domains and
subsequently classify them based on their respective tasks using a proposed
taxonomy. Our aim is to shed light on the existing potential and future
possibilities of transformers for enthusiastic researchers, thus contributing
to the broader understanding of this groundbreaking technology.

中文翻译:
Transformer是一种采用自注意力机制来理解序列数据中上下文关系的深度神经网络。不同于传统神经网络或循环神经网络（RNN）的改进版本（如长短期记忆网络LSTM），Transformer模型擅长处理输入序列元素间的长距离依赖，并支持并行计算。因此，基于Transformer的模型在人工智能领域引发了研究者们的广泛关注，这归功于其不仅在自然语言处理（NLP）任务中展现出巨大潜力和卓越成就，还广泛应用于计算机视觉、音频语音处理、医疗保健和物联网（IoT）等多个领域。尽管已有若干综述论文聚焦Transformer在特定领域的贡献、架构差异或性能评估，但目前仍缺乏全面涵盖其跨领域主要应用的系统性综述。为此，我们对2017至2022年间提出的Transformer模型进行了广泛调研，识别出五大核心应用领域：自然语言处理、计算机视觉、多模态技术、音频语音处理以及信号处理。通过分析这些领域中具有高度影响力的Transformer模型，我们依据提出的分类框架按任务类型对其进行系统归类。本研究旨在为热衷探索的研究者揭示Transformer的现有潜力与未来可能性，从而推动对这一突破性技术的更深入理解。
