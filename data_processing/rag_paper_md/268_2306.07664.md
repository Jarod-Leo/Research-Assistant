# Rethink the Effectiveness of Text Data Augmentation: An Empirical Analysis

链接: http://arxiv.org/abs/2306.07664v1

原文摘要:
In recent years, language models (LMs) have made remarkable progress in
advancing the field of natural language processing (NLP). However, the impact
of data augmentation (DA) techniques on the fine-tuning (FT) performance of
these LMs has been a topic of ongoing debate. In this study, we evaluate the
effectiveness of three different FT methods in conjugation with
back-translation across an array of 7 diverse NLP tasks, including
classification and regression types, covering single-sentence and sentence-pair
tasks. Contrary to prior assumptions that DA does not contribute to the
enhancement of LMs' FT performance, our findings reveal that continued
pre-training on augmented data can effectively improve the FT performance of
the downstream tasks. In the most favourable case, continued pre-training
improves the performance of FT by more than 10% in the few-shot learning
setting. Our finding highlights the potential of DA as a powerful tool for
bolstering LMs' performance.

中文翻译:
近年来，语言模型（LMs）在推动自然语言处理（NLP）领域发展方面取得了显著进展。然而，数据增强（DA）技术对这些语言模型微调（FT）性能的影响始终存在争议。本研究评估了三种不同微调方法结合回译技术在7项多样化NLP任务中的有效性，这些任务涵盖分类与回归类型，包括单句和句对任务。与先前认为DA无助于提升语言模型微调性能的假设相反，我们的研究发现：在增强数据上持续进行预训练能有效提升下游任务的微调性能。在最理想的情况下，持续预训练使小样本学习场景中的微调性能提升超过10%。这一发现揭示了数据增强作为提升语言模型性能的强大工具潜力。

（翻译说明：采用学术论文摘要的简洁风格，通过以下处理实现专业性与可读性平衡：
1. 专业术语保留英文缩写并首次出现时标注全称（如"LMs（语言模型）"）
2. 复杂句式拆分重组（如将原文复合长句分解为符合中文表达习惯的短句）
3. 关键概念准确对应（"back-translation"译为"回译"，"few-shot learning"译为"小样本学习"）
4. 被动语态转化（如"has been a topic"转为主动句式"始终存在争议"）
5. 数据呈现方式本地化（"more than 10%"译为"超过10%"而非"10%以上"以符合中文科技文献惯例））
