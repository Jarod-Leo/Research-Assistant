# AraSTEM: A Native Arabic Multiple Choice Question Benchmark for Evaluating LLMs Knowledge In STEM Subjects

链接: http://arxiv.org/abs/2501.00559v1

原文摘要:
Large Language Models (LLMs) have shown remarkable capabilities, not only in
generating human-like text, but also in acquiring knowledge. This highlights
the need to go beyond the typical Natural Language Processing downstream
benchmarks and asses the various aspects of LLMs including knowledge and
reasoning. Numerous benchmarks have been developed to evaluate LLMs knowledge,
but they predominantly focus on the English language. Given that many LLMs are
multilingual, relying solely on benchmarking English knowledge is insufficient.
To address this issue, we introduce AraSTEM, a new Arabic multiple-choice
question dataset aimed at evaluating LLMs knowledge in STEM subjects. The
dataset spans a range of topics at different levels which requires models to
demonstrate a deep understanding of scientific Arabic in order to achieve high
accuracy. Our findings show that publicly available models of varying sizes
struggle with this dataset, and underscores the need for more localized
language models. The dataset is freely accessible on Hugging Face.

中文翻译:
大型语言模型（LLMs）不仅展现出生成类人文本的卓越能力，还在知识获取方面表现突出。这凸显了超越传统自然语言处理下游基准测试、全面评估模型知识与推理等多维能力的必要性。尽管目前已涌现大量针对LLMs知识评估的基准数据集，但绝大多数仅聚焦于英语领域。鉴于许多LLMs具备多语言处理能力，仅依赖英语知识测评显然不够全面。为此，我们推出AraSTEM——一个全新的阿拉伯语多项选择题数据集，专门用于评估LLMs在STEM学科领域的知识掌握度。该数据集涵盖多层级主题，要求模型必须对阿拉伯语科学术语有深刻理解才能获得高准确率。实验结果表明，当前公开的不同规模模型在该数据集上均表现欠佳，这印证了开发更多本土化语言模型的迫切需求。本数据集已在Hugging Face平台开源共享。
