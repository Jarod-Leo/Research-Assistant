# CMed-GPT: Prompt Tuning for Entity-Aware Chinese Medical Dialogue Generation

链接: http://arxiv.org/abs/2311.14539v1

原文摘要:
Medical dialogue generation relies on natural language generation techniques
to enable online medical consultations. Recently, the widespread adoption of
large-scale models in the field of natural language processing has facilitated
rapid advancements in this technology. Existing medical dialogue models are
mostly based on BERT and pre-trained on English corpora, but there is a lack of
high-performing models on the task of Chinese medical dialogue generation. To
solve the above problem, this paper proposes CMed-GPT, which is the GPT
pre-training language model based on Chinese medical domain text. The model is
available in two versions, namely, base and large, with corresponding
perplexity values of 8.64 and 8.01. Additionally, we incorporate lexical and
entity embeddings into the dialogue text in a uniform manner to meet the
requirements of downstream dialogue generation tasks. By applying both
fine-tuning and p-tuning to CMed-GPT, we lowered the PPL from 8.44 to 7.35.
This study not only confirms the exceptional performance of the CMed-GPT model
in generating Chinese biomedical text but also highlights the advantages of
p-tuning over traditional fine-tuning with prefix prompts. Furthermore, we
validate the significance of incorporating external information in medical
dialogue generation, which enhances the quality of dialogue generation.

中文翻译:
医疗对话生成依赖自然语言生成技术实现线上医疗问诊。近年来自然语言处理领域大规模模型的广泛应用推动了该技术的快速发展。现有医疗对话模型多基于BERT架构并在英文语料上预训练，但中文医疗对话生成任务上缺乏高性能模型。为解决上述问题，本文提出基于中文医疗领域文本的GPT预训练语言模型CMed-GPT。该模型提供base和large两个版本，困惑度分别为8.64和8.01。我们创新性地将词级和实体级嵌入以统一方式融入对话文本，以满足下游对话生成任务需求。通过对CMed-GPT同时进行微调和p-tuning，模型困惑度从8.44降至7.35。本研究不仅验证了CMed-GPT模型在生成中文生物医学文本方面的卓越性能，还证明了带前缀提示的p-tuning相较于传统微调的优势。此外，我们验证了外部信息融入对医疗对话生成的重要意义，能有效提升对话生成质量。
