# Contrastive Learning for Task-Independent SpeechLLM-Pretraining

链接: http://arxiv.org/abs/2412.15712v1

原文摘要:
Large language models (LLMs) excel in natural language processing but
adapting these LLMs to speech processing tasks efficiently is not
straightforward. Direct task-specific fine-tuning is limited by overfitting
risks, data requirements, and computational costs. To address these challenges,
we propose a scalable, two-stage training approach: (1) A task-independent
speech pretraining stage using contrastive learning to align text and speech
representations over all layers, followed by (2) a task-specific fine-tuning
stage requiring minimal data. This approach outperforms traditional ASR
pretraining and enables the model to surpass models specialized on speech
translation and question answering while being trained on only 10% of the
task-specific data.

中文翻译:
大型语言模型（LLMs）在自然语言处理领域表现卓越，但如何高效地将其适配至语音处理任务并非易事。直接针对特定任务进行微调会面临过拟合风险、数据需求量大及计算成本高等限制。为解决这些挑战，我们提出了一种可扩展的两阶段训练方法：（1）采用对比学习进行与任务无关的语音预训练，对齐所有网络层的文本与语音表征；（2）仅需少量数据的任务特定微调阶段。该方法不仅超越了传统自动语音识别（ASR）预训练效果，还能在仅使用10%任务专用数据训练的情况下，使模型在语音翻译和问答任务上超越专用模型的性能。
