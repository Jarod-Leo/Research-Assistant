# Large Language Models are legal but they are not: Making the case for a powerful LegalLLM

链接: http://arxiv.org/abs/2311.08890v1

原文摘要:
Realizing the recent advances in Natural Language Processing (NLP) to the
legal sector poses challenging problems such as extremely long sequence
lengths, specialized vocabulary that is usually only understood by legal
professionals, and high amounts of data imbalance. The recent surge of Large
Language Models (LLMs) has begun to provide new opportunities to apply NLP in
the legal domain due to their ability to handle lengthy, complex sequences.
Moreover, the emergence of domain-specific LLMs has displayed extremely
promising results on various tasks. In this study, we aim to quantify how
general LLMs perform in comparison to legal-domain models (be it an LLM or
otherwise). Specifically, we compare the zero-shot performance of three
general-purpose LLMs (ChatGPT-20b, LLaMA-2-70b, and Falcon-180b) on the LEDGAR
subset of the LexGLUE benchmark for contract provision classification. Although
the LLMs were not explicitly trained on legal data, we observe that they are
still able to classify the theme correctly in most cases. However, we find that
their mic-F1/mac-F1 performance is up to 19.2/26.8\% lesser than smaller models
fine-tuned on the legal domain, thus underscoring the need for more powerful
legal-domain LLMs.

中文翻译:
将自然语言处理（NLP）领域的最新进展应用于法律行业，面临着诸多挑战性问题，例如极长的序列长度、通常仅由法律专业人士理解的专有词汇，以及高度不平衡的数据分布。近期大型语言模型（LLM）的兴起，因其处理冗长复杂序列的能力，为NLP在法律领域的应用开辟了新机遇。此外，领域专用LLM的出现已在多项任务中展现出极具前景的效果。本研究旨在量化通用LLM与法律领域模型（无论是LLM还是其他类型）的性能差异。具体而言，我们对比了三种通用LLM（ChatGPT-20b、LLaMA-2-70b和Falcon-180b）在LexGLUE基准测试LEDGAR子集（合同条款分类任务）上的零样本表现。尽管这些LLM未经过专门的法律数据训练，我们观察到它们在多数情况下仍能正确识别条款主题。然而研究发现，其微观F1/宏观F1分数比经过法律领域微调的小型模型低至19.2%/26.8%，这凸显了开发更强大法律领域专用LLM的必要性。
