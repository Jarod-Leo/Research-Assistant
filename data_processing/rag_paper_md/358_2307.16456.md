# Camoscio: an Italian Instruction-tuned LLaMA

链接: http://arxiv.org/abs/2307.16456v1

原文摘要:
In recent years Large Language Models (LLMs) have increased the state of the
art on several natural language processing tasks. However, their accessibility
is often limited to paid API services, posing challenges for researchers in
conducting extensive investigations. On the other hand, while some open-source
models have been proposed by the community, they are typically English-centric
or multilingual without a specific adaptation for the Italian language. In an
effort to democratize the available and open resources for the Italian
language, in this paper we introduce Camoscio: a language model specifically
tuned to follow users' prompts in Italian. Specifically, we finetuned the
smallest variant of LLaMA (7b) with LoRA on a corpus of instruction prompts
translated to Italian via ChatGPT. Results indicate that the model's zero-shot
performance on various downstream tasks in Italian competes favorably with
existing models specifically finetuned for those tasks. All the artifacts
(code, dataset, model) are released to the community at the following url:
https://github.com/teelinsan/camoscio

中文翻译:
近年来，大型语言模型（LLMs）在多项自然语言处理任务中提升了技术前沿水平。然而，这些模型通常仅限于付费API服务，这为研究人员开展深入探索带来了挑战。另一方面，尽管社区已提出部分开源模型，但它们大多以英语为核心或为多语言设计，并未针对意大利语进行专门优化。为推动意大利语资源的民主化与开放共享，本文推出Camoscio——一款专为遵循意大利语用户指令而优化的语言模型。具体而言，我们采用LoRA技术对LLaMA最小版本（7b参数）进行微调，训练数据为通过ChatGPT翻译成意大利语的指令提示语料库。实验结果表明，该模型在意大利语各类下游任务中的零样本性能，与专为这些任务微调的现有模型相比具有显著竞争力。所有资源（代码、数据集、模型）已通过以下链接向社区开源：https://github.com/teelinsan/camoscio
