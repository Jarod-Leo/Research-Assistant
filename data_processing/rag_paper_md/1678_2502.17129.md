# Thus Spake Long-Context Large Language Model

链接: http://arxiv.org/abs/2502.17129v1

原文摘要:
Long context is an important topic in Natural Language Processing (NLP),
running through the development of NLP architectures, and offers immense
opportunities for Large Language Models (LLMs) giving LLMs the lifelong
learning potential akin to humans. Unfortunately, the pursuit of a long context
is accompanied by numerous obstacles. Nevertheless, long context remains a core
competitive advantage for LLMs. In the past two years, the context length of
LLMs has achieved a breakthrough extension to millions of tokens. Moreover, the
research on long-context LLMs has expanded from length extrapolation to a
comprehensive focus on architecture, infrastructure, training, and evaluation
technologies.
  Inspired by the symphonic poem, Thus Spake Zarathustra, we draw an analogy
between the journey of extending the context of LLM and the attempts of humans
to transcend its mortality. In this survey, We will illustrate how LLM
struggles between the tremendous need for a longer context and its equal need
to accept the fact that it is ultimately finite. To achieve this, we give a
global picture of the lifecycle of long-context LLMs from four perspectives:
architecture, infrastructure, training, and evaluation, showcasing the full
spectrum of long-context technologies. At the end of this survey, we will
present 10 unanswered questions currently faced by long-context LLMs. We hope
this survey can serve as a systematic introduction to the research on
long-context LLMs.

中文翻译:
长上下文是自然语言处理（NLP）领域的重要课题，贯穿了NLP架构的发展历程，并为大语言模型（LLM）提供了类似人类终身学习的巨大潜力。然而，追求长上下文的过程中伴随着诸多障碍。尽管如此，长上下文仍是LLM的核心竞争优势。过去两年间，LLM的上下文长度实现了突破性延伸，达到百万级token规模。同时，长上下文LLM的研究已从长度外推拓展至对架构、基础设施、训练与评估技术的全面关注。

受交响诗《查拉图斯特拉如是说》启发，我们将LLM扩展上下文的探索历程类比于人类超越生命极限的尝试。本综述将阐释LLM如何在"对更长上下文的迫切需求"与"接受其终究有限的事实"之间艰难平衡。为此，我们从架构、基础设施、训练和评估四个维度全景式展现长上下文LLM的技术生命周期，系统梳理其完整技术图谱。在综述结尾，我们将提出当前长上下文LLM面临的十大未解难题。希望本工作能为长上下文LLM研究提供体系化的导引。
