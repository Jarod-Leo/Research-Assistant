# Investigating the translation capabilities of Large Language Models trained on parallel data only

链接: http://arxiv.org/abs/2406.09140v1

原文摘要:
In recent years, Large Language Models (LLMs) have demonstrated exceptional
proficiency across a broad spectrum of Natural Language Processing (NLP) tasks,
including Machine Translation. However, previous methods predominantly relied
on iterative processes such as instruction fine-tuning or continual
pre-training, leaving unexplored the challenges of training LLMs solely on
parallel data. In this work, we introduce PLUME (Parallel Language Model), a
collection of three 2B LLMs featuring varying vocabulary sizes (32k, 128k, and
256k) trained exclusively on Catalan-centric parallel examples. These models
perform comparably to previous encoder-decoder architectures on 16 supervised
translation directions and 56 zero-shot ones. Utilizing this set of models, we
conduct a thorough investigation into the translation capabilities of LLMs,
probing their performance, the impact of the different elements of the prompt,
and their cross-lingual representation space.

中文翻译:
近年来，大型语言模型（LLMs）在包括机器翻译在内的广泛自然语言处理（NLP）任务中展现出卓越性能。然而，现有方法主要依赖指令微调或持续预训练等迭代过程，尚未充分探索仅基于平行数据训练LLMs的挑战。本研究提出PLUME（并行语言模型）——包含三个20亿参数量的LLM系列，其词汇量分别为32k、128k和256k，这些模型完全基于以加泰罗尼亚语为中心的平行语料训练。实验表明，该系列模型在16个有监督翻译方向和56个零样本翻译方向上的表现与先前编码器-解码器架构相当。通过这组模型，我们深入探究了LLMs的翻译能力，包括其性能表现、提示模板各要素的影响机制以及跨语言表征空间特性。
