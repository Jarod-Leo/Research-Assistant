# Investigating the translation capabilities of Large Language Models trained on parallel data only

链接: http://arxiv.org/abs/2406.09140v1

原文摘要:
In recent years, Large Language Models (LLMs) have demonstrated exceptional
proficiency across a broad spectrum of Natural Language Processing (NLP) tasks,
including Machine Translation. However, previous methods predominantly relied
on iterative processes such as instruction fine-tuning or continual
pre-training, leaving unexplored the challenges of training LLMs solely on
parallel data. In this work, we introduce PLUME (Parallel Language Model), a
collection of three 2B LLMs featuring varying vocabulary sizes (32k, 128k, and
256k) trained exclusively on Catalan-centric parallel examples. These models
perform comparably to previous encoder-decoder architectures on 16 supervised
translation directions and 56 zero-shot ones. Utilizing this set of models, we
conduct a thorough investigation into the translation capabilities of LLMs,
probing their performance, the impact of the different elements of the prompt,
and their cross-lingual representation space.

中文翻译:
近年来，大型语言模型（LLMs）在包括机器翻译在内的广泛自然语言处理（NLP）任务中展现出卓越能力。然而，先前方法主要依赖指令微调或持续预训练等迭代过程，尚未探索仅基于平行数据训练LLMs的挑战。本研究提出PLUME（平行语言模型）——包含三个20亿参数规模的LLM，其词汇量分别为32k、128k和256k，这些模型完全通过以加泰罗尼亚语为中心的平行语料训练而成。在16个有监督翻译方向和56个零样本方向上，这些模型性能与先前编码器-解码器架构相当。借助该模型组，我们深入探究了LLMs的翻译能力，包括其性能表现、提示要素的影响及其跨语言表征空间特性。
