# CritiqueLLM: Scaling LLM-as-Critic for Effective and Explainable Evaluation of Large Language Model Generation

链接: http://arxiv.org/abs/2311.18702v1

原文摘要:
Since the natural language processing (NLP) community started to make large
language models (LLMs) act as a critic to evaluate the quality of generated
texts, most of the existing works train a critique generation model on the
evaluation data labeled by GPT-4's direct prompting. We observe that these
models lack the ability to generate informative critiques in both pointwise
grading and pairwise comparison especially without references. As a result,
their generated critiques cannot provide fine-grained distinguishability on
generated texts, causing unsatisfactory evaluation performance. In this paper,
we propose a simple yet effective method called Eval-Instruct, which can first
acquire pointwise grading critiques with pseudo references and then revise
these critiques via multi-path prompting to obtain informative evaluation data
in different tasks and settings, including pointwise grading and pairwise
comparison with / without references. After fine-tuning on these data, the
resulting model CritiqueLLM is empirically shown to outperform ChatGPT and all
the open-source baselines and even achieve comparable evaluation performance to
GPT-4 in system-level correlations of pointwise grading. We also demonstrate
that our generated critiques can act as scalable feedback to further improve
the generation quality of strong LLMs like ChatGPT.

中文翻译:
自自然语言处理（NLP）领域开始利用大语言模型（LLMs）作为评估生成文本质量的评判者以来，现有研究大多基于GPT-4直接提示标注的评估数据训练评论生成模型。我们发现这些模型在无参考文本时，无论是逐点评分还是成对比较中均难以生成信息量充分的评论，导致其生成的评论无法对文本质量提供细粒度区分，评估效果欠佳。本文提出一种简单有效的方法Eval-Instruct：首先生成带有伪参考文本的逐点评分评论，再通过多路径提示修订这些评论，从而获取不同任务与场景（含参考/无参考的逐点评分及成对比较）下的高信息量评估数据。基于这些数据微调得到的CritiqueLLM模型，实证表明其评估表现超越ChatGPT与所有开源基线模型，在系统级逐点评分相关性上甚至达到与GPT-4相当的水平。我们还证明所生成的评论可作为可扩展反馈，进一步提升ChatGPT等强语言模型的生成质量。
