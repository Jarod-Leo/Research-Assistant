# Breaking Memory Limits: Gradient Wavelet Transform Enhances LLMs Training

链接: http://arxiv.org/abs/2501.07237v1

原文摘要:
Large language models (LLMs) have shown impressive performance across a range
of natural language processing tasks. However, their vast number of parameters
introduces significant memory challenges during training, particularly when
using memory-intensive optimizers like Adam. Existing memory-efficient
algorithms often rely on techniques such as singular value decomposition
projection or weight freezing. While these approaches help alleviate memory
constraints, they generally produce suboptimal results compared to full-rank
updates. In this paper, we investigate the memory-efficient method beyond
low-rank training, proposing a novel solution called Gradient Wavelet Transform
(GWT), which applies wavelet transforms to gradients in order to significantly
reduce the memory requirements for maintaining optimizer states. We demonstrate
that GWT can be seamlessly integrated with memory-intensive optimizers,
enabling efficient training without sacrificing performance. Through extensive
experiments on both pre-training and fine-tuning tasks, we show that GWT
achieves state-of-the-art performance compared with advanced memory-efficient
optimizers and full-rank approaches in terms of both memory usage and training
performance.

中文翻译:
大型语言模型（LLMs）在各类自然语言处理任务中展现出卓越性能，但其庞大的参数量在训练过程中——尤其是使用Adam等内存密集型优化器时——会带来显著的内存挑战。现有内存高效算法通常依赖于奇异值分解投影或权重冻结等技术。虽然这些方法有助于缓解内存限制，但与全秩更新相比，其效果往往欠佳。本文探索了一种超越低秩训练的内存高效方法，提出名为梯度小波变换（GWT）的创新方案：通过对梯度施加小波变换，大幅降低维护优化器状态所需的内存开销。我们证明GWT可与内存密集型优化器无缝集成，在保证性能的同时实现高效训练。通过在预训练和微调任务上的大量实验表明，相较于先进的内存高效优化器和全秩方法，GWT在内存占用与训练性能方面均达到当前最优水平。
