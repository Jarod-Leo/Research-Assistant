# Effective Structured Prompting by Meta-Learning and Representative Verbalizer

链接: http://arxiv.org/abs/2306.00618v1

原文摘要:
Prompt tuning for pre-trained masked language models (MLM) has shown
promising performance in natural language processing tasks with few labeled
examples. It tunes a prompt for the downstream task, and a verbalizer is used
to bridge the predicted token and label prediction. Due to the limited training
data, prompt initialization is crucial for prompt tuning. Recently,
MetaPrompting (Hou et al., 2022) uses meta-learning to learn a shared
initialization for all task-specific prompts. However, a single initialization
is insufficient to obtain good prompts for all tasks and samples when the tasks
are complex. Moreover, MetaPrompting requires tuning the whole MLM, causing a
heavy burden on computation and memory as the MLM is usually large. To address
these issues, we use a prompt pool to extract more task knowledge and construct
instance-dependent prompts via attention. We further propose a novel soft
verbalizer (RepVerb) which constructs label embedding from feature embeddings
directly. Combining meta-learning the prompt pool and RepVerb, we propose
MetaPrompter for effective structured prompting. MetaPrompter is
parameter-efficient as only the pool is required to be tuned. Experimental
results demonstrate that MetaPrompter performs better than the recent
state-of-the-arts and RepVerb outperforms existing soft verbalizers.

中文翻译:
针对预训练掩码语言模型（MLM）的提示调优在少标注样本的自然语言处理任务中展现出优异性能。该方法通过为下游任务定制提示模板，并借助解释器将预测标记与标签预测关联。由于训练数据有限，提示初始化成为关键因素。近期MetaPrompting（Hou等人，2022）采用元学习为所有任务特定提示获取共享初始化参数。然而当任务复杂度较高时，单一初始化难以保证所有任务和样本都能获得优质提示。此外，该方法需对整个MLM进行调优，对于参数量庞大的模型会带来沉重的计算和内存负担。

为解决这些问题，我们引入提示池机制以提取更丰富的任务知识，并通过注意力机制构建样本自适应的动态提示。进一步提出新型软解释器RepVerb，直接从特征嵌入空间构建标签嵌入表示。通过结合元学习优化的提示池与RepVerb，我们提出结构化提示框架MetaPrompter。该框架仅需调优提示池参数，具有显著参数效率优势。实验结果表明MetaPrompter性能超越当前最先进方法，且RepVerb优于现有软解释器方案。
