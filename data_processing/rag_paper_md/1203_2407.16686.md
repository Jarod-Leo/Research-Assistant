# Can Large Language Models Automatically Jailbreak GPT-4V?

链接: http://arxiv.org/abs/2407.16686v1

原文摘要:
GPT-4V has attracted considerable attention due to its extraordinary capacity
for integrating and processing multimodal information. At the same time, its
ability of face recognition raises new safety concerns of privacy leakage.
Despite researchers' efforts in safety alignment through RLHF or preprocessing
filters, vulnerabilities might still be exploited. In our study, we introduce
AutoJailbreak, an innovative automatic jailbreak technique inspired by prompt
optimization. We leverage Large Language Models (LLMs) for red-teaming to
refine the jailbreak prompt and employ weak-to-strong in-context learning
prompts to boost efficiency. Furthermore, we present an effective search method
that incorporates early stopping to minimize optimization time and token
expenditure. Our experiments demonstrate that AutoJailbreak significantly
surpasses conventional methods, achieving an Attack Success Rate (ASR)
exceeding 95.3\%. This research sheds light on strengthening GPT-4V security,
underscoring the potential for LLMs to be exploited in compromising GPT-4V
integrity.

中文翻译:
GPT-4V因其卓越的多模态信息整合与处理能力备受瞩目，但其人脸识别功能也引发了隐私泄露的新安全隐患。尽管研究者已通过RLHF或预处理过滤器进行安全对齐，系统漏洞仍可能被恶意利用。本研究提出创新性自动越狱技术AutoJailbreak，其灵感源自提示词优化方法：我们利用大语言模型（LLMs）进行红队测试以精炼越狱提示词，并采用弱监督到强监督的上下文学习提示策略提升效率。此外，我们开发了一种融合早停机制的高效搜索方法，显著减少优化时间与token消耗。实验表明，AutoJailbreak以超过95.3%的攻击成功率（ASR）大幅超越传统方法。这项研究不仅为强化GPT-4V安全性提供了新思路，更揭示了大语言模型可能被用于破坏GPT-4V完整性的潜在风险。
