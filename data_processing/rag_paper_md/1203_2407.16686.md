# Can Large Language Models Automatically Jailbreak GPT-4V?

链接: http://arxiv.org/abs/2407.16686v1

原文摘要:
GPT-4V has attracted considerable attention due to its extraordinary capacity
for integrating and processing multimodal information. At the same time, its
ability of face recognition raises new safety concerns of privacy leakage.
Despite researchers' efforts in safety alignment through RLHF or preprocessing
filters, vulnerabilities might still be exploited. In our study, we introduce
AutoJailbreak, an innovative automatic jailbreak technique inspired by prompt
optimization. We leverage Large Language Models (LLMs) for red-teaming to
refine the jailbreak prompt and employ weak-to-strong in-context learning
prompts to boost efficiency. Furthermore, we present an effective search method
that incorporates early stopping to minimize optimization time and token
expenditure. Our experiments demonstrate that AutoJailbreak significantly
surpasses conventional methods, achieving an Attack Success Rate (ASR)
exceeding 95.3\%. This research sheds light on strengthening GPT-4V security,
underscoring the potential for LLMs to be exploited in compromising GPT-4V
integrity.

中文翻译:
GPT-4V因其卓越的多模态信息整合与处理能力而备受瞩目。与此同时，其人脸识别功能也引发了隐私泄露的新安全隐患。尽管研究者已通过强化学习人类反馈（RLHF）或预处理过滤器进行安全对齐，系统漏洞仍可能被恶意利用。本研究提出AutoJailbreak——一种受提示词优化启发的创新型自动越狱技术：我们利用大语言模型（LLMs）进行红队测试以优化越狱提示词，并采用弱监督到强监督的上下文学习提示策略提升效率。此外，我们开发了一种融合早停机制的高效搜索方法，显著减少优化时间与token消耗。实验表明，AutoJailbreak以超过95.3%的攻击成功率（ASR）大幅超越传统方法。该研究不仅为强化GPT-4V安全性提供新见解，更揭示了大语言模型在破坏GPT-4V完整性方面被恶意利用的潜在风险。  

（翻译说明：  
1. 专业术语处理："red-teaming"译为"红队测试"符合网络安全领域术语规范  
2. 技术概念转化："weak-to-strong in-context learning prompts"意译为"弱监督到强监督的上下文学习提示策略"以保持技术准确性  
3. 句式重构：将原文三个长句拆分为符合中文表达习惯的短句结构  
4. 被动语态转换："vulnerabilities might still be exploited"主动化为"系统漏洞仍可能被恶意利用"  
5. 数据呈现：保留精确数值"95.3%"并采用中文百分号规范格式  
6. 学术风格保持：使用"本研究""揭示"等符合学术论文摘要的规范表述）
