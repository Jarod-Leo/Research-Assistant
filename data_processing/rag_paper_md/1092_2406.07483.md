# Advancing Annotation of Stance in Social Media Posts: A Comparative Analysis of Large Language Models and Crowd Sourcing

链接: http://arxiv.org/abs/2406.07483v1

原文摘要:
In the rapidly evolving landscape of Natural Language Processing (NLP), the
use of Large Language Models (LLMs) for automated text annotation in social
media posts has garnered significant interest. Despite the impressive
innovations in developing LLMs like ChatGPT, their efficacy, and accuracy as
annotation tools are not well understood. In this paper, we analyze the
performance of eight open-source and proprietary LLMs for annotating the stance
expressed in social media posts, benchmarking their performance against human
annotators' (i.e., crowd-sourced) judgments. Additionally, we investigate the
conditions under which LLMs are likely to disagree with human judgment. A
significant finding of our study is that the explicitness of text expressing a
stance plays a critical role in how faithfully LLMs' stance judgments match
humans'. We argue that LLMs perform well when human annotators do, and when
LLMs fail, it often corresponds to situations in which human annotators
struggle to reach an agreement. We conclude with recommendations for a
comprehensive approach that combines the precision of human expertise with the
scalability of LLM predictions. This study highlights the importance of
improving the accuracy and comprehensiveness of automated stance detection,
aiming to advance these technologies for more efficient and unbiased analysis
of social media.

中文翻译:
在自然语言处理（NLP）领域快速发展的背景下，利用大语言模型（LLMs）对社交媒体帖子进行自动化文本标注已引发广泛关注。尽管ChatGPT等大语言模型的开发取得了显著创新，但其作为标注工具的有效性和准确性尚未得到充分认知。本文通过对比八种开源与商用大语言模型在社交媒体立场标注任务中的表现，以众包人工标注结果为基准展开分析，并探究大语言模型与人类判断产生分歧的情境。研究发现：文本立场表达的显性程度对大语言模型与人类标注结果的一致性具有决定性影响。我们论证了大语言模型在人类标注者能达成共识的场景中表现良好，而其失效案例往往对应人类标注者难以形成一致意见的情况。最后，我们提出应构建融合人类专业判断精确性与大语言模型可扩展性的综合方案。本研究对提升自动化立场检测的准确性与全面性具有重要意义，旨在推动相关技术发展以实现更高效、无偏见的社交媒体分析。
