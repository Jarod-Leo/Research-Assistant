# P-MMEval: A Parallel Multilingual Multitask Benchmark for Consistent Evaluation of LLMs

链接: http://arxiv.org/abs/2411.09116v1

原文摘要:
Recent advancements in large language models (LLMs) showcase varied
multilingual capabilities across tasks like translation, code generation, and
reasoning. Previous assessments often limited their scope to fundamental
natural language processing (NLP) or isolated capability-specific tasks. To
alleviate this drawback, we aim to present a comprehensive multilingual
multitask benchmark. First, we present a pipeline for selecting available and
reasonable benchmarks from massive ones, addressing the oversight in previous
work regarding the utility of these benchmarks, i.e., their ability to
differentiate between models being evaluated. Leveraging this pipeline, we
introduce P-MMEval, a large-scale benchmark covering effective fundamental and
capability-specialized datasets. Furthermore, P-MMEval delivers consistent
language coverage across various datasets and provides parallel samples.
Finally, we conduct extensive experiments on representative multilingual model
series to compare performances across models, analyze dataset effectiveness,
examine prompt impacts on model performances, and explore the relationship
between multilingual performances and factors such as tasks, model sizes, and
languages. These insights offer valuable guidance for future research. The
dataset is available at https://huggingface.co/datasets/Qwen/P-MMEval.

中文翻译:
近期，大型语言模型（LLMs）在多语言能力方面展现出多样化表现，涵盖翻译、代码生成与推理等任务。然而既有评估研究往往局限于基础自然语言处理（NLP）任务或单一能力专项测试。为突破这一局限，我们致力于构建一个全面的多语言多任务评估基准。首先，我们设计了一套从海量基准中筛选可用且合理评估集的流程，解决了前人工作中对基准效用性（即区分被评估模型能力）的忽视问题。基于此流程，我们推出了P-MMEval——一个覆盖基础能力与专项能力的大规模评估基准。该基准不仅确保各数据集间语言覆盖的一致性，还提供平行语料样本。最后，我们对代表性多语言模型系列展开广泛实验，通过横向性能对比、数据集效用分析、提示词对模型表现的影响研究，以及多语言性能与任务类型、模型规模、语言特性等因素的关联探索，为后续研究提供了重要启示。数据集已发布于https://huggingface.co/datasets/Qwen/P-MMEval。
