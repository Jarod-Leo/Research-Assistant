# Quantifying Logical Consistency in Transformers via Query-Key Alignment

链接: http://arxiv.org/abs/2502.17017v1

原文摘要:
Large language models (LLMs) have demonstrated impressive performance in
various natural language processing tasks, yet their ability to perform
multi-step logical reasoning remains an open challenge. Although
Chain-of-Thought prompting has improved logical reasoning by enabling models to
generate intermediate steps, it lacks mechanisms to assess the coherence of
these logical transitions. In this paper, we propose a novel, lightweight
evaluation strategy for logical reasoning that uses query-key alignments inside
transformer attention heads. By computing a single forward pass and extracting
a "QK-score" from carefully chosen heads, our method reveals latent
representations that reliably separate valid from invalid inferences, offering
a scalable alternative to traditional ablation-based techniques. We also
provide an empirical validation on multiple logical reasoning benchmarks,
demonstrating improved robustness of our evaluation method against distractors
and increased reasoning depth. The experiments were conducted on a diverse set
of models, ranging from 1.5B to 70B parameters.

中文翻译:
大型语言模型（LLMs）在各类自然语言处理任务中展现出卓越性能，但其多步逻辑推理能力仍存在显著挑战。尽管思维链提示技术通过生成中间推理步骤提升了逻辑推理表现，但缺乏评估这些逻辑过渡连贯性的内在机制。本文提出一种新颖、轻量级的逻辑推理评估策略，利用Transformer注意力头中的查询-键值对齐机制。通过单次前向传播计算并从精选的注意力头中提取"QK分数"，我们的方法能可靠地区分有效与无效推理的潜在表征，为传统基于消融的技术提供了可扩展的替代方案。我们在多个逻辑推理基准测试上进行了实证验证，结果表明该评估方法对干扰因素具有更强鲁棒性，并能支持更深层次的推理。实验覆盖了参数量从15亿到700亿不等的多样化模型集。

（翻译说明：1. 专业术语如"Chain-of-Thought"采用学界通用译法"思维链"；2. 长难句进行合理切分，如将原文最后两句合并为符合中文表达习惯的复合句；3. 技术概念"query-key alignments"保留核心意象译为"查询-键值对齐"；4. 被动语态转换为主动句式，如"demonstrating improved..."处理为"结果表明..."；5. 数字单位"B"统一转换为中文计量习惯"亿"）
