# Quantifying Logical Consistency in Transformers via Query-Key Alignment

链接: http://arxiv.org/abs/2502.17017v1

原文摘要:
Large language models (LLMs) have demonstrated impressive performance in
various natural language processing tasks, yet their ability to perform
multi-step logical reasoning remains an open challenge. Although
Chain-of-Thought prompting has improved logical reasoning by enabling models to
generate intermediate steps, it lacks mechanisms to assess the coherence of
these logical transitions. In this paper, we propose a novel, lightweight
evaluation strategy for logical reasoning that uses query-key alignments inside
transformer attention heads. By computing a single forward pass and extracting
a "QK-score" from carefully chosen heads, our method reveals latent
representations that reliably separate valid from invalid inferences, offering
a scalable alternative to traditional ablation-based techniques. We also
provide an empirical validation on multiple logical reasoning benchmarks,
demonstrating improved robustness of our evaluation method against distractors
and increased reasoning depth. The experiments were conducted on a diverse set
of models, ranging from 1.5B to 70B parameters.

中文翻译:
大型语言模型（LLMs）在各类自然语言处理任务中展现出卓越性能，但其多步逻辑推理能力仍面临挑战。尽管思维链提示技术通过生成中间推理步骤提升了逻辑推理表现，却缺乏评估这些逻辑过渡连贯性的机制。本文提出一种新颖的轻量级逻辑推理评估策略，利用Transformer注意力头中的查询-键对齐机制。通过单次前向传播计算并精选注意力头提取"QK分数"，我们的方法揭示了能可靠区分有效与无效推理的潜在表征，为传统消融技术提供了可扩展的替代方案。我们在多个逻辑推理基准测试上进行实证验证，表明该方法对干扰因素具有更强鲁棒性，并能支持更深层次的推理。实验覆盖了1.5B至70B参数规模的多样化模型集。
