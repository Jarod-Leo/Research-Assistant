# Less is More: Extreme Gradient Boost Rank-1 Adaption for Efficient Finetuning of LLMs

链接: http://arxiv.org/abs/2410.19694v1

原文摘要:
Fine-tuning Large Language Models (LLMs) has become a crucial technique for
adapting pre-trained models to downstream tasks. However, the enormous size of
LLMs poses significant challenges in terms of computational complexity and
resource requirements. Low-Rank Adaptation (LoRA) has emerged as a promising
solution. However, there exists a gap between the practical performance of
low-rank adaptations and its theoretical optimum. In this work, we propose
eXtreme Gradient Boosting LoRA (XGBLoRA), a novel framework that bridges this
gap by leveraging the power of ensemble learning. Inspired by gradient
boosting, XGBLoRA iteratively learns and merges a sequence of LoRA adaptations
to refine model predictions. It achieves better performance than the standard
LoRA, while enjoying the computational efficiency of rank-1 adaptations. We
provide theoretical analysis to show the convergence and optimality of our
approach, and conduct extensive experiments on a range of natural language
processing tasks. The results demonstrate that XGBLoRA consistently outperforms
standard LoRA and achieves performance comparable to full fine-tuning with
significantly fewer trainable parameters. This work advances
parameter-efficient fine-tuning for LLMs, and offers a promising solution for
adapting LLMs to downstream tasks while optimizing performance and efficiency.

中文翻译:
微调大型语言模型（LLMs）已成为将预训练模型适配下游任务的关键技术。然而，LLMs庞大的参数量带来了计算复杂度和资源需求的重大挑战。低秩自适应（LoRA）作为一种有前景的解决方案应运而生，但其实际性能与理论最优值之间仍存在差距。本研究提出极端梯度提升低秩自适应（XGBLoRA）框架，通过集成学习的力量弥合这一差距。受梯度提升思想启发，XGBLoRA通过迭代学习和合并一系列LoRA适配来优化模型预测，在保持秩-1适配计算效率的同时，性能优于标准LoRA。我们通过理论分析证明了该方法的收敛性和最优性，并在多项自然语言处理任务上进行了广泛实验。结果表明，XGBLoRA在可训练参数显著减少的情况下，不仅持续超越标准LoRA，更能达到与全参数微调相当的性能。这项工作推动了LLMs的参数高效微调技术发展，为在优化性能与效率的同时适配下游任务提供了创新解决方案。
