# Precision Where It Matters: A Novel Spike Aware Mixed-Precision Quantization Strategy for LLaMA-based Language Models

链接: http://arxiv.org/abs/2504.21553v1

原文摘要:
Large Language Models (LLMs) have demonstrated remarkable capabilities in
various natural language processing tasks. However, their size presents
significant challenges for deployment and inference. This paper investigates
the quantization of LLMs, focusing on the LLaMA architecture and its
derivatives. We challenge existing assumptions about activation outliers in
LLMs and propose a novel mixed-precision quantization approach tailored for
LLaMA-like models. Our method leverages the observation that activation spikes
in LLaMA architectures are predominantly concentrated in specific projection
layers. By applying higher precision (FP16 or FP8) to these layers while
quantizing the rest of the model to lower bit-widths, we achieve superior
performance compared to existing quantization techniques. Experimental results
on LLaMA2, LLaMA3, and Mistral models demonstrate significant improvements in
perplexity and zero-shot accuracy, particularly for 8-bit per-tensor
quantization. Our approach outperforms general-purpose methods designed to
handle outliers across all architecture types, highlighting the benefits of
architecture-specific quantization strategies. This research contributes to the
ongoing efforts to make LLMs more efficient and deployable, potentially
enabling their use in resource-constrained environments. Our findings emphasize
the importance of considering model-specific characteristics in developing
effective quantization pipelines for state-of-the-art language models by
identifying and targeting a small number of projections that concentrate
activation spikes.

中文翻译:
大型语言模型（LLMs）在各种自然语言处理任务中展现出卓越能力，但其庞大规模给部署与推理带来显著挑战。本文以LLaMA架构及其衍生模型为研究对象，对LLM量化技术展开深入探讨。我们质疑现有关于LLM激活异常值的假设，提出了一种专为类LLaMA模型设计的混合精度量化新方法。该方法基于关键发现：LLaMA架构中的激活峰值现象主要集中于特定投影层。通过对这些层采用更高精度（FP16或FP8）处理，同时将模型其余部分量化至更低比特位宽，我们的方案在性能上超越了现有量化技术。基于LLaMA2、LLaMA3和Mistral模型的实验结果表明，该方法在困惑度和零样本准确率指标上取得显著提升，尤其在全张量8比特量化场景表现突出。相较于通用型异常值处理方法，我们的架构专用策略展现出明显优势，这凸显了针对特定模型特性开发量化方案的重要性。本研究通过精准定位并处理少数集中出现激活峰值的投影层，为提升前沿语言模型的量化效率提供了新思路，有望推动LLM在资源受限环境中的实际应用。
