# Scientific Language Modeling: A Quantitative Review of Large Language Models in Molecular Science

链接: http://arxiv.org/abs/2402.04119v1

原文摘要:
Deep learning has significantly advanced molecular modeling and design,
enabling efficient understanding and discovery of novel molecules. In
particular, large language models (LLMs) introduce a fresh research paradigm to
tackle scientific problems from a natural language processing (NLP)
perspective. LLMs significantly enhance our understanding and generation of
molecules, often surpassing existing methods with their capabilities to decode
and synthesize complex molecular patterns. However, two key issues remain: how
to quantify the match between model and data modalities and how to identify the
knowledge-learning preferences of models. To address these challenges, we
propose a multi-modal benchmark, named ChEBI-20-MM, and perform 1263
experiments to assess the model's compatibility with data modalities and
knowledge acquisition. Through the modal transition probability matrix, we
provide insights into the most suitable modalities for tasks. Furthermore, we
introduce a statistically interpretable approach to discover context-specific
knowledge mapping by localized feature filtering. Our analysis offers an
exploration of the learning mechanism and paves the way for advancing LLMs in
molecular science.

中文翻译:
深度学习显著推动了分子建模与设计的发展，为高效理解与发现新型分子提供了强大工具。特别是大语言模型（LLMs）引入了一种全新的研究范式，从自然语言处理（NLP）视角来攻克科学难题。LLMs通过解析与合成复杂分子模式的能力，极大提升了我们对分子的认知与生成水平，其表现往往超越传统方法。然而仍存在两个核心问题：如何量化模型与数据模态之间的匹配度，以及如何识别模型的知识学习偏好。

为解决这些挑战，我们提出了名为ChEBI-20-MM的多模态基准测试，通过1263组实验系统评估了模型对不同数据模态的适应性与知识获取能力。借助模态转移概率矩阵，我们揭示了任务最适配的模态选择方案。更进一步，我们开发了一种基于局部特征过滤的统计可解释方法，用于发现特定上下文的知识映射规律。这项研究不仅深化了对LLMs学习机制的探索，更为推动分子科学领域的大模型发展开辟了新路径。
