# LLASP: Fine-tuning Large Language Models for Answer Set Programming

链接: http://arxiv.org/abs/2407.18723v1

原文摘要:
Recently, Large Language Models (LLMs) have showcased their potential in
various natural language processing tasks, including code generation. However,
while significant progress has been made in adapting LLMs to generate code for
several imperative programming languages and tasks, there remains a notable gap
in their application to declarative formalisms, such as Answer Set Programming
(ASP). In this paper, we move a step towards exploring the capabilities of LLMs
for ASP code generation. First, we perform a systematic evaluation of several
state-of-the-art LLMs. Despite their power in terms of number of parameters,
training data and computational resources, empirical results demonstrate
inadequate performances in generating correct ASP programs. Therefore, we
propose LLASP, a fine-tuned lightweight model specifically trained to encode
fundamental ASP program patterns. To this aim, we create an ad-hoc dataset
covering a wide variety of fundamental problem specifications that can be
encoded in ASP. Our experiments demonstrate that the quality of ASP programs
generated by LLASP is remarkable. This holds true not only when compared to the
non-fine-tuned counterpart but also when compared to the majority of eager LLM
candidates, particularly from a semantic perspective. All the code and data
used to perform the experiments are publicly available at
https://anonymous.4open.science/r/LLASP-D86C/.

中文翻译:
近年来，大型语言模型（LLMs）在各类自然语言处理任务中展现出巨大潜力，包括代码生成领域。然而，尽管LLMs在适配生成多种命令式编程语言代码方面已取得显著进展，但其在声明式范式（如答案集编程ASP）中的应用仍存在明显不足。本文迈出了探索LLMs在ASP代码生成能力的重要一步：首先对多个前沿LLMs进行系统评估，发现即便这些模型拥有海量参数、训练数据和计算资源，实证结果仍显示其在生成正确ASP程序方面表现欠佳。为此，我们提出LLASP——一个经过微调的轻量级模型，专门训练用于编码基础ASP程序模式。为此我们构建了专用数据集，涵盖可被ASP编码的多样化基础问题规范。实验表明，LLASP生成的ASP程序质量显著提升，不仅优于未微调的基线模型，在语义层面更超越了多数主流LLM候选方案。全部实验代码与数据已公开于https://anonymous.4open.science/r/LLASP-D86C/。
