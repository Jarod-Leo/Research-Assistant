# Complexity of Symbolic Representation in Working Memory of Transformer Correlates with the Complexity of a Task

链接: http://arxiv.org/abs/2406.14213v1

原文摘要:
Even though Transformers are extensively used for Natural Language Processing
tasks, especially for machine translation, they lack an explicit memory to
store key concepts of processed texts. This paper explores the properties of
the content of symbolic working memory added to the Transformer model decoder.
Such working memory enhances the quality of model predictions in machine
translation task and works as a neural-symbolic representation of information
that is important for the model to make correct translations. The study of
memory content revealed that translated text keywords are stored in the working
memory, pointing to the relevance of memory content to the processed text.
Also, the diversity of tokens and parts of speech stored in memory correlates
with the complexity of the corpora for machine translation task.

中文翻译:
尽管Transformer模型在自然语言处理任务中（尤其是机器翻译）得到广泛应用，但其缺乏显式记忆机制来存储已处理文本的关键概念。本研究探讨了在Transformer解码器中加入符号化工作记忆内容所呈现的特性。这种工作记忆不仅提升了机器翻译任务中模型预测的质量，还形成了对翻译决策至关重要的神经符号化信息表征。通过对记忆内容的分析发现，工作记忆中存储着待翻译文本的关键词，证实了记忆内容与处理文本的高度相关性。此外，记忆单元内存储的词符多样性和词性分布与机器翻译任务语料库的复杂度呈现显著相关性。
