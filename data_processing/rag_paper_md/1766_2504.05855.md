# Enhancing Coreference Resolution with Pretrained Language Models: Bridging the Gap Between Syntax and Semantics

链接: http://arxiv.org/abs/2504.05855v1

原文摘要:
Large language models have made significant advancements in various natural
language processing tasks, including coreference resolution. However,
traditional methods often fall short in effectively distinguishing referential
relationships due to a lack of integration between syntactic and semantic
information. This study introduces an innovative framework aimed at enhancing
coreference resolution by utilizing pretrained language models. Our approach
combines syntax parsing with semantic role labeling to accurately capture finer
distinctions in referential relationships. By employing state-of-the-art
pretrained models to gather contextual embeddings and applying an attention
mechanism for fine-tuning, we improve the performance of coreference tasks.
Experimental results across diverse datasets show that our method surpasses
conventional coreference resolution systems, achieving notable accuracy in
disambiguating references. This development not only improves coreference
resolution outcomes but also positively impacts other natural language
processing tasks that depend on precise referential understanding.

中文翻译:
大语言模型在指代消解等自然语言处理任务中取得了显著进展，但传统方法因缺乏句法与语义信息的有效融合，往往难以精准区分指代关系。本研究提出一种创新框架，通过整合预训练语言模型来提升指代消解性能。该框架结合句法分析与语义角色标注技术，能更精确捕捉指代关系的细微差异。我们采用前沿预训练模型获取上下文嵌入表示，并运用注意力机制进行微调，从而优化指代任务的性能表现。跨数据集实验表明，该方法在指代消歧准确率上显著优于传统指代消解系统。这一进展不仅提升了指代消解效果，对依赖精确指代理解的其他自然语言处理任务也具有积极影响。
