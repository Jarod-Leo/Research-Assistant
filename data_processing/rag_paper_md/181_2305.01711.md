# Don't Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner

链接: http://arxiv.org/abs/2305.01711v1

原文摘要:
Language models (LMs) trained on vast quantities of unlabelled data have
greatly advanced the field of natural language processing (NLP). In this study,
we re-visit the widely accepted notion in NLP that continued pre-training LMs
on task-related texts improves the performance of fine-tuning (FT) in
downstream tasks. Through experiments on eight single-sentence tasks and eight
sentence-pair tasks in both semi-supervised and fully-supervised settings, we
find that conventional continued pre-training does not consistently provide
benefits and can even be detrimental for sentence-pair tasks or when
prompt-based FT is used. To tackle these issues, we propose Prompt-based
Continued Pre-training (PCP), which combines the idea of instruction tuning
with conventional continued pre-training. Our approach aims to improve the
performance of prompt-based FT by presenting both task-related texts and prompt
templates to LMs through unsupervised pre-training objectives before
fine-tuning for the target task. Our empirical evaluations on 21 benchmarks
demonstrate that the PCP consistently improves the performance of
state-of-the-art prompt-based FT approaches (up to 20.1% absolute) in both
semi-supervised and fully-supervised settings, even with only hundreds of
unlabelled examples. Additionally, prompt-based FT with the PCP outperforms
state-of-the-art semi-supervised approaches with greater simplicity,
eliminating the need for an iterative process and extra data augmentation. Our
further analysis explores the performance lower bound of the PCP and reveals
that the advantages of PCP persist across different sizes of models and
datasets.

中文翻译:
在大规模无标注数据上训练的语言模型（LMS）显著推动了自然语言处理（NLP）领域的发展。本研究重新审视了NLP中一个被广泛接受的观念：在任务相关文本上持续预训练语言模型能提升下游任务微调（FT）的性能。通过对半监督和全监督场景下八项单句任务与八项句对任务的实验，我们发现传统持续预训练不仅无法稳定带来增益，甚至可能损害句对任务或基于提示微调的效果。针对这些问题，我们提出基于提示的持续预训练（PCP），该方法将指令调优思想与传统持续预训练相结合。我们的方案通过在目标任务微调前，以无监督预训练目标向语言模型同时呈现任务相关文本和提示模板，旨在提升基于提示微调的性能。在21个基准测试上的实证评估表明，PCP能持续提升当前最优基于提示微调方法的性能（最高达20.1%绝对提升），且在半监督和全监督场景下仅需数百个无标注样本即可生效。此外，结合PCP的基于提示微调以更简化的方式超越了当前最先进的半监督方法，无需迭代过程与额外数据增强。进一步分析揭示了PCP的性能下限，并证实其优势在不同模型规模和数据集上均具有持续性。
