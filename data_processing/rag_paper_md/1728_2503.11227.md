# GKG-LLM: A Unified Framework for Generalized Knowledge Graph Construction

链接: http://arxiv.org/abs/2503.11227v1

原文摘要:
The construction of Generalized Knowledge Graph (GKG), including knowledge
graph, event knowledge graph and commonsense knowledge graph, is fundamental
for various natural language processing tasks. Current studies typically
construct these types of graph separately, overlooking holistic insights and
potential unification that could be beneficial in computing resources and usage
perspectives. However, a key challenge in developing a unified framework for
GKG is obstacles arising from task-specific differences. In this study, we
propose a unified framework for constructing generalized knowledge graphs to
address this challenge. First, we collect data from 15 sub-tasks in 29 datasets
across the three types of graphs, categorizing them into in-sample,
counter-task, and out-of-distribution (OOD) data. Then, we propose a
three-stage curriculum learning fine-tuning framework, by iteratively injecting
knowledge from the three types of graphs into the Large Language Models.
Extensive experiments show that our proposed model improves the construction of
all three graph types across in-domain, OOD and counter-task data.

中文翻译:
广义知识图谱（GKG）的构建涵盖知识图谱、事件知识图谱与常识知识图谱，是支撑各类自然语言处理任务的基础。当前研究通常独立构建这三类图谱，忽视了整体视角的洞察力以及潜在的统一性可能带来的计算资源与使用效率优势。然而，开发统一GKG框架的核心挑战在于任务特异性差异导致的障碍。本研究提出一个构建广义知识图谱的统一框架以应对该挑战：首先从三类图谱涉及的29个数据集中收集15个子任务数据，将其分类为同分布样本、对抗任务样本和分布外（OOD）数据；随后设计三阶段课程学习微调框架，通过迭代方式将三类图谱知识注入大语言模型。大量实验表明，该模型在领域内数据、OOD数据和对抗任务数据上均能提升三类图谱的构建效果。
