# idT5: Indonesian Version of Multilingual T5 Transformer

链接: http://arxiv.org/abs/2302.00856v1

原文摘要:
Indonesian language is spoken by almost 200 million people and is the 10th
most spoken language in the world, but it is under-represented in NLP (Natural
Language Processing) research. A sparsity of language resources has hampered
previous work on Indonesian. The Transformer is a new architecture rapidly
becoming dominant for NLP, surpassing alternatives like convolutional and
recurrent neural networks. T5 (Text-to-Text Transfer Transformer) is a
Transformer model that converts all text-based language problems to
text-to-text format for English. The multilingual variant is mT5 (multilingual
T5) which has shown promising results on many NLP tasks across languages.
However, the size of this multilingual model is a drawback for its application
in real production applications, which sometimes require only one language. In
this study, the mT5 model was adapted for only one language, Indonesian,
resulting in a pre-trained T5 model that was specific only for Indonesian with
a smaller size. For performance comparison, we fine-tuned this model and the
mT5 model to the Sentiment Analysis (SA), Question Generation (QG), and
Question Answering (QA) tasks with the exact mechanism and dataset. Fine-tuned
model based on our model achieved 77.18% accuracy on SA, 8% higher than the
mT5-based model, and obtained nearly the same score as the mT5-based model on
QG and QA. The results confirm that it is possible to produce a smaller
pre-trained model that maintains comparable yields while reducing the model
size by up to 58%. In addition, the resulting model requires less memory, loads
faster, and inference times faster.

中文翻译:
印尼语拥有近2亿使用者，是全球第十大语言，但在自然语言处理（NLP）研究中却鲜少受到关注。语言资源的匮乏长期制约着印尼语的相关研究。Transformer作为一种新兴架构，正迅速成为NLP领域的主导技术，其性能已超越卷积神经网络和循环神经网络等传统方案。T5（文本到文本转换Transformer）模型通过将英语文本任务统一转化为文本到文本格式而广受关注，其多语言版本mT5（多语言T5）在跨语言NLP任务中展现出卓越性能。然而，该多语言模型的庞大体量限制了其在实际生产环境中的应用——尤其是当应用场景仅需单一语言支持时。本研究将mT5模型适配为印尼语专用版本，最终获得了一个体积更小、专为印尼语优化的预训练T5模型。为评估性能，我们采用相同机制和数据集，分别对印尼语T5模型和mT5模型进行了情感分析（SA）、问题生成（QG）和问答（QA）任务的微调。实验表明：基于本研究的模型在SA任务中达到77.18%准确率，较mT5基础模型提升8%；在QG和QA任务中则与mT5基础模型表现相当。这些结果证实，通过最高58%的模型压缩，完全可以获得性能相当的小型预训练模型。此外，所得模型还具有内存占用更低、加载速度更快、推理耗时更短等优势。
