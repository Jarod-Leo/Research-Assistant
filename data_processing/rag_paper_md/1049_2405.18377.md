# LLaMA-NAS: Efficient Neural Architecture Search for Large Language Models

链接: http://arxiv.org/abs/2405.18377v1

原文摘要:
The abilities of modern large language models (LLMs) in solving natural
language processing, complex reasoning, sentiment analysis and other tasks have
been extraordinary which has prompted their extensive adoption. Unfortunately,
these abilities come with very high memory and computational costs which
precludes the use of LLMs on most hardware platforms. To mitigate this, we
propose an effective method of finding Pareto-optimal network architectures
based on LLaMA2-7B using one-shot NAS. In particular, we fine-tune LLaMA2-7B
only once and then apply genetic algorithm-based search to find smaller, less
computationally complex network architectures. We show that, for certain
standard benchmark tasks, the pre-trained LLaMA2-7B network is unnecessarily
large and complex. More specifically, we demonstrate a 1.5x reduction in model
size and 1.3x speedup in throughput for certain tasks with negligible drop in
accuracy. In addition to finding smaller, higher-performing network
architectures, our method does so more effectively and efficiently than certain
pruning or sparsification techniques. Finally, we demonstrate how quantization
is complementary to our method and that the size and complexity of the networks
we find can be further decreased using quantization. We believe that our work
provides a way to automatically create LLMs which can be used on less expensive
and more readily available hardware platforms.

中文翻译:
现代大语言模型（LLMs）在自然语言处理、复杂推理、情感分析等任务中展现出的能力令人瞩目，这促使了其广泛应用。然而，这些能力伴随着极高的内存与计算成本，导致大多数硬件平台难以部署LLMs。为缓解这一问题，我们提出一种基于LLaMA2-7B的高效帕累托最优网络架构搜索方法，采用一次性神经架构搜索（one-shot NAS）。具体而言，我们仅需对LLaMA2-7B进行一次微调，随后应用基于遗传算法的搜索来寻找更小、计算复杂度更低的网络架构。实验表明，对于某些标准基准任务，预训练的LLaMA2-7B网络存在不必要的规模冗余与复杂度过剩。我们成功将模型尺寸缩小1.5倍，特定任务吞吐速度提升1.3倍，且精度损失可忽略不计。相较于剪枝或稀疏化技术，我们的方法能以更高效率发现性能更优的小型架构。此外，我们验证了量化技术与本方法的互补性——通过量化可进一步降低所发现网络的规模与复杂度。本研究为自动生成适用于低成本、易获取硬件平台的LLMs提供了可行路径。
