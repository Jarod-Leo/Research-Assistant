# LLaMA-NAS: Efficient Neural Architecture Search for Large Language Models

链接: http://arxiv.org/abs/2405.18377v1

原文摘要:
The abilities of modern large language models (LLMs) in solving natural
language processing, complex reasoning, sentiment analysis and other tasks have
been extraordinary which has prompted their extensive adoption. Unfortunately,
these abilities come with very high memory and computational costs which
precludes the use of LLMs on most hardware platforms. To mitigate this, we
propose an effective method of finding Pareto-optimal network architectures
based on LLaMA2-7B using one-shot NAS. In particular, we fine-tune LLaMA2-7B
only once and then apply genetic algorithm-based search to find smaller, less
computationally complex network architectures. We show that, for certain
standard benchmark tasks, the pre-trained LLaMA2-7B network is unnecessarily
large and complex. More specifically, we demonstrate a 1.5x reduction in model
size and 1.3x speedup in throughput for certain tasks with negligible drop in
accuracy. In addition to finding smaller, higher-performing network
architectures, our method does so more effectively and efficiently than certain
pruning or sparsification techniques. Finally, we demonstrate how quantization
is complementary to our method and that the size and complexity of the networks
we find can be further decreased using quantization. We believe that our work
provides a way to automatically create LLMs which can be used on less expensive
and more readily available hardware platforms.

中文翻译:
现代大型语言模型（LLM）在自然语言处理、复杂推理、情感分析等任务中展现出的能力令人瞩目，这推动了其广泛应用。然而，这些能力伴随着极高的内存和计算成本，导致LLM难以在多数硬件平台上部署。为此，我们提出了一种基于LLaMA2-7B模型、利用一次性神经架构搜索（NAS）寻找帕累托最优网络架构的有效方法。具体而言，我们仅对LLaMA2-7B进行一次微调，随后应用基于遗传算法的搜索来发现更小、计算复杂度更低的网络架构。实验表明，针对某些标准基准任务，预训练的LLaMA2-7B网络存在不必要的规模和复杂度冗余。通过我们的方法，在精度损失可忽略的前提下，特定任务的模型尺寸可缩减1.5倍，吞吐速度提升1.3倍。与剪枝或稀疏化技术相比，我们的方法能以更高效率和效果发现更小且性能更优的架构。此外，我们还验证了量化技术与本方法的互补性——通过量化可进一步降低所发现网络的规模和复杂度。这项工作为自动创建适用于低成本、易获取硬件平台的LLM提供了可行路径。
