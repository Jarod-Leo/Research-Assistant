# Automated Interviewer or Augmented Survey? Collecting Social Data with Large Language Models

链接: http://arxiv.org/abs/2309.10187v1

原文摘要:
Chatbots have shown promise as tools to scale qualitative data collection.
Recent advances in Large Language Models (LLMs) could accelerate this process
by allowing researchers to easily deploy sophisticated interviewing chatbots.
We test this assumption by conducting a large-scale user study (n=399)
evaluating 3 different chatbots, two of which are LLM-based and a baseline
which employs hard-coded questions. We evaluate the results with respect to
participant engagement and experience, established metrics of chatbot quality
grounded in theories of effective communication, and a novel scale evaluating
"richness" or the extent to which responses capture the complexity and
specificity of the social context under study. We find that, while the chatbots
were able to elicit high-quality responses based on established evaluation
metrics, the responses rarely capture participants' specific motives or
personalized examples, and thus perform poorly with respect to richness. We
further find low inter-rater reliability between LLMs and humans in the
assessment of both quality and richness metrics. Our study offers a cautionary
tale for scaling and evaluating qualitative research with LLMs.

中文翻译:
聊天机器人作为扩展定性数据收集的工具已展现出潜力。随着大语言模型（LLM）技术的进步，研究者能够便捷部署复杂的访谈机器人，从而加速这一进程。我们通过一项大规模用户研究（n=399）验证该假设，评估了三种不同聊天机器人（其中两个基于LLM，另一个采用硬编码问题作为基线）的表现。从参与者参与度与体验、基于有效沟通理论建立的聊天机器人质量评价指标，以及衡量"丰富性"（即回答捕捉研究对象社会情境复杂性和具体程度）的新量表三个维度进行分析。研究发现：虽然聊天机器人能基于传统评估指标获得高质量回答，但这些回答很少体现参与者的具体动机或个性化案例，因此在丰富性维度表现欠佳。进一步分析表明，LLM与人类评估者在质量与丰富性指标上的评分者间信度较低。本研究为利用LLM开展规模化定性研究及其评估提供了警示性案例。
