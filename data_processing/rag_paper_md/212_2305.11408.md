# AlignAtt: Using Attention-based Audio-Translation Alignments as a Guide for Simultaneous Speech Translation

链接: http://arxiv.org/abs/2305.11408v1

原文摘要:
Attention is the core mechanism of today's most used architectures for
natural language processing and has been analyzed from many perspectives,
including its effectiveness for machine translation-related tasks. Among these
studies, attention resulted to be a useful source of information to get
insights about word alignment also when the input text is substituted with
audio segments, as in the case of the speech translation (ST) task. In this
paper, we propose AlignAtt, a novel policy for simultaneous ST (SimulST) that
exploits the attention information to generate source-target alignments that
guide the model during inference. Through experiments on the 8 language pairs
of MuST-C v1.0, we show that AlignAtt outperforms previous state-of-the-art
SimulST policies applied to offline-trained models with gains in terms of BLEU
of 2 points and latency reductions ranging from 0.5s to 0.8s across the 8
languages.

中文翻译:
注意力机制是当今自然语言处理领域最常用架构的核心机制，其有效性已从多个角度得到分析，包括在机器翻译相关任务中的应用。研究表明，即使在语音翻译（ST）任务中将输入文本替换为音频片段时，注意力机制仍是获取词对齐信息的重要来源。本文提出AlignAtt——一种创新的同声传译（SimulST）策略，该策略利用注意力信息生成源-目标对齐关系，在推理过程中引导模型。通过在MuST-C v1.0数据集的8种语言对上进行的实验表明，AlignAtt显著优于先前应用于离线训练模型的最先进SimulST策略：在BLEU指标上平均提升2个点，同时在8种语言中实现0.5至0.8秒的延迟降低。

（翻译说明：
1. 专业术语处理："attention mechanism"规范译为"注意力机制"，"SimulST"采用行业通用缩略语"同声传译"并保留英文缩写
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句，如将"has been analyzed..."处理为独立分句
3. 被动语态转换：将"has been analyzed"等被动式转为主动式"得到分析"
4. 数据呈现优化：将"gains in terms of BLEU of 2 points"转化为"平均提升2个点"更符合中文科技论文表述
5. 概念显化："policy"在机器学习语境下译为"策略"而非直译"政策"
6. 保持技术准确性：严格区分"inference"译为"推理"而非"推断"，符合深度学习领域术语规范）
