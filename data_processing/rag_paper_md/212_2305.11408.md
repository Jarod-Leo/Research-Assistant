# AlignAtt: Using Attention-based Audio-Translation Alignments as a Guide for Simultaneous Speech Translation

链接: http://arxiv.org/abs/2305.11408v1

原文摘要:
Attention is the core mechanism of today's most used architectures for
natural language processing and has been analyzed from many perspectives,
including its effectiveness for machine translation-related tasks. Among these
studies, attention resulted to be a useful source of information to get
insights about word alignment also when the input text is substituted with
audio segments, as in the case of the speech translation (ST) task. In this
paper, we propose AlignAtt, a novel policy for simultaneous ST (SimulST) that
exploits the attention information to generate source-target alignments that
guide the model during inference. Through experiments on the 8 language pairs
of MuST-C v1.0, we show that AlignAtt outperforms previous state-of-the-art
SimulST policies applied to offline-trained models with gains in terms of BLEU
of 2 points and latency reductions ranging from 0.5s to 0.8s across the 8
languages.

中文翻译:
注意力机制是当前自然语言处理领域主流架构的核心组件，其有效性已在机器翻译等任务中通过多角度研究得到验证。研究表明，即使在语音翻译（ST）任务中将输入文本替换为音频片段时，注意力仍能提供有价值的词对齐信息。本文提出AlignAtt——一种创新的同步语音翻译（SimulST）策略，该策略利用注意力信息生成源-目标对齐关系来指导推理过程。基于MuST-C v1.0数据集中8种语言对的实验表明，AlignAtt在应用于离线训练模型时，不仅以2个BLEU值的优势超越现有最优SimulST策略，还在8种语言中实现了0.5至0.8秒的延迟降低。
