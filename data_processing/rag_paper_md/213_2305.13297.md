# Parallel Attention and Feed-Forward Net Design for Pre-training and Inference on Transformers

链接: http://arxiv.org/abs/2305.13297v1

原文摘要:
This paper investigates the key role of Feed-Forward Networks (FFNs) in
transformer models by utilizing the Parallel Attention and Feed-Forward Net
Design (PAF) architecture, and comparing it to their Series Attention and
Feed-Forward Net Design (SAF) counterparts. Central to the effectiveness of PAF
are two main assumptions regarding the FFN block and the attention block within
a layer: 1) the primary function of the FFN block is to maintain isotropy among
token embeddings and prevent their degeneration, and 2) the residual norm
computed in the attention block is substantially smaller than the input token
embedding norm. To empirically validate these assumptions, we train PAF
variants of two large language models (RoBERTa-large and bert-large-uncased).
Our results demonstrate that both assumptions hold true in the PAF design. This
study contributes to a deeper understanding of the roles and interactions
between FFNs and self-attention mechanisms in transformer architectures.

中文翻译:
本文通过采用并行注意力与前馈网络设计（PAF）架构，并与串行结构（SAF）进行对比，深入探究了前馈网络（FFN）在Transformer模型中的关键作用。PAF架构的有效性基于两个核心假设：1）FFN模块的核心功能是保持词元嵌入的各向同性并防止其退化；2）注意力模块中的残差范数远小于输入词元嵌入的范数。为验证这些假设，我们训练了RoBERTa-large和bert-large-uncased两种大语言模型的PAF变体。实验结果表明，在PAF设计中这两个假设均成立。本研究为深入理解Transformer架构中FFN与自注意力机制的作用及交互关系提供了新的见解。
