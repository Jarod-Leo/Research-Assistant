# OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models

链接: http://arxiv.org/abs/2308.13137v1

原文摘要:
Large language models (LLMs) have revolutionized natural language processing
tasks. However, their practical deployment is hindered by their immense memory
and computation requirements. Although recent post-training quantization (PTQ)
methods are effective in reducing memory footprint and improving the
computational efficiency of LLM, they hand-craft quantization parameters,
leading to low performance, especially in extremely low-bit quantization. To
tackle this issue, we introduce an Omnidirectionally calibrated Quantization
(\textbf{OmniQuant}) technique for LLMs, which achieves good performance in
diverse quantization settings while maintaining the computational efficiency of
PTQ by efficiently optimizing various quantization parameters. OmniQuant
comprises two innovative components including Learnable Weight Clipping (LWC)
and Learnable Equivalent Transformation (LET). LWC modulates the extreme values
of weights by optimizing the clipping threshold. Meanwhile, LET tackles
activation outliers by shifting the challenge of quantization from activations
to weights. Operating within a differentiable framework using block-wise error
minimization, OmniQuant can optimize the quantization process efficiently for
both weight-only and weight-activation quantization. For instance, the LLaMA-2
model family size 7-70B can be processed with OmniQuant on a single A100-40G
GPU within 1-16 hours using 128 samples. Extensive experiments validate
OmniQuant's superior performance across diverse quantization configurations
such as W4A4 (4-bit weight, 4-bit activation), W6A6, W4A16, W3A16, and W2A16.
Additionally, OmniQuant demonstrates effectiveness in instruction-tuned models
and delivers notable improvements in inference speed and memory reduction on
real devices. Codes are available at
\url{https://github.com/OpenGVLab/OmniQuant}.

中文翻译:
大型语言模型（LLMs）已彻底改变了自然语言处理任务，但其庞大的内存与计算需求阻碍了实际部署。尽管近期训练后量化（PTQ）方法能有效减少内存占用并提升LLM计算效率，但其手工设计量化参数导致性能受限，尤其在极低位宽量化场景下表现不佳。为解决这一问题，我们提出全方位校准量化技术（OmniQuant），通过高效优化各类量化参数，在多样化量化配置中保持优异性能，同时维持PTQ的计算效率。该技术包含两大创新模块：可学习权重截断（LWC）和可学习等效变换（LET）。LWC通过优化截断阈值调节权重极值分布，而LET通过将激活量化的挑战转移至权重端来处理异常值。基于分块误差最小化的可微分框架，OmniQuant能高效优化权重单独量化及权重-激活联合量化流程。例如，LLaMA-2系列7B-70B模型在单张A100-40G GPU上仅需1-16小时（使用128个样本）即可完成处理。大量实验验证了OmniQuant在W4A4（4位权重/4位激活）、W6A6、W4A16、W3A16及W2A16等量化配置中的卓越性能，同时在指令微调模型上展现显著效果，并在实际设备中实现推理加速与内存占用的双重优化。代码已开源于\url{https://github.com/OpenGVLab/OmniQuant}。
