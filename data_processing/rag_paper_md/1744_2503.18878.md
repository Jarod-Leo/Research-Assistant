# I Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders

链接: http://arxiv.org/abs/2503.18878v1

原文摘要:
Large Language Models (LLMs) have achieved remarkable success in natural
language processing. Recent advances have led to the developing of a new class
of reasoning LLMs; for example, open-source DeepSeek-R1 has achieved
state-of-the-art performance by integrating deep thinking and complex
reasoning. Despite these impressive capabilities, the internal reasoning
mechanisms of such models remain unexplored. In this work, we employ Sparse
Autoencoders (SAEs), a method to learn a sparse decomposition of latent
representations of a neural network into interpretable features, to identify
features that drive reasoning in the DeepSeek-R1 series of models. First, we
propose an approach to extract candidate ''reasoning features'' from SAE
representations. We validate these features through empirical analysis and
interpretability methods, demonstrating their direct correlation with the
model's reasoning abilities. Crucially, we demonstrate that steering these
features systematically enhances reasoning performance, offering the first
mechanistic account of reasoning in LLMs. Code available at
https://github.com/AIRI-Institute/SAE-Reasoning

中文翻译:
大语言模型（LLMs）在自然语言处理领域取得了显著成就。近期进展催生了一类新型推理大语言模型，例如开源的DeepSeek-R1通过深度融合深度思考与复杂推理能力，实现了最先进的性能表现。尽管这些模型展现出令人瞩目的能力，其内部推理机制仍未被充分探索。本研究采用稀疏自编码器（SAEs）方法——通过将神经网络潜在表征分解为可解释的稀疏特征——系统性地识别了DeepSeek-R1系列模型中驱动推理的关键特征。我们首先提出从SAE表征中提取候选"推理特征"的方法框架，随后通过实证分析与可解释性技术验证了这些特征与模型推理能力的直接关联。最关键的是，我们证明了定向调控这些特征能系统性提升推理性能，首次为大语言模型的推理机制提供了可解释的机理说明。代码已开源于https://github.com/AIRI-Institute/SAE-Reasoning。
