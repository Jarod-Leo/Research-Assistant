# CLLMFS: A Contrastive Learning enhanced Large Language Model Framework for Few-Shot Named Entity Recognition

链接: http://arxiv.org/abs/2408.12834v1

原文摘要:
Few-shot Named Entity Recognition (NER), the task of identifying named
entities with only a limited amount of labeled data, has gained increasing
significance in natural language processing. While existing methodologies have
shown some effectiveness, such as enriching label semantics through various
prompting modes or employing metric learning techniques, their performance
exhibits limited robustness across diverse domains due to the lack of rich
knowledge in their pre-trained models. To address this issue, we propose
CLLMFS, a Contrastive Learning enhanced Large Language Model (LLM) Framework
for Few-Shot Named Entity Recognition, achieving promising results with limited
training data. Considering the impact of LLM's internal representations on
downstream tasks, CLLMFS integrates Low-Rank Adaptation (LoRA) and contrastive
learning mechanisms specifically tailored for few-shot NER. By enhancing the
model's internal representations, CLLMFS effectively improves both entity
boundary awareness ability and entity recognition accuracy. Our method has
achieved state-of-the-art performance improvements on F1-score ranging from
2.58\% to 97.74\% over existing best-performing methods across several
recognized benchmarks. Furthermore, through cross-domain NER experiments
conducted on multiple datasets, we have further validated the robust
generalization capability of our method. Our code will be released in the near
future.

中文翻译:
小样本命名实体识别（Few-shot Named Entity Recognition, NER）作为仅需少量标注数据即可识别命名实体的任务，在自然语言处理领域日益重要。现有方法虽展现出一定效果——例如通过多样化提示模式丰富标签语义，或采用度量学习技术——但由于预训练模型缺乏丰富知识，其性能在不同领域间的鲁棒性有限。为此，我们提出CLLMFS框架，即基于对比学习增强的大语言模型小样本命名实体识别方案，在有限训练数据下取得了显著效果。针对大语言模型内部表征对下游任务的影响，CLLMFS创新性地融合了低秩适配（LoRA）技术与专为小样本NER设计的对比学习机制。通过优化模型内部表征，该框架有效提升了实体边界感知能力与识别准确率。在多个权威基准测试中，我们的方法相较现有最优方案实现了F1值2.58%至97.74%的性能突破。此外，通过跨领域NER实验的验证，进一步证实了该方法具备强大的泛化能力。相关代码将于近期开源。
