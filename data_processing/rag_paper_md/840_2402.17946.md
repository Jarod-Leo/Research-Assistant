# Gradient-Free Adaptive Global Pruning for Pre-trained Language Models

链接: http://arxiv.org/abs/2402.17946v1

原文摘要:
The transformative impact of large language models (LLMs) like LLaMA and GPT
on natural language processing is countered by their prohibitive computational
demands. Pruning has emerged as a pivotal compression strategy, introducing
sparsity to enhance both memory and computational efficiency. Yet, traditional
global pruning is impractical for LLMs due to scalability issues, while local
pruning, despite its efficiency, leads to suboptimal solutions. Addressing
these challenges, we propose SparseLLM, a novel framework that redefines the
global pruning process into manageable, coordinated subproblems, allowing for
resource-efficient optimization with global optimality. SparseLLM's approach,
which conceptualizes LLMs as a chain of modular functions and leverages
auxiliary variables for problem decomposition, not only facilitates a pragmatic
application on LLMs but also demonstrates significant performance improvements,
particularly in high-sparsity regimes where it surpasses current
state-of-the-art methods.

中文翻译:
诸如LLaMA和GPT等大语言模型对自然语言处理领域的变革性影响，正被其高昂的计算需求所制约。剪枝技术作为一种关键压缩策略应运而生，通过引入稀疏性来提升内存与计算效率。然而传统全局剪枝方法因扩展性问题难以适用于大语言模型，而局部剪枝虽具效率优势却会导致次优解。针对这些挑战，我们提出创新框架SparseLLM，将全局剪枝过程重构为可管理的协同子问题，在保证全局最优性的同时实现资源高效优化。该框架将大语言模型概念化为模块化函数链，并利用辅助变量进行问题分解，不仅实现了大语言模型上的实用化应用，更在高稀疏度场景下展现出显著性能提升，超越了当前最先进方法。
