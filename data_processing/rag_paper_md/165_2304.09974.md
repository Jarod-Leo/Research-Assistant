# SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery

链接: http://arxiv.org/abs/2304.09974v1

原文摘要:
Advances in GPT-based large language models (LLMs) are revolutionizing
natural language processing, exponentially increasing its use across various
domains. Incorporating uni-directional attention, these autoregressive LLMs can
generate long and coherent paragraphs. However, for visual question answering
(VQA) tasks that require both vision and language processing, models with
bi-directional attention or models employing fusion techniques are often
employed to capture the context of multiple modalities all at once. As GPT does
not natively process vision tokens, to exploit the advancements in GPT models
for VQA in robotic surgery, we design an end-to-end trainable Language-Vision
GPT (LV-GPT) model that expands the GPT2 model to include vision input (image).
The proposed LV-GPT incorporates a feature extractor (vision tokenizer) and
vision token embedding (token type and pose). Given the limitations of
unidirectional attention in GPT models and their ability to generate coherent
long paragraphs, we carefully sequence the word tokens before vision tokens,
mimicking the human thought process of understanding the question to infer an
answer from an image. Quantitatively, we prove that the LV-GPT model
outperforms other state-of-the-art VQA models on two publically available
surgical-VQA datasets (based on endoscopic vision challenge robotic scene
segmentation 2018 and CholecTriplet2021) and on our newly annotated dataset
(based on the holistic surgical scene dataset). We further annotate all three
datasets to include question-type annotations to allow sub-type analysis.
Furthermore, we extensively study and present the effects of token sequencing,
token type and pose embedding for vision tokens in the LV-GPT model.

中文翻译:
基于GPT的大型语言模型（LLMs）的进步正在彻底改变自然语言处理领域，其应用范围正呈指数级增长。这类自回归LLMs采用单向注意力机制，能够生成连贯的长段落。然而，对于需要同时处理视觉与语言信息的视觉问答（VQA）任务，通常需采用双向注意力模型或融合技术模型来一次性捕捉多模态上下文。由于GPT本身不具备视觉标记处理能力，为将GPT模型的优势应用于机器人手术中的VQA任务，我们设计了一个端到端可训练的语言-视觉GPT（LV-GPT）模型，通过扩展GPT2架构以整合视觉输入（图像）。

提出的LV-GPT模型包含特征提取器（视觉标记生成器）和视觉标记嵌入（标记类型与位置编码）。针对GPT模型单向注意力的局限性及其生成连贯长文本的能力，我们精心设计了词标记先于视觉标记的序列编排方式，模拟人类通过理解问题从图像推断答案的思维过程。定量实验表明，LV-GPT模型在两个公开的手术VQA数据集（基于2018年内窥镜视觉挑战赛机器人场景分割和CholecTriplet2021）及我们新标注的数据集（基于整体手术场景数据集）上均优于其他最先进的VQA模型。我们进一步为所有三个数据集添加问题类型标注以实现子类型分析，并深入研究了LV-GPT模型中视觉标记的序列编排、标记类型及位置嵌入的影响机制。
