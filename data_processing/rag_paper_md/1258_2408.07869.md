# A Systematic Evaluation of Generated Time Series and Their Effects in Self-Supervised Pretraining

链接: http://arxiv.org/abs/2408.07869v1

原文摘要:
Self-supervised Pretrained Models (PTMs) have demonstrated remarkable
performance in computer vision and natural language processing tasks. These
successes have prompted researchers to design PTMs for time series data. In our
experiments, most self-supervised time series PTMs were surpassed by simple
supervised models. We hypothesize this undesired phenomenon may be caused by
data scarcity. In response, we test six time series generation methods, use the
generated data in pretraining in lieu of the real data, and examine the effects
on classification performance. Our results indicate that replacing a real-data
pretraining set with a greater volume of only generated samples produces
noticeable improvement.

中文翻译:
自监督预训练模型（PTMs）在计算机视觉和自然语言处理任务中展现出卓越性能，这一成功促使研究者开始为时间序列数据设计PTMs。然而实验表明，多数自监督时间序列PTMs的表现甚至不及简单的监督模型。我们推测这一现象可能源于数据稀缺问题。为此，我们测试了六种时间序列生成方法，在预训练阶段用生成数据替代真实数据，并评估其对分类性能的影响。结果表明，当使用更大规模的纯生成样本替代真实预训练数据集时，模型性能获得了显著提升。
