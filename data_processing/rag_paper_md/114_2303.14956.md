# Unified Text Structuralization with Instruction-tuned Language Models

链接: http://arxiv.org/abs/2303.14956v1

原文摘要:
Text structuralization is one of the important fields of natural language
processing (NLP) consists of information extraction (IE) and structure
formalization. However, current studies of text structuralization suffer from a
shortage of manually annotated high-quality datasets from different domains and
languages, which require specialized professional knowledge. In addition, most
IE methods are designed for a specific type of structured data, e.g., entities,
relations, and events, making them hard to generalize to others. In this work,
we propose a simple and efficient approach to instruct large language model
(LLM) to extract a variety of structures from texts. More concretely, we add a
prefix and a suffix instruction to indicate the desired IE task and structure
type, respectively, before feeding the text into a LLM. Experiments on two LLMs
show that this approach can enable language models to perform comparable with
other state-of-the-art methods on datasets of a variety of languages and
knowledge, and can generalize to other IE sub-tasks via changing the content of
instruction. Another benefit of our approach is that it can help researchers to
build datasets in low-source and domain-specific scenarios, e.g., fields in
finance and law, with low cost.

中文翻译:
文本结构化是自然语言处理（NLP）的重要领域之一，涵盖信息抽取（IE）与结构形式化两个核心环节。然而当前研究面临两大挑战：一是跨领域、多语言的高质量标注数据稀缺，这类数据往往需要专业领域知识进行人工标注；二是现有IE方法通常针对特定结构化数据（如实体、关系、事件）设计，难以泛化至其他任务。本研究提出一种简洁高效的大语言模型（LLM）指令调控方法，通过添加前缀和后缀指令分别指定IE任务与目标结构类型，引导模型从文本中提取多样化结构。实验表明，该方法使语言模型在多种语言和知识领域的基准数据集上达到与最先进方法相当的性能，仅需调整指令内容即可泛化至不同IE子任务。该方法的另一优势在于能够以较低成本，帮助研究者在资源匮乏的垂直领域（如金融、法律）快速构建专业数据集。
