# Semantic-Aware Autoregressive Image Modeling for Visual Representation Learning

链接: http://arxiv.org/abs/2312.10457v1

原文摘要:
The development of autoregressive modeling (AM) in computer vision lags
behind natural language processing (NLP) in self-supervised pre-training. This
is mainly caused by the challenge that images are not sequential signals and
lack a natural order when applying autoregressive modeling. In this study,
inspired by human beings' way of grasping an image, i.e., focusing on the main
object first, we present a semantic-aware autoregressive image modeling
(SemAIM) method to tackle this challenge. The key insight of SemAIM is to
autoregressive model images from the semantic patches to the less semantic
patches. To this end, we first calculate a semantic-aware permutation of
patches according to their feature similarities and then perform the
autoregression procedure based on the permutation. In addition, considering
that the raw pixels of patches are low-level signals and are not ideal
prediction targets for learning high-level semantic representation, we also
explore utilizing the patch features as the prediction targets. Extensive
experiments are conducted on a broad range of downstream tasks, including image
classification, object detection, and instance/semantic segmentation, to
evaluate the performance of SemAIM. The results demonstrate SemAIM achieves
state-of-the-art performance compared with other self-supervised methods.
Specifically, with ViT-B, SemAIM achieves 84.1% top-1 accuracy for fine-tuning
on ImageNet, 51.3% AP and 45.4% AP for object detection and instance
segmentation on COCO, which outperforms the vanilla MAE by 0.5%, 1.0%, and
0.5%, respectively.

中文翻译:
自回归建模（AM）在计算机视觉领域的自监督预训练发展中落后于自然语言处理（NLP），主要源于图像作为非序列信号在应用自回归建模时缺乏天然顺序的挑战。本研究受人类观察图像时优先关注主体对象的启发，提出了一种语义感知的自回归图像建模方法（SemAIM）来解决这一难题。SemAIM的核心思想是依照语义强弱对图像块进行排序，从高语义块向低语义块实施自回归建模。具体实现中，我们首先根据图像块特征相似度计算语义感知的排列顺序，再基于该顺序执行自回归过程。此外，考虑到原始像素属于低级视觉信号，不利于学习高级语义表征，我们还探索了以图像块特征作为预测目标的方法。通过在图像分类、目标检测及实例/语义分割等多样化下游任务上的大量实验表明，SemAIM相较其他自监督方法取得了最先进的性能表现。以ViT-B模型为例，SemAIM在ImageNet微调任务中达到84.1%的Top-1准确率，在COCO数据集上分别获得51.3%的目标检测AP和45.4%的实例分割AP，较原始MAE方法分别提升0.5%、1.0%和0.5%。
