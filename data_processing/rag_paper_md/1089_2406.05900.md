# Large Language Models Memorize Sensor Datasets! Implications on Human Activity Recognition Research

链接: http://arxiv.org/abs/2406.05900v1

原文摘要:
The astonishing success of Large Language Models (LLMs) in Natural Language
Processing (NLP) has spurred their use in many application domains beyond text
analysis, including wearable sensor-based Human Activity Recognition (HAR). In
such scenarios, often sensor data are directly fed into an LLM along with text
instructions for the model to perform activity classification. Seemingly
remarkable results have been reported for such LLM-based HAR systems when they
are evaluated on standard benchmarks from the field. Yet, we argue, care has to
be taken when evaluating LLM-based HAR systems in such a traditional way. Most
contemporary LLMs are trained on virtually the entire (accessible) internet --
potentially including standard HAR datasets. With that, it is not unlikely that
LLMs actually had access to the test data used in such benchmark
experiments.The resulting contamination of training data would render these
experimental evaluations meaningless. In this paper we investigate whether LLMs
indeed have had access to standard HAR datasets during training. We apply
memorization tests to LLMs, which involves instructing the models to extend
given snippets of data. When comparing the LLM-generated output to the original
data we found a non-negligible amount of matches which suggests that the LLM
under investigation seems to indeed have seen wearable sensor data from the
benchmark datasets during training. For the Daphnet dataset in particular,
GPT-4 is able to reproduce blocks of sensor readings. We report on our
investigations and discuss potential implications on HAR research, especially
with regards to reporting results on experimental evaluation

中文翻译:
大型语言模型（LLM）在自然语言处理（NLP）领域的惊人成功，推动了其在文本分析之外的诸多应用领域的使用，包括基于可穿戴传感器的人类活动识别（HAR）。在此类场景中，传感器数据通常与文本指令一同直接输入LLM，由模型执行活动分类。当基于LLM的HAR系统在领域标准基准测试中评估时，报告的结果看似非常出色。然而，我们认为，以这种传统方式评估基于LLM的HAR系统时必须谨慎。大多数当代LLM的训练数据几乎涵盖了整个（可访问的）互联网——可能包括标准HAR数据集。因此，LLM实际上可能已经接触过这些基准实验中的测试数据。由此导致的训练数据污染将使这些实验评估失去意义。  

本文研究了LLM在训练过程中是否确实接触过标准HAR数据集。我们对LLM进行了记忆测试，即指示模型扩展给定的数据片段。通过将LLM生成的输出与原始数据对比，我们发现存在不可忽视的匹配情况，这表明被测试的LLM似乎在训练过程中见过基准数据集中的可穿戴传感器数据。特别是对于Daphnet数据集，GPT-4能够复现传感器读数块。我们报告了相关研究，并讨论了其对HAR研究的潜在影响，尤其是在实验评估结果的报告方面。
