# Confidant: Customizing Transformer-based LLMs via Collaborative Edge Training

链接: http://arxiv.org/abs/2311.13381v1

原文摘要:
Transformer-based large language models (LLMs) have demonstrated impressive
capabilities in a variety of natural language processing (NLP) tasks.
Nonetheless, it is challenging to deploy and fine-tune LLMs on mobile edge
devices with limited computing, memory, and energy budgets. In this paper, we
propose Confidant, a multi-backend collaborative training framework for
customizing state-of-the-art LLMs on commodity mobile devices like smartphones.
Confidant partitions an LLM into several sub-models so that each fits into a
mobile device's memory. A pipeline parallel training mechanism is further
developed to ensure fast and efficient distributed training. In addition, we
propose a novel backend scheduler to allocate different attention heads to
heterogeneous compute hardware, including mobile CPU and GPUs, to maximize the
compute resource utilization on each edge device. Our preliminary experimental
results show that Confidant achieves at most 45.3% memory reduction and 8.03x
inference speedup in practical settings.

中文翻译:
基于Transformer架构的大规模语言模型(LLM)在各类自然语言处理任务中展现出卓越性能。然而，在计算、内存和能源资源受限的移动边缘设备上部署与微调这类模型仍面临巨大挑战。本文提出Confidant框架——一种面向智能手机等商用移动设备的多后端协同训练系统，用于定制最先进的大语言模型。该框架通过将大模型拆分为多个能适配移动设备内存容量的子模型，并创新性地采用流水线并行训练机制实现高效分布式训练。此外，我们设计了一种新型后端调度器，能够将注意力头动态分配给包括移动端CPU和GPU在内的异构计算硬件，从而最大化每个边缘设备的计算资源利用率。初步实验表明，Confidant在实际场景中最高可实现45.3%的内存压缩和8.03倍的推理加速。
