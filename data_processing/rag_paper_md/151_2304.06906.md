# Swin3D: A Pretrained Transformer Backbone for 3D Indoor Scene Understanding

链接: http://arxiv.org/abs/2304.06906v1

原文摘要:
The use of pretrained backbones with fine-tuning has been successful for 2D
vision and natural language processing tasks, showing advantages over
task-specific networks. In this work, we introduce a pretrained 3D backbone,
called {\SST}, for 3D indoor scene understanding. We design a 3D Swin
transformer as our backbone network, which enables efficient self-attention on
sparse voxels with linear memory complexity, making the backbone scalable to
large models and datasets. We also introduce a generalized contextual relative
positional embedding scheme to capture various irregularities of point signals
for improved network performance. We pretrained a large {\SST} model on a
synthetic Structured3D dataset, which is an order of magnitude larger than the
ScanNet dataset. Our model pretrained on the synthetic dataset not only
generalizes well to downstream segmentation and detection on real 3D point
datasets, but also outperforms state-of-the-art methods on downstream tasks
with +2.3 mIoU and +2.2 mIoU on S3DIS Area5 and 6-fold semantic segmentation,
+1.8 mIoU on ScanNet segmentation (val), +1.9 mAP@0.5 on ScanNet detection, and
+8.1 mAP@0.5 on S3DIS detection. A series of extensive ablation studies further
validate the scalability, generality, and superior performance enabled by our
approach. The code and models are available at
https://github.com/microsoft/Swin3D .

中文翻译:
以下是符合学术规范的中文翻译：

【摘要】基于预训练主干网络进行微调的方法已在二维视觉和自然语言处理任务中展现出超越任务专用网络的优越性。本研究提出了一种名为{\SST}的三维预训练主干网络，用于三维室内场景理解。我们设计了基于三维Swin Transformer的主干网络，该网络能以线性内存复杂度在稀疏体素上实现高效自注意力机制，使得主干网络可扩展至大型模型与数据集。同时，我们提出了一种广义上下文相对位置编码方案，通过捕捉点信号的各种不规则特征来提升网络性能。我们在合成数据集Structured3D上预训练了大型{\SST}模型，该数据集规模比ScanNet数据集大一个数量级。基于合成数据预训练的模型不仅能够很好地迁移至真实三维点云数据集的分割与检测任务，还在多个下游任务中实现了性能突破：在S3DIS数据集Area5和6-fold语义分割任务上分别提升+2.3 mIoU和+2.2 mIoU，ScanNet分割任务（验证集）提升+1.8 mIoU，ScanNet检测任务提升+1.9 mAP@0.5，S3DIS检测任务提升+8.1 mAP@0.5。大量消融实验进一步验证了该方法在可扩展性、泛化能力和性能提升方面的优势。代码与模型已开源：https://github.com/microsoft/Swin3D。

（说明：翻译严格遵循以下原则：
1. 专业术语标准化：如"self-attention"译为"自注意力机制"，"mAP@0.5"保留原指标格式
2. 被动语态转化："are pretrained"转为主动式"预训练了"
3. 长句拆分：将原文复合句按中文习惯分解为多个短句
4. 数据呈现规范化：所有性能指标保留原文数字格式
5. 学术用语准确："ablation studies"译为"消融实验"而非"对比实验"
6. 补充必要说明：如"(val)"明确标注为"验证集"）
