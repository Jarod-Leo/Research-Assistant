# MixLoRA: Enhancing Large Language Models Fine-Tuning with LoRA based Mixture of Experts

链接: http://arxiv.org/abs/2404.15159v1

原文摘要:
Fine-tuning Large Language Models (LLMs) is a common practice to adapt
pre-trained models for specific applications. While methods like LoRA have
effectively addressed GPU memory constraints during fine-tuning, their
performance often falls short, especially in multi-task scenarios. In contrast,
Mixture-of-Expert (MoE) models, such as Mixtral 8x7B, demonstrate remarkable
performance in multi-task learning scenarios while maintaining a reduced
parameter count. However, the resource requirements of these MoEs remain
challenging, particularly for consumer-grade GPUs with less than 24GB memory.
To tackle these challenges, we propose MixLoRA, an approach to construct a
resource-efficient sparse MoE model based on LoRA. MixLoRA inserts multiple
LoRA-based experts within the feed-forward network block of a frozen
pre-trained dense model and employs a commonly used top-k router. Unlike other
LoRA-based MoE methods, MixLoRA enhances model performance by utilizing
independent attention-layer LoRA adapters. Additionally, an auxiliary load
balance loss is employed to address the imbalance problem of the router. Our
evaluations show that MixLoRA improves about 9% accuracy compared to
state-of-the-art PEFT methods in multi-task learning scenarios. We also propose
a new high-throughput framework to alleviate the computation and memory
bottlenecks during the training and inference of MOE models. This framework
reduces GPU memory consumption by 40% and token computation latency by 30%
during both training and inference.

中文翻译:
微调大型语言模型（LLMs）是使预训练模型适应特定应用的常见方法。尽管LoRA等方法有效缓解了微调过程中的GPU内存限制，但其性能表现往往不足，尤其在多任务场景下。相比之下，混合专家模型（MoE）如Mixtral 8x7B在多任务学习中展现出卓越性能，同时保持较低参数量。然而，这类MoE模型的资源需求仍具挑战性，尤其对内存小于24GB的消费级GPU而言。

为解决这些问题，我们提出MixLoRA——一种基于LoRA构建高效稀疏MoE模型的方法。该方法在冻结的预训练稠密模型前馈网络块中插入多个基于LoRA的专家模块，并采用常见的top-k路由机制。与其他基于LoRA的MoE方法不同，MixLoRA通过独立的注意力层LoRA适配器提升模型性能，同时引入辅助负载均衡损失以解决路由不均衡问题。评估表明，MixLoRA在多任务学习场景中比当前最先进的PEFT方法准确率提升约9%。我们还提出新型高吞吐量框架，缓解MoE模型训练和推理时的计算与内存瓶颈，使GPU内存消耗降低40%，令牌计算延迟减少30%。
