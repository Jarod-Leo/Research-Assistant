# RET-LLM: Towards a General Read-Write Memory for Large Language Models

链接: http://arxiv.org/abs/2305.14322v1

原文摘要:
Large language models (LLMs) have significantly advanced the field of natural
language processing (NLP) through their extensive parameters and comprehensive
data utilization. However, existing LLMs lack a dedicated memory unit, limiting
their ability to explicitly store and retrieve knowledge for various tasks. In
this paper, we propose RET-LLM a novel framework that equips LLMs with a
general write-read memory unit, allowing them to extract, store, and recall
knowledge from the text as needed for task performance. Inspired by Davidsonian
semantics theory, we extract and save knowledge in the form of triplets. The
memory unit is designed to be scalable, aggregatable, updatable, and
interpretable. Through qualitative evaluations, we demonstrate the superiority
of our proposed framework over baseline approaches in question answering tasks.
Moreover, our framework exhibits robust performance in handling temporal-based
question answering tasks, showcasing its ability to effectively manage
time-dependent information.

中文翻译:
大型语言模型（LLMs）凭借其庞大的参数量与全面的数据利用能力，显著推动了自然语言处理（NLP）领域的发展。然而现有LLMs缺乏专用记忆单元，限制了其显式存储和调用知识以应对多样化任务的能力。本文提出RET-LLM创新框架，通过为LLMs配备通用读写记忆单元，使其能够根据任务需求从文本中提取、存储和调用知识。受戴维森语义学理论启发，我们采用三元组形式进行知识提取与存储。该记忆单元具备可扩展性、可聚合性、可更新性和可解释性等特性。通过定性评估，我们证明所提框架在问答任务中优于基线方法。此外，该框架在处理时序性问答任务时展现出强劲性能，体现了其有效管理时间依存信息的能力。
