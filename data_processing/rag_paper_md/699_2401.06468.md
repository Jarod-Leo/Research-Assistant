# Adapting Large Language Models for Document-Level Machine Translation

链接: http://arxiv.org/abs/2401.06468v1

原文摘要:
Large language models (LLMs) have significantly advanced various natural
language processing (NLP) tasks. Recent research indicates that
moderately-sized LLMs often outperform larger ones after task-specific
fine-tuning. This study focuses on adapting LLMs for document-level machine
translation (DocMT) for specific language pairs. We first investigate the
impact of prompt strategies on translation performance and then conduct
extensive experiments using two fine-tuning methods, three LLM backbones, and
18 translation tasks across nine language pairs. Our results show that
specialized models can sometimes surpass GPT-4 in translation performance but
still face issues like off-target translation due to error propagation in
decoding. We provide an in-depth analysis of these LLMs tailored for DocMT,
examining translation errors, discourse phenomena, strategies for training and
inference, the data efficiency of parallel documents, recent test set
evaluations, and zero-shot crosslingual transfer. Our findings highlight the
strengths and limitations of LLM-based DocMT models and provide a foundation
for future research.

中文翻译:
大型语言模型（LLMs）显著推动了各类自然语言处理（NLP）任务的发展。近期研究表明，经过特定任务微调的中等规模LLMs往往能超越更大规模的模型。本研究专注于将LLMs适配于特定语言对的文档级机器翻译（DocMT）。我们首先探究了提示策略对翻译性能的影响，随后采用两种微调方法、三种LLM主干架构，在九组语言对的18项翻译任务上展开广泛实验。结果表明：专用模型在翻译性能上有时能超越GPT-4，但仍存在解码过程中错误传播导致的译文偏离等问题。我们对这些专为DocMT优化的LLMs进行了深度分析，涵盖翻译错误类型、篇章现象处理、训练与推理策略、平行语料的数据效率、最新测试集评估以及零样本跨语言迁移等方面。研究发现既揭示了基于LLM的DocMT模型优势，也明确了其局限性，为未来研究奠定了基础。
