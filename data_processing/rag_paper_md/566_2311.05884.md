# Hiformer: Heterogeneous Feature Interactions Learning with Transformers for Recommender Systems

链接: http://arxiv.org/abs/2311.05884v1

原文摘要:
Learning feature interaction is the critical backbone to building recommender
systems. In web-scale applications, learning feature interaction is extremely
challenging due to the sparse and large input feature space; meanwhile,
manually crafting effective feature interactions is infeasible because of the
exponential solution space. We propose to leverage a Transformer-based
architecture with attention layers to automatically capture feature
interactions. Transformer architectures have witnessed great success in many
domains, such as natural language processing and computer vision. However,
there has not been much adoption of Transformer architecture for feature
interaction modeling in industry. We aim at closing the gap. We identify two
key challenges for applying the vanilla Transformer architecture to web-scale
recommender systems: (1) Transformer architecture fails to capture the
heterogeneous feature interactions in the self-attention layer; (2) The serving
latency of Transformer architecture might be too high to be deployed in
web-scale recommender systems. We first propose a heterogeneous self-attention
layer, which is a simple yet effective modification to the self-attention layer
in Transformer, to take into account the heterogeneity of feature interactions.
We then introduce \textsc{Hiformer} (\textbf{H}eterogeneous
\textbf{I}nteraction Trans\textbf{former}) to further improve the model
expressiveness. With low-rank approximation and model pruning, \hiformer enjoys
fast inference for online deployment. Extensive offline experiment results
corroborates the effectiveness and efficiency of the \textsc{Hiformer} model.
We have successfully deployed the \textsc{Hiformer} model to a real world large
scale App ranking model at Google Play, with significant improvement in key
engagement metrics (up to +2.66\%).

中文翻译:
学习特征交互是构建推荐系统的核心支柱。在网络级应用中，由于输入特征空间的高维稀疏性，特征交互学习面临极大挑战；同时，由于解空间呈指数级增长，人工设计有效特征交互几乎不可行。我们提出采用基于Transformer的注意力架构来自动捕捉特征交互。Transformer架构已在自然语言处理和计算机视觉等领域取得巨大成功，但在工业界的特征交互建模中尚未得到广泛应用。本研究致力于弥合这一差距。

我们发现将标准Transformer架构应用于网络级推荐系统存在两大关键挑战：（1）自注意力层难以捕捉异构特征交互；（2）架构的推理延迟可能过高而无法满足线上服务需求。首先，我们提出异构自注意力层——通过对Transformer自注意力层进行简洁而有效的改进，使其能够建模特征交互的异质性。继而推出\textsc{Hiformer}（\textbf{异}构\textbf{交}互Trans\textbf{former}）架构以进一步增强模型表达能力。通过低秩近似和模型剪枝技术，\hiformer实现了高效的线上推理部署。大量离线实验验证了\textsc{Hiformer}模型的有效性与高效性。目前该模型已成功部署于Google Play的大规模应用排序系统，关键用户参与指标获得显著提升（最高达+2.66%）。
