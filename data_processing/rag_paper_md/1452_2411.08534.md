# Neural Topic Modeling with Large Language Models in the Loop

链接: http://arxiv.org/abs/2411.08534v1

原文摘要:
Topic modeling is a fundamental task in natural language processing, allowing
the discovery of latent thematic structures in text corpora. While Large
Language Models (LLMs) have demonstrated promising capabilities in topic
discovery, their direct application to topic modeling suffers from issues such
as incomplete topic coverage, misalignment of topics, and inefficiency. To
address these limitations, we propose LLM-ITL, a novel LLM-in-the-loop
framework that integrates LLMs with Neural Topic Models (NTMs). In LLM-ITL,
global topics and document representations are learned through the NTM.
Meanwhile, an LLM refines these topics using an Optimal Transport (OT)-based
alignment objective, where the refinement is dynamically adjusted based on the
LLM's confidence in suggesting topical words for each set of input words. With
the flexibility of being integrated into many existing NTMs, the proposed
approach enhances the interpretability of topics while preserving the
efficiency of NTMs in learning topics and document representations. Extensive
experiments demonstrate that LLM-ITL helps NTMs significantly improve their
topic interpretability while maintaining the quality of document
representation. Our code and datasets will be available at Github.

中文翻译:
主题建模是自然语言处理中的一项基础任务，旨在发现文本语料库中潜在的语义结构。尽管大语言模型（LLMs）在主题发现方面展现出卓越能力，但其直接应用于主题建模时仍存在主题覆盖不全、主题错位和效率低下等问题。为此，我们提出LLM-ITL——一种创新的大语言模型循环框架，通过将LLMs与神经主题模型（NTMs）相融合来解决这些局限。在该框架中，全局主题和文档表征由NTM学习生成，同时LLM基于最优传输（OT）对齐目标对这些主题进行优化：系统会根据LLM对每组输入词生成主题词的置信度，动态调整优化过程。该方法可灵活集成至多种现有NTM架构，在保持NTMs高效学习能力的同时显著提升主题可解释性。大量实验表明，LLM-ITL能帮助NTMs在维持文档表征质量的前提下，使主题可解释性获得显著提升。相关代码与数据集将在Github平台开源。
