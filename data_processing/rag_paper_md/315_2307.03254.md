# Vision Language Transformers: A Survey

链接: http://arxiv.org/abs/2307.03254v1

原文摘要:
Vision language tasks, such as answering questions about or generating
captions that describe an image, are difficult tasks for computers to perform.
A relatively recent body of research has adapted the pretrained transformer
architecture introduced in \citet{vaswani2017attention} to vision language
modeling. Transformer models have greatly improved performance and versatility
over previous vision language models. They do so by pretraining models on a
large generic datasets and transferring their learning to new tasks with minor
changes in architecture and parameter values. This type of transfer learning
has become the standard modeling practice in both natural language processing
and computer vision. Vision language transformers offer the promise of
producing similar advancements in tasks which require both vision and language.
In this paper, we provide a broad synthesis of the currently available research
on vision language transformer models and offer some analysis of their
strengths, limitations and some open questions that remain.

中文翻译:
视觉语言任务，例如回答关于图像的问题或生成描述图像的标题，对计算机而言是极具挑战性的任务。近期一系列研究借鉴了\citet{vaswani2017attention}提出的预训练Transformer架构，将其应用于视觉语言建模。相较于早期的视觉语言模型，Transformer模型在性能与泛化能力上实现了显著提升。其核心在于利用大规模通用数据集进行预训练，并通过微调架构与参数将学习成果迁移至新任务。此类迁移学习已成为自然语言处理与计算机视觉领域的标准建模范式。视觉语言Transformer模型有望在需要跨模态理解的任务中带来同等突破。本文系统梳理了当前视觉语言Transformer模型的研究进展，并深入分析了其优势、局限性以及尚未解决的关键问题。
