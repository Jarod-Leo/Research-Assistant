# Extracting Paragraphs from LLM Token Activations

链接: http://arxiv.org/abs/2409.06328v1

原文摘要:
Generative large language models (LLMs) excel in natural language processing
tasks, yet their inner workings remain underexplored beyond token-level
predictions. This study investigates the degree to which these models decide
the content of a paragraph at its onset, shedding light on their contextual
understanding. By examining the information encoded in single-token
activations, specifically the "\textbackslash n\textbackslash n" double newline
token, we demonstrate that patching these activations can transfer significant
information about the context of the following paragraph, providing further
insights into the model's capacity to plan ahead.

中文翻译:
生成式大型语言模型（LLMs）在自然语言处理任务中表现卓越，但其内部工作机制在词汇级预测之外仍缺乏深入探索。本研究通过分析模型在段落起始阶段对内容决策的程度，揭示了其对上下文的理解能力。通过考察单标记激活（特别是“\n\n”双换行符标记）中编码的信息，我们证明修补这些激活能传递关于后续段落上下文的重要信息，从而进一步阐明模型进行前瞻性规划的能力。
