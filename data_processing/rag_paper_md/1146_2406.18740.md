# Re-Ranking Step by Step: Investigating Pre-Filtering for Re-Ranking with Large Language Models

链接: http://arxiv.org/abs/2406.18740v1

原文摘要:
Large Language Models (LLMs) have been revolutionizing a myriad of natural
language processing tasks with their diverse zero-shot capabilities. Indeed,
existing work has shown that LLMs can be used to great effect for many tasks,
such as information retrieval (IR), and passage ranking. However, current
state-of-the-art results heavily lean on the capabilities of the LLM being
used. Currently, proprietary, and very large LLMs such as GPT-4 are the highest
performing passage re-rankers. Hence, users without the resources to leverage
top of the line LLMs, or ones that are closed source, are at a disadvantage. In
this paper, we investigate the use of a pre-filtering step before passage
re-ranking in IR. Our experiments show that by using a small number of human
generated relevance scores, coupled with LLM relevance scoring, it is
effectively possible to filter out irrelevant passages before re-ranking. Our
experiments also show that this pre-filtering then allows the LLM to perform
significantly better at the re-ranking task. Indeed, our results show that
smaller models such as Mixtral can become competitive with much larger
proprietary models (e.g., ChatGPT and GPT-4).

中文翻译:
大型语言模型（LLMs）凭借其多样化的零样本能力，正在彻底改变众多自然语言处理任务。现有研究已证实，LLMs能高效应用于信息检索（IR）和段落排序等任务。然而，当前最先进的结果高度依赖于所使用LLM的性能。目前，诸如GPT-4等专有且规模庞大的LLMs在段落重排序任务中表现最优。因此，无法利用顶尖LLMs或闭源模型的用户处于明显劣势。本文探讨了在信息检索段落重排序前增加预过滤步骤的可行性。实验表明，通过结合少量人工生成的相关性评分与LLM相关性评分，可有效在重排序前筛除无关段落。研究还发现，这种预过滤能显著提升LLM在重排序任务中的表现。值得注意的是，实验结果显示，Mixtral等较小模型经此处理后，其性能可与ChatGPT、GPT-4等大型专有模型相媲美。
