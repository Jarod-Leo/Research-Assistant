# Re-Ranking Step by Step: Investigating Pre-Filtering for Re-Ranking with Large Language Models

链接: http://arxiv.org/abs/2406.18740v1

原文摘要:
Large Language Models (LLMs) have been revolutionizing a myriad of natural
language processing tasks with their diverse zero-shot capabilities. Indeed,
existing work has shown that LLMs can be used to great effect for many tasks,
such as information retrieval (IR), and passage ranking. However, current
state-of-the-art results heavily lean on the capabilities of the LLM being
used. Currently, proprietary, and very large LLMs such as GPT-4 are the highest
performing passage re-rankers. Hence, users without the resources to leverage
top of the line LLMs, or ones that are closed source, are at a disadvantage. In
this paper, we investigate the use of a pre-filtering step before passage
re-ranking in IR. Our experiments show that by using a small number of human
generated relevance scores, coupled with LLM relevance scoring, it is
effectively possible to filter out irrelevant passages before re-ranking. Our
experiments also show that this pre-filtering then allows the LLM to perform
significantly better at the re-ranking task. Indeed, our results show that
smaller models such as Mixtral can become competitive with much larger
proprietary models (e.g., ChatGPT and GPT-4).

中文翻译:
大型语言模型（LLMs）凭借其多样化的零样本能力，正在彻底改变众多自然语言处理任务。现有研究表明，LLMs在信息检索（IR）和段落重排序等任务中能发挥卓越效果。然而，当前最先进的成果高度依赖于所使用LLM的性能水平。目前，诸如GPT-4等闭源的超大规模专有LLM是表现最优异的段落重排序工具，这使得无法获取顶尖LLM资源或只能使用闭源模型的用户处于劣势地位。本文研究了在信息检索的段落重排序环节前增加预过滤步骤的可行性。实验表明，通过结合少量人工生成的相关性评分与LLM相关性评分，能有效在重排序前过滤无关段落。我们的实验还证实，这种预过滤机制能显著提升LLM在重排序任务中的表现。研究结果显示，采用该方案后，Mixtral等较小规模模型的性能甚至可以与ChatGPT、GPT-4等庞大专有模型相媲美。
