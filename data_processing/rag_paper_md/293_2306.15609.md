# Exploring Durham University Physics exams with Large Language Models

链接: http://arxiv.org/abs/2306.15609v1

原文摘要:
The emergence of advanced Natural Language Processing (NLP) models like
ChatGPT has raised concerns among universities regarding AI-driven exam
completion. This paper provides a comprehensive evaluation of the proficiency
of GPT-4 and GPT-3.5 in answering a set of 42 exam papers derived from 10
distinct physics courses, administered at Durham University over the span of
2018 to 2022, totalling 593 questions and 2504 available marks. These exams,
spanning both undergraduate and postgraduate levels, include traditional
pre-COVID and adaptive COVID-era formats. Questions from the years 2018-2020
were designed for pre-COVID in person adjudicated examinations whereas the
2021-2022 exams were set for varying COVID-adapted conditions including
open-book conditions. To ensure a fair evaluation of AI performances, the exams
completed by AI were assessed by the original exam markers. However, due to
staffing constraints, only the aforementioned 593 out of the total 1280
questions were marked. GPT-4 and GPT-3.5 scored an average of 49.4\% and
38.6\%, respectively, suggesting only the weaker students would potential
improve their marks if using AI. For exams from the pre-COVID era, the average
scores for GPT-4 and GPT-3.5 were 50.8\% and 41.6\%, respectively. However,
post-COVID, these dropped to 47.5\% and 33.6\%. Thus contrary to expectations,
the change to less fact-based questions in the COVID era did not significantly
impact AI performance for the state-of-the-art models such as GPT-4. These
findings suggest that while current AI models struggle with university-level
Physics questions, an improving trend is observable. The code used for
automated AI completion is made publicly available for further research.

中文翻译:
随着ChatGPT等先进自然语言处理（NLP）模型的出现，高校对人工智能完成考试的担忧日益加剧。本研究对GPT-4和GPT-3.5在解答杜伦大学2018至2022年间10门物理课程42份考卷（共593道试题，总分2504分）的表现进行了全面评估。这些涵盖本科与研究生阶段的考试，既包含传统疫情前线下监考模式，也包含疫情时期适应性考试形式。2018-2020年试题为疫情前闭卷考试设计，2021-2022年则调整为适应疫情的开卷考试等多样化形式。为确保公平性，人工智能答卷由原阅卷人评分，但因人力限制仅完成全部1280道题中593道的批改。结果显示，GPT-4与GPT-3.5平均得分率分别为49.4%和38.6%，表明当前仅有学业表现较弱的学生可能通过AI提升成绩。疫情前考试中，两模型得分率分别为50.8%和41.6%；而疫情后降至47.5%和33.6%。与预期相反，疫情时期减少事实型试题的转变并未对GPT-4等前沿模型产生显著影响。研究表明，虽然现有AI模型应对大学物理试题仍存在困难，但已呈现进步趋势。本研究使用的自动答题代码已开源以供后续研究。
