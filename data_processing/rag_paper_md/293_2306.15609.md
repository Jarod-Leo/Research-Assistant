# Exploring Durham University Physics exams with Large Language Models

链接: http://arxiv.org/abs/2306.15609v1

原文摘要:
The emergence of advanced Natural Language Processing (NLP) models like
ChatGPT has raised concerns among universities regarding AI-driven exam
completion. This paper provides a comprehensive evaluation of the proficiency
of GPT-4 and GPT-3.5 in answering a set of 42 exam papers derived from 10
distinct physics courses, administered at Durham University over the span of
2018 to 2022, totalling 593 questions and 2504 available marks. These exams,
spanning both undergraduate and postgraduate levels, include traditional
pre-COVID and adaptive COVID-era formats. Questions from the years 2018-2020
were designed for pre-COVID in person adjudicated examinations whereas the
2021-2022 exams were set for varying COVID-adapted conditions including
open-book conditions. To ensure a fair evaluation of AI performances, the exams
completed by AI were assessed by the original exam markers. However, due to
staffing constraints, only the aforementioned 593 out of the total 1280
questions were marked. GPT-4 and GPT-3.5 scored an average of 49.4\% and
38.6\%, respectively, suggesting only the weaker students would potential
improve their marks if using AI. For exams from the pre-COVID era, the average
scores for GPT-4 and GPT-3.5 were 50.8\% and 41.6\%, respectively. However,
post-COVID, these dropped to 47.5\% and 33.6\%. Thus contrary to expectations,
the change to less fact-based questions in the COVID era did not significantly
impact AI performance for the state-of-the-art models such as GPT-4. These
findings suggest that while current AI models struggle with university-level
Physics questions, an improving trend is observable. The code used for
automated AI completion is made publicly available for further research.

中文翻译:
随着ChatGPT等先进自然语言处理(NLP)模型的出现，高校对人工智能完成考试的担忧日益加剧。本文对GPT-4和GPT-3.5在解答杜伦大学2018至2022年间10门物理课程42份试卷（共593道题，2504分）的表现进行了全面评估。这些涵盖本科与研究生阶段的考试，既包含传统线下模式，也包含疫情时期的适应性调整形式。2018-2020年试题为疫情前闭卷考试设计，2021-2022年则调整为开卷等疫情应对形式。为确保公平性，AI答卷由原阅卷人评分，但因人力限制仅完成全部1280题中的593题批改。

研究显示，GPT-4与GPT-3.5平均得分率分别为49.4%和38.6%，表明仅成绩较弱学生可能通过AI提升分数。疫情前考试中，两者得分率为50.8%和41.6%；疫情后则降至47.5%和33.6%。与预期相反，疫情时期减少事实性命题的转变对GPT-4等前沿模型表现影响有限。这表明当前AI虽难以应对大学物理试题，但进步趋势明显。本研究使用的自动答题代码已开源以供后续研究。
