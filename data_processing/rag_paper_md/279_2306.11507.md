# TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models

链接: http://arxiv.org/abs/2306.11507v1

原文摘要:
Large Language Models (LLMs) such as ChatGPT, have gained significant
attention due to their impressive natural language processing capabilities. It
is crucial to prioritize human-centered principles when utilizing these models.
Safeguarding the ethical and moral compliance of LLMs is of utmost importance.
However, individual ethical issues have not been well studied on the latest
LLMs. Therefore, this study aims to address these gaps by introducing a new
benchmark -- TrustGPT. TrustGPT provides a comprehensive evaluation of LLMs in
three crucial areas: toxicity, bias, and value-alignment. Initially, TrustGPT
examines toxicity in language models by employing toxic prompt templates
derived from social norms. It then quantifies the extent of bias in models by
measuring quantifiable toxicity values across different groups. Lastly,
TrustGPT assesses the value of conversation generation models from both active
value-alignment and passive value-alignment tasks. Through the implementation
of TrustGPT, this research aims to enhance our understanding of the performance
of conversation generation models and promote the development of language
models that are more ethical and socially responsible.

中文翻译:
以ChatGPT为代表的大型语言模型(LLMs)凭借卓越的自然语言处理能力引发广泛关注。在应用这类模型时，必须坚持以人为本的原则，确保其输出内容符合伦理道德规范。然而目前针对最新LLMs的个体伦理问题研究尚不充分。为此，本研究通过构建新型评估基准TrustGPT来填补这一空白。TrustGPT从毒性、偏见和价值对齐三个关键维度对语言模型进行全面评估：首先基于社会规范构建毒性提示模板检测语言模型的毒性；其次通过测量不同群体间的可量化毒性值来评估模型偏见程度；最后从主动价值对齐和被动价值对齐两个任务维度评估对话生成模型的价值取向。通过TrustGPT的实施，本研究旨在深化对对话生成模型性能的理解，推动开发更具伦理性和社会责任感的语言模型。
