# TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models

链接: http://arxiv.org/abs/2306.11507v1

原文摘要:
Large Language Models (LLMs) such as ChatGPT, have gained significant
attention due to their impressive natural language processing capabilities. It
is crucial to prioritize human-centered principles when utilizing these models.
Safeguarding the ethical and moral compliance of LLMs is of utmost importance.
However, individual ethical issues have not been well studied on the latest
LLMs. Therefore, this study aims to address these gaps by introducing a new
benchmark -- TrustGPT. TrustGPT provides a comprehensive evaluation of LLMs in
three crucial areas: toxicity, bias, and value-alignment. Initially, TrustGPT
examines toxicity in language models by employing toxic prompt templates
derived from social norms. It then quantifies the extent of bias in models by
measuring quantifiable toxicity values across different groups. Lastly,
TrustGPT assesses the value of conversation generation models from both active
value-alignment and passive value-alignment tasks. Through the implementation
of TrustGPT, this research aims to enhance our understanding of the performance
of conversation generation models and promote the development of language
models that are more ethical and socially responsible.

中文翻译:
以下是符合要求的学术中文翻译：

以ChatGPT为代表的大语言模型（LLMs）凭借卓越的自然语言处理能力引发广泛关注。在应用这些模型时，必须坚持以人为本的原则，确保其符合伦理道德规范至关重要。然而目前针对最新大语言模型的个体伦理问题研究尚不充分。为此，本研究通过构建新型评估基准TrustGPT来填补这一空白。该基准从毒性、偏见和价值对齐三个核心维度对大语言模型进行全面评估：首先基于社会规范构建毒性提示模板来检测语言模型的毒性水平；其次通过测量不同群体间的量化毒性值来评估模型偏见程度；最后分别通过主动价值对齐和被动价值对齐任务来评估对话生成模型的价值取向。通过TrustGPT评估体系的实施，本研究旨在深化对对话生成模型性能的理解，推动开发更具伦理性和社会责任感的大语言模型。

（翻译说明：采用学术论文摘要的标准结构，保持被动语态与客观表述；专业术语如"value-alignment"统一译为"价值对齐"；长句拆分符合中文表达习惯；关键概念首次出现标注英文缩写；通过"构建""评估""检测"等动词体现研究动作的递进性）
