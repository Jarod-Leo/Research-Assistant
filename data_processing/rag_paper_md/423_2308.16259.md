# Materials Informatics Transformer: A Language Model for Interpretable Materials Properties Prediction

链接: http://arxiv.org/abs/2308.16259v1

原文摘要:
Recently, the remarkable capabilities of large language models (LLMs) have
been illustrated across a variety of research domains such as natural language
processing, computer vision, and molecular modeling. We extend this paradigm by
utilizing LLMs for material property prediction by introducing our model
Materials Informatics Transformer (MatInFormer). Specifically, we introduce a
novel approach that involves learning the grammar of crystallography through
the tokenization of pertinent space group information. We further illustrate
the adaptability of MatInFormer by incorporating task-specific data pertaining
to Metal-Organic Frameworks (MOFs). Through attention visualization, we uncover
the key features that the model prioritizes during property prediction. The
effectiveness of our proposed model is empirically validated across 14 distinct
datasets, hereby underscoring its potential for high throughput screening
through accurate material property prediction.

中文翻译:
近年来，大型语言模型（LLMs）在自然语言处理、计算机视觉和分子建模等多个研究领域展现出卓越能力。本研究通过开发材料信息学Transformer模型（MatInFormer），将这一范式拓展至材料性能预测领域。我们提出了一种创新方法，通过对相关空间群信息进行标记化处理来学习晶体学语法规则。进一步以金属有机框架（MOFs）材料为例，展示了MatInFormer整合任务特异性数据的适应能力。通过注意力可视化技术，揭示了模型在性能预测过程中关注的关键特征。在14个不同数据集上的实证研究表明，该模型能通过精确的材料性能预测实现高通量筛选，充分验证了其有效性。
