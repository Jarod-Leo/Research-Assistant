# TransPrompt v2: A Transferable Prompting Framework for Cross-task Text Classification

链接: http://arxiv.org/abs/2308.15010v1

原文摘要:
Text classification is one of the most imperative tasks in natural language
processing (NLP). Recent advances with pre-trained language models (PLMs) have
shown remarkable success on this task. However, the satisfying results obtained
by PLMs heavily depend on the large amounts of task-specific labeled data,
which may not be feasible in many application scenarios due to data access and
privacy constraints. The recently-proposed prompt-based fine-tuning paradigm
improves the performance of PLMs for few-shot text classification with
task-specific templates. Yet, it is unclear how the prompting knowledge can be
transferred across tasks, for the purpose of mutual reinforcement. We propose
TransPrompt v2, a novel transferable prompting framework for few-shot learning
across similar or distant text classification tasks. For learning across
similar tasks, we employ a multi-task meta-knowledge acquisition (MMA)
procedure to train a meta-learner that captures the cross-task transferable
knowledge. For learning across distant tasks, we further inject the task type
descriptions into the prompt, and capture the intra-type and inter-type prompt
embeddings among multiple distant tasks. Additionally, two de-biasing
techniques are further designed to make the trained meta-learner more
task-agnostic and unbiased towards any tasks. After that, the meta-learner can
be adapted to each specific task with better parameters initialization.
Extensive experiments show that TransPrompt v2 outperforms single-task and
cross-task strong baselines over multiple NLP tasks and datasets. We further
show that the meta-learner can effectively improve the performance of PLMs on
previously unseen tasks. In addition, TransPrompt v2 also outperforms strong
fine-tuning baselines when learning with full training sets.

中文翻译:
文本分类是自然语言处理（NLP）中最关键的任务之一。预训练语言模型（PLMs）的最新进展已在该任务上展现出显著成效。然而，PLMs所取得的理想效果高度依赖于大量任务特定的标注数据，这在数据获取受限或隐私保护要求的实际场景中往往难以实现。近期提出的基于提示微调的方法通过设计任务相关模板，提升了PLMs在小样本文本分类中的表现。但如何实现提示知识在任务间的迁移以达成协同增强，仍是一个未解难题。

本文提出TransPrompt v2——一个面向相似或异构文本分类任务的小样本学习可迁移提示框架。针对相似任务间的学习，我们采用多任务元知识获取（MMA）机制训练元学习器，以捕捉跨任务可迁移知识；对于异构任务，则通过注入任务类型描述至提示模板，并建模多任务间的类型内与类型间提示嵌入关系。此外，设计两种去偏技术使元学习器更具任务无关性，避免对特定任务的偏好。经此优化后，元学习器可通过更优的参数初始化适配具体任务。

大量实验表明，TransPrompt v2在多个NLP任务和数据集上均超越单任务及跨任务基线模型。研究进一步证实，该元学习器能有效提升PLMs在未见任务上的表现。值得注意的是，即使使用全量训练集时，TransPrompt v2也显著优于主流微调基线方法。
