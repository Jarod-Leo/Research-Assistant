# Blacks is to Anger as Whites is to Joy? Understanding Latent Affective Bias in Large Pre-trained Neural Language Models

链接: http://arxiv.org/abs/2301.09003v1

原文摘要:
Groundbreaking inventions and highly significant performance improvements in
deep learning based Natural Language Processing are witnessed through the
development of transformer based large Pre-trained Language Models (PLMs). The
wide availability of unlabeled data within human generated data deluge along
with self-supervised learning strategy helps to accelerate the success of large
PLMs in language generation, language understanding, etc. But at the same time,
latent historical bias/unfairness in human minds towards a particular gender,
race, etc., encoded unintentionally/intentionally into the corpora harms and
questions the utility and efficacy of large PLMs in many real-world
applications, particularly for the protected groups. In this paper, we present
an extensive investigation towards understanding the existence of "Affective
Bias" in large PLMs to unveil any biased association of emotions such as anger,
fear, joy, etc., towards a particular gender, race or religion with respect to
the downstream task of textual emotion detection. We conduct our exploration of
affective bias from the very initial stage of corpus level affective bias
analysis by searching for imbalanced distribution of affective words within a
domain, in large scale corpora that are used to pre-train and fine-tune PLMs.
Later, to quantify affective bias in model predictions, we perform an extensive
set of class-based and intensity-based evaluations using various bias
evaluation corpora. Our results show the existence of statistically significant
affective bias in the PLM based emotion detection systems, indicating biased
association of certain emotions towards a particular gender, race, and
religion.

中文翻译:
基于Transformer架构的大型预训练语言模型（PLMs）的发展，为深度学习驱动的自然语言处理领域带来了突破性发明和显著性能提升。海量人类生成数据中未标注信息的广泛可获取性，结合自监督学习策略，加速了大型PLMs在文本生成、语言理解等任务上的成功。然而与此同时，人类思维中潜藏的对特定性别、种族等群体的历史偏见/不公，无论有意或无意地编码进训练语料，损害并质疑了大型PLMs在现实应用（尤其是涉及受保护群体时）的实用性与有效性。本文通过系统性研究，揭示了大型PLMs中存在的"情感偏见"现象——即在文本情感检测这一下游任务中，模型对愤怒、恐惧、喜悦等情感与特定性别、种族或宗教之间存在的偏颇关联。我们从语料层面开始探索，通过分析预训练与微调PLMs所用的大规模语料库中情感词的不均衡分布，进行领域级情感偏见检测。进而采用基于类别和强度的多维评估方法，借助多种偏见评估语料库量化模型预测中的情感偏见。实验结果表明，基于PLMs的情感检测系统存在统计学上显著的情感偏见，证实了某些情感与特定性别、种族及宗教之间存在偏颇关联。
