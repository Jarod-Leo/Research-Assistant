# Blacks is to Anger as Whites is to Joy? Understanding Latent Affective Bias in Large Pre-trained Neural Language Models

链接: http://arxiv.org/abs/2301.09003v1

原文摘要:
Groundbreaking inventions and highly significant performance improvements in
deep learning based Natural Language Processing are witnessed through the
development of transformer based large Pre-trained Language Models (PLMs). The
wide availability of unlabeled data within human generated data deluge along
with self-supervised learning strategy helps to accelerate the success of large
PLMs in language generation, language understanding, etc. But at the same time,
latent historical bias/unfairness in human minds towards a particular gender,
race, etc., encoded unintentionally/intentionally into the corpora harms and
questions the utility and efficacy of large PLMs in many real-world
applications, particularly for the protected groups. In this paper, we present
an extensive investigation towards understanding the existence of "Affective
Bias" in large PLMs to unveil any biased association of emotions such as anger,
fear, joy, etc., towards a particular gender, race or religion with respect to
the downstream task of textual emotion detection. We conduct our exploration of
affective bias from the very initial stage of corpus level affective bias
analysis by searching for imbalanced distribution of affective words within a
domain, in large scale corpora that are used to pre-train and fine-tune PLMs.
Later, to quantify affective bias in model predictions, we perform an extensive
set of class-based and intensity-based evaluations using various bias
evaluation corpora. Our results show the existence of statistically significant
affective bias in the PLM based emotion detection systems, indicating biased
association of certain emotions towards a particular gender, race, and
religion.

中文翻译:
基于Transformer架构的大型预训练语言模型（PLMs）的发展，为深度学习驱动的自然语言处理领域带来了突破性创新与显著性能提升。海量人类生成数据中未标注文本的广泛可获得性，结合自监督学习策略，极大促进了大型PLMs在文本生成、语言理解等任务中的成功。然而与此同时，人类思维中潜藏的针对特定性别、种族等群体的历史性偏见/不公，无论有意或无意地编码进训练语料，损害了大型PLMs在现实场景（尤其是涉及受保护群体时）的实用性与有效性。

本文通过系统性研究，首次揭示了大型PLMs中存在的"情感偏见"现象——即在文本情感检测任务中，模型对愤怒、恐惧、喜悦等情感与特定性别、种族或宗教之间存在的偏颇关联。我们的探索始于语料层级的初始分析：通过检测PLMs预训练与微调所用的大规模语料库中，情感词汇在不同领域内的非均衡分布状况。继而采用基于类别与强度的多维评估框架，借助多种偏见评测语料库对模型预测中的情感偏见进行量化。实验结果表明，基于PLMs的情感检测系统存在统计学显著的情感偏见，证实了某些情感与特定性别、种族及宗教之间存在系统性偏态关联。
