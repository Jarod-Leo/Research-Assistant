# LegalPro-BERT: Classification of Legal Provisions by fine-tuning BERT Large Language Model

链接: http://arxiv.org/abs/2404.10097v1

原文摘要:
A contract is a type of legal document commonly used in organizations.
Contract review is an integral and repetitive process to avoid business risk
and liability. Contract analysis requires the identification and classification
of key provisions and paragraphs within an agreement. Identification and
validation of contract clauses can be a time-consuming and challenging task
demanding the services of trained and expensive lawyers, paralegals or other
legal assistants. Classification of legal provisions in contracts using
artificial intelligence and natural language processing is complex due to the
requirement of domain-specialized legal language for model training and the
scarcity of sufficient labeled data in the legal domain. Using general-purpose
models is not effective in this context due to the use of specialized legal
vocabulary in contracts which may not be recognized by a general model. To
address this problem, we propose the use of a pre-trained large language model
which is subsequently calibrated on legal taxonomy. We propose LegalPro-BERT, a
BERT transformer architecture model that we fine-tune to efficiently handle
classification task for legal provisions. We conducted experiments to measure
and compare metrics with current benchmark results. We found that LegalPro-BERT
outperforms the previous benchmark used for comparison in this research.

中文翻译:
合同是各类组织广泛采用的法律文书形式。合同审查作为规避商业风险与责任的关键环节，其过程具有重复性和系统性特征。合同分析的核心在于识别并归类协议中的核心条款与段落，这一过程往往耗时费力，通常需要依赖专业律师、法律助理等高价人力资源。由于法律领域需要专业术语进行模型训练且标注数据稀缺，运用人工智能和自然语言处理技术对合同法律条款进行分类面临巨大挑战。通用模型在此场景下效果欠佳，因其难以识别合同中特有的法律专业术语。针对这一问题，我们提出采用预训练大语言模型并结合法律分类体系进行校准的方案。具体而言，我们开发了LegalPro-BERT模型——基于BERT架构的改进模型，通过微调使其高效完成法律条款分类任务。实验结果表明，与现有基准相比，LegalPro-BERT在各项评估指标上均展现出更优性能。
