# Sharif-STR at SemEval-2024 Task 1: Transformer as a Regression Model for Fine-Grained Scoring of Textual Semantic Relations

链接: http://arxiv.org/abs/2407.12426v1

原文摘要:
Semantic Textual Relatedness holds significant relevance in Natural Language
Processing, finding applications across various domains. Traditionally,
approaches to STR have relied on knowledge-based and statistical methods.
However, with the emergence of Large Language Models, there has been a paradigm
shift, ushering in new methodologies. In this paper, we delve into the
investigation of sentence-level STR within Track A (Supervised) by leveraging
fine-tuning techniques on the RoBERTa transformer. Our study focuses on
assessing the efficacy of this approach across different languages. Notably,
our findings indicate promising advancements in STR performance, particularly
in Latin languages. Specifically, our results demonstrate notable improvements
in English, achieving a correlation of 0.82 and securing a commendable 19th
rank. Similarly, in Spanish, we achieved a correlation of 0.67, securing the
15th position. However, our approach encounters challenges in languages like
Arabic, where we observed a correlation of only 0.38, resulting in a 20th rank.

中文翻译:
语义文本关联性在自然语言处理领域具有重要价值，其应用场景广泛多样。传统研究方法主要依赖知识库和统计手段，但随着大语言模型的出现，该领域正经历方法论上的范式转变。本研究聚焦Track A（监督学习）中的句子级语义关联任务，通过对RoBERTa Transformer模型进行微调，系统评估了该方法在跨语言环境下的适用性。实验结果表明，该技术尤其在拉丁语系中表现突出：英语任务获得0.82的相关性分数（排名第19位），西班牙语达到0.67相关性（排名第15位）。然而在阿拉伯语等语言中效果欠佳，仅取得0.38相关性（排名第20位），揭示了当前方法存在的局限性。
