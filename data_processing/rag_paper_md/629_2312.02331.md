# Revisiting Topic-Guided Language Models

链接: http://arxiv.org/abs/2312.02331v1

原文摘要:
A recent line of work in natural language processing has aimed to combine
language models and topic models. These topic-guided language models augment
neural language models with topic models, unsupervised learning methods that
can discover document-level patterns of word use. This paper compares the
effectiveness of these methods in a standardized setting. We study four
topic-guided language models and two baselines, evaluating the held-out
predictive performance of each model on four corpora. Surprisingly, we find
that none of these methods outperform a standard LSTM language model baseline,
and most fail to learn good topics. Further, we train a probe of the neural
language model that shows that the baseline's hidden states already encode
topic information. We make public all code used for this study.

中文翻译:
近期自然语言处理领域的一系列研究致力于将语言模型与主题模型相结合。这类主题引导的语言模型通过主题模型（一种能发现文档层面词汇使用模式的无监督学习方法）来增强神经语言模型。本文在标准化环境下系统比较了这些方法的有效性，研究了四种主题引导语言模型和两种基线模型，并在四个语料库上评估了各模型的留出预测性能。研究发现：出乎意料的是，这些方法均未能超越标准LSTM语言模型基线，且大多数无法学习到优质主题；进一步通过设计神经语言模型的探测实验，证实基线模型的隐藏状态已编码了主题信息。本研究所有代码均已公开。
