# Automatic Generation of Behavioral Test Cases For Natural Language Processing Using Clustering and Prompting

链接: http://arxiv.org/abs/2408.00161v1

原文摘要:
Recent work in behavioral testing for natural language processing (NLP)
models, such as Checklist, is inspired by related paradigms in software
engineering testing. They allow evaluation of general linguistic capabilities
and domain understanding, hence can help evaluate conceptual soundness and
identify model weaknesses. However, a major challenge is the creation of test
cases. The current packages rely on semi-automated approach using manual
development which requires domain expertise and can be time consuming. This
paper introduces an automated approach to develop test cases by exploiting the
power of large language models and statistical techniques. It clusters the text
representations to carefully construct meaningful groups and then apply
prompting techniques to automatically generate Minimal Functionality Tests
(MFT). The well-known Amazon Reviews corpus is used to demonstrate our
approach. We analyze the behavioral test profiles across four different
classification algorithms and discuss the limitations and strengths of those
models.

中文翻译:
近期在自然语言处理（NLP）模型行为测试领域的研究，例如Checklist框架，借鉴了软件工程测试的相关范式。这类方法能够评估模型的语言理解能力与领域知识掌握程度，从而验证概念合理性并识别模型缺陷。然而，测试用例的创建面临重大挑战——现有工具包采用半自动化方法依赖人工开发，既需要领域专业知识又耗时费力。本文提出一种自动化测试用例生成方法，通过结合大语言模型与统计技术，先对文本表征进行聚类以构建语义分组，再运用提示工程自动生成最小功能测试集（MFT）。研究以知名亚马逊评论数据集为实验对象，对比分析了四种分类算法的行为测试特征剖面，并探讨了各模型的优势与局限性。
