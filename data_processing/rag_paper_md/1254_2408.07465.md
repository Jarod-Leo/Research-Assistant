# Large Language Models Prompting With Episodic Memory

链接: http://arxiv.org/abs/2408.07465v1

原文摘要:
Prompt optimization is essential for enhancing the performance of Large
Language Models (LLMs) in a range of Natural Language Processing (NLP) tasks,
particularly in scenarios of few-shot learning where training examples are
incorporated directly into the prompt. Despite the growing interest in
optimizing prompts with few-shot examples, existing methods for prompt
optimization are often resource-intensive or perform inadequately. In this
work, we propose PrOmpting with Episodic Memory (POEM), a novel prompt
optimization technique that is simple, efficient, and demonstrates strong
generalization capabilities. We approach prompt optimization as a Reinforcement
Learning (RL) challenge, using episodic memory to archive combinations of input
data, permutations of few-shot examples, and the rewards observed during
training. In the testing phase, we optimize the sequence of examples for each
test query by selecting the sequence that yields the highest total rewards from
the top-k most similar training examples in the episodic memory. Our results
show that POEM outperforms recent techniques like TEMPERA and RLPrompt by over
5.3% in various text classification tasks. Furthermore, our approach adapts
well to broader language understanding tasks, consistently outperforming
conventional heuristic methods for ordering examples.

中文翻译:
提示优化对于提升大语言模型（LLM）在自然语言处理（NLP）任务中的表现至关重要，尤其是在将训练样本直接嵌入提示的少样本学习场景中。尽管优化含少样本示例的提示日益受到关注，但现有方法往往资源消耗大或效果欠佳。本研究提出基于情景记忆的提示优化技术POEM，该方法简洁高效且展现出强大的泛化能力。我们将提示优化构建为强化学习（RL）问题，利用情景记忆归档输入数据组合、少样本示例排列及训练期间观测到的奖励值。在测试阶段，通过从情景记忆中选取与测试查询最相似的前k个训练样本所对应奖励总和最高的示例序列进行优化。实验结果表明，POEM在多种文本分类任务中性能超越TEMPERA和RLPrompt等最新技术达5.3%以上。此外，该方法能良好适配更广泛的语言理解任务，其表现持续优于传统的示例排序启发式方法。
