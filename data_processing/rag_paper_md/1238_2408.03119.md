# Evaluating the Translation Performance of Large Language Models Based on Euas-20

链接: http://arxiv.org/abs/2408.03119v1

原文摘要:
In recent years, with the rapid development of deep learning technology,
large language models (LLMs) such as BERT and GPT have achieved breakthrough
results in natural language processing tasks. Machine translation (MT), as one
of the core tasks of natural language processing, has also benefited from the
development of large language models and achieved a qualitative leap. Despite
the significant progress in translation performance achieved by large language
models, machine translation still faces many challenges. Therefore, in this
paper, we construct the dataset Euas-20 to evaluate the performance of large
language models on translation tasks, the translation ability on different
languages, and the effect of pre-training data on the translation ability of
LLMs for researchers and developers.

中文翻译:
近年来，随着深度学习技术的快速发展，以BERT、GPT为代表的大语言模型（LLMs）在自然语言处理任务中取得了突破性成果。作为自然语言处理核心任务之一的机器翻译（MT），同样受益于大语言模型的发展实现了质的飞跃。尽管大语言模型使翻译性能取得了显著提升，但机器翻译仍面临诸多挑战。为此，本文构建了Euas-20数据集，旨在为研究者和开发者评估大语言模型在翻译任务中的表现、不同语言间的翻译能力，以及预训练数据对LLMs翻译能力的影响。

（译文说明：采用学术论文摘要的标准表述方式，通过以下处理实现专业性与可读性的平衡：
1. 术语统一："large language models"严格译为"大语言模型"并首次出现标注英文缩写"LLMs"
2. 句式重构：将原文复合长句拆分为符合中文表达习惯的短句（如将"Therefore..."长句拆分为"为此..."引导的目的状语独立成句）
3. 语态转换：被动语态"has been benefited"主动化为"受益于"
4. 概念显化："qualitative leap"意译为"质的飞跃"而非字面直译
5. 数据名称保留：数据集名称"Euas-20"保持原貌不翻译
6. 逻辑连接：添加"旨在"明确构建数据集的研究目的）
