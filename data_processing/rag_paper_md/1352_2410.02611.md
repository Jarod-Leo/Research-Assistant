# IndicSentEval: How Effectively do Multilingual Transformer Models encode Linguistic Properties for Indic Languages?

链接: http://arxiv.org/abs/2410.02611v1

原文摘要:
Transformer-based models have revolutionized the field of natural language
processing. To understand why they perform so well and to assess their
reliability, several studies have focused on questions such as: Which
linguistic properties are encoded by these models, and to what extent? How
robust are these models in encoding linguistic properties when faced with
perturbations in the input text? However, these studies have mainly focused on
BERT and the English language. In this paper, we investigate similar questions
regarding encoding capability and robustness for 8 linguistic properties across
13 different perturbations in 6 Indic languages, using 9 multilingual
Transformer models (7 universal and 2 Indic-specific). To conduct this study,
we introduce a novel multilingual benchmark dataset, IndicSentEval, containing
approximately $\sim$47K sentences. Surprisingly, our probing analysis of
surface, syntactic, and semantic properties reveals that while almost all
multilingual models demonstrate consistent encoding performance for English,
they show mixed results for Indic languages. As expected, Indic-specific
multilingual models capture linguistic properties in Indic languages better
than universal models. Intriguingly, universal models broadly exhibit better
robustness compared to Indic-specific models, particularly under perturbations
such as dropping both nouns and verbs, dropping only verbs, or keeping only
nouns. Overall, this study provides valuable insights into probing and
perturbation-specific strengths and weaknesses of popular multilingual
Transformer-based models for different Indic languages. We make our code and
dataset publicly available [https://tinyurl.com/IndicSentEval}].

中文翻译:
基于Transformer的模型彻底改变了自然语言处理领域。为探究其卓越性能背后的原因并评估其可靠性，多项研究聚焦于以下问题：这些模型编码了哪些语言属性？编码程度如何？当输入文本受到干扰时，这些模型编码语言属性的鲁棒性表现如何？然而现有研究主要集中于BERT模型和英语。本文通过构建包含约47K句子的新型多语言基准数据集IndicSentEval，对6种印度语言在13种文本干扰下8种语言属性的编码能力与鲁棒性展开研究，涉及9种多语言Transformer模型（7种通用型与2种印度语言专用型）。令人惊讶的是，针对表层、句法和语义属性的探测分析表明：虽然几乎所有多语言模型对英语都表现出稳定的编码性能，但对印度语言却呈现参差不齐的结果。印度语言专用模型对印度语言属性的捕捉能力确实优于通用模型，但有趣的是，通用模型在鲁棒性方面普遍优于专用模型——尤其在同时删除名词动词、仅删除动词或仅保留名词等干扰场景下表现突出。本研究为不同印度语言下主流多语言Transformer模型的探测特性和干扰特异性优劣提供了重要见解。相关代码与数据集已公开[https://tinyurl.com/IndicSentEval]。
