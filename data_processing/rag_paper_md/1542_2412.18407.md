# A Statistical Framework for Ranking LLM-Based Chatbots

链接: http://arxiv.org/abs/2412.18407v1

原文摘要:
Large language models (LLMs) have transformed natural language processing,
with frameworks like Chatbot Arena providing pioneering platforms for
evaluating these models. By facilitating millions of pairwise comparisons based
on human judgments, Chatbot Arena has become a cornerstone in LLM evaluation,
offering rich datasets for ranking models in open-ended conversational tasks.
Building upon this foundation, we propose a statistical framework that
incorporates key advancements to address specific challenges in pairwise
comparison analysis. First, we introduce a factored tie model that enhances the
ability to handle ties -- an integral aspect of human-judged comparisons --
significantly improving the model's fit to observed data. Second, we extend the
framework to model covariance between competitors, enabling deeper insights
into performance relationships and facilitating intuitive groupings into
performance tiers. Third, we resolve optimization challenges arising from
parameter non-uniqueness by introducing novel constraints, ensuring stable and
interpretable parameter estimation. Through rigorous evaluation and extensive
experimentation, our framework demonstrates substantial improvements over
existing methods in modeling pairwise comparison data. To support
reproducibility and practical adoption, we release leaderbot, an open-source
Python package implementing our models and analyses.

中文翻译:
大型语言模型（LLMs）已彻底改变了自然语言处理领域，而Chatbot Arena等评估框架通过基于人类判断的数百万次两两比较，为开放式对话任务中的模型排名提供了丰富数据集，成为LLM评估的基石。在此基础之上，我们提出一个统计框架，通过三项关键创新解决配对比较分析中的特定挑战：首先，引入因子化平局模型，显著提升处理人类评判中固有平局现象的能力，使模型对观测数据的拟合度大幅提高；其次，扩展框架以建模竞争者间的协方差关系，不仅能深入解析性能关联，还可实现性能层级的直观分组；第三，通过创新性约束条件解决参数非唯一性导致的优化难题，确保参数估计的稳定性和可解释性。经严格验证与大量实验表明，该框架在配对比较数据建模上较现有方法有显著提升。为促进研究复现与实际应用，我们开源了实现该模型的Python工具包leaderbot。
