# Towards Better Understanding Table Instruction Tuning: Decoupling the Effects from Data versus Models

链接: http://arxiv.org/abs/2501.14717v1

原文摘要:
Recent advances in natural language processing have leveraged instruction
tuning to enhance Large Language Models (LLMs) for table-related tasks.
However, previous works train different base models with different training
data, lacking an apples-to-apples comparison across the result table LLMs. To
address this, we fine-tune base models from the Mistral, OLMo, and Phi families
on existing public training datasets. Our replication achieves performance on
par with or surpassing existing table LLMs, establishing new state-of-the-art
performance on Hitab, a table question-answering dataset. More importantly,
through systematic out-of-domain evaluation, we decouple the contributions of
training data and the base model, providing insight into their individual
impacts. In addition, we assess the effects of table-specific instruction
tuning on general-purpose benchmarks, revealing trade-offs between
specialization and generalization.

中文翻译:
自然语言处理领域的最新进展通过指令微调技术优化了大语言模型（LLMs）在表格相关任务中的表现。然而，现有研究采用不同基础模型和训练数据进行差异化训练，导致各表格LLM之间缺乏直接可比性。为此，我们基于Mistral、OLMo和Phi系列的基础模型，利用现有公开训练数据集进行微调。复现实验表明，其性能达到或超越了现有表格LLM水平，并在表格问答数据集Hitab上创造了新的性能标杆。更重要的是，通过系统性跨领域评估，我们解构了训练数据与基础模型的独立贡献，揭示了二者对模型性能的具体影响。此外，我们还评估了表格专用指令微调对通用基准测试的影响，发现了专业化与泛化能力之间的权衡关系。
