# Open Generative Large Language Models for Galician

链接: http://arxiv.org/abs/2406.13893v1

原文摘要:
Large language models (LLMs) have transformed natural language processing.
Yet, their predominantly English-centric training has led to biases and
performance disparities across languages. This imbalance marginalizes
minoritized languages, making equitable access to NLP technologies more
difficult for languages with lower resources, such as Galician. We present the
first two generative LLMs focused on Galician to bridge this gap. These models,
freely available as open-source resources, were trained using a GPT
architecture with 1.3B parameters on a corpus of 2.1B words. Leveraging
continual pretraining, we adapt to Galician two existing LLMs trained on larger
corpora, thus mitigating the data constraints that would arise if the training
were performed from scratch. The models were evaluated using human judgments
and task-based datasets from standardized benchmarks. These evaluations reveal
a promising performance, underscoring the importance of linguistic diversity in
generative models.

中文翻译:
大型语言模型（LLM）已经彻底改变了自然语言处理领域。然而，这些模型主要以英语为中心的训练方式导致了跨语言偏见和性能差异。这种不平衡使得资源较少的少数语言（如加利西亚语）被边缘化，难以公平获取自然语言处理技术。为弥合这一差距，我们推出了首个专注于加利西亚语的两个生成式大型语言模型。这些采用GPT架构、具有13亿参数的开源模型基于21亿单词语料库训练而成，现已作为开放资源免费发布。通过持续预训练技术，我们将两个基于更大规模语料库训练的语言模型适配至加利西亚语，从而避免了从零开始训练面临的数据限制问题。基于人工评估和标准化基准测试数据集的实验表明，这些模型展现出令人期待的性能表现，凸显了生成式模型中语言多样性的重要意义。

（翻译说明：采用学术论文摘要的标准表述方式，通过以下处理实现专业性与可读性的平衡：
1. 术语统一："continual pretraining"译为专业术语"持续预训练"，"generative models"译为"生成式模型"
2. 句式重构：将原文复合长句拆分为符合中文表达习惯的短句（如第二句的因果逻辑处理）
3. 概念显化："lower resources"译为"资源较少"而非字面直译，更符合中文技术文献表述
4. 被动语态转化："were evaluated"译为主动式的"实验表明"
5. 文化适配：保留"Galician"专业译名"加利西亚语"并首次出现时标注原文）
