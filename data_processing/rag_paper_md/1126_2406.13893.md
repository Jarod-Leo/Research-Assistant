# Open Generative Large Language Models for Galician

链接: http://arxiv.org/abs/2406.13893v1

原文摘要:
Large language models (LLMs) have transformed natural language processing.
Yet, their predominantly English-centric training has led to biases and
performance disparities across languages. This imbalance marginalizes
minoritized languages, making equitable access to NLP technologies more
difficult for languages with lower resources, such as Galician. We present the
first two generative LLMs focused on Galician to bridge this gap. These models,
freely available as open-source resources, were trained using a GPT
architecture with 1.3B parameters on a corpus of 2.1B words. Leveraging
continual pretraining, we adapt to Galician two existing LLMs trained on larger
corpora, thus mitigating the data constraints that would arise if the training
were performed from scratch. The models were evaluated using human judgments
and task-based datasets from standardized benchmarks. These evaluations reveal
a promising performance, underscoring the importance of linguistic diversity in
generative models.

中文翻译:
大型语言模型（LLMs）已彻底改变了自然语言处理领域。然而，其以英语为核心的训练模式导致不同语言间存在偏见与性能差异。这种不平衡使得资源较少的少数语言（如加利西亚语）被边缘化，更难公平获取NLP技术。为弥合这一鸿沟，我们推出了首个专注于加利西亚语的两款生成式LLM。这些开源模型采用GPT架构，基于21亿单词语料库训练而成，参数量达13亿。通过持续预训练技术，我们将两个已训练于更大语料库的现有LLM适配至加利西亚语，从而避免了从零训练面临的数据限制问题。模型评估结合了人工评判与标准化基准测试中的任务数据集，结果显示其性能表现优异，凸显了生成式模型中语言多样性的重要性。
