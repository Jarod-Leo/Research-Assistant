# Cheaply Evaluating Inference Efficiency Metrics for Autoregressive Transformer APIs

链接: http://arxiv.org/abs/2305.02440v1

原文摘要:
Large language models (LLMs) power many state-of-the-art systems in natural
language processing. However, these models are extremely computationally
expensive, even at inference time, raising the natural question: when is the
extra cost of deploying a larger model worth the anticipated boost in
capabilities? Better understanding this tradeoff fundamentally could benefit
from an inference efficiency metric that is both (i) easily comparable across
models from different providers, and (ii) representative of the true cost of
running queries in an isolated performance environment. Unfortunately, access
to LLMs today is largely restricted to black-box text generation APIs and raw
runtimes measured through this interface do not satisfy these desiderata: model
providers can apply various software and hardware optimizations orthogonal to
the model, and models served on shared infrastructure are susceptible to
performance contention. To circumvent these problems, we propose a new metric
for comparing inference efficiency across models. This metric puts models on
equal footing as though they were served (i) on uniform hardware and software,
and (ii) without performance contention. We call this metric the
\emph{idealized runtime}, and we propose a methodology to efficiently estimate
this metric for autoregressive Transformer models. We also propose cost-aware
variants that incorporate the number of accelerators needed to serve the model.
Using these metrics, we compare ten state-of-the-art LLMs to provide the first
analysis of inference efficiency-capability tradeoffs; we make several
observations from this analysis, including the fact that the superior inference
runtime performance of certain APIs is often a byproduct of optimizations
within the API rather than the underlying model. Our methodology also
facilitates the efficient comparison of different software and hardware stacks.

中文翻译:
大型语言模型（LLM）已成为自然语言处理领域众多前沿系统的核心驱动力。然而这些模型即使在推理阶段也需消耗巨大的计算资源，这引发了一个本质问题：部署更大模型带来的预期能力提升，何时能抵消其额外成本？要深入理解这种权衡关系，关键在于建立一种同时满足以下两个条件的推理效率评估指标：(i) 能够跨不同供应商的模型进行便捷比较；(ii) 能真实反映在独立性能环境中运行查询的实际成本。当前LLM主要通过黑箱文本生成API提供服务，基于此接口测量的原始运行时数据无法满足这些要求：模型供应商可能采用与模型无关的软硬件优化方案，且在共享基础设施上部署的模型易受性能争用影响。

为此，我们提出了一种新型跨模型推理效率比较指标。该指标通过模拟两种理想条件实现模型间的公平对比：(i) 在统一的软硬件环境下运行；(ii) 无性能争用干扰。我们将此指标命名为"理想化运行时"，并针对自回归Transformer模型提出高效估算方法。同时我们还设计了考虑部署所需加速器数量的成本感知变体指标。

基于这些指标，我们对十种前沿LLM进行了首次推理效率-能力权衡分析，获得多项重要发现：某些API表现出的卓越推理运行时性能，往往源自API内部的优化策略而非底层模型本身。该评估方法还能有效支持不同软硬件技术栈的对比分析，为模型部署决策提供科学依据。
