# Large Language Models are Good Attackers: Efficient and Stealthy Textual Backdoor Attacks

链接: http://arxiv.org/abs/2408.11587v1

原文摘要:
With the burgeoning advancements in the field of natural language processing
(NLP), the demand for training data has increased significantly. To save costs,
it has become common for users and businesses to outsource the labor-intensive
task of data collection to third-party entities. Unfortunately, recent research
has unveiled the inherent risk associated with this practice, particularly in
exposing NLP systems to potential backdoor attacks. Specifically, these attacks
enable malicious control over the behavior of a trained model by poisoning a
small portion of the training data. Unlike backdoor attacks in computer vision,
textual backdoor attacks impose stringent requirements for attack stealthiness.
However, existing attack methods meet significant trade-off between
effectiveness and stealthiness, largely due to the high information entropy
inherent in textual data. In this paper, we introduce the Efficient and
Stealthy Textual backdoor attack method, EST-Bad, leveraging Large Language
Models (LLMs). Our EST-Bad encompasses three core strategies: optimizing the
inherent flaw of models as the trigger, stealthily injecting triggers with
LLMs, and meticulously selecting the most impactful samples for backdoor
injection. Through the integration of these techniques, EST-Bad demonstrates an
efficient achievement of competitive attack performance while maintaining
superior stealthiness compared to prior methods across various text classifier
datasets.

中文翻译:
随着自然语言处理（NLP）领域的蓬勃发展，对训练数据的需求显著增加。为节约成本，用户和企业普遍将数据收集这一劳动密集型任务外包给第三方机构。然而最新研究揭示，这种做法存在固有风险，尤其可能导致NLP系统遭受后门攻击。此类攻击通过污染少量训练数据，即可实现对训练模型行为的恶意操控。与计算机视觉领域的后门攻击不同，文本后门攻击对隐蔽性有着严苛要求。但由于文本数据固有的高信息熵特性，现有攻击方法在有效性与隐蔽性之间面临显著权衡。本文提出基于大语言模型（LLMs）的高效隐蔽文本后门攻击方法EST-Bad，其核心策略包含三点：以模型固有缺陷为触发机制的优化设计、利用LLMs实现触发器的隐蔽注入，以及精准筛选最具影响力的样本进行后门植入。通过整合这些技术，EST-Bad在多种文本分类数据集上展现出卓越性能——在保持优于现有方法隐蔽性的同时，高效实现了具有竞争力的攻击效果。
