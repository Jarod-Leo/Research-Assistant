# Generalized Probabilistic Attention Mechanism in Transformers

链接: http://arxiv.org/abs/2410.15578v1

原文摘要:
The Transformer architecture has become widely adopted due to its
demonstrated success, attributed to the attention mechanism at its core.
Despite these successes, the attention mechanism of Transformers is associated
with two well-known issues: rank-collapse and gradient vanishing. In this
paper, we present a theoretical analysis that it is inherently difficult to
address both issues simultaneously in the conventional attention mechanism. To
handle these issues, we introduce a novel class of attention mechanism,
referred to as generalized probabilistic attention mechanism (GPAM), and its
dual-attention implementation within the Transformer architecture. Unlike
conventional attention mechanisms, GPAM allows for negative attention scores
while preserving a fixed total sum. We provide theoretical evidence that the
proposed dual-attention GPAM (daGPAM) effectively mitigates both the
rank-collapse and gradient vanishing issues which are difficult to resolve
simultaneously with the conventional attention mechanisms. Furthermore, we
empirically validate this theoretical evidence, demonstrating the superiority
of daGPAM compared to other alternative attention mechanisms that were proposed
to address the same issues. Additionally, we demonstrate the practical benefits
of GPAM in natural language processing tasks, such as language modeling and
neural machine translation.

中文翻译:
Transformer架构因其显著的成功而得到广泛采用，这主要归功于其核心的注意力机制。然而，这种注意力机制存在两个公认的问题：秩崩溃（rank-collapse）和梯度消失。本文通过理论分析证明，在传统注意力机制框架下，同时解决这两个问题存在固有困难。为此，我们提出了一类新型注意力机制——广义概率注意力机制（GPAM），并在Transformer架构中实现了其双注意力版本。与传统机制不同，GPAM在保持注意力分数总和恒定的前提下允许负值存在。我们通过理论论证表明，所提出的双注意力GPAM（daGPAM）能有效缓解传统注意力机制难以同时解决的秩崩溃和梯度消失问题。此外，我们通过实验验证了这一理论结论，证明daGPAM在解决同类问题时优于其他替代性注意力机制。最后，我们在自然语言处理任务（如语言建模和神经机器翻译）中验证了GPAM的实际应用价值。
