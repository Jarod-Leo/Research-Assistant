# Generalized Probabilistic Attention Mechanism in Transformers

链接: http://arxiv.org/abs/2410.15578v1

原文摘要:
The Transformer architecture has become widely adopted due to its
demonstrated success, attributed to the attention mechanism at its core.
Despite these successes, the attention mechanism of Transformers is associated
with two well-known issues: rank-collapse and gradient vanishing. In this
paper, we present a theoretical analysis that it is inherently difficult to
address both issues simultaneously in the conventional attention mechanism. To
handle these issues, we introduce a novel class of attention mechanism,
referred to as generalized probabilistic attention mechanism (GPAM), and its
dual-attention implementation within the Transformer architecture. Unlike
conventional attention mechanisms, GPAM allows for negative attention scores
while preserving a fixed total sum. We provide theoretical evidence that the
proposed dual-attention GPAM (daGPAM) effectively mitigates both the
rank-collapse and gradient vanishing issues which are difficult to resolve
simultaneously with the conventional attention mechanisms. Furthermore, we
empirically validate this theoretical evidence, demonstrating the superiority
of daGPAM compared to other alternative attention mechanisms that were proposed
to address the same issues. Additionally, we demonstrate the practical benefits
of GPAM in natural language processing tasks, such as language modeling and
neural machine translation.

中文翻译:
Transformer架构因其核心的注意力机制所展现的成功而得到广泛应用。尽管如此，该注意力机制仍存在两个公认问题：秩塌缩与梯度消失。本文通过理论分析表明，传统注意力机制本质上难以同时解决这两个问题。为此，我们提出了一类新型注意力机制——广义概率注意力机制（GPAM），及其在Transformer架构中的双注意力实现形式。与传统机制不同，GPAM在保持注意力分数总和恒定的前提下允许负值存在。我们通过理论证明，所提出的双注意力GPAM（daGPAM）能有效缓解传统注意力机制难以兼顾的秩塌缩和梯度消失问题。进一步通过实证研究验证了这一理论结论，显示出daGPAM相较于其他针对相同问题提出的替代方案具有显著优势。此外，我们在自然语言处理任务（如语言建模和神经机器翻译）中验证了GPAM的实际应用价值。
