# Transformer-Based Contextualized Language Models Joint with Neural Networks for Natural Language Inference in Vietnamese

链接: http://arxiv.org/abs/2411.13407v2

原文摘要:
Natural Language Inference (NLI) is a task within Natural Language Processing
(NLP) that holds value for various AI applications. However, there have been
limited studies on Natural Language Inference in Vietnamese that explore the
concept of joint models. Therefore, we conducted experiments using various
combinations of contextualized language models (CLM) and neural networks. We
use CLM to create contextualized work presentations and use Neural Networks for
classification. Furthermore, we have evaluated the strengths and weaknesses of
each joint model and identified the model failure points in the Vietnamese
context. The highest F1 score in this experiment, up to 82.78% in the benchmark
dataset (ViNLI). By conducting experiments with various models, the most
considerable size of the CLM is XLM-R (355M). That combination has consistently
demonstrated superior performance compared to fine-tuning strong pre-trained
language models like PhoBERT (+6.58%), mBERT (+19.08%), and XLM-R (+0.94%) in
terms of F1-score. This article aims to introduce a novel approach or model
that attains improved performance for Vietnamese NLI. Overall, we find that the
joint approach of CLM and neural networks is simple yet capable of achieving
high-quality performance, which makes it suitable for applications that require
efficient resource utilization.

中文翻译:
自然语言推理（Natural Language Inference, NLI）作为自然语言处理（NLP）领域的一项重要任务，在多种人工智能应用中具有重要价值。然而目前针对越南语自然语言推理的研究较为有限，特别是涉及联合模型架构的探索更为匮乏。为此，我们通过结合多种上下文语言模型（CLM）与神经网络进行了系列实验：采用CLM生成上下文敏感的语义表征，并利用神经网络进行分类决策。此外，我们系统评估了各联合模型的优劣特性，特别识别了这些模型在越南语环境下的失效场景。实验在基准数据集（ViNLI）中取得了82.78%的最高F1值。通过多模型对比实验发现，当采用参数量达3.55亿的XLM-R作为CLM时，该组合在F1分数上始终优于直接微调PhoBERT（+6.58%）、mBERT（+19.08%）和XLM-R（+0.94%）等强预训练语言模型的表现。本文旨在提出一种针对越南语NLI任务的新型优化方案。总体而言，CLM与神经网络的联合策略虽结构简洁，却能实现高性能表现，特别适合需要高效利用计算资源的应用场景。
