# Transformer-Based Contextualized Language Models Joint with Neural Networks for Natural Language Inference in Vietnamese

链接: http://arxiv.org/abs/2411.13407v2

原文摘要:
Natural Language Inference (NLI) is a task within Natural Language Processing
(NLP) that holds value for various AI applications. However, there have been
limited studies on Natural Language Inference in Vietnamese that explore the
concept of joint models. Therefore, we conducted experiments using various
combinations of contextualized language models (CLM) and neural networks. We
use CLM to create contextualized work presentations and use Neural Networks for
classification. Furthermore, we have evaluated the strengths and weaknesses of
each joint model and identified the model failure points in the Vietnamese
context. The highest F1 score in this experiment, up to 82.78% in the benchmark
dataset (ViNLI). By conducting experiments with various models, the most
considerable size of the CLM is XLM-R (355M). That combination has consistently
demonstrated superior performance compared to fine-tuning strong pre-trained
language models like PhoBERT (+6.58%), mBERT (+19.08%), and XLM-R (+0.94%) in
terms of F1-score. This article aims to introduce a novel approach or model
that attains improved performance for Vietnamese NLI. Overall, we find that the
joint approach of CLM and neural networks is simple yet capable of achieving
high-quality performance, which makes it suitable for applications that require
efficient resource utilization.

中文翻译:
自然语言推理（NLI）作为自然语言处理（NLP）领域的重要任务，对多种人工智能应用具有显著价值。然而当前针对越南语自然语言推理的研究中，联合模型概念的探索仍较为有限。为此，我们采用多种语境化语言模型（CLM）与神经网络的组合方案进行实验：通过CLM生成语境化词表征，并利用神经网络进行分类。研究系统评估了各联合模型的优劣特性，同时识别了这些模型在越南语环境中的失效点。实验在基准数据集（ViNLI）中取得的最高F1值达82.78%。通过多模型对比测试发现，参数量达3.55亿的XLM-R作为最大规模CLM时，其组合方案在F1分数上始终优于PhoBERT（+6.58%）、mBERT（+19.08%）和XLM-R（+0.94%）等强预训练语言模型的微调效果。本文旨在提出一种能提升越南语NLI性能的创新方法或模型。总体而言，CLM与神经网络的联合方法虽结构简洁，却能实现高质量性能，特别适合需要高效资源利用的应用场景。
