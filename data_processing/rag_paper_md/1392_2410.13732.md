# Reducing the Transformer Architecture to a Minimum

链接: http://arxiv.org/abs/2410.13732v1

原文摘要:
Transformers are a widespread and successful model architecture, particularly
in Natural Language Processing (NLP) and Computer Vision (CV). The essential
innovation of this architecture is the Attention Mechanism, which solves the
problem of extracting relevant context information from long sequences in NLP
and realistic scenes in CV. A classical neural network component, a Multi-Layer
Perceptron (MLP), complements the attention mechanism. Its necessity is
frequently justified by its capability of modeling nonlinear relationships.
However, the attention mechanism itself is nonlinear through its internal use
of similarity measures. A possible hypothesis is that this nonlinearity is
sufficient for modeling typical application problems. As the MLPs usually
contain the most trainable parameters of the whole model, their omission would
substantially reduce the parameter set size. Further components can also be
reorganized to reduce the number of parameters. Under some conditions, query
and key matrices can be collapsed into a single matrix of the same size. The
same is true about value and projection matrices, which can also be omitted
without eliminating the substance of the attention mechanism. Initially, the
similarity measure was defined asymmetrically, with peculiar properties such as
that a token is possibly dissimilar to itself. A possible symmetric definition
requires only half of the parameters. We have laid the groundwork by testing
widespread CV benchmarks: MNIST and CIFAR-10. The tests have shown that
simplified transformer architectures (a) without MLP, (b) with collapsed
matrices, and (c) symmetric similarity matrices exhibit similar performance as
the original architecture, saving up to 90% of parameters without hurting the
classification performance.

中文翻译:
Transformer作为一种广泛且成功的模型架构，在自然语言处理（NLP）和计算机视觉（CV）领域表现尤为突出。其核心创新在于注意力机制，该机制有效解决了从NLP长序列和CV真实场景中提取相关上下文信息的关键问题。传统神经网络组件多层感知机（MLP）通常作为注意力机制的补充模块，其必要性常被归因于非线性关系建模能力。然而注意力机制本身通过内部相似度计算已具备非线性特性，我们推测这种非线性可能足以应对典型应用问题的建模需求。由于MLP通常包含整个模型中最多的可训练参数，移除该模块将大幅缩减参数量。其他组件亦可进行重构优化：查询矩阵与键矩阵在某些条件下可合并为单一矩阵；值矩阵与投影矩阵同样可合并甚至完全移除，而不会破坏注意力机制的实质功能。最初提出的非对称相似度度量定义存在特殊性质（如标记可能与自身不相似），而对称化定义仅需半数参数即可实现。我们通过在MNIST和CIFAR-10等主流CV基准测试上的实验验证：简化版Transformer架构（a）去除MLP模块（b）合并矩阵（c）采用对称相似度矩阵后，在保持分类性能不变的前提下，参数量最高可减少90%，且模型表现与原架构相当。
