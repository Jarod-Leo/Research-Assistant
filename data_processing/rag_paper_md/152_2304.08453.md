# Improving Autoregressive NLP Tasks via Modular Linearized Attention

链接: http://arxiv.org/abs/2304.08453v1

原文摘要:
Various natural language processing (NLP) tasks necessitate models that are
efficient and small based on their ultimate application at the edge or in other
resource-constrained environments. While prior research has reduced the size of
these models, increasing computational efficiency without considerable
performance impacts remains difficult, especially for autoregressive tasks.
This paper proposes modular linearized attention (MLA), which combines multiple
efficient attention mechanisms, including cosFormer, to maximize inference
quality while achieving notable speedups. We validate this approach on several
autoregressive NLP tasks, including speech-to-text neural machine translation
(S2T NMT), speech-to-text simultaneous translation (SimulST), and
autoregressive text-to-spectrogram, noting efficiency gains on TTS and
competitive performance for NMT and SimulST during training and inference.

中文翻译:
多种自然语言处理（NLP）任务需要基于边缘计算或其他资源受限环境最终应用的轻量化高效模型。尽管已有研究成功压缩了模型规模，但在不明显影响性能的前提下提升计算效率仍具挑战性，尤其对于自回归任务。本文提出模块化线性注意力机制（MLA），通过融合包括cosFormer在内的多种高效注意力机制，在显著加速的同时保持最优推理质量。我们在语音到文本神经机器翻译（S2T NMT）、语音到文本同步翻译（SimulST）以及自回归文本到声谱图等任务上验证了该方案，在训练和推理阶段均观察到TTS任务的效率提升，同时NMT与SimulST任务保持了竞争力性能表现。
