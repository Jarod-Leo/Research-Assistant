# UQABench: Evaluating User Embedding for Prompting LLMs in Personalized Question Answering

链接: http://arxiv.org/abs/2502.19178v1

原文摘要:
Large language models (LLMs) achieve remarkable success in natural language
processing (NLP). In practical scenarios like recommendations, as users
increasingly seek personalized experiences, it becomes crucial to incorporate
user interaction history into the context of LLMs to enhance personalization.
However, from a practical utility perspective, user interactions' extensive
length and noise present challenges when used directly as text prompts. A
promising solution is to compress and distill interactions into compact
embeddings, serving as soft prompts to assist LLMs in generating personalized
responses. Although this approach brings efficiency, a critical concern
emerges: Can user embeddings adequately capture valuable information and prompt
LLMs? To address this concern, we propose \name, a benchmark designed to
evaluate the effectiveness of user embeddings in prompting LLMs for
personalization. We establish a fair and standardized evaluation process,
encompassing pre-training, fine-tuning, and evaluation stages. To thoroughly
evaluate user embeddings, we design three dimensions of tasks: sequence
understanding, action prediction, and interest perception. These evaluation
tasks cover the industry's demands in traditional recommendation tasks, such as
improving prediction accuracy, and its aspirations for LLM-based methods, such
as accurately understanding user interests and enhancing the user experience.
We conduct extensive experiments on various state-of-the-art methods for
modeling user embeddings. Additionally, we reveal the scaling laws of
leveraging user embeddings to prompt LLMs. The benchmark is available online.

中文翻译:
大语言模型（LLMs）在自然语言处理（NLP）领域取得了显著成就。在推荐等实际场景中，随着用户对个性化体验需求的日益增长，将用户交互历史融入LLM上下文以增强个性化变得至关重要。然而从实用角度看，用户交互数据因长度冗长且含有噪声，直接作为文本提示面临挑战。一种可行方案是将交互信息压缩蒸馏为紧凑的嵌入向量，作为软提示辅助LLM生成个性化响应。尽管这种方法提升了效率，但核心问题随之浮现：用户嵌入能否充分捕获有效信息并成功驱动LLM？

针对该问题，我们提出\name基准测试框架，用于评估用户嵌入在驱动LLM实现个性化时的有效性。我们建立了公平标准化的三阶段评估流程，涵盖预训练、微调与评估环节。为全面检验用户嵌入性能，设计了三大任务维度：序列理解、行为预测和兴趣感知。这些评估任务既包含行业对传统推荐任务（如提升预测准确率）的需求，也涵盖对基于LLM方法（如精准理解用户兴趣、优化用户体验）的期待。我们对多种前沿的用户嵌入建模方法进行了广泛实验，同时揭示了利用用户嵌入驱动LLM的规模效应规律。本基准测试平台已开源。
