# A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks

链接: http://arxiv.org/abs/2307.12114v1

原文摘要:
We evaluate four state-of-the-art instruction-tuned large language models
(LLMs) -- ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca -- on a set of 13
real-world clinical and biomedical natural language processing (NLP) tasks in
English, such as named-entity recognition (NER), question-answering (QA),
relation extraction (RE), etc. Our overall results demonstrate that the
evaluated LLMs begin to approach performance of state-of-the-art models in
zero- and few-shot scenarios for most tasks, and particularly well for the QA
task, even though they have never seen examples from these tasks before.
However, we observed that the classification and RE tasks perform below what
can be achieved with a specifically trained model for the medical field, such
as PubMedBERT. Finally, we noted that no LLM outperforms all the others on all
the studied tasks, with some models being better suited for certain tasks than
others.

中文翻译:
我们评估了四种先进的指令微调大语言模型（LLMs）——ChatGPT、Flan-T5 UL2、Tk-Instruct和Alpaca——在英语临床与生物医学自然语言处理（NLP）领域的13项实际任务中的表现，包括命名实体识别（NER）、问答（QA）、关系抽取（RE）等。总体结果表明，尽管这些LLMs从未接触过相关任务样本，但在多数任务的零样本和小样本场景下，其性能已开始接近最先进模型水平，尤其在QA任务中表现突出。然而我们发现，分类任务和RE任务的性能仍低于PubMedBERT等医疗领域专用训练模型。最后值得注意的是，在所有研究任务中没有任何LLM能全面超越其他模型，不同模型对特定任务展现出各自的优势。
