# TPPoet: Transformer-Based Persian Poem Generation using Minimal Data and Advanced Decoding Techniques

链接: http://arxiv.org/abs/2312.02125v1

原文摘要:
Recent advances in language models (LMs), have demonstrated significant
efficacy in tasks related to the arts and humanities. While LMs have exhibited
exceptional performance across a wide range of natural language processing
tasks, there are notable challenges associated with their utilization on small
datasets and their ability to replicate more creative human capacities. In this
study, we aim to address these challenges by training a Persian classical
poetry generation model using a transformer architecture on a specialized
dataset with no pretraining. Additionally, we propose a novel decoding method
to enhance coherence and meaningfulness in the generated poetry, effectively
managing the tradeoff between diversity and quality. Furthermore, the results
of our training approach and the proposed decoding method are evaluated through
comprehensive set of automatic and human evaluations and showed its superior
capability to generate coherent and meaningful poetry in compare to other
decoding methods and an existing Persian large language model (LLM).

中文翻译:
语言模型（LMs）的最新进展在艺术与人文学科相关任务中展现出显著效能。尽管LMs在广泛自然语言处理任务中表现卓越，但其在小数据集上的应用及对人类创造性能力的复现仍存在明显挑战。本研究通过基于专业数据集（无预训练）采用Transformer架构训练波斯古典诗歌生成模型，旨在解决这些问题。我们提出了一种新颖的解码方法以增强生成诗歌的连贯性与意义表达，有效平衡多样性与质量之间的权衡。此外，通过自动化与人工评估相结合的全面测试，我们的训练方案及所提解码方法相较于其他解码方式及现有波斯大型语言模型（LLM），展现出生成连贯且富有意义诗歌的卓越能力。
