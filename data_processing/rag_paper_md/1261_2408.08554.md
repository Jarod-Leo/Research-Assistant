# ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large Language Models

链接: http://arxiv.org/abs/2408.08554v1

原文摘要:
Large Language Models (LLMs) have revolutionized natural language processing
tasks. However, their practical application is constrained by substantial
memory and computational demands. Post-training quantization (PTQ) is
considered an effective method to accelerate LLM inference. Despite its growing
popularity in LLM model compression, PTQ deployment faces two major challenges.
First, low-bit quantization leads to performance degradation. Second,
restricted by the limited integer computing unit type on GPUs, quantized matrix
operations with different precisions cannot be effectively accelerated. To
address these issues, we introduce a novel arbitrary-bit quantization algorithm
and inference framework, ABQ-LLM. It achieves superior performance across
various quantization settings and enables efficient arbitrary-precision
quantized inference on the GPU. ABQ-LLM introduces several key innovations: (1)
a distribution correction method for transformer blocks to mitigate
distribution differences caused by full quantization of weights and
activations, improving performance at low bit-widths. (2) the bit balance
strategy to counteract performance degradation from asymmetric distribution
issues at very low bit-widths (e.g., 2-bit). (3) an innovative quantization
acceleration framework that reconstructs the quantization matrix multiplication
of arbitrary precision combinations based on BTC (Binary TensorCore)
equivalents, gets rid of the limitations of INT4/INT8 computing units. ABQ-LLM
can convert each component bit width gain into actual acceleration gain,
maximizing performance under mixed precision(e.g., W6A6, W2A8). Based on W2*A8
quantization configuration on LLaMA-7B model, it achieved a WikiText2
perplexity of 7.59 (2.17$\downarrow $ vs 9.76 in AffineQuant). Compared to
SmoothQuant, we realized 1.6$\times$ acceleration improvement and 2.7$\times$
memory compression gain.

中文翻译:
大型语言模型（LLMs）已彻底改变了自然语言处理任务，但其实际应用受限于巨大的内存与计算需求。后训练量化（PTQ）被视为加速LLM推理的有效方法。尽管PTQ在LLM模型压缩中日益流行，其部署仍面临两大挑战：一是低位量化导致性能下降；二是受限于GPU上有限的整数计算单元类型，不同精度的量化矩阵运算无法有效加速。为此，我们提出了一种创新的任意位量化算法与推理框架ABQ-LLM，该方案在多种量化设置下均能实现卓越性能，并支持GPU上高效的任意精度量化推理。

ABQ-LLM包含多项关键创新：（1）针对Transformer块的分布校正方法，通过缓解权重和激活值全量化引发的分布差异，显著提升低位宽下的性能表现；（2）位平衡策略有效抵消极低位宽（如2比特）下非对称分布问题导致的性能衰减；（3）基于BTC（二进制张量核）等效原理重构任意精度组合的量化矩阵乘法框架，突破INT4/INT8计算单元限制的创新加速架构。该框架能将各组件位宽增益转化为实际加速收益，在混合精度场景（如W6A6、W2A8）下实现性能最大化。

基于LLaMA-7B模型的W2*A8量化配置，ABQ-LLM在WikiText2数据集上达到7.59的困惑度（较AffineQuant的9.76降低2.17）。相比SmoothQuant方案，我们实现了1.6倍的加速提升与2.7倍的内存压缩增益。
