# Leveraging FourierKAN Classification Head for Pre-Trained Transformer-based Text Classification

链接: http://arxiv.org/abs/2408.08803v1

原文摘要:
In resource constraint settings, adaptation to downstream classification
tasks involves fine-tuning the final layer of a classifier (i.e. classification
head) while keeping rest of the model weights frozen. Multi-Layer Perceptron
(MLP) heads fine-tuned with pre-trained transformer backbones have long been
the de facto standard for text classification head fine-tuning. However, the
fixed non-linearity of MLPs often struggles to fully capture the nuances of
contextual embeddings produced by pre-trained models, while also being
computationally expensive. In our work, we investigate the efficacy of KAN and
its variant, Fourier KAN (FR-KAN), as alternative text classification heads.
Our experiments reveal that FR-KAN significantly outperforms MLPs with an
average improvement of 10% in accuracy and 11% in F1-score across seven
pre-trained transformer models and four text classification tasks. Beyond
performance gains, FR-KAN is more computationally efficient and trains faster
with fewer parameters. These results underscore the potential of FR-KAN to
serve as a lightweight classification head, with broader implications for
advancing other Natural Language Processing (NLP) tasks.

中文翻译:
在资源受限的环境中，适应下游分类任务通常涉及微调分类器的最后一层（即分类头），同时保持模型其余权重冻结。长期以来，采用预训练Transformer主干网络微调的多层感知机（MLP）分类头被视为文本分类微调的事实标准。然而，MLP固有的固定非线性特性往往难以充分捕捉预训练模型生成的上下文嵌入的细微差异，且计算成本较高。本研究探讨了KAN及其变体傅里叶KAN（FR-KAN）作为替代文本分类头的有效性。实验表明，在七种预训练Transformer模型和四项文本分类任务中，FR-KAN平均准确率提升10%、F1分数提高11%，显著优于MLP。除性能优势外，FR-KAN还具有更高的计算效率，能以更少参数实现更快训练。这些发现凸显了FR-KAN作为轻量级分类头的潜力，对推动其他自然语言处理（NLP）任务发展具有更广泛的启示意义。
