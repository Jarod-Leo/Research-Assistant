# United We Pretrain, Divided We Fail! Representation Learning for Time Series by Pretraining on 75 Datasets at Once

链接: http://arxiv.org/abs/2402.15404v1

原文摘要:
In natural language processing and vision, pretraining is utilized to learn
effective representations. Unfortunately, the success of pretraining does not
easily carry over to time series due to potential mismatch between sources and
target. Actually, common belief is that multi-dataset pretraining does not work
for time series! Au contraire, we introduce a new self-supervised contrastive
pretraining approach to learn one encoding from many unlabeled and diverse time
series datasets, so that the single learned representation can then be reused
in several target domains for, say, classification. Specifically, we propose
the XD-MixUp interpolation method and the Soft Interpolation Contextual
Contrasting (SICC) loss. Empirically, this outperforms both supervised training
and other self-supervised pretraining methods when finetuning on low-data
regimes. This disproves the common belief: We can actually learn from multiple
time series datasets, even from 75 at once.

中文翻译:
在自然语言处理与视觉领域，预训练技术被广泛用于学习有效表征。然而由于源域与目标域间可能存在的失配问题，这一成功经验难以直接迁移至时间序列数据。当前学界普遍认为：跨数据集预训练对时间序列无效！与此相反，我们提出了一种新型自监督对比预训练方法，通过从多个未标注的异构时间序列数据集中学习统一编码，使得该单一表征可复用于分类等下游任务。具体而言，我们设计了XD-MixUp插值方法和软插值上下文对比（SICC）损失函数。实证研究表明，在低数据量微调场景下，该方法性能显著优于有监督训练及其他自监督预训练方案。这一发现颠覆了传统认知：我们不仅能从多时间序列数据集中学习，甚至可同时利用多达75个数据集进行联合预训练。
