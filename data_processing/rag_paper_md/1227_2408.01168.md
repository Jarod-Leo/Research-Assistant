# Misinforming LLMs: vulnerabilities, challenges and opportunities

链接: http://arxiv.org/abs/2408.01168v1

原文摘要:
Large Language Models (LLMs) have made significant advances in natural
language processing, but their underlying mechanisms are often misunderstood.
Despite exhibiting coherent answers and apparent reasoning behaviors, LLMs rely
on statistical patterns in word embeddings rather than true cognitive
processes. This leads to vulnerabilities such as "hallucination" and
misinformation. The paper argues that current LLM architectures are inherently
untrustworthy due to their reliance on correlations of sequential patterns of
word embedding vectors. However, ongoing research into combining generative
transformer-based models with fact bases and logic programming languages may
lead to the development of trustworthy LLMs capable of generating statements
based on given truth and explaining their self-reasoning process.

中文翻译:
大型语言模型（LLMs）在自然语言处理领域取得了显著进展，但其底层机制常被误解。尽管这些模型能生成连贯答案并表现出看似合理的推理行为，其本质仍是依赖词嵌入向量的统计模式，而非真实的认知过程。这种特性导致模型易产生"幻觉"和错误信息等缺陷。本文指出，当前LLM架构由于完全依赖词嵌入向量序列模式的相关性，本质上具有不可靠性。然而，将基于生成式Transformer的模型与事实库及逻辑编程语言相结合的研究，有望推动可信LLM的发展——这类新型模型不仅能基于给定事实生成陈述，还能解释其自主推理过程。
