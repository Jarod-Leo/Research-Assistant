# ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning

链接: http://arxiv.org/abs/2304.05613v1

原文摘要:
Over the last few years, large language models (LLMs) have emerged as the
most important breakthroughs in natural language processing (NLP) that
fundamentally transform research and developments in the field. ChatGPT
represents one of the most exciting LLM systems developed recently to showcase
impressive skills for language generation and highly attract public attention.
Among various exciting applications discovered for ChatGPT in English, the
model can process and generate texts for multiple languages due to its
multilingual training data. Given the broad adoption of ChatGPT for English in
different problems and areas, a natural question is whether ChatGPT can also be
applied effectively for other languages or it is necessary to develop more
language-specific technologies. The answer to this question requires a thorough
evaluation of ChatGPT over multiple tasks with diverse languages and large
datasets (i.e., beyond reported anecdotes), which is still missing or limited
in current research. Our work aims to fill this gap for the evaluation of
ChatGPT and similar LLMs to provide more comprehensive information for
multilingual NLP applications. While this work will be an ongoing effort to
include additional experiments in the future, our current paper evaluates
ChatGPT on 7 different tasks, covering 37 diverse languages with high, medium,
low, and extremely low resources. We also focus on the zero-shot learning
setting for ChatGPT to improve reproducibility and better simulate the
interactions of general users. Compared to the performance of previous models,
our extensive experimental results demonstrate a worse performance of ChatGPT
for different NLP tasks and languages, calling for further research to develop
better models and understanding for multilingual learning.

中文翻译:
近年来，大型语言模型（LLMs）已成为自然语言处理（NLP）领域最具突破性的进展，从根本上改变了该领域的研究与发展格局。ChatGPT作为近期最受瞩目的LLM系统之一，凭借其卓越的语言生成能力吸引了广泛关注。尽管ChatGPT在英语应用中展现出令人振奋的多样性，但由于其多语言训练数据的特性，该模型同样具备处理与生成多语种文本的能力。鉴于ChatGPT在英语场景下的广泛应用，一个核心问题随之产生：该模型是否能同等高效地适用于其他语言？抑或仍需发展更具语言针对性的技术？解答这一问题需基于多任务、多语言和大规模数据集（而非零散案例）对ChatGPT进行全面评估，而这正是当前研究中所欠缺或不足的。

本研究致力于填补ChatGPT及同类LLMs在多语言评估方面的空白，为NLP的多语言应用提供更全面的参考依据。尽管未来将持续扩展实验范围，本文已针对7项不同任务、涵盖37种资源水平各异（从高资源到极低资源）的语言展开评估。我们特别聚焦ChatGPT的零样本学习设置，以提升结果可复现性，更真实地模拟普通用户的使用场景。与既有模型性能的对比显示，ChatGPT在多数NLP任务和语言中的表现均逊于专用模型，这一发现呼吁学界进一步探索更优的多语言学习模型与机制。
