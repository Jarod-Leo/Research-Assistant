# Systematic Evaluation of Long-Context LLMs on Financial Concepts

链接: http://arxiv.org/abs/2412.15386v1

原文摘要:
Long-context large language models (LC LLMs) promise to increase reliability
of LLMs in real-world tasks requiring processing and understanding of long
input documents. However, this ability of LC LLMs to reliably utilize their
growing context windows remains under investigation. In this work, we evaluate
the performance of state-of-the-art GPT-4 suite of LC LLMs in solving a series
of progressively challenging tasks, as a function of factors such as context
length, task difficulty, and position of key information by creating a real
world financial news dataset. Our findings indicate that LC LLMs exhibit
brittleness at longer context lengths even for simple tasks, with performance
deteriorating sharply as task complexity increases. At longer context lengths,
these state-of-the-art models experience catastrophic failures in instruction
following resulting in degenerate outputs. Our prompt ablations also reveal
unfortunate continued sensitivity to both the placement of the task instruction
in the context window as well as minor markdown formatting. Finally, we
advocate for more rigorous evaluation of LC LLMs by employing holistic metrics
such as F1 (rather than recall) and reporting confidence intervals, thereby
ensuring robust and conclusive findings.

中文翻译:
以下是符合您要求的中文翻译：

长上下文大语言模型（LC LLMs）有望提升大模型在现实任务中的可靠性，这些任务通常需要处理和理解长篇幅输入文档。然而，这类模型能否有效利用其不断扩展的上下文窗口仍待验证。本研究通过构建真实世界金融新闻数据集，系统评估了当前最先进的GPT-4系列LC LLMs在完成渐进式复杂任务时的表现，重点考察了上下文长度、任务难度及关键信息位置等因素的影响。研究发现：即便执行简单任务，LC LLMs在长上下文场景下也表现出明显的性能脆弱性，且随着任务复杂度提升，模型表现急剧恶化。在长上下文环境中，这些顶尖模型会出现指令遵循方面的灾难性失败，导致输出结果严重退化。提示词消融实验还揭示了两个持续存在的敏感性问题：任务指令在上下文窗口中的位置差异，以及轻微标记格式变化都会显著影响输出。最后，我们主张采用更严格的评估标准（如使用F1值而非单纯召回率）并报告置信区间，以此确保研究结论的稳健性。

（翻译严格遵循以下原则：
1. 专业术语准确统一（如"context window"译为"上下文窗口"）
2. 长句合理切分，符合中文表达习惯
3. 被动语态转换为主动表述（如"remain under investigation"译为"仍待验证"）
4. 关键概念首次出现标注英文缩写
5. 学术用语规范（如"prompt ablations"译为"提示词消融实验"）
6. 保持原文严谨客观的学术风格）
