# Systematic Evaluation of Long-Context LLMs on Financial Concepts

链接: http://arxiv.org/abs/2412.15386v1

原文摘要:
Long-context large language models (LC LLMs) promise to increase reliability
of LLMs in real-world tasks requiring processing and understanding of long
input documents. However, this ability of LC LLMs to reliably utilize their
growing context windows remains under investigation. In this work, we evaluate
the performance of state-of-the-art GPT-4 suite of LC LLMs in solving a series
of progressively challenging tasks, as a function of factors such as context
length, task difficulty, and position of key information by creating a real
world financial news dataset. Our findings indicate that LC LLMs exhibit
brittleness at longer context lengths even for simple tasks, with performance
deteriorating sharply as task complexity increases. At longer context lengths,
these state-of-the-art models experience catastrophic failures in instruction
following resulting in degenerate outputs. Our prompt ablations also reveal
unfortunate continued sensitivity to both the placement of the task instruction
in the context window as well as minor markdown formatting. Finally, we
advocate for more rigorous evaluation of LC LLMs by employing holistic metrics
such as F1 (rather than recall) and reporting confidence intervals, thereby
ensuring robust and conclusive findings.

中文翻译:
长上下文大语言模型（LC LLMs）有望提升大语言模型在处理和理解长输入文档的实际任务中的可靠性。然而，这些模型能否有效利用其不断扩展的上下文窗口仍待验证。本研究通过构建真实世界金融新闻数据集，评估了顶尖GPT-4系列LC LLMs在解决一系列渐进式挑战性任务时的表现，分析因素包括上下文长度、任务难度及关键信息位置。研究发现：即便对于简单任务，LC LLMs在较长上下文条件下也表现出脆弱性，且性能随任务复杂度提升急剧下降；在长上下文场景中，这些先进模型会出现指令遵循的灾难性失败，导致输出质量退化。提示消融实验进一步揭示了模型对任务指令在上下文窗口中的位置及细微标记格式变化的持续敏感性。最后，我们主张采用F1值（而非单纯召回率）等综合性指标并报告置信区间，以更严谨地评估LC LLMs，确保研究结论的稳健性和确定性。
