# Approximation and Estimation Ability of Transformers for Sequence-to-Sequence Functions with Infinite Dimensional Input

链接: http://arxiv.org/abs/2305.18699v1

原文摘要:
Despite the great success of Transformer networks in various applications
such as natural language processing and computer vision, their theoretical
aspects are not well understood. In this paper, we study the approximation and
estimation ability of Transformers as sequence-to-sequence functions with
infinite dimensional inputs. Although inputs and outputs are both infinite
dimensional, we show that when the target function has anisotropic smoothness,
Transformers can avoid the curse of dimensionality due to their feature
extraction ability and parameter sharing property. In addition, we show that
even if the smoothness changes depending on each input, Transformers can
estimate the importance of features for each input and extract important
features dynamically. Then, we proved that Transformers achieve similar
convergence rate as in the case of the fixed smoothness. Our theoretical
results support the practical success of Transformers for high dimensional
data.

中文翻译:
尽管Transformer网络在自然语言处理和计算机视觉等诸多领域取得了巨大成功，但其理论机制尚未得到充分阐释。本文研究了Transformer作为处理无限维输入的序列到序列函数时的逼近与估计能力。研究表明：当目标函数具有各向异性平滑特性时，Transformer凭借其特征提取能力和参数共享特性，能够有效规避维度灾难问题。进一步发现，即使输入数据的平滑度存在个体差异，Transformer仍能动态评估各特征的重要性并进行自适应特征提取。我们通过理论证明，在此类动态平滑场景下，Transformer仍能保持与固定平滑度情形相当的收敛速率。这些理论发现为Transformer处理高维数据时的卓越表现提供了理论支撑。
