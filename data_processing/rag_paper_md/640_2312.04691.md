# Simul-LLM: A Framework for Exploring High-Quality Simultaneous Translation with Large Language Models

链接: http://arxiv.org/abs/2312.04691v1

原文摘要:
Large language models (LLMs) with billions of parameters and pretrained on
massive amounts of data are now capable of near or better than state-of-the-art
performance in a variety of downstream natural language processing tasks.
Neural machine translation (NMT) is one such task that LLMs have been applied
to with great success. However, little research has focused on applying LLMs to
the more difficult subset of NMT called simultaneous translation (SimulMT),
where translation begins before the entire source context is available to the
model. In this paper, we address key challenges facing LLMs fine-tuned for
SimulMT, validate classical SimulMT concepts and practices in the context of
LLMs, explore adapting LLMs that are fine-tuned for NMT to the task of SimulMT,
and introduce Simul-LLM, the first open-source fine-tuning and evaluation
pipeline development framework for LLMs focused on SimulMT.

中文翻译:
拥有数十亿参数并基于海量数据预训练的大型语言模型（LLMs），如今在各类下游自然语言处理任务中已能实现接近或超越最先进水平的性能表现。神经机器翻译（NMT）正是LLMs取得显著成功的应用领域之一。然而，针对LLMs在NMT更复杂分支——同步翻译（SimulMT）中的应用研究却鲜少涉及，该任务要求模型在未获取完整源文本时即开始翻译。本文系统研究了LLMs在SimulMT微调过程中的核心挑战，验证了经典SimulMT理论在LLMs语境下的适用性，探索了将NMT微调后的LLMs迁移至SimulMT任务的方法，并推出首个专注于SimulMT的开源LLMs微调与评估框架Simul-LLM。
