# Anonymization by Design of Language Modeling

链接: http://arxiv.org/abs/2501.02407v1

原文摘要:
Rapid advances in Natural Language Processing (NLP) have revolutionized many
fields, including healthcare. However, these advances raise significant privacy
concerns, especially when pre-trained models fine-tuned and specialized on
sensitive data can memorize and then expose and regurgitate personal
information. This paper presents a privacy-preserving language modeling
approach to address the problem of language models anonymization, and thus
promote their sharing. Specifically, we propose both a Masking Language
Modeling (MLM) methodology to specialize a BERT-like language model, and a
Causal Language Modeling (CLM) methodology to specialize a GPT-like model that
avoids the model from memorizing direct and indirect identifying information
present in the training data. We have comprehensively evaluated our approaches
using a medical dataset and compared them against different baselines. Our
results indicate that by avoiding memorizing both direct and indirect
identifiers during model specialization, our masking and causal language
modeling schemes offer a good tradeoff for maintaining high privacy while
retaining high utility.

中文翻译:
自然语言处理（NLP）技术的飞速发展已深刻变革了医疗健康等诸多领域，但同时也引发了重大隐私隐忧——尤其是当基于敏感数据微调的专业化预训练模型可能记忆并泄露个人信息时。本文提出一种保护隐私的语言建模方法，旨在解决语言模型匿名化问题以促进模型共享。具体而言，我们分别针对BERT类模型设计了掩码语言建模（MLM）方法，针对GPT类模型开发了因果语言建模（CLM）方法，通过这两种模型专业化方案有效防止模型记忆训练数据中的直接与间接身份标识信息。基于医疗数据集的多维度评估显示，我们的掩码与因果语言建模方案在模型专业化过程中规避两类标识符记忆，实现了隐私保护强度与模型实用性的最佳平衡。
