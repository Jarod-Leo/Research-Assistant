# Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models

链接: http://arxiv.org/abs/2308.14149v1

原文摘要:
Generative pre-trained transformer (GPT) models have revolutionized the field
of natural language processing (NLP) with remarkable performance in various
tasks and also extend their power to multimodal domains. Despite their success,
large GPT models like GPT-4 face inherent limitations such as considerable
size, high computational requirements, complex deployment processes, and closed
development loops. These constraints restrict their widespread adoption and
raise concerns regarding their responsible development and usage. The need for
user-friendly, relatively small, and open-sourced alternative GPT models arises
from the desire to overcome these limitations while retaining high performance.
In this survey paper, we provide an examination of alternative open-sourced
models of large GPTs, focusing on user-friendly and relatively small models
that facilitate easier deployment and accessibility. Through this extensive
survey, we aim to equip researchers, practitioners, and enthusiasts with a
thorough understanding of user-friendly and relatively small open-sourced
models of large GPTs, their current state, challenges, and future research
directions, inspiring the development of more efficient, accessible, and
versatile GPT models that cater to the broader scientific community and advance
the field of general artificial intelligence. The source contents are
continuously updating in https://github.com/GPT-Alternatives/gpt_alternatives.

中文翻译:
生成式预训练变换器（GPT）模型凭借在各类任务中的卓越表现革新了自然语言处理领域，并将其影响力扩展至多模态领域。尽管成效显著，GPT-4等大型模型仍存在固有缺陷：参数量庞大、算力需求高、部署流程复杂以及开发闭环化。这些限制不仅阻碍了其广泛应用，更引发了关于负责任开发与使用的隐忧。为突破这些局限同时保持高性能，市场亟需用户友好、体量适中且开源的可替代GPT模型。本综述论文系统考察了大型GPT的开源替代模型，重点关注便于部署和获取的中小型用户友好型方案。通过全面梳理，我们力图帮助研究者、从业者和技术爱好者深入理解这类模型的发展现状、核心挑战与未来方向，从而推动开发更高效、普惠且通用的GPT模型，服务更广泛的科学社群并推进通用人工智能领域发展。相关资源持续更新于https://github.com/GPT-Alternatives/gpt_alternatives。
