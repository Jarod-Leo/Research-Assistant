# Instruct and Extract: Instruction Tuning for On-Demand Information Extraction

链接: http://arxiv.org/abs/2310.16040v1

原文摘要:
Large language models with instruction-following capabilities open the door
to a wider group of users. However, when it comes to information extraction - a
classic task in natural language processing - most task-specific systems cannot
align well with long-tail ad hoc extraction use cases for non-expert users. To
address this, we propose a novel paradigm, termed On-Demand Information
Extraction, to fulfill the personalized demands of real-world users. Our task
aims to follow the instructions to extract the desired content from the
associated text and present it in a structured tabular format. The table
headers can either be user-specified or inferred contextually by the model. To
facilitate research in this emerging area, we present a benchmark named
InstructIE, inclusive of both automatically generated training data, as well as
the human-annotated test set. Building on InstructIE, we further develop an
On-Demand Information Extractor, ODIE. Comprehensive evaluations on our
benchmark reveal that ODIE substantially outperforms the existing open-source
models of similar size. Our code and dataset are released on
https://github.com/yzjiao/On-Demand-IE.

中文翻译:
具备指令跟随能力的大型语言模型为更广泛的用户群体打开了大门。然而在信息抽取这一自然语言处理经典任务上，大多数专用系统难以满足非专业用户的长尾临时抽取需求。为此，我们提出"按需信息抽取"这一新范式，旨在满足真实用户的个性化需求。该任务要求根据指令从关联文本中提取目标内容，并以结构化表格形式呈现，表头既可由用户指定也可由模型根据上下文推断。为推进这一新兴领域研究，我们构建了包含自动生成训练数据和人工标注测试集的InstructIE基准。基于此，我们进一步开发了按需信息抽取系统ODIE。全面评估表明，ODIE在性能上显著优于同类规模的开源模型。相关代码和数据集已发布于https://github.com/yzjiao/On-Demand-IE。
