# Leveraging Knowledge and Reinforcement Learning for Enhanced Reliability of Language Models

链接: http://arxiv.org/abs/2308.13467v1

原文摘要:
The Natural Language Processing(NLP) community has been using crowd sourcing
techniques to create benchmark datasets such as General Language Understanding
and Evaluation(GLUE) for training modern Language Models such as BERT. GLUE
tasks measure the reliability scores using inter annotator metrics i.e. Cohens
Kappa. However, the reliability aspect of LMs has often been overlooked. To
counter this problem, we explore a knowledge-guided LM ensembling approach that
leverages reinforcement learning to integrate knowledge from ConceptNet and
Wikipedia as knowledge graph embeddings. This approach mimics human annotators
resorting to external knowledge to compensate for information deficits in the
datasets. Across nine GLUE datasets, our research shows that ensembling
strengthens reliability and accuracy scores, outperforming state of the art.

中文翻译:
自然语言处理（NLP）领域正广泛采用众包技术构建基准数据集（如通用语言理解评估基准GLUE），用于训练BERT等现代语言模型。GLUE任务通过科恩卡帕等标注者间一致性指标衡量模型可靠性，但语言模型自身的可靠性常被忽视。为此，我们提出一种知识引导的集成学习方法：利用强化学习融合ConceptNet和维基百科知识图谱嵌入，模拟人类标注者借助外部知识弥补数据信息缺失的机制。在九项GLUE任务上的实验表明，该集成策略显著提升了模型可靠性与准确率指标，性能超越现有最优水平。
