# Enhancing E-Commerce Recommendation using Pre-Trained Language Model and Fine-Tuning

链接: http://arxiv.org/abs/2302.04443v1

原文摘要:
Pretrained Language Models (PLM) have been greatly successful on a board
range of natural language processing (NLP) tasks. However, it has just started
being applied to the domain of recommendation systems. Traditional
recommendation algorithms failed to incorporate the rich textual information in
e-commerce datasets, which hinderss the performance of those models. We present
a thorough investigation on the effect of various strategy of incorporating
PLMs into traditional recommender algorithms on one of the e-commerce datasets,
and we compare the results with vanilla recommender baseline models. We show
that the application of PLMs and domain specific fine-tuning lead to an
increase on the predictive capability of combined models. These results
accentuate the importance of utilizing textual information in the context of
e-commerce, and provides insight on how to better apply PLMs alongside
traditional recommender system algorithms. The code used in this paper is
available on Github: https://github.com/NuofanXu/bert_retail_recommender.

中文翻译:
以下是符合要求的学术摘要中文翻译：

预训练语言模型（PLM）在自然语言处理（NLP）领域已取得显著成功，但其在推荐系统领域的应用才刚刚起步。传统推荐算法未能有效利用电商数据集中的丰富文本信息，这限制了模型性能的提升。本研究基于某电商数据集，系统考察了将PLM融入传统推荐算法的不同策略效果，并与基准推荐模型进行对比。实验表明：通过应用PLM并结合领域适应性微调，能显著提升混合模型的预测能力。这些结果不仅凸显了电商场景中文本信息利用的重要性，更为传统推荐算法与PLM的有机结合提供了实践指导。本文代码已开源：https://github.com/NuofanXu/bert_retail_recommender。

（注：根据学术规范要求，翻译中进行了以下处理：
1. 将长句拆分为符合中文表达习惯的短句结构
2. 专业术语保持统一（如PLM/微调等）
3. 被动语态转换为主动表述（如"hinders"译为"限制"）
4. 补充逻辑连接词（如"基于/通过"）增强连贯性
5. 保留技术术语（BERT）及数字信息的准确性
6. 调整语序符合中文前置修饰习惯（如"domain specific fine-tuning"译为"领域适应性微调"））
