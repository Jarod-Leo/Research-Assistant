# Can Large Language Models Understand Context?

链接: http://arxiv.org/abs/2402.00858v1

原文摘要:
Understanding context is key to understanding human language, an ability
which Large Language Models (LLMs) have been increasingly seen to demonstrate
to an impressive extent. However, though the evaluation of LLMs encompasses
various domains within the realm of Natural Language Processing, limited
attention has been paid to probing their linguistic capability of understanding
contextual features. This paper introduces a context understanding benchmark by
adapting existing datasets to suit the evaluation of generative models. This
benchmark comprises of four distinct tasks and nine datasets, all featuring
prompts designed to assess the models' ability to understand context. First, we
evaluate the performance of LLMs under the in-context learning pretraining
scenario. Experimental results indicate that pre-trained dense models struggle
with understanding more nuanced contextual features when compared to
state-of-the-art fine-tuned models. Second, as LLM compression holds growing
significance in both research and real-world applications, we assess the
context understanding of quantized models under in-context-learning settings.
We find that 3-bit post-training quantization leads to varying degrees of
performance reduction on our benchmark. We conduct an extensive analysis of
these scenarios to substantiate our experimental results.

中文翻译:
理解上下文是把握人类语言的关键，大型语言模型(LLMs)在这方面的能力正展现出日益惊人的水平。然而，尽管对LLMs的评估涵盖了自然语言处理领域的多个维度，但针对其语境特征理解能力的系统性探究仍显不足。本文通过改造现有数据集构建了一个生成式模型语境理解评估基准，包含四项差异化任务和九个数据集，所有测试 prompts 均专门设计用于检验模型的上下文理解能力。首先，我们在上下文学习预训练场景下评估了LLMs的表现，实验结果表明与经过微调的最优模型相比，预训练的稠密模型在捕捉细微语境特征方面存在明显不足。其次，鉴于LLM压缩技术在研究和实际应用中的重要性日益凸显，我们量化评估了上下文学习设置下量化模型的语境理解能力，发现3比特训练后量化会导致模型在本基准测试中出现不同程度的性能下降。我们通过多维度场景分析对实验结果进行了充分验证。
