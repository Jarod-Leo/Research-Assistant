# Improving Domain Generalization for Sound Classification with Sparse Frequency-Regularized Transformer

链接: http://arxiv.org/abs/2307.09723v1

原文摘要:
Sound classification models' performance suffers from generalizing on
out-of-distribution (OOD) data. Numerous methods have been proposed to help the
model generalize. However, most either introduce inference overheads or focus
on long-lasting CNN-variants, while Transformers has been proven to outperform
CNNs on numerous natural language processing and computer vision tasks. We
propose FRITO, an effective regularization technique on Transformer's
self-attention, to improve the model's generalization ability by limiting each
sequence position's attention receptive field along the frequency dimension on
the spectrogram. Experiments show that our method helps Transformer models
achieve SOTA generalization performance on TAU 2020 and Nsynth datasets while
saving 20% inference time.

中文翻译:
声音分类模型在分布外（OOD）数据上的泛化性能往往欠佳。尽管已有大量方法被提出以提升模型泛化能力，但多数方案要么引入推理开销，要么仅针对长期占据主导地位的CNN变体进行优化，而Transformer架构已在众多自然语言处理和计算机视觉任务中被证明优于CNN。我们提出FRITO——一种针对Transformer自注意力机制的有效正则化技术，该方法通过限制频谱图上每个序列位置沿频率维度的注意力感受野，显著提升模型的泛化能力。实验表明，我们的方法使Transformer模型在TAU 2020和Nsynth数据集上实现了最先进的泛化性能，同时节省了20%的推理时间。

（翻译说明：
1. 专业术语处理："out-of-distribution"译为"分布外"，"self-attention"保留专业表述"自注意力"，"spectrogram"译为"频谱图"
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句，如将"limiting each...spectrogram"处理为独立分句
3. 被动语态转换："has been proven"译为主动式"已被证明"更符合中文习惯
4. 技术概念显化："SOTA"展开为"最先进的"便于理解
5. 逻辑连接优化：添加"尽管"、"而"等连接词增强段落连贯性
6. 数字规范：保留"20%"的统一表述）
