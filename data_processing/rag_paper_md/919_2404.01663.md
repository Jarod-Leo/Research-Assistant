# CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small Language Models

链接: http://arxiv.org/abs/2404.01663v1

原文摘要:
Open large language models (LLMs) have significantly advanced the field of
natural language processing, showcasing impressive performance across various
tasks.Despite the significant advancements in LLMs, their effective operation
still relies heavily on human input to accurately guide the dialogue flow, with
agent tuning being a crucial optimization technique that involves human
adjustments to the model for better response to such guidance.Addressing this
dependency, our work introduces the TinyAgent model, trained on a meticulously
curated high-quality dataset. We also present the Collaborative Multi-Agent
Tuning (CMAT) framework, an innovative system designed to augment language
agent capabilities through adaptive weight updates based on environmental
feedback. This framework fosters collaborative learning and real-time
adaptation among multiple intelligent agents, enhancing their context-awareness
and long-term memory. In this research, we propose a new communication agent
framework that integrates multi-agent systems with environmental feedback
mechanisms, offering a scalable method to explore cooperative behaviors.
Notably, our TinyAgent-7B model exhibits performance on par with GPT-3.5,
despite having fewer parameters, signifying a substantial improvement in the
efficiency and effectiveness of LLMs.

中文翻译:
以下是符合学术规范的中文翻译：

开放大语言模型（LLMs）显著推动了自然语言处理领域的发展，在各种任务中展现出卓越性能。尽管LLMs取得了重大进展，其有效运行仍高度依赖人工输入以精确引导对话流程，其中智能体调优作为关键优化技术，需要通过人工调整模型来提升其对引导指令的响应能力。针对这一依赖性，本研究提出基于精细筛选的高质量数据集训练的TinyAgent模型，并创新性地开发了协同多智能体调优（CMAT）框架。该框架通过环境反馈驱动的自适应权重更新机制，增强语言智能体的多维度能力，促进多智能体间的协作学习与实时适应，显著提升其情境感知与长时记忆性能。本研究构建的新型通信智能体框架，将多智能体系统与环境反馈机制相融合，为探索协同行为提供了可扩展的研究路径。值得注意的是，我们的TinyAgent-7B模型在参数量更少的情况下实现了与GPT-3.5相当的性能，标志着大语言模型效率与效能的实质性突破。

（翻译说明：
1. 专业术语统一处理："agent"译为"智能体"，"tuning"译为"调优"，"framework"译为"框架"
2. 长句拆分重构：将原文复合句按中文表达习惯分解为多个短句
3. 被动语态转化："being a crucial optimization technique"处理为判断句式"作为关键优化技术"
4. 概念显化表达："context-awareness"译为"情境感知"，"long-term memory"译为"长时记忆"
5. 学术用语规范："substantial improvement"译为"实质性突破"，符合中文论文表述习惯
6. 保持数字准确性：完整保留"7B"、"GPT-3.5"等技术参数表述）
