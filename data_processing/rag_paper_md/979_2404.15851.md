# Porting Large Language Models to Mobile Devices for Question Answering

链接: http://arxiv.org/abs/2404.15851v1

原文摘要:
Deploying Large Language Models (LLMs) on mobile devices makes all the
capabilities of natural language processing available on the device. An
important use case of LLMs is question answering, which can provide accurate
and contextually relevant answers to a wide array of user queries. We describe
how we managed to port state of the art LLMs to mobile devices, enabling them
to operate natively on the device. We employ the llama.cpp framework, a
flexible and self-contained C++ framework for LLM inference. We selected a
6-bit quantized version of the Orca-Mini-3B model with 3 billion parameters and
present the correct prompt format for this model. Experimental results show
that LLM inference runs in interactive speed on a Galaxy S21 smartphone and
that the model delivers high-quality answers to user queries related to
questions from different subjects like politics, geography or history.

中文翻译:
在移动设备上部署大型语言模型（LLM）使得自然语言处理的所有能力都能在设备端实现。LLM的一个重要应用场景是问答系统，它可以为用户的各种查询提供准确且上下文相关的答案。我们描述了如何成功将最先进的LLM移植到移动设备，使其能在设备上原生运行。我们采用了llama.cpp框架，这是一个灵活且独立的C++框架，用于LLM推理。我们选择了具有30亿参数的Orca-Mini-3B模型的6位量化版本，并展示了适用于该模型的正确提示格式。实验结果表明，在Galaxy S21智能手机上，LLM推理能以交互速度运行，并且该模型能够针对政治、地理或历史等不同学科的相关问题，为用户查询提供高质量答案。
