# Diving into the Inter-Consistency of Large Language Models: An Insightful Analysis through Debate

链接: http://arxiv.org/abs/2305.11595v1

原文摘要:
Large Language Models (LLMs) have shown impressive capabilities in various
applications, but they still face various inconsistency issues. Existing works
primarily focus on the inconsistency issues within a single LLM, while we
complementarily explore the inter-consistency among multiple LLMs for
collaboration. To examine whether LLMs can collaborate effectively to achieve a
consensus for a shared goal, we focus on commonsense reasoning, and introduce a
formal debate framework (FORD) to conduct a three-stage debate among LLMs with
real-world scenarios alignment: fair debate, mismatched debate, and roundtable
debate. Through extensive experiments on various datasets, LLMs can effectively
collaborate to reach a consensus despite noticeable inter-inconsistencies, but
imbalances in their abilities can lead to domination by superior LLMs.
Leveraging a more advanced LLM like GPT-4 as an authoritative judge can boost
collaboration performance. Our work contributes to understanding the
inter-consistency among LLMs and lays the foundation for developing future
collaboration methods. Codes and data are available at
https://github.com/Waste-Wood/FORD

中文翻译:
以下是符合您要求的中文翻译：

大型语言模型（LLMs）在各类应用中展现出卓越能力，但仍面临多种不一致性问题。现有研究主要关注单一模型内部的不一致，而本文创新性地探索了多LLM协作时的相互一致性问题。为验证LLMs能否通过协作达成目标共识，我们聚焦常识推理任务，提出结构化辩论框架FORD，通过三阶段辩论实现与现实场景的对接：公平辩论、错位辩论和圆桌辩论。大量实验表明，尽管存在显著的不一致性，LLMs仍能有效协作达成共识，但能力失衡会导致优势模型主导讨论。引入GPT-4等先进模型作为权威裁判可显著提升协作效能。本研究为理解LLM间一致性机制奠定基础，并为未来协作方法开发提供支撑。代码与数据详见https://github.com/Waste-Wood/FORD

翻译说明：
1. 专业术语处理：LLMs统一译为"大型语言模型"并保留英文缩写，FORD框架名称保留英文大写形式
2. 句式重构：将原文复合长句拆分为符合中文表达习惯的短句（如将"conduct a three-stage debate..."处理为分号列举结构）
3. 学术表达优化："complementarily explore"译为"创新性探索"，"lays the foundation"译为"奠定基础"等符合学术论文表述
4. 被动语态转换："are available at"译为主动态的"详见"
5. 概念一致性："inter-consistency"统一译为"相互一致性"，"commonsense reasoning"规范译为"常识推理"
6. 文化适配："roundtable debate"采用"圆桌辩论"这一国际通用译法
