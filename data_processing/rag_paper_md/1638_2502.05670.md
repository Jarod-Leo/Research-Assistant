# Language Models Largely Exhibit Human-like Constituent Ordering Preferences

链接: http://arxiv.org/abs/2502.05670v1

原文摘要:
Though English sentences are typically inflexible vis-\`a-vis word order,
constituents often show far more variability in ordering. One prominent theory
presents the notion that constituent ordering is directly correlated with
constituent weight: a measure of the constituent's length or complexity. Such
theories are interesting in the context of natural language processing (NLP),
because while recent advances in NLP have led to significant gains in the
performance of large language models (LLMs), much remains unclear about how
these models process language, and how this compares to human language
processing. In particular, the question remains whether LLMs display the same
patterns with constituent movement, and may provide insights into existing
theories on when and how the shift occurs in human language. We compare a
variety of LLMs with diverse properties to evaluate broad LLM performance on
four types of constituent movement: heavy NP shift, particle movement, dative
alternation, and multiple PPs. Despite performing unexpectedly around particle
movement, LLMs generally align with human preferences around constituent
ordering.

中文翻译:
尽管英语句子在词序上通常缺乏灵活性，但其成分的排列顺序往往展现出更高的可变性。一种重要理论提出，成分排序与成分权重（即成分长度或复杂度的度量）直接相关。这类理论在自然语言处理（NLP）领域尤为引人关注：虽然NLP的最新进展使得大语言模型（LLMs）性能显著提升，但这些模型如何处理语言、其机制与人类语言处理有何异同，仍存在诸多未解之谜。特别是LLMs是否呈现与人类相似的结构成分移位模式，可能为现有理论中关于人类语言何时及如何发生成分移位的问题提供新见解。本研究通过对比多种特性各异的大语言模型，评估其在四类成分移位现象（重名词短语移位、小品词移位、与格交替、多重介词短语）上的整体表现。尽管在小品词移位任务中表现异常，但LLMs总体上与人类对成分排序的偏好保持一致。
