# mChartQA: A universal benchmark for multimodal Chart Question Answer based on Vision-Language Alignment and Reasoning

链接: http://arxiv.org/abs/2404.01548v1

原文摘要:
In the fields of computer vision and natural language processing, multimodal
chart question-answering, especially involving color, structure, and textless
charts, poses significant challenges. Traditional methods, which typically
involve either direct multimodal processing or a table-to-text conversion
followed by language model analysis, have limitations in effectively handling
these complex scenarios. This paper introduces a novel multimodal chart
question-answering model, specifically designed to address these intricate
tasks. Our model integrates visual and linguistic processing, overcoming the
constraints of existing methods. We adopt a dual-phase training approach: the
initial phase focuses on aligning image and text representations, while the
subsequent phase concentrates on optimizing the model's interpretative and
analytical abilities in chart-related queries. This approach has demonstrated
superior performance on multiple public datasets, particularly in handling
color, structure, and textless chart questions, indicating its effectiveness in
complex multimodal tasks.

中文翻译:
在计算机视觉与自然语言处理领域，多模态图表问答——尤其是涉及颜色、结构及无文本图表的任务——构成了重大挑战。传统方法通常采用直接多模态处理或先进行表格到文本转换再结合语言模型分析的策略，但这类方案在应对复杂场景时存在明显局限。本文提出了一种新型多模态图表问答模型，专门针对上述复杂任务设计。该模型通过融合视觉与语言处理能力，有效突破了现有方法的瓶颈。我们采用双阶段训练框架：初始阶段着重图像与文本表征的对齐学习，进阶阶段则专注于优化模型对图表类问题的解析与分析能力。实验表明，该模型在多个公开数据集上展现出卓越性能，特别是在处理颜色、结构及无文本图表问题时优势显著，印证了其在复杂多模态任务中的有效性。
