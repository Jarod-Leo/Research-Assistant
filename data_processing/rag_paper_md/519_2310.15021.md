# Efficient Data Learning for Open Information Extraction with Pre-trained Language Models

链接: http://arxiv.org/abs/2310.15021v1

原文摘要:
Open Information Extraction (OpenIE) is a fundamental yet challenging task in
Natural Language Processing, which involves extracting all triples (subject,
predicate, object) from a given sentence. While labeling-based methods have
their merits, generation-based techniques offer unique advantages, such as the
ability to generate tokens not present in the original sentence. However, these
generation-based methods often require a significant amount of training data to
learn the task form of OpenIE and substantial training time to overcome slow
model convergence due to the order penalty. In this paper, we introduce a novel
framework, OK-IE, that ingeniously transforms the task form of OpenIE into the
pre-training task form of the T5 model, thereby reducing the need for extensive
training data. Furthermore, we introduce an innovative concept of Anchor to
control the sequence of model outputs, effectively eliminating the impact of
order penalty on model convergence and significantly reducing training time.
Experimental results indicate that, compared to previous SOTA methods, OK-IE
requires only 1/100 of the training data (900 instances) and 1/120 of the
training time (3 minutes) to achieve comparable results.

中文翻译:
开放信息抽取（OpenIE）作为自然语言处理领域一项基础而富有挑战性的任务，旨在从给定句子中提取所有三元组（主语、谓语、宾语）。尽管基于标注的方法具有特定优势，但基于生成的技术展现出独特优点，例如能够生成原句中未出现的词汇。然而，这类生成式方法通常需要大量训练数据来学习OpenIE的任务形式，并耗费可观训练时间以克服顺序惩罚导致的模型收敛缓慢问题。本文提出创新框架OK-IE，通过巧妙地将OpenIE任务形式转化为T5模型的预训练任务形式，显著降低了对海量训练数据的需求。此外，我们引入"锚点"这一创新概念来控制模型输出序列，有效消除了顺序惩罚对模型收敛的影响，大幅缩减了训练时长。实验结果表明，相较于先前最优方法，OK-IE仅需1/100的训练数据（900个实例）和1/120的训练时间（3分钟）即可取得相当的效果。
