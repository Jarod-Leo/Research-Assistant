# VBART: The Turkish LLM

链接: http://arxiv.org/abs/2403.01308v1

原文摘要:
We present VBART, the first Turkish sequence-to-sequence Large Language
Models (LLMs) pre-trained on a large corpus from scratch. VBART are compact
LLMs based on good ideas leveraged from BART and mBART models and come in two
sizes, Large and XLarge. Fine-tuned VBART models surpass the prior
state-of-the-art results in abstractive text summarization, title generation,
text paraphrasing, question answering and question generation tasks. They allow
fine-tuning for future text generation tasks and datasets, carving a new path
for Turkish Natural Language Processing (NLP) research. Our work shows that
having a pre-trained LLM for Turkish outperforms up to 3x multilingual models,
improving existing results and providing efficient models for training and
inference. Moreover, we show that our monolingual tokenizer is up to 11x more
efficient than multilingual tokenizers. Last but not least, we introduce a
method to enlarge an existing pre-trained LLM and question the relevancy of
Chinchilla Scaling Law to sequence-to-sequence masked language models. Our
fine-tuned models, tokenizer and cleaned vngrs-web-corpus of 135 GB are
publicly available at huggingface.co/vngrs-ai.

中文翻译:
我们推出了VBART，这是首个基于土耳其语、从头开始在大规模语料库上预训练的序列到序列大语言模型（LLMs）。VBART作为紧凑型LLM，融合了BART和mBART模型的优势设计，提供Large和XLarge两种规格。经微调后的VBART模型在抽象文本摘要、标题生成、文本复述、问答及问题生成任务中均超越了现有最优成果。该模型支持针对未来文本生成任务和数据集的微调，为土耳其语自然语言处理（NLP）研究开辟了新路径。研究表明，土耳其语专用预训练LLM的性能最高可达多语言模型的3倍，不仅提升了现有成果，还提供了高效的训练与推理模型。此外，我们的单语分词器效率比多语言分词器最高提升11倍。最后，我们提出了一种扩展现有预训练LLM的方法，并对Chinchilla缩放定律在序列到序列掩码语言模型中的适用性提出质疑。微调模型、分词器及135GB经清洗的vngrs-web-corpus已公开于huggingface.co/vngrs-ai平台。
