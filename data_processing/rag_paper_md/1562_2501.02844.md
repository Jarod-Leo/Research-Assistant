# Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text Classification

链接: http://arxiv.org/abs/2501.02844v1

原文摘要:
Text classification is a fundamental task in data mining, pivotal to various
applications such as tabular understanding and recommendation. Although neural
network-based models, such as CNN and BERT, have demonstrated remarkable
performance in text classification, their effectiveness heavily relies on
abundant labeled training data. This dependency makes these models less
effective in dynamic few-shot text classification, where labeled data is
scarce, and new target labels frequently appear based on application needs.
Recently, large language models (LLMs) have shown promise due to their
extensive pretraining and contextual understanding ability. Current approaches
provide LLMs with text inputs, candidate labels, and additional side
information (e.g., descriptions) to classify texts. However, their
effectiveness is hindered by the increased input size and the noise introduced
through side information processing. To address these limitations, we propose a
graph-based online retrieval-augmented generation framework, namely GORAG, for
dynamic few-shot text classification. Rather than treating each input
independently, GORAG constructs and maintains a weighted graph by extracting
side information across all target texts. In this graph, text keywords and
labels are represented as nodes, with edges indicating the correlations between
them. To model these correlations, GORAG employs an edge weighting mechanism to
prioritize the importance and reliability of extracted information and
dynamically retrieves relevant context using a minimum-cost spanning tree
tailored for each text input. Empirical evaluations demonstrate that GORAG
outperforms existing approaches by providing more comprehensive and precise
contextual information.

中文翻译:
文本分类是数据挖掘中的基础任务，对表格理解、推荐系统等应用至关重要。尽管基于神经网络的模型（如CNN和BERT）在文本分类中表现出色，但其性能高度依赖大量标注训练数据。这一局限性使此类模型难以应对动态少样本文本分类场景——即标注数据稀缺且目标标签随应用需求频繁更新的情境。近期，大语言模型（LLM）凭借其广泛的预训练和上下文理解能力展现出潜力。现有方法通过向LLM提供文本输入、候选标签及附加信息（如描述）进行分类，但输入规模扩大和辅助信息处理引入的噪声会降低效果。为此，我们提出基于图的在线检索增强生成框架GORAG，用于动态少样本文本分类。该框架通过提取所有目标文本的辅助信息构建加权图，将文本关键词和标签表示为节点，边表示其相关性。GORAG采用边权重机制评估提取信息的重要性与可靠性，并基于最小生成树为每个文本动态检索相关上下文。实验表明，GORAG通过提供更全面精确的上下文信息，性能优于现有方法。
