# FlexiGPT: Pruning and Extending Large Language Models with Low-Rank Weight Sharing

链接: http://arxiv.org/abs/2501.14713v1

原文摘要:
The rapid proliferation of large language models (LLMs) in natural language
processing (NLP) has created a critical need for techniques that enable
efficient deployment on memory-constrained devices without compromising
performance. We present a method to prune LLMs that selectively prunes model
blocks based on an importance score and replaces them with a low-parameter
replacement strategy. Specifically, we propose a principled metric to replace
each pruned block using a weight-sharing mechanism that leverages unpruned
counterparts from the model and block-specific low-rank adapters. Furthermore,
we facilitate the learning of these replacement blocks with output feature
normalization and an adapter initialization scheme built on low-rank SVD
reconstructions. Empirical evaluations demonstrate substantial performance
gains over existing methods, achieving state-of-the-art performance on 5/6
benchmarks for a compression rate of 30% and 6/6 benchmarks for a compression
rate of 40%. We also demonstrate that our approach can extend smaller models,
boosting performance on 6/6 benchmarks using only ~0.3% tokens of extended
training with minimal additional parameter costs.

中文翻译:
以下是符合要求的学术中文翻译：

自然语言处理领域大规模语言模型（LLMs）的快速普及，亟需在保证性能前提下实现内存受限设备高效部署的技术。本研究提出一种LLM剪枝方法，通过重要性评分选择性剪除模型块，并采用低参数量替代策略。具体而言：1）设计基于权重共享机制的原则性度量标准，利用模型中未剪除块及块特异性低秩适配器实现剪除块替换；2）通过输出特征归一化和基于低秩SVD重构的适配器初始化方案优化替代块的学习过程。实验结果表明：在30%压缩率下实现5/6基准测试、40%压缩率下实现6/6基准测试的当前最优性能。该方法还能扩展小型模型，仅需约0.3%的扩展训练token和极小参数量开销即可在6/6基准测试上提升性能。

（翻译严格遵循以下原则：
1. 专业术语准确统一："low-rank adapters"译为"低秩适配器"，"SVD"保留专业缩写
2. 被动语态转化："is proposed"转为主动式"设计"
3. 长句拆分：将原文复合句分解为符合中文表达习惯的短句结构
4. 数据呈现规范：严格保留"5/6 benchmarks"等量化表述
5. 学术风格保持：使用"本研究""实现...性能"等规范学术用语
6. 逻辑关系显化：通过冒号、数字编号等方式清晰呈现方法论层次）
