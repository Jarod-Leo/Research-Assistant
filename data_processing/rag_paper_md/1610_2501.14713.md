# FlexiGPT: Pruning and Extending Large Language Models with Low-Rank Weight Sharing

链接: http://arxiv.org/abs/2501.14713v1

原文摘要:
The rapid proliferation of large language models (LLMs) in natural language
processing (NLP) has created a critical need for techniques that enable
efficient deployment on memory-constrained devices without compromising
performance. We present a method to prune LLMs that selectively prunes model
blocks based on an importance score and replaces them with a low-parameter
replacement strategy. Specifically, we propose a principled metric to replace
each pruned block using a weight-sharing mechanism that leverages unpruned
counterparts from the model and block-specific low-rank adapters. Furthermore,
we facilitate the learning of these replacement blocks with output feature
normalization and an adapter initialization scheme built on low-rank SVD
reconstructions. Empirical evaluations demonstrate substantial performance
gains over existing methods, achieving state-of-the-art performance on 5/6
benchmarks for a compression rate of 30% and 6/6 benchmarks for a compression
rate of 40%. We also demonstrate that our approach can extend smaller models,
boosting performance on 6/6 benchmarks using only ~0.3% tokens of extended
training with minimal additional parameter costs.

中文翻译:
自然语言处理（NLP）领域大型语言模型（LLMs）的快速扩散，催生了对高效部署技术的迫切需求——这些技术需在内存受限设备上实现无损性能。我们提出一种LLMs剪枝方法，通过重要性评分选择性移除模型块，并采用低参数量替换策略。具体而言，设计了一种原则性度量标准：利用模型中未剪枝块的权重共享机制，结合针对特定块的低秩适配器进行替换。此外，通过输出特征归一化和基于低秩SVD重构的适配器初始化方案，有效促进了替换块的学习。实证评估显示，该方法在30%压缩率下5/6基准测试、40%压缩率下6/6基准测试中均超越现有技术，达到最优性能。实验还表明该方法可扩展至小模型，仅需约0.3%的扩展训练token及微量参数成本，即在6/6基准测试中实现性能提升。
