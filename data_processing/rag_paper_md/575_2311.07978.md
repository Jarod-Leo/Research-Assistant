# How good are Large Language Models on African Languages?

链接: http://arxiv.org/abs/2311.07978v1

原文摘要:
Large-scale multilingual evaluations, such as MEGA, often include only a
handful of African languages due to the scarcity of high-quality evaluation
data and the limited discoverability of existing African datasets. This lack of
representation hinders comprehensive LLM evaluation across a diverse range of
languages and tasks. To address these challenges, we introduce AfroBench -- a
multi-task benchmark for evaluating the performance of LLMs across 64 African
languages, 15 tasks and 22 datasets. AfroBench consists of nine natural
language understanding datasets, six text generation datasets, six knowledge
and question answering tasks, and one mathematical reasoning task. We present
results comparing the performance of prompting LLMs to fine-tuned baselines
based on BERT and T5-style models. Our results suggest large gaps in
performance between high-resource languages, such as English, and African
languages across most tasks; but performance also varies based on the
availability of monolingual data resources. Our findings confirm that
performance on African languages continues to remain a hurdle for current LLMs,
underscoring the need for additional efforts to close this gap.
  https://mcgill-nlp.github.io/AfroBench/

中文翻译:
诸如MEGA等大规模多语言评估项目，通常仅涵盖少数非洲语言，这源于高质量评估数据的匮乏及现有非洲数据集可发现性不足。这种代表性缺失阻碍了跨多样语言与任务的LLM全面评估。为应对这些挑战，我们推出AfroBench——一个涵盖64种非洲语言、15项任务和22个数据集的多任务基准测试平台。AfroBench包含九组自然语言理解数据集、六组文本生成数据集、六项知识与问答任务，以及一项数学推理任务。我们通过对比基于BERT和T5架构微调基线模型与提示式LLM的表现，发现英语等高资源语言与非洲语言在多数任务上存在显著性能差距；但性能差异也受单语数据资源可用性影响。研究证实当前LLM在非洲语言处理上仍面临持续挑战，凸显了缩小这一差距的迫切需求。
https://mcgill-nlp.github.io/AfroBench/
