# Investigating Energy Efficiency and Performance Trade-offs in LLM Inference Across Tasks and DVFS Settings

链接: http://arxiv.org/abs/2501.08219v1

原文摘要:
Large language models (LLMs) have shown significant improvements in many
natural language processing (NLP) tasks, accelerating their rapid adoption
across many industries. These models are resource-intensive, requiring
extensive computational resources both during training and inference, leading
to increased energy consumption and negative environmental impact. As their
adoption accelerates, the sustainability of LLMs has become a critical issue,
necessitating strategies to optimize their runtime efficiency without
compromising performance. Hence, it is imperative to identify the parameters
that significantly influence the performance and energy efficiency of LLMs. To
that end, in this work, we investigate the effect of important parameters on
the performance and energy efficiency of LLMs during inference and examine
their trade-offs.
  First, we analyze how different types of models with varying numbers of
parameters and architectures perform on tasks like text generation, question
answering, and summarization by benchmarking LLMs such as Falcon-7B,
Mistral-7B-v0.1, T5-3B, GPT-2, GPT-J-6B, and GPT-Neo-2.7B. Second, we study
input and output sequence characteristics such as sequence length concerning
energy consumption, performance, and throughput. Finally, we explore the impact
of hardware-based power-saving techniques, i.e., Dynamic Voltage Frequency
Scaling (DVFS), on the models' latency and energy efficiency. Our extensive
benchmarking and statistical analysis reveal many interesting findings,
uncovering how specific optimizations can reduce energy consumption while
maintaining throughput and accuracy. This study provides actionable insights
for researchers and practitioners to design energy-efficient LLM inference
systems.

中文翻译:
大型语言模型（LLMs）在众多自然语言处理（NLP）任务中展现出显著性能提升，推动了其跨行业的快速应用。然而这类模型对资源需求极高，训练与推理阶段均需消耗大量算力，导致能耗激增并引发环境负面影响。随着应用规模扩大，LLMs的可持续性已成为关键议题，亟需在不牺牲性能的前提下优化其运行效率的策略。因此，准确识别显著影响LLMs性能与能效的关键参数至关重要。为此，本研究系统探究了重要参数对LLMs推理阶段性能与能效的影响机制及其权衡关系。

首先，我们通过基准测试（涵盖Falcon-7B、Mistral-7B-v0.1、T5-3B、GPT-2、GPT-J-6B及GPT-Neo-2.7B等模型），分析了不同参数量级与架构的模型在文本生成、问答和摘要等任务中的表现差异。其次，研究了输入输出序列特征（如序列长度）对能耗、性能及吞吐量的影响规律。最后，探索了基于硬件的节能技术——动态电压频率调节（DVFS）对模型延迟与能效的作用效果。通过大规模基准测试与统计分析，我们揭示了多项重要发现，阐明了特定优化策略如何在保持吞吐量与精度的同时有效降低能耗。本研究为学术界与工业界设计高能效LLM推理系统提供了可操作的实践指导。
