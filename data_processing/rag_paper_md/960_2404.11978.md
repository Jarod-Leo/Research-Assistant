# EVIT: Event-Oriented Instruction Tuning for Event Reasoning

链接: http://arxiv.org/abs/2404.11978v1

原文摘要:
Events refer to specific occurrences, incidents, or happenings that take
place under a particular background. Event reasoning aims to infer events
according to certain relations and predict future events. The cutting-edge
techniques for event reasoning play a crucial role in various natural language
processing applications. Large language models (LLMs) have made significant
advancements in event reasoning owing to their wealth of knowledge and
reasoning capabilities. However, smaller instruction-tuned models currently in
use do not consistently demonstrate exceptional proficiency in managing these
tasks. This discrepancy arises from the absence of explicit modeling of events
and the interconnections of them within their instruction data. Consequently,
these models face challenges in comprehending event structures and semantics
while struggling to bridge the gap between their interpretations and human
understanding of events. Additionally, their limitations in grasping event
relations lead to constrained event reasoning abilities to effectively deduce
and incorporate pertinent event knowledge. In this paper, we propose
Event-Oriented Instruction Tuning (EvIT) to train our LLM. Specifically, we
first propose a novel structure named event quadruple which contains the
structure and semantics of events and is complete in the event representation.
We then design event-relation learning based on the structures. We encapsulate
the learning into the instruction-tuning formulation to better stimulate the
event reasoning capacity of our model. We design a heuristic unsupervised
method to mine event quadruple from a large-scale corpus. At last, we finetune
a Llama model on our Event-Oriented Instruction Tuning. We conduct extensive
experiments on event reasoning tasks on several datasets. Automatic and human
evaluations demonstrate EvIT achieves competitive performances on event
reasoning.

中文翻译:
事件是指在特定背景下发生的具体活动、事故或现象。事件推理旨在依据特定关系推断事件并预测未来事件，其前沿技术在各类自然语言处理应用中具有关键作用。得益于丰富的知识储备与推理能力，大语言模型（LLMs）在事件推理领域取得了显著进展。然而当前使用的指令微调小模型尚未展现出处理此类任务的稳定优势，其根源在于指令数据中缺乏对事件及其关联性的显式建模，导致这些模型在理解事件结构与语义时面临挑战，难以弥合模型解释与人类事件认知之间的鸿沟。此外，对事件关系把握的局限性也制约了其有效推导与整合相关事件知识的推理能力。

本文提出面向事件的指令微调方法（EvIT）来训练大语言模型。具体而言，我们首先设计了一种包含完整事件结构与语义表征的新型四元组结构；随后基于该结构构建事件关系学习框架，并将学习过程封装为指令微调范式以充分激发模型的事件推理能力；接着开发启发式无监督方法从大规模语料中挖掘事件四元组；最终对Llama模型实施面向事件的指令微调。我们在多个数据集的事件推理任务上展开广泛实验，自动评估与人工评测均表明EvIT在事件推理方面具有竞争优势。
