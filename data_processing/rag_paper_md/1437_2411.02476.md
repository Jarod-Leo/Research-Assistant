# A Comparative Analysis of Instruction Fine-Tuning LLMs for Financial Text Classification

链接: http://arxiv.org/abs/2411.02476v1

原文摘要:
Large Language Models (LLMs) have demonstrated impressive capabilities across
diverse Natural Language Processing (NLP) tasks, including language
understanding, reasoning, and generation. However, general-domain LLMs often
struggle with financial tasks due to the technical and specialized nature of
financial texts. This study investigates the efficacy of instruction
fine-tuning smaller-scale LLMs, including Mistral-7B, Llama3-8B, and Phi3-mini,
to enhance their performance in financial text classification tasks. We
fine-tuned both instruction-tuned and base models across four financial
classification tasks, achieving significant improvements in task-specific
performance. Furthermore, we evaluated the zero-shot capabilities of these
fine-tuned models on three unseen complex financial tasks, including argument
classification, deal completeness classification, and causal classification.
Our results indicate while base model fine-tuning led to greater degradation,
instruction-tuned models maintained more robust performance. To address this
degradation, we employed model merging techniques, integrating single-task
domain-specific fine-tuned models with the base model. Using this merging
method resulted in significant enhancements in zero-shot performance, even
exceeding the original model's accuracy on certain datasets. Our findings
underscore the effectiveness of instruction fine-tuning and model merging for
adapting LLMs to specialized financial text classification tasks.

中文翻译:
大型语言模型（LLMs）在自然语言处理（NLP）任务中展现出卓越能力，涵盖语言理解、推理与生成等多个领域。然而，由于金融文本的专业性与技术性，通用领域LLMs往往难以胜任金融相关任务。本研究探讨了通过指令微调较小规模LLMs（包括Mistral-7B、Llama3-8B和Phi3-mini）来提升其在金融文本分类任务中的表现。我们在四项金融分类任务上对指令微调模型和基础模型进行微调，均实现了任务性能的显著提升。进一步地，我们评估了这些微调模型在三项未见过的复杂金融任务（包括论点分类、交易完整性分类和因果分类）上的零样本能力。结果表明，基础模型微调会导致性能明显下降，而指令微调模型则保持了更强的鲁棒性。为应对性能退化问题，我们采用模型融合技术，将单任务领域微调模型与基础模型进行整合。该方法显著提升了零样本性能，在某些数据集上甚至超越了原始模型的准确率。本研究证实了指令微调与模型融合在适配LLMs至专业金融文本分类任务中的有效性。
