# Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method

链接: http://arxiv.org/abs/2310.17918v1

原文摘要:
Large Language Models (LLMs) have shown great potential in Natural Language
Processing (NLP) tasks. However, recent literature reveals that LLMs generate
nonfactual responses intermittently, which impedes the LLMs' reliability for
further utilization. In this paper, we propose a novel self-detection method to
detect which questions that a LLM does not know that are prone to generate
nonfactual results. Specifically, we first diversify the textual expressions
for a given question and collect the corresponding answers. Then we examine the
divergencies between the generated answers to identify the questions that the
model may generate falsehoods. All of the above steps can be accomplished by
prompting the LLMs themselves without referring to any other external
resources. We conduct comprehensive experiments and demonstrate the
effectiveness of our method on recently released LLMs, e.g., Vicuna, ChatGPT,
and GPT-4.

中文翻译:
大型语言模型（LLMs）在自然语言处理（NLP）任务中展现出巨大潜力。然而，近期研究表明，LLMs会间歇性生成不真实的回答，这影响了模型进一步应用的可靠性。本文提出了一种创新的自检测方法，用于识别模型自身无法正确回答、易产生虚假结果的问题。具体而言，我们首先对给定问题生成多样化的文本表述并收集对应答案，随后通过分析回答之间的差异性来定位可能产生谬误的问题。上述所有步骤仅需通过提示LLMs自身即可完成，无需依赖任何外部资源。我们在最新发布的Vicuna、ChatGPT和GPT-4等模型上进行了全面实验，验证了该方法的有效性。
