# From Structured Prompts to Open Narratives: Measuring Gender Bias in LLMs Through Open-Ended Storytelling

链接: http://arxiv.org/abs/2503.15904v1

原文摘要:
Large Language Models (LLMs) have revolutionized natural language processing,
yet concerns persist regarding their tendency to reflect or amplify social
biases present in their training data. This study introduces a novel evaluation
framework to uncover gender biases in LLMs, focusing on their occupational
narratives. Unlike previous methods relying on structured scenarios or
carefully crafted prompts, our approach leverages free-form storytelling to
reveal biases embedded in the models. Systematic analyses show an
overrepresentation of female characters across occupations in six widely used
LLMs. Additionally, our findings reveal that LLM-generated occupational gender
rankings align more closely with human stereotypes than actual labor
statistics. These insights underscore the need for balanced mitigation
strategies to ensure fairness while avoiding the reinforcement of new
stereotypes.

中文翻译:
大型语言模型（LLMs）彻底改变了自然语言处理领域，但其反映或放大训练数据中社会偏见的倾向仍引发持续担忧。本研究提出一种新颖的评估框架，通过聚焦职业叙事来揭示LLMs中的性别偏见。与以往依赖结构化情境或精心设计提示的方法不同，我们的方法利用自由叙事形式来暴露模型内嵌的偏见。系统分析表明，六种广泛使用的LLMs在所有职业类别中都存在女性角色过度代表现象。更值得注意的是，这些模型生成的职业性别排名与人类刻板印象的吻合度，远高于与实际劳动力统计数据的匹配度。这些发现凸显了在确保公平性的同时，需要采取平衡的缓解策略以避免强化新的刻板印象。
