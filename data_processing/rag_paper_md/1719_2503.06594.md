# Beyond Decoder-only: Large Language Models Can be Good Encoders for Machine Translation

链接: http://arxiv.org/abs/2503.06594v1

原文摘要:
The field of neural machine translation (NMT) has changed with the advent of
large language models (LLMs). Much of the recent emphasis in natural language
processing (NLP) has been on modeling machine translation and many other
problems using a single pre-trained Transformer decoder, while encoder-decoder
architectures, which were the standard in earlier NMT models, have received
relatively less attention. In this paper, we explore translation models that
are universal, efficient, and easy to optimize, by marrying the world of LLMs
with the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder
unchanged. We also develop methods for adapting LLMs to work better with the
NMT decoder. Furthermore, we construct a new dataset involving multiple tasks
to assess how well the machine translation system generalizes across various
tasks. Evaluations on the WMT and our datasets show that results using our
method match or surpass a range of baselines in terms of translation quality,
but achieve $2.4 \sim 6.5 \times$ inference speedups and a $75\%$ reduction in
the memory footprint of the KV cache. It also demonstrates strong
generalization across a variety of translation-related tasks.

中文翻译:
随着大语言模型（LLMs）的出现，神经机器翻译（NMT）领域发生了变革。当前自然语言处理（NLP）的研究重点多集中于使用单一预训练Transformer解码器来建模机器翻译及其他任务，而早期NMT模型标配的编码器-解码器架构则相对受到冷落。本文通过融合LLMs与NMT技术，探索兼具通用性、高效性且易于优化的翻译模型：我们创新性地将LLMs应用于NMT编码环节，同时保留传统NMT解码器架构，并开发了适配方法以增强LLMs与NMT解码器的协同效能。此外，我们构建了包含多任务的新数据集以评估机器翻译系统的跨任务泛化能力。在WMT数据集和自建数据集上的实验表明，本方法在翻译质量上达到或超越多种基线模型，同时实现2.4~6.5倍的推理加速和75%的KV缓存内存占用缩减，且在各类翻译相关任务中展现出强大的泛化性能。

（翻译说明：
1. 专业术语准确处理："KV cache"译为技术界通用的"KV缓存"，"memory footprint"译为"内存占用"
2. 长句拆分重组：将原文复合句按中文表达习惯分解为多个短句，如第一句拆分为背景陈述+现状说明
3. 被动语态转化："have been on modeling"等被动结构转为"集中于...研究"的主动表达
4. 数据呈现优化：保留原文$2.4 \sim 6.5 \times$等数学符号格式，符合中文技术文献规范
5. 逻辑连接强化：使用"此外"、"同时"等连接词保持论证连贯性
6. 技术概念统一："generalization"在机器学习语境下统一译为"泛化"而非"通用化"
7. 文化适配：保留"WMT"国际评测名称不翻译，符合学术惯例）
