# Beyond Decoder-only: Large Language Models Can be Good Encoders for Machine Translation

链接: http://arxiv.org/abs/2503.06594v1

原文摘要:
The field of neural machine translation (NMT) has changed with the advent of
large language models (LLMs). Much of the recent emphasis in natural language
processing (NLP) has been on modeling machine translation and many other
problems using a single pre-trained Transformer decoder, while encoder-decoder
architectures, which were the standard in earlier NMT models, have received
relatively less attention. In this paper, we explore translation models that
are universal, efficient, and easy to optimize, by marrying the world of LLMs
with the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder
unchanged. We also develop methods for adapting LLMs to work better with the
NMT decoder. Furthermore, we construct a new dataset involving multiple tasks
to assess how well the machine translation system generalizes across various
tasks. Evaluations on the WMT and our datasets show that results using our
method match or surpass a range of baselines in terms of translation quality,
but achieve $2.4 \sim 6.5 \times$ inference speedups and a $75\%$ reduction in
the memory footprint of the KV cache. It also demonstrates strong
generalization across a variety of translation-related tasks.

中文翻译:
随着大语言模型（LLMs）的出现，神经机器翻译（NMT）领域发生了变革。当前自然语言处理（NLP）的研究重心多集中于利用单一预训练Transformer解码器建模机器翻译及其他任务，而早期NMT模型标配的编码器-解码器架构则相对受到冷落。本文通过融合LLM与NMT技术，探索兼具通用性、高效性且易于优化的翻译模型：我们将LLMs应用于NMT编码环节，同时保留传统NMT解码器架构，并开发了适配方法以优化LLMs与NMT解码器的协同工作。此外，我们构建了包含多任务的新数据集以评估机器翻译系统在不同任务间的泛化能力。在WMT及自建数据集上的实验表明，该方法在翻译质量上达到或超越多种基线模型，同时实现$2.4\sim6.5$倍的推理加速和键值缓存内存占用降低75%，且在各类翻译相关任务中展现出卓越的泛化性能。
