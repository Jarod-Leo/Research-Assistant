# Noise-Robust Fine-Tuning of Pretrained Language Models via External Guidance

链接: http://arxiv.org/abs/2311.01108v1

原文摘要:
Adopting a two-stage paradigm of pretraining followed by fine-tuning,
Pretrained Language Models (PLMs) have achieved substantial advancements in the
field of natural language processing. However, in real-world scenarios, data
labels are often noisy due to the complex annotation process, making it
essential to develop strategies for fine-tuning PLMs with such noisy labels. To
this end, we introduce an innovative approach for fine-tuning PLMs using noisy
labels, which incorporates the guidance of Large Language Models (LLMs) like
ChatGPT. This guidance assists in accurately distinguishing between clean and
noisy samples and provides supplementary information beyond the noisy labels,
thereby boosting the learning process during fine-tuning PLMs. Extensive
experiments on synthetic and real-world noisy datasets further demonstrate the
superior advantages of our framework over the state-of-the-art baselines.

中文翻译:
采用预训练后微调的两阶段范式，预训练语言模型(PLMs)在自然语言处理领域取得了显著进展。然而在实际场景中，由于标注过程的复杂性，数据标签往往存在噪声，这使得开发针对带噪声标签的PLMs微调策略变得尤为重要。为此，我们提出了一种创新性的带噪声标签微调方法，该方法引入ChatGPT等大型语言模型(LLMs)的指导。这种指导能有效区分干净样本与噪声样本，并提供超越噪声标签的补充信息，从而增强PLMs在微调阶段的学习效果。通过在合成与真实噪声数据集上的大量实验，我们进一步验证了该框架相较于现有最优基线模型的显著优势。
