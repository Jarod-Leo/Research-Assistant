# Few-shot learning for automated content analysis: Efficient coding of arguments and claims in the debate on arms deliveries to Ukraine

链接: http://arxiv.org/abs/2312.16975v1

原文摘要:
Pre-trained language models (PLM) based on transformer neural networks
developed in the field of natural language processing (NLP) offer great
opportunities to improve automatic content analysis in communication science,
especially for the coding of complex semantic categories in large datasets via
supervised machine learning. However, three characteristics so far impeded the
widespread adoption of the methods in the applying disciplines: the dominance
of English language models in NLP research, the necessary computing resources,
and the effort required to produce training data to fine-tune PLMs. In this
study, we address these challenges by using a multilingual transformer model in
combination with the adapter extension to transformers, and few-shot learning
methods. We test our approach on a realistic use case from communication
science to automatically detect claims and arguments together with their stance
in the German news debate on arms deliveries to Ukraine. In three experiments,
we evaluate (1) data preprocessing strategies and model variants for this task,
(2) the performance of different few-shot learning methods, and (3) how well
the best setup performs on varying training set sizes in terms of validity,
reliability, replicability and reproducibility of the results. We find that our
proposed combination of transformer adapters with pattern exploiting training
provides a parameter-efficient and easily shareable alternative to fully
fine-tuning PLMs. It performs on par in terms of validity, while overall,
provides better properties for application in communication studies. The
results also show that pre-fine-tuning for a task on a near-domain dataset
leads to substantial improvement, in particular in the few-shot setting.
Further, the results indicate that it is useful to bias the dataset away from
the viewpoints of specific prominent individuals.

中文翻译:
基于自然语言处理（NLP）领域开发的Transformer神经网络预训练语言模型（PLM），为传播科学中的自动内容分析提供了重要机遇，尤其适用于通过监督式机器学习对大规模数据集中的复杂语义类别进行编码。然而，该方法在应用学科中的广泛采用仍受三个因素制约：NLP研究中英语语言模型的主导地位、所需的计算资源，以及微调PLM所需训练数据的制备成本。本研究通过结合多语言Transformer模型与适配器扩展技术，并采用小样本学习方法应对这些挑战。我们在传播科学实际案例中测试了该方法——针对德国新闻界关于向乌克兰输送武器辩论中的主张、论据及其立场进行自动识别。通过三项实验，我们评估了：（1）该任务的数据预处理策略与模型变体；（2）不同小样本学习方法的性能表现；（3）最佳方案在不同训练集规模下结果的有效性、可靠性、可复现性与可重复性。研究发现，将Transformer适配器与模式利用训练相结合的方法，既能实现参数高效化又便于共享，可作为完全微调PLM的替代方案。其在有效性方面表现相当，同时整体上更适用于传播学研究应用。结果还表明，在近领域数据集上进行任务预微调能显著提升性能，尤其在小样本场景下。此外，研究指出应避免数据集中特定知名人士观点占比过高。
