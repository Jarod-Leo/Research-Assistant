# Few-shot learning for automated content analysis: Efficient coding of arguments and claims in the debate on arms deliveries to Ukraine

链接: http://arxiv.org/abs/2312.16975v1

原文摘要:
Pre-trained language models (PLM) based on transformer neural networks
developed in the field of natural language processing (NLP) offer great
opportunities to improve automatic content analysis in communication science,
especially for the coding of complex semantic categories in large datasets via
supervised machine learning. However, three characteristics so far impeded the
widespread adoption of the methods in the applying disciplines: the dominance
of English language models in NLP research, the necessary computing resources,
and the effort required to produce training data to fine-tune PLMs. In this
study, we address these challenges by using a multilingual transformer model in
combination with the adapter extension to transformers, and few-shot learning
methods. We test our approach on a realistic use case from communication
science to automatically detect claims and arguments together with their stance
in the German news debate on arms deliveries to Ukraine. In three experiments,
we evaluate (1) data preprocessing strategies and model variants for this task,
(2) the performance of different few-shot learning methods, and (3) how well
the best setup performs on varying training set sizes in terms of validity,
reliability, replicability and reproducibility of the results. We find that our
proposed combination of transformer adapters with pattern exploiting training
provides a parameter-efficient and easily shareable alternative to fully
fine-tuning PLMs. It performs on par in terms of validity, while overall,
provides better properties for application in communication studies. The
results also show that pre-fine-tuning for a task on a near-domain dataset
leads to substantial improvement, in particular in the few-shot setting.
Further, the results indicate that it is useful to bias the dataset away from
the viewpoints of specific prominent individuals.

中文翻译:
基于Transformer神经网络的预训练语言模型（PLM）在自然语言处理（NLP）领域的发展，为传播科学中的自动内容分析——尤其是通过监督式机器学习对大规模数据集中复杂语义类别进行编码——提供了重要机遇。然而目前有三个因素阻碍了该方法在应用学科中的广泛普及：NLP研究中英语语言模型的主导地位、所需的计算资源，以及微调PLM所需训练数据的制备成本。本研究通过采用多语言Transformer模型结合适配器扩展技术与小样本学习方法，系统应对这些挑战。我们以传播科学中关于德国向乌克兰输送武器新闻辩论的真实案例为测试场景，自动识别其中的主张、论据及其立场倾向。通过三项实验，我们分别评估：（1）针对该任务的数据预处理策略与模型变体；（2）不同小样本学习方法的性能表现；（3）最佳配置在不同训练集规模下结果的效度、信度、可复现性与可重复性。研究发现，我们提出的Transformer适配器与模式利用训练相结合的方法，可作为完全微调PLM的高效参数化且易于共享的替代方案，在效度指标上表现相当，同时整体上更适用于传播学研究应用。结果还表明，在近领域数据集上进行任务预微调能带来显著改进，尤其在小样本场景下。此外，研究显示避免数据集过度偏向特定知名人士的观点具有积极意义。
