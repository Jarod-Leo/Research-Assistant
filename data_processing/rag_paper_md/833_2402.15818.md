# Linguistic Intelligence in Large Language Models for Telecommunications

链接: http://arxiv.org/abs/2402.15818v1

原文摘要:
Large Language Models (LLMs) have emerged as a significant advancement in the
field of Natural Language Processing (NLP), demonstrating remarkable
capabilities in language generation and other language-centric tasks. Despite
their evaluation across a multitude of analytical and reasoning tasks in
various scientific domains, a comprehensive exploration of their knowledge and
understanding within the realm of natural language tasks in the
telecommunications domain is still needed. This study, therefore, seeks to
evaluate the knowledge and understanding capabilities of LLMs within this
domain. To achieve this, we conduct an exhaustive zero-shot evaluation of four
prominent LLMs-Llama-2, Falcon, Mistral, and Zephyr. These models require fewer
resources than ChatGPT, making them suitable for resource-constrained
environments. Their performance is compared with state-of-the-art, fine-tuned
models. To the best of our knowledge, this is the first work to extensively
evaluate and compare the understanding of LLMs across multiple language-centric
tasks in this domain. Our evaluation reveals that zero-shot LLMs can achieve
performance levels comparable to the current state-of-the-art fine-tuned
models. This indicates that pretraining on extensive text corpora equips LLMs
with a degree of specialization, even within the telecommunications domain. We
also observe that no single LLM consistently outperforms others, and the
performance of different LLMs can fluctuate. Although their performance lags
behind fine-tuned models, our findings underscore the potential of LLMs as a
valuable resource for understanding various aspects of this field that lack
large annotated data.

中文翻译:
大语言模型（LLMs）作为自然语言处理（NLP）领域的重要突破，在文本生成及其他语言核心任务中展现出卓越能力。尽管这些模型已在多个科学领域的分析与推理任务中接受评估，但针对电信领域自然语言任务的知识理解能力仍需系统探究。本研究旨在评估LLMs在该领域的知识掌握与理解水平，为此我们对Llama-2、Falcon、Mistral和Zephyr四款主流LLMs进行了详尽的零样本评估。这些模型较ChatGPT资源需求更低，适合资源受限环境，其表现与当前最优的微调模型进行了对比。据我们所知，这是首个在电信领域多语言核心任务上对LLMs理解能力开展系统评估与比较的研究。评估结果表明，零样本LLMs能达到与当前最优微调模型相当的性能水平，说明海量文本预训练使LLMs具备了一定程度的领域专精能力。我们还发现没有单一LLM能持续领先，不同模型表现存在波动。虽然其性能仍落后于微调模型，但本研究证实了LLMs作为理解该领域缺乏标注数据问题的宝贵资源的潜力。
