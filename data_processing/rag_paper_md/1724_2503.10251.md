# Numerical Error Analysis of Large Language Models

链接: http://arxiv.org/abs/2503.10251v1

原文摘要:
Large language models based on transformer architectures have become integral
to state-of-the-art natural language processing applications. However, their
training remains computationally expensive and exhibits instabilities, some of
which are expected to be caused by finite-precision computations. We provide a
theoretical analysis of the impact of round-off errors within the forward pass
of a transformer architecture which yields fundamental bounds for these
effects. In addition, we conduct a series of numerical experiments which
demonstrate the practical relevance of our bounds. Our results yield concrete
guidelines for choosing hyperparameters that mitigate round-off errors, leading
to more robust and stable inference.

中文翻译:
基于Transformer架构的大规模语言模型已成为当前自然语言处理领域不可或缺的核心技术。然而，其训练过程仍面临计算成本高昂和稳定性不足的挑战，其中部分问题可归因于有限精度计算带来的影响。本研究通过理论分析揭示了Transformer前向传播过程中舍入误差的作用机制，并建立了量化这些影响的基础边界条件。进一步开展的数值实验验证了所提出边界条件的实际适用性。研究成果为超参数选择提供了具体指导方案，能有效抑制舍入误差，从而增强模型推理过程的鲁棒性与稳定性。
