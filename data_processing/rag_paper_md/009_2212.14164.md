# On Transforming Reinforcement Learning by Transformer: The Development Trajectory

链接: http://arxiv.org/abs/2212.14164v1

原文摘要:
Transformer, originally devised for natural language processing, has also
attested significant success in computer vision. Thanks to its super expressive
power, researchers are investigating ways to deploy transformers to
reinforcement learning (RL) and the transformer-based models have manifested
their potential in representative RL benchmarks. In this paper, we collect and
dissect recent advances on transforming RL by transformer (transformer-based RL
or TRL), in order to explore its development trajectory and future trend. We
group existing developments in two categories: architecture enhancement and
trajectory optimization, and examine the main applications of TRL in robotic
manipulation, text-based games, navigation and autonomous driving. For
architecture enhancement, these methods consider how to apply the powerful
transformer structure to RL problems under the traditional RL framework, which
model agents and environments much more precisely than deep RL methods, but
they are still limited by the inherent defects of traditional RL algorithms,
such as bootstrapping and "deadly triad". For trajectory optimization, these
methods treat RL problems as sequence modeling and train a joint state-action
model over entire trajectories under the behavior cloning framework, which are
able to extract policies from static datasets and fully use the long-sequence
modeling capability of the transformer. Given these advancements, extensions
and challenges in TRL are reviewed and proposals about future direction are
discussed. We hope that this survey can provide a detailed introduction to TRL
and motivate future research in this rapidly developing field.

中文翻译:
Transformer最初为自然语言处理设计，在计算机视觉领域也取得了显著成功。凭借其卓越的表达能力，研究者正探索如何将transformer应用于强化学习（RL），基于transformer的模型已在代表性RL基准测试中展现出巨大潜力。本文系统梳理并剖析了transformer革新强化学习（TRL）的最新进展，以揭示其发展轨迹与未来趋势。我们将现有成果归纳为架构增强与轨迹优化两大方向，并考察TRL在机器人操控、文本游戏、导航及自动驾驶等领域的典型应用。

在架构增强方面，这些研究致力于将强大的transformer结构融入传统RL框架，其建模智能体与环境的能力远超深度RL方法，但仍受限于传统RL算法固有的自举问题和"致命三要素"等缺陷。轨迹优化方法则将RL问题视为序列建模任务，在行为克隆框架下训练覆盖完整轨迹的状态-动作联合模型，既能从静态数据集中提取策略，又能充分发挥transformer的长序列建模优势。

本文在综述TRL领域技术进步与现存挑战的基础上，对未来研究方向进行了展望。我们期望通过这项调研为读者提供TRL的全面介绍，并推动这一快速发展领域的后续研究。
