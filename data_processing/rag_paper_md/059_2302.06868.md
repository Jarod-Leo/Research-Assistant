# SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains

链接: http://arxiv.org/abs/2302.06868v1

原文摘要:
Prompting pre-trained language models leads to promising results across
natural language processing tasks but is less effective when applied in
low-resource domains, due to the domain gap between the pre-training data and
the downstream task. In this work, we bridge this gap with a novel and
lightweight prompting methodology called SwitchPrompt for the adaptation of
language models trained on datasets from the general domain to diverse
low-resource domains. Using domain-specific keywords with a trainable gated
prompt, SwitchPrompt offers domain-oriented prompting, that is, effective
guidance on the target domains for general-domain language models. Our few-shot
experiments on three text classification benchmarks demonstrate the efficacy of
the general-domain pre-trained language models when used with SwitchPrompt.
They often even outperform their domain-specific counterparts trained with
baseline state-of-the-art prompting methods by up to 10.7% performance increase
in accuracy. This result indicates that SwitchPrompt effectively reduces the
need for domain-specific language model pre-training.

中文翻译:
针对预训练语言模型进行提示（prompting）在自然语言处理任务中展现出显著效果，但在低资源领域应用时，由于预训练数据与下游任务间的领域差异，其效能往往受限。本研究提出了一种名为SwitchPrompt的新型轻量级提示方法，通过结合领域关键词与可训练门控提示机制，实现了通用领域语言模型向多样化低资源领域的自适应迁移。该方法提供面向领域的动态提示，为通用模型在目标领域执行任务时给予有效引导。我们在三个文本分类基准数据集上的少样本实验表明：结合SwitchPrompt的通用预训练语言模型不仅显著提升了性能，其准确率最高较采用现有先进提示方法的领域专用模型提升10.7%。这一成果证实了该方法能有效降低领域专用模型预训练的需求。
