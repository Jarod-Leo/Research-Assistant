# A Survey of Techniques for Optimizing Transformer Inference

链接: http://arxiv.org/abs/2307.07982v1

原文摘要:
Recent years have seen a phenomenal rise in performance and applications of
transformer neural networks. The family of transformer networks, including
Bidirectional Encoder Representations from Transformer (BERT), Generative
Pretrained Transformer (GPT) and Vision Transformer (ViT), have shown their
effectiveness across Natural Language Processing (NLP) and Computer Vision (CV)
domains. Transformer-based networks such as ChatGPT have impacted the lives of
common men. However, the quest for high predictive performance has led to an
exponential increase in transformers' memory and compute footprint. Researchers
have proposed techniques to optimize transformer inference at all levels of
abstraction. This paper presents a comprehensive survey of techniques for
optimizing the inference phase of transformer networks. We survey techniques
such as knowledge distillation, pruning, quantization, neural architecture
search and lightweight network design at the algorithmic level. We further
review hardware-level optimization techniques and the design of novel hardware
accelerators for transformers. We summarize the quantitative results on the
number of parameters/FLOPs and accuracy of several models/techniques to
showcase the tradeoff exercised by them. We also outline future directions in
this rapidly evolving field of research. We believe that this survey will
educate both novice and seasoned researchers and also spark a plethora of
research efforts in this field.

中文翻译:
近年来，Transformer神经网络在性能与应用领域实现了显著突破。以双向编码器表示（BERT）、生成式预训练模型（GPT）和视觉Transformer（ViT）为代表的Transformer家族，已在自然语言处理（NLP）和计算机视觉（CV）领域展现出卓越效能。以ChatGPT为代表的Transformer网络正深刻影响着普通人的生活。然而，对高预测性能的追求导致模型内存需求和计算量呈指数级增长。研究者们提出了多层次的Transformer推理优化技术。

本文系统综述了Transformer网络推理阶段的优化方法：在算法层面梳理了知识蒸馏、剪枝、量化、神经架构搜索和轻量化网络设计等技术；在硬件层面探讨了优化方案及专用加速器设计。通过对比不同模型/技术的参数量/浮点运算次数（FLOPs）与准确率量化数据，揭示了其性能权衡机制。最后展望了这个快速发展领域的未来研究方向。本综述旨在为初阶与资深研究者提供知识参考，并激发该领域更广泛的研究探索。
