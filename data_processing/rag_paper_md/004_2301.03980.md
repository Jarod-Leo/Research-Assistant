# Language Models sounds the Death Knell of Knowledge Graphs

链接: http://arxiv.org/abs/2301.03980v1

原文摘要:
Healthcare domain generates a lot of unstructured and semi-structured text.
Natural Language processing (NLP) has been used extensively to process this
data. Deep Learning based NLP especially Large Language Models (LLMs) such as
BERT have found broad acceptance and are used extensively for many
applications. A Language Model is a probability distribution over a word
sequence. Self-supervised Learning on a large corpus of data automatically
generates deep learning-based language models. BioBERT and Med-BERT are
language models pre-trained for the healthcare domain. Healthcare uses typical
NLP tasks such as question answering, information extraction, named entity
recognition, and search to simplify and improve processes. However, to ensure
robust application of the results, NLP practitioners need to normalize and
standardize them. One of the main ways of achieving normalization and
standardization is the use of Knowledge Graphs. A Knowledge Graph captures
concepts and their relationships for a specific domain, but their creation is
time-consuming and requires manual intervention from domain experts, which can
prove expensive. SNOMED CT (Systematized Nomenclature of Medicine -- Clinical
Terms), Unified Medical Language System (UMLS), and Gene Ontology (GO) are
popular ontologies from the healthcare domain. SNOMED CT and UMLS capture
concepts such as disease, symptoms and diagnosis and GO is the world's largest
source of information on the functions of genes. Healthcare has been dealing
with an explosion in information about different types of drugs, diseases, and
procedures. This paper argues that using Knowledge Graphs is not the best
solution for solving problems in this domain. We present experiments using LLMs
for the healthcare domain to demonstrate that language models provide the same
functionality as knowledge graphs, thereby making knowledge graphs redundant.

中文翻译:
医疗领域产生了大量非结构化和半结构化文本数据。自然语言处理技术已被广泛应用于此类数据的处理，其中基于深度学习的自然语言处理方法——尤其是BERT等大语言模型——获得了广泛认可，并在众多应用场景中得到深入运用。语言模型本质上是针对词序列的概率分布，通过对海量语料库进行自监督学习，可自动生成基于深度学习的语言模型。BioBERT和Med-BERT就是专为医疗领域预训练的语言模型。

医疗领域通过典型自然语言处理任务（如问答系统、信息抽取、命名实体识别和智能检索）来简化和优化业务流程。但为确保应用结果的稳健性，自然语言处理从业者需对输出进行归一化和标准化处理。实现归一化与标准化的主要途径之一是知识图谱的应用。知识图谱能够捕捉特定领域的实体概念及其关联关系，但其构建过程耗时费力，需要领域专家的人工干预，成本往往十分高昂。SNOMED CT（医学系统命名法-临床术语）、统一医学语言系统（UMLS）和基因本体论（GO）是医疗领域广为人知的本体系统：SNOMED CT与UMLS涵盖疾病、症状和诊断等临床概念，GO则是全球最大的基因功能信息库。

当前医疗领域正面临着药物种类、疾病类型和治疗方案等信息爆炸式增长的挑战。本文通过基于大语言模型的医疗领域实验论证：语言模型可提供与知识图谱等同的功能，这使得知识图谱成为冗余方案。研究表明，在此领域采用知识图谱并非最优解决路径。
