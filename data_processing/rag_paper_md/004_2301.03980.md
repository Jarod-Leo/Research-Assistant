# Language Models sounds the Death Knell of Knowledge Graphs

链接: http://arxiv.org/abs/2301.03980v1

原文摘要:
Healthcare domain generates a lot of unstructured and semi-structured text.
Natural Language processing (NLP) has been used extensively to process this
data. Deep Learning based NLP especially Large Language Models (LLMs) such as
BERT have found broad acceptance and are used extensively for many
applications. A Language Model is a probability distribution over a word
sequence. Self-supervised Learning on a large corpus of data automatically
generates deep learning-based language models. BioBERT and Med-BERT are
language models pre-trained for the healthcare domain. Healthcare uses typical
NLP tasks such as question answering, information extraction, named entity
recognition, and search to simplify and improve processes. However, to ensure
robust application of the results, NLP practitioners need to normalize and
standardize them. One of the main ways of achieving normalization and
standardization is the use of Knowledge Graphs. A Knowledge Graph captures
concepts and their relationships for a specific domain, but their creation is
time-consuming and requires manual intervention from domain experts, which can
prove expensive. SNOMED CT (Systematized Nomenclature of Medicine -- Clinical
Terms), Unified Medical Language System (UMLS), and Gene Ontology (GO) are
popular ontologies from the healthcare domain. SNOMED CT and UMLS capture
concepts such as disease, symptoms and diagnosis and GO is the world's largest
source of information on the functions of genes. Healthcare has been dealing
with an explosion in information about different types of drugs, diseases, and
procedures. This paper argues that using Knowledge Graphs is not the best
solution for solving problems in this domain. We present experiments using LLMs
for the healthcare domain to demonstrate that language models provide the same
functionality as knowledge graphs, thereby making knowledge graphs redundant.

中文翻译:
医疗领域产生了大量非结构化和半结构化文本。自然语言处理技术（NLP）已被广泛用于处理这类数据。基于深度学习的NLP技术——尤其是BERT等大语言模型（LLMs）——已获得广泛认可，并被大量应用于各类场景。语言模型本质上是针对词序列的概率分布，通过对海量语料库进行自监督学习，可自动生成基于深度学习的语言模型。BioBERT和Med-BERT就是针对医疗领域预训练的语言模型。

医疗领域通常运用问答系统、信息抽取、命名实体识别和智能搜索等NLP任务来简化和优化流程。但为确保应用结果的稳健性，NLP从业者需对输出进行归一化和标准化处理，而知识图谱是实现这一目标的主要手段。知识图谱能捕捉特定领域的实体概念及其关联关系，但其构建过程耗时费力，需要领域专家人工介入，成本高昂。SNOMED CT（医学系统命名法-临床术语）、统一医学语言系统（UMLS）和基因本体（GO）是医疗领域的经典本体库——SNOMED CT与UMLS涵盖疾病、症状和诊断等临床概念，GO则是全球最大的基因功能信息库。

当前医疗领域正面临药物、疾病和治疗方案相关信息的爆炸式增长。本文论证了知识图谱并非解决该领域问题的最佳方案。我们通过大语言模型在医疗领域的实验证明：语言模型能提供与知识图谱等同的功能，从而使知识图谱变得冗余。
