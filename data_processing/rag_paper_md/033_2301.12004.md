# Understanding the Effectiveness of Very Large Language Models on Dialog Evaluation

链接: http://arxiv.org/abs/2301.12004v1

原文摘要:
Language models have steadily increased in size over the past few years. They
achieve a high level of performance on various natural language processing
(NLP) tasks such as question answering and summarization. Large language models
(LLMs) have been used for generation and can now output human-like text. Due to
this, there are other downstream tasks in the realm of dialog that can now
harness the LLMs' language understanding capabilities. Dialog evaluation is one
task that this paper will explore. It concentrates on prompting with LLMs:
BLOOM, OPT, GPT-3, Flan-T5, InstructDial and TNLGv2. The paper shows that the
choice of datasets used for training a model contributes to how well it
performs on a task as well as on how the prompt should be structured.
Specifically, the more diverse and relevant the group of datasets that a model
is trained on, the better dialog evaluation performs. This paper also
investigates how the number of examples in the prompt and the type of example
selection used affect the model's performance.

中文翻译:
近年来，语言模型的规模持续扩大，在问答、摘要生成等多种自然语言处理任务中展现出卓越性能。大型语言模型（LLMs）已能生成高度拟人化的文本，这使得对话领域的下游任务得以充分利用其语言理解能力。本文重点探讨对话评估任务，聚焦于对BLOOM、OPT、GPT-3、Flan-T5、InstructDial和TNLGv2等模型的提示策略研究。研究表明：模型训练数据集的选取不仅影响任务表现，更直接决定了提示结构的有效性——当模型在更多样化且相关性强的数据集组合上训练时，其对话评估表现更优。本文还深入分析了提示中示例数量及示例选择类型对模型性能的影响机制。
