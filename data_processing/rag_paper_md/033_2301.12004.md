# Understanding the Effectiveness of Very Large Language Models on Dialog Evaluation

链接: http://arxiv.org/abs/2301.12004v1

原文摘要:
Language models have steadily increased in size over the past few years. They
achieve a high level of performance on various natural language processing
(NLP) tasks such as question answering and summarization. Large language models
(LLMs) have been used for generation and can now output human-like text. Due to
this, there are other downstream tasks in the realm of dialog that can now
harness the LLMs' language understanding capabilities. Dialog evaluation is one
task that this paper will explore. It concentrates on prompting with LLMs:
BLOOM, OPT, GPT-3, Flan-T5, InstructDial and TNLGv2. The paper shows that the
choice of datasets used for training a model contributes to how well it
performs on a task as well as on how the prompt should be structured.
Specifically, the more diverse and relevant the group of datasets that a model
is trained on, the better dialog evaluation performs. This paper also
investigates how the number of examples in the prompt and the type of example
selection used affect the model's performance.

中文翻译:
过去几年，语言模型的规模持续扩大。这些模型在问答、文本摘要等多种自然语言处理（NLP）任务中展现出卓越性能。大型语言模型（LLM）已被用于文本生成领域，如今能输出高度拟人化的文本。基于此，对话领域的其他下游任务现在也能充分利用LLM的语言理解能力。本文重点探讨的对话评估正是其中一项任务，研究聚焦于对BLOOM、OPT、GPT-3、Flan-T5、InstructDial和TNLGv2等LLM的提示策略。研究表明：模型训练所用数据集的选取既影响其任务表现，也决定了提示模板的最优结构——当模型在更多样化且相关度高的数据集组合上训练时，其对话评估表现更优异。本文还探究了提示中示例数量及示例选择类型对模型性能的影响。
