# DrivingWorld: ConstructingWorld Model for Autonomous Driving via Video GPT

链接: http://arxiv.org/abs/2412.19505v1

原文摘要:
Recent successes in autoregressive (AR) generation models, such as the GPT
series in natural language processing, have motivated efforts to replicate this
success in visual tasks. Some works attempt to extend this approach to
autonomous driving by building video-based world models capable of generating
realistic future video sequences and predicting ego states. However, prior
works tend to produce unsatisfactory results, as the classic GPT framework is
designed to handle 1D contextual information, such as text, and lacks the
inherent ability to model the spatial and temporal dynamics essential for video
generation. In this paper, we present DrivingWorld, a GPT-style world model for
autonomous driving, featuring several spatial-temporal fusion mechanisms. This
design enables effective modeling of both spatial and temporal dynamics,
facilitating high-fidelity, long-duration video generation. Specifically, we
propose a next-state prediction strategy to model temporal coherence between
consecutive frames and apply a next-token prediction strategy to capture
spatial information within each frame. To further enhance generalization
ability, we propose a novel masking strategy and reweighting strategy for token
prediction to mitigate long-term drifting issues and enable precise control.
Our work demonstrates the ability to produce high-fidelity and consistent video
clips of over 40 seconds in duration, which is over 2 times longer than
state-of-the-art driving world models. Experiments show that, in contrast to
prior works, our method achieves superior visual quality and significantly more
accurate controllable future video generation. Our code is available at
https://github.com/YvanYin/DrivingWorld.

中文翻译:
自回归（AR）生成模型近期取得的成功，例如自然语言处理中的GPT系列，激发了在视觉任务中复制这一成就的努力。一些研究尝试将这一方法扩展到自动驾驶领域，通过构建基于视频的世界模型来生成逼真的未来视频序列并预测自车状态。然而，先前的研究往往效果不尽如人意，因为经典的GPT框架专为处理一维上下文信息（如文本）而设计，缺乏对视频生成至关重要的时空动态建模能力。

本文提出DrivingWorld，一种面向自动驾驶的GPT风格世界模型，融合了多项时空机制。该设计能有效建模空间与时间动态，支持高保真度、长时段的视频生成。具体而言，我们提出"下一状态预测"策略来建模连续帧间的时间连贯性，并采用"下一标记预测"策略捕捉单帧内的空间信息。为进一步增强泛化能力，我们创新性地提出标记预测的掩码策略和重加权策略，以缓解长期漂移问题并实现精准控制。

实验表明，相较于现有方法，我们的模型能够生成持续40秒以上的高保真连贯视频片段，时长达到当前最优驾驶世界模型的2倍以上。研究证实，该方法在视觉质量和可控未来视频生成的准确性上均显著优于先前工作。代码已开源在https://github.com/YvanYin/DrivingWorld。
