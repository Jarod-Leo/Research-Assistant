# Efficient Long-Range Transformers: You Need to Attend More, but Not Necessarily at Every Layer

链接: http://arxiv.org/abs/2310.12442v1

原文摘要:
Pretrained transformer models have demonstrated remarkable performance across
various natural language processing tasks. These models leverage the attention
mechanism to capture long- and short-range dependencies in the sequence.
However, the (full) attention mechanism incurs high computational cost -
quadratic in the sequence length, which is not affordable in tasks with long
sequences, e.g., inputs with 8k tokens. Although sparse attention can be used
to improve computational efficiency, as suggested in existing work, it has
limited modeling capacity and often fails to capture complicated dependencies
in long sequences. To tackle this challenge, we propose MASFormer, an
easy-to-implement transformer variant with Mixed Attention Spans. Specifically,
MASFormer is equipped with full attention to capture long-range dependencies,
but only at a small number of layers. For the remaining layers, MASformer only
employs sparse attention to capture short-range dependencies. Our experiments
on natural language modeling and generation tasks show that a decoder-only
MASFormer model of 1.3B parameters can achieve competitive performance to
vanilla transformers with full attention while significantly reducing
computational cost (up to 75%). Additionally, we investigate the effectiveness
of continual training with long sequence data and how sequence length impacts
downstream generation performance, which may be of independent interest.

中文翻译:
预训练Transformer模型在各类自然语言处理任务中展现出卓越性能，其核心注意力机制能有效捕捉序列中的长程与短程依赖关系。然而，传统（全）注意力机制的计算成本高昂——与序列长度呈平方级增长，在处理长序列任务（如包含8000个标记的输入）时难以承受。尽管现有研究表明稀疏注意力可提升计算效率，但其建模能力有限，往往难以捕捉长序列中的复杂依赖关系。为应对这一挑战，我们提出MASFormer——一种易于实现的混合注意力跨度Transformer变体。该模型在少数层级保留全注意力以捕获长程依赖，其余层级仅采用稀疏注意力处理短程依赖。在自然语言建模与生成任务上的实验表明，仅含13亿参数的纯解码器MASFormer模型在显著降低计算成本（最高达75%）的同时，性能可与全注意力标准Transformer相媲美。此外，我们还探究了长序列数据持续训练的有效性，以及序列长度对下游生成任务的影响，这些发现可能具有独立研究价值。
