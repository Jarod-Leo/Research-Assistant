# BBT-Fin: Comprehensive Construction of Chinese Financial Domain Pre-trained Language Model, Corpus and Benchmark

链接: http://arxiv.org/abs/2302.09432v1

原文摘要:
To advance Chinese financial natural language processing (NLP), we introduce
BBT-FinT5, a new Chinese financial pre-training language model based on the T5
model. To support this effort, we have built BBT-FinCorpus, a large-scale
financial corpus with approximately 300GB of raw text from four different
sources. In general domain NLP, comprehensive benchmarks like GLUE and
SuperGLUE have driven significant advancements in language model pre-training
by enabling head-to-head comparisons among models. Drawing inspiration from
these benchmarks, we propose BBT-CFLEB, a Chinese Financial Language
understanding and generation Evaluation Benchmark, which includes six datasets
covering both understanding and generation tasks. Our aim is to facilitate
research in the development of NLP within the Chinese financial domain. Our
model, corpus and benchmark are released at
https://github.com/ssymmetry/BBT-FinCUGE-Applications. Our work belongs to the
Big Bang Transformer (BBT), a large-scale pre-trained language model project.

中文翻译:
为推动中文金融自然语言处理（NLP）的发展，我们推出了基于T5架构的新型中文金融预训练语言模型BBT-FinT5。为支撑该研究，我们构建了BBT-FinCorpus大规模金融语料库，整合了来自四个不同来源、总量约300GB的原始文本数据。在通用领域NLP中，GLUE和SuperGLUE等综合性基准测试通过实现模型间的直接性能对比，显著推动了语言模型预训练技术的进步。受此启发，我们提出了中文金融语言理解与生成评估基准BBT-CFLEB，该基准包含六个数据集，覆盖理解与生成两类任务。我们的目标是为中文金融领域NLP研究提供基础设施支持。相关模型、语料库及基准测试已发布于https://github.com/ssymmetry/BBT-FinCUGE-Applications。本项工作隶属于"大爆炸Transformer"（BBT）大规模预训练语言模型项目框架。
