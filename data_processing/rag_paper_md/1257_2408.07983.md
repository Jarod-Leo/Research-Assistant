# ArabLegalEval: A Multitask Benchmark for Assessing Arabic Legal Knowledge in Large Language Models

链接: http://arxiv.org/abs/2408.07983v1

原文摘要:
The rapid advancements in Large Language Models (LLMs) have led to
significant improvements in various natural language processing tasks. However,
the evaluation of LLMs' legal knowledge, particularly in non-English languages
such as Arabic, remains under-explored. To address this gap, we introduce
ArabLegalEval, a multitask benchmark dataset for assessing the Arabic legal
knowledge of LLMs. Inspired by the MMLU and LegalBench datasets, ArabLegalEval
consists of multiple tasks sourced from Saudi legal documents and synthesized
questions. In this work, we aim to analyze the capabilities required to solve
legal problems in Arabic and benchmark the performance of state-of-the-art
LLMs. We explore the impact of in-context learning and investigate various
evaluation methods. Additionally, we explore workflows for generating questions
with automatic validation to enhance the dataset's quality. We benchmark
multilingual and Arabic-centric LLMs, such as GPT-4 and Jais, respectively. We
also share our methodology for creating the dataset and validation, which can
be generalized to other domains. We hope to accelerate AI research in the
Arabic Legal domain by releasing the ArabLegalEval dataset and code:
https://github.com/Thiqah/ArabLegalEval

中文翻译:
大型语言模型（LLM）的快速发展显著提升了各类自然语言处理任务的性能。然而，针对LLM法律知识评估的研究，特别是在阿拉伯语等非英语语言领域仍显不足。为填补这一空白，我们推出ArabLegalEval——一个多任务基准数据集，用于评估LLM的阿拉伯语法律知识。该数据集受MMLU和LegalBench启发，包含源自沙特法律文件及合成问题的多项任务。本研究旨在分析解决阿拉伯语法律问题所需的能力，并对前沿LLM进行性能基准测试。我们探究了上下文学习的影响，并比较了不同评估方法。此外，我们还探索了通过自动验证生成问题以提升数据集质量的工作流程。我们对多语言模型（如GPT-4）和阿拉伯语专用模型（如Jais）进行了基准测试，同时公开了可推广至其他领域的数据集构建与验证方法。希望通过发布ArabLegalEval数据集及代码（https://github.com/Thiqah/ArabLegalEval），加速阿拉伯法律领域的人工智能研究。
