# MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models

链接: http://arxiv.org/abs/2403.19913v1

原文摘要:
Large language models such as ChatGPT and GPT-4 have recently achieved
astonishing performance on a variety of natural language processing tasks. In
this paper, we propose MANGO, a benchmark to evaluate their capabilities to
perform text-based mapping and navigation. Our benchmark includes 53 mazes
taken from a suite of textgames: each maze is paired with a walkthrough that
visits every location but does not cover all possible paths. The task is
question-answering: for each maze, a large language model reads the walkthrough
and answers hundreds of mapping and navigation questions such as "How should
you go to Attic from West of House?" and "Where are we if we go north and east
from Cellar?". Although these questions are easy to humans, it turns out that
even GPT-4, the best-to-date language model, performs poorly at answering them.
Further, our experiments suggest that a strong mapping and navigation ability
would benefit large language models in performing relevant downstream tasks,
such as playing textgames. Our MANGO benchmark will facilitate future research
on methods that improve the mapping and navigation capabilities of language
models. We host our leaderboard, data, code, and evaluation program at
https://mango.ttic.edu and https://github.com/oaklight/mango/.

中文翻译:
诸如ChatGPT和GPT-4等大型语言模型近期在多种自然语言处理任务中展现出惊人性能。本文提出MANGO评估基准，用于检验其执行基于文本的路径规划与导航任务的能力。该基准包含从文本冒险游戏套件中选取的53个迷宫场景：每个迷宫均配有遍历所有地点但未覆盖全部路径的攻略文本。任务形式为问答测试：针对每个迷宫，语言模型需阅读攻略后回答数百道路径规划与导航类问题，例如"如何从西屋到达阁楼？"或"从地窖向北再向东会到达哪里？"。虽然这些问题对人类而言十分简单，但实验表明即便是当前最先进的GPT-4模型也表现欠佳。进一步研究表明，强大的空间映射与导航能力将显著提升语言模型在相关下游任务（如文本游戏交互）中的表现。MANGO基准将为改进语言模型空间认知能力的后续研究提供支持，相关排行榜、数据、代码及评估程序已发布于https://mango.ttic.edu 与 https://github.com/oaklight/mango/。
