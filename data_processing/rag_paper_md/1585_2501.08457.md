# Large Language Models For Text Classification: Case Study And Comprehensive Review

链接: http://arxiv.org/abs/2501.08457v1

原文摘要:
Unlocking the potential of Large Language Models (LLMs) in data
classification represents a promising frontier in natural language processing.
In this work, we evaluate the performance of different LLMs in comparison with
state-of-the-art deep-learning and machine-learning models, in two different
classification scenarios: i) the classification of employees' working locations
based on job reviews posted online (multiclass classification), and 2) the
classification of news articles as fake or not (binary classification). Our
analysis encompasses a diverse range of language models differentiating in
size, quantization, and architecture. We explore the impact of alternative
prompting techniques and evaluate the models based on the weighted F1-score.
Also, we examine the trade-off between performance (F1-score) and time
(inference response time) for each language model to provide a more nuanced
understanding of each model's practical applicability. Our work reveals
significant variations in model responses based on the prompting strategies. We
find that LLMs, particularly Llama3 and GPT-4, can outperform traditional
methods in complex classification tasks, such as multiclass classification,
though at the cost of longer inference times. In contrast, simpler ML models
offer better performance-to-time trade-offs in simpler binary classification
tasks.

中文翻译:
释放大型语言模型（LLMs）在数据分类领域的潜力，是自然语言处理中一个极具前景的前沿方向。本研究评估了不同LLMs与当前最先进的深度学习和机器学习模型在两种分类场景下的表现：1）基于在线发布的职位评价对员工工作地点进行分类（多类别分类）；2）将新闻文章分类为虚假或非虚假（二分类）。我们的分析涵盖了规模、量化和架构各异的多种语言模型，探究了不同提示技术的影响，并以加权F1分数作为评估标准。同时，我们考察了各语言模型在性能（F1分数）与时间（推理响应时间）之间的权衡关系，从而更细致地理解每个模型的实际适用性。

研究发现，模型响应会因提示策略的不同而产生显著差异。结果表明，在复杂分类任务（如多类别分类）中，LLMs（尤其是Llama3和GPT-4）能够超越传统方法，但代价是更长的推理时间。相比之下，在简单的二分类任务中，传统机器学习模型能提供更优的性能与时间平衡。
