# Translating Natural Language to Planning Goals with Large-Language Models

链接: http://arxiv.org/abs/2302.05128v1

原文摘要:
Recent large language models (LLMs) have demonstrated remarkable performance
on a variety of natural language processing (NLP) tasks, leading to intense
excitement about their applicability across various domains. Unfortunately,
recent work has also shown that LLMs are unable to perform accurate reasoning
nor solve planning problems, which may limit their usefulness for
robotics-related tasks. In this work, our central question is whether LLMs are
able to translate goals specified in natural language to a structured planning
language. If so, LLM can act as a natural interface between the planner and
human users; the translated goal can be handed to domain-independent AI
planners that are very effective at planning. Our empirical results on GPT 3.5
variants show that LLMs are much better suited towards translation rather than
planning. We find that LLMs are able to leverage commonsense knowledge and
reasoning to furnish missing details from under-specified goals (as is often
the case in natural language). However, our experiments also reveal that LLMs
can fail to generate goals in tasks that involve numerical or physical (e.g.,
spatial) reasoning, and that LLMs are sensitive to the prompts used. As such,
these models are promising for translation to structured planning languages,
but care should be taken in their use.

中文翻译:
近期的大型语言模型（LLM）在各类自然语言处理任务中展现出卓越性能，引发了对其跨领域应用潜力的广泛关注。然而研究表明，这类模型无法进行精确推理或解决规划问题，这可能限制其在机器人相关任务中的实用性。本研究聚焦于探究LLM能否将自然语言描述的目标转化为结构化规划语言。若可行，LLM便能作为规划器与人类用户间的自然接口——转化后的目标可交由擅长规划任务的领域无关AI规划器处理。基于GPT-3.5系列模型的实验表明，LLM更擅长语言转换而非直接规划：它们能利用常识知识和推理补全自然语言中常见的模糊目标细节。但实验也揭示，当任务涉及数值或物理（如空间）推理时，LLM可能生成错误目标，且对提示词选择敏感。因此，这类模型在结构化规划语言转换方面前景可观，但实际应用中需谨慎对待。
