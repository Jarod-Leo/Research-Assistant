# Fisher Mask Nodes for Language Model Merging

链接: http://arxiv.org/abs/2403.09891v1

原文摘要:
Fine-tuning pre-trained models provides significant advantages in downstream
performance. The ubiquitous nature of pre-trained models such as BERT and its
derivatives in natural language processing has also led to a proliferation of
task-specific fine-tuned models. As these models typically only perform one
task well, additional training or ensembling is required in multi-task
scenarios. The growing field of model merging provides a solution, dealing with
the challenge of combining multiple task-specific models into a single
multi-task model. In this study, we introduce a novel model merging method for
Transformers, combining insights from previous work in Fisher-weighted
averaging and the use of Fisher information in model pruning. Utilizing the
Fisher information of mask nodes within the Transformer architecture, we devise
a computationally efficient weighted-averaging scheme. Our method exhibits a
regular and significant performance increase across various models in the BERT
family, outperforming full-scale Fisher-weighted averaging in a fraction of the
computational cost, with baseline performance improvements of up to +6.5 and a
speedup between 57.4x and 321.7x across models. Our results prove the potential
of our method in current multi-task learning environments and suggest its
scalability and adaptability to new model architectures and learning scenarios.

中文翻译:
微调预训练模型在下游任务性能上展现出显著优势。以BERT及其衍生模型为代表的预训练模型在自然语言处理领域的普及，也催生了大量针对特定任务进行微调的模型。由于这些模型通常仅擅长单一任务，在多任务场景中需要额外训练或集成处理。快速发展的模型融合领域为此提供了解决方案，致力于将多个专用模型整合为统一的多任务模型。本研究提出了一种创新的Transformer模型融合方法，结合了Fisher加权平均的既有研究成果与模型剪枝中Fisher信息的应用经验。通过利用Transformer架构中掩码节点的Fisher信息，我们设计出计算高效的加权平均方案。该方法在BERT系列各类模型中均表现出稳定且显著的性能提升：以仅需部分计算成本的代价超越全量Fisher加权平均的效果，基线性能最高提升达+6.5，计算速度在不同模型上实现57.4至321.7倍的加速。实验结果证实了该方法在当前多任务学习环境中的潜力，并表明其对新模型架构和学习场景的扩展性与适应能力。
