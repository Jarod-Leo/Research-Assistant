# Augmented Large Language Models with Parametric Knowledge Guiding

链接: http://arxiv.org/abs/2305.04757v1

原文摘要:
Large Language Models (LLMs) have significantly advanced natural language
processing (NLP) with their impressive language understanding and generation
capabilities. However, their performance may be suboptimal for domain-specific
tasks that require specialized knowledge due to limited exposure to the related
data. Additionally, the lack of transparency of most state-of-the-art (SOTA)
LLMs, which can only be accessed via APIs, impedes further fine-tuning with
domain custom data. Moreover, providing private data to the LLMs' owner leads
to data privacy problems. To address these challenges, we propose the novel
Parametric Knowledge Guiding (PKG) framework, which equips LLMs with a
knowledge-guiding module to access relevant knowledge without altering the
LLMs' parameters. Our PKG is based on open-source "white-box" language models,
allowing offline memory of any knowledge that LLMs require. We demonstrate that
our PKG framework can enhance the performance of "black-box" LLMs on a range of
domain knowledge-intensive tasks that require factual (+7.9%), tabular
(+11.9%), medical (+3.0%), and multimodal (+8.1%) knowledge.

中文翻译:
大型语言模型（LLMs）凭借其卓越的语言理解与生成能力，显著推动了自然语言处理（NLP）领域的发展。然而，由于缺乏相关领域数据的充分接触，这类模型在需要专业知识的特定领域任务中表现可能欠佳。此外，当前最先进（SOTA）的LLMs大多缺乏透明度（仅能通过API访问），这阻碍了利用领域定制数据进行微调的可能性。更值得注意的是，向LLMs所有者提供私有数据会引发数据隐私问题。为应对这些挑战，我们提出创新的参数化知识引导（PKG）框架，该框架通过为LLMs配备知识引导模块，使其无需修改模型参数即可获取相关知识。我们的PKG基于开源"白盒"语言模型，可离线存储LLMs所需的任何知识。实验表明，PKG框架能显著提升"黑盒"LLMs在多个领域知识密集型任务中的表现，包括事实性（+7.9%）、表格（+11.9%）、医学（+3.0%）和多模态（+8.1%）知识需求场景。

（翻译说明：采用技术文献的严谨表述风格，通过以下处理确保专业性与可读性：
1. 术语统一处理："state-of-the-art"译为"最先进"，"white-box/black-box"保留比喻性译法
2. 长句拆分重构：将原文复合句分解为符合中文表达习惯的短句，如API访问限制相关描述
3. 被动语态转化："can only be accessed"转为主动式"仅能通过API访问"
4. 数据呈现优化：百分比增幅保留原文数字形式，符合科技论文规范
5. 概念准确传达："parametric knowledge guiding"译为"参数化知识引导"既保留术语特征又体现技术内涵）
