# Augmented Large Language Models with Parametric Knowledge Guiding

链接: http://arxiv.org/abs/2305.04757v1

原文摘要:
Large Language Models (LLMs) have significantly advanced natural language
processing (NLP) with their impressive language understanding and generation
capabilities. However, their performance may be suboptimal for domain-specific
tasks that require specialized knowledge due to limited exposure to the related
data. Additionally, the lack of transparency of most state-of-the-art (SOTA)
LLMs, which can only be accessed via APIs, impedes further fine-tuning with
domain custom data. Moreover, providing private data to the LLMs' owner leads
to data privacy problems. To address these challenges, we propose the novel
Parametric Knowledge Guiding (PKG) framework, which equips LLMs with a
knowledge-guiding module to access relevant knowledge without altering the
LLMs' parameters. Our PKG is based on open-source "white-box" language models,
allowing offline memory of any knowledge that LLMs require. We demonstrate that
our PKG framework can enhance the performance of "black-box" LLMs on a range of
domain knowledge-intensive tasks that require factual (+7.9%), tabular
(+11.9%), medical (+3.0%), and multimodal (+8.1%) knowledge.

中文翻译:
大型语言模型（LLMs）凭借卓越的语言理解与生成能力，显著推动了自然语言处理（NLP）领域的发展。然而，在需要专业知识的特定领域任务中，由于相关数据接触有限，其表现可能不尽如人意。加之当前最先进的LLMs多为仅能通过API访问的"黑箱"系统，缺乏透明度，导致无法利用领域定制数据进行微调。此外，向模型所有者提供私有数据还会引发隐私泄露风险。为应对这些挑战，我们提出创新的参数化知识引导（PKG）框架，通过为LLMs配备知识引导模块，使其无需修改模型参数即可获取相关知识。该框架基于开源的"白盒"语言模型，可离线存储LLMs所需的任何知识。实验表明，PKG能显著提升"黑箱"LLMs在需要事实性（+7.9%）、表格（+11.9%）、医学（+3.0%）及多模态（+8.1%）知识的领域密集型任务中的表现。
