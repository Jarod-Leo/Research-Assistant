# SGL-PT: A Strong Graph Learner with Graph Prompt Tuning

链接: http://arxiv.org/abs/2302.12449v1

原文摘要:
Recently, much exertion has been paid to design graph self-supervised methods
to obtain generalized pre-trained models, and adapt pre-trained models onto
downstream tasks through fine-tuning. However, there exists an inherent gap
between pretext and downstream graph tasks, which insufficiently exerts the
ability of pre-trained models and even leads to negative transfer. Meanwhile,
prompt tuning has seen emerging success in natural language processing by
aligning pre-training and fine-tuning with consistent training objectives. In
this paper, we identify the challenges for graph prompt tuning: The first is
the lack of a strong and universal pre-training task across sundry pre-training
methods in graph domain. The second challenge lies in the difficulty of
designing a consistent training objective for both pre-training and downstream
tasks. To overcome above obstacles, we propose a novel framework named SGL-PT
which follows the learning strategy ``Pre-train, Prompt, and Predict''.
Specifically, we raise a strong and universal pre-training task coined as SGL
that acquires the complementary merits of generative and contrastive
self-supervised graph learning. And aiming for graph classification task, we
unify pre-training and fine-tuning by designing a novel verbalizer-free
prompting function, which reformulates the downstream task in a similar format
as pretext task. Empirical results show that our method surpasses other
baselines under unsupervised setting, and our prompt tuning method can greatly
facilitate models on biological datasets over fine-tuning methods.

中文翻译:
近年来，研究者们投入大量精力设计图自监督学习方法，旨在获取通用预训练模型，并通过微调将预训练模型适配至下游任务。然而，预训练任务与下游图任务之间存在固有差距，这不仅限制了预训练模型性能的充分发挥，甚至可能导致负迁移现象。与此同时，提示调优（prompt tuning）技术通过保持预训练与微调阶段目标的一致性，已在自然语言处理领域展现出显著成效。本文揭示了图提示调优面临的两大核心挑战：其一，图领域缺乏跨多种预训练方法的强效通用预训练任务；其二，设计预训练与下游任务间统一的训练目标存在困难。为突破这些障碍，我们提出名为SGL-PT的创新框架，遵循"预训练-提示-预测"的学习范式。具体而言，我们提出名为SGL的强效通用预训练任务，融合了生成式与对比式自监督图学习的互补优势。针对图分类任务，我们设计了一种无需词表转换的新型提示函数，通过将下游任务重构为与预训练任务相似的格式实现目标统一。实验结果表明，在无监督设定下我们的方法显著优于基线模型，且在生物数据集上，相较于传统微调方法，提示调优能大幅提升模型性能。
