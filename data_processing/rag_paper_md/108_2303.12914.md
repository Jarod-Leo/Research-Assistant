# TRON: Transformer Neural Network Acceleration with Non-Coherent Silicon Photonics

链接: http://arxiv.org/abs/2303.12914v1

原文摘要:
Transformer neural networks are rapidly being integrated into
state-of-the-art solutions for natural language processing (NLP) and computer
vision. However, the complex structure of these models creates challenges for
accelerating their execution on conventional electronic platforms. We propose
the first silicon photonic hardware neural network accelerator called TRON for
transformer-based models such as BERT, and Vision Transformers. Our analysis
demonstrates that TRON exhibits at least 14x better throughput and 8x better
energy efficiency, in comparison to state-of-the-art transformer accelerators.

中文翻译:
Transformer神经网络正迅速融入自然语言处理(NLP)与计算机视觉领域的前沿解决方案。然而，这类模型的复杂结构为其在传统电子平台上的加速执行带来了挑战。我们提出了首个名为TRON的硅基光子硬件神经网络加速器，专为BERT、Vision Transformers等基于Transformer的模型设计。分析表明，与当前最先进的Transformer加速器相比，TRON在吞吐量上至少提升14倍，能效比提高8倍。
