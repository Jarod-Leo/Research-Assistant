# TRON: Transformer Neural Network Acceleration with Non-Coherent Silicon Photonics

链接: http://arxiv.org/abs/2303.12914v1

原文摘要:
Transformer neural networks are rapidly being integrated into
state-of-the-art solutions for natural language processing (NLP) and computer
vision. However, the complex structure of these models creates challenges for
accelerating their execution on conventional electronic platforms. We propose
the first silicon photonic hardware neural network accelerator called TRON for
transformer-based models such as BERT, and Vision Transformers. Our analysis
demonstrates that TRON exhibits at least 14x better throughput and 8x better
energy efficiency, in comparison to state-of-the-art transformer accelerators.

中文翻译:
Transformer神经网络正迅速成为自然语言处理（NLP）与计算机视觉领域前沿解决方案的核心组件。然而，这类模型的复杂结构为其在传统电子平台上的运算加速带来了挑战。我们首次提出名为TRON的硅基光子硬件神经网络加速器，专为BERT、Vision Transformers等基于Transformer的模型设计。分析表明，相较于当前最先进的Transformer加速器，TRON在吞吐量上至少提升14倍，能效比提高8倍以上。

（翻译说明：
1. 专业术语处理：采用"硅基光子"对应"silicon photonic"，"能效比"对应"energy efficiency"等业界通用译法
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句，如将"accelerating their execution..."处理为独立分句
3. 被动语态转换："are rapidly being integrated"译为主动态"正迅速成为"
4. 数据呈现：保留"14x/8x"的精确倍数关系，采用"提升14倍"的规范科技文本表述
5. 品牌名称保留：TRON作为专有名称保持大写不译，BERT/Vision Transformers等模型名称维持原貌）
