# MoSECroT: Model Stitching with Static Word Embeddings for Crosslingual Zero-shot Transfer

链接: http://arxiv.org/abs/2401.04821v1

原文摘要:
Transformer-based pre-trained language models (PLMs) have achieved remarkable
performance in various natural language processing (NLP) tasks. However,
pre-training such models can take considerable resources that are almost only
available to high-resource languages. On the contrary, static word embeddings
are easier to train in terms of computing resources and the amount of data
required. In this paper, we introduce MoSECroT Model Stitching with Static Word
Embeddings for Crosslingual Zero-shot Transfer), a novel and challenging task
that is especially relevant to low-resource languages for which static word
embeddings are available. To tackle the task, we present the first framework
that leverages relative representations to construct a common space for the
embeddings of a source language PLM and the static word embeddings of a target
language. In this way, we can train the PLM on source-language training data
and perform zero-shot transfer to the target language by simply swapping the
embedding layer. However, through extensive experiments on two classification
datasets, we show that although our proposed framework is competitive with weak
baselines when addressing MoSECroT, it fails to achieve competitive results
compared with some strong baselines. In this paper, we attempt to explain this
negative result and provide several thoughts on possible improvement.

中文翻译:
基于Transformer的预训练语言模型（PLMs）在各类自然语言处理（NLP）任务中展现出卓越性能。然而，此类模型的预训练过程需要消耗大量计算资源，通常仅适用于高资源语言。相比之下，静态词向量在计算资源和数据需求量方面更易训练。本文提出MoSECroT（基于静态词向量的跨语言零样本迁移模型拼接）这一新颖且富有挑战性的任务，该任务尤其适用于已具备静态词向量的低资源语言。为解决该任务，我们首次提出利用相对表示法构建源语言PLM嵌入与目标语言静态词向量共同空间的框架。通过这种方式，可在源语言训练数据上训练PLM，并通过简单替换嵌入层实现向目标语言的零样本迁移。然而，在两个分类数据集上的大量实验表明，尽管该框架在处理MoSECroT任务时能与弱基线方法竞争，但相较于某些强基线方法仍无法取得理想效果。本文尝试对这一负面结果进行解释，并就可能的改进方向提出若干思考。
