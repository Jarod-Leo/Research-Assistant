# RELIEF: Reinforcement Learning Empowered Graph Feature Prompt Tuning

链接: http://arxiv.org/abs/2408.03195v1

原文摘要:
The advent of the "pre-train, prompt" paradigm has recently extended its
generalization ability and data efficiency to graph representation learning,
following its achievements in Natural Language Processing (NLP). Initial graph
prompt tuning approaches tailored specialized prompting functions for Graph
Neural Network (GNN) models pre-trained with specific strategies, such as edge
prediction, thus limiting their applicability. In contrast, another pioneering
line of research has explored universal prompting via adding prompts to the
input graph's feature space, thereby removing the reliance on specific
pre-training strategies. However, the necessity to add feature prompts to all
nodes remains an open question. Motivated by findings from prompt tuning
research in the NLP domain, which suggest that highly capable pre-trained
models need less conditioning signal to achieve desired behaviors, we advocate
for strategically incorporating necessary and lightweight feature prompts to
certain graph nodes to enhance downstream task performance. This introduces a
combinatorial optimization problem, requiring a policy to decide 1) which nodes
to prompt and 2) what specific feature prompts to attach. We then address the
problem by framing the prompt incorporation process as a sequential
decision-making problem and propose our method, RELIEF, which employs
Reinforcement Learning (RL) to optimize it. At each step, the RL agent selects
a node (discrete action) and determines the prompt content (continuous action),
aiming to maximize cumulative performance gain. Extensive experiments on graph
and node-level tasks with various pre-training strategies in few-shot scenarios
demonstrate that our RELIEF outperforms fine-tuning and other prompt-based
approaches in classification performance and data efficiency. The code is
available at https://github.com/JasonZhujp/RELIEF.

中文翻译:
“预训练-提示”范式的出现，在自然语言处理（NLP）领域取得成就后，近期将其泛化能力和数据效率优势扩展至图表示学习领域。早期的图提示调优方法针对特定预训练策略（如边预测）的图神经网络（GNN）模型定制专用提示函数，导致适用性受限。相比之下，另一开创性研究探索了通过向输入图的特征空间添加提示来实现通用提示，从而摆脱对特定预训练策略的依赖。然而，是否需要对所有节点添加特征提示仍存疑问。受NLP领域提示调优研究的启发——该研究表明能力强的预训练模型只需较少调节信号即可实现预期行为——我们主张战略性地向特定图节点注入必要且轻量的特征提示以提升下游任务性能。这引出了一个组合优化问题：需要制定策略来决定1）对哪些节点添加提示，2）附加何种具体特征提示。我们将提示注入过程建模为序列决策问题，提出RELIEF方法，利用强化学习（RL）进行优化。RL智能体在每一步选择节点（离散动作）并确定提示内容（连续动作），以最大化累积性能增益。在少样本场景下，针对多种预训练策略的图级和节点级任务实验表明，RELIEF在分类性能和数据效率上均优于微调及其他基于提示的方法。代码已开源：https://github.com/JasonZhujp/RELIEF。
