# Graph Convolutions Enrich the Self-Attention in Transformers!

链接: http://arxiv.org/abs/2312.04234v1

原文摘要:
Transformers, renowned for their self-attention mechanism, have achieved
state-of-the-art performance across various tasks in natural language
processing, computer vision, time-series modeling, etc. However, one of the
challenges with deep Transformer models is the oversmoothing problem, where
representations across layers converge to indistinguishable values, leading to
significant performance degradation. We interpret the original self-attention
as a simple graph filter and redesign it from a graph signal processing (GSP)
perspective. We propose a graph-filter-based self-attention (GFSA) to learn a
general yet effective one, whose complexity, however, is slightly larger than
that of the original self-attention mechanism. We demonstrate that GFSA
improves the performance of Transformers in various fields, including computer
vision, natural language processing, graph-level tasks, speech recognition, and
code classification.

中文翻译:
Transformer模型凭借其自注意力机制，在自然语言处理、计算机视觉、时间序列建模等多个领域取得了最先进的性能表现。然而，深度Transformer模型面临的核心挑战之一是过度平滑问题——随着网络层数加深，各层表征会逐渐收敛至难以区分的相似值，从而导致模型性能显著下降。本研究将原始自注意力机制重新解读为一种简单的图滤波器，并从图信号处理（GSP）的理论视角进行重构。我们提出基于图滤波的自注意力机制（GFSA），该机制能够学习到通用且高效的注意力模式，尽管其计算复杂度略高于原始自注意力结构。实验验证表明，GFSA有效提升了Transformer在计算机视觉、自然语言处理、图级别任务、语音识别及代码分类等跨领域任务中的性能表现。
