# Taipan: Efficient and Expressive State Space Language Models with Selective Attention

链接: http://arxiv.org/abs/2410.18572v1

原文摘要:
Efficient long-context language modeling remains a significant challenge in
Natural Language Processing (NLP). While Transformers dominate language tasks,
they struggle with long sequences due to quadratic computational complexity in
training and linearly scaling memory costs during inference. Recent State Space
Models (SSMs) such as Mamba offer alternatives with constant memory usage, but
they underperform in tasks requiring extensive in-context retrieval. We
introduce Taipan, a novel hybrid architecture that combines Mamba-2 with
Selective Attention Layers (SALs). These SALs identify tokens requiring
long-range interactions, remove less important features, and then augment their
representations using the attention module. This approach balances Mamba's
efficiency with Transformer-like performance in memory-intensive tasks. By
constraining the attention budget, Taipan extends accurate predictions to
context lengths of up to 1 million tokens while preserving computational
efficiency. Our experiments demonstrate Taipan's superior performance across
various scales and tasks, offering a promising solution for efficient
long-context language modeling.

中文翻译:
高效的长上下文语言建模仍然是自然语言处理（NLP）领域的一项重大挑战。尽管Transformer模型在语言任务中占据主导地位，但由于训练时的二次计算复杂度和推理时线性增长的内存开销，其处理长序列时表现受限。近期如Mamba等状态空间模型（SSMs）虽能保持恒定内存占用，但在需要大量上下文检索的任务中表现欠佳。我们提出Taipan——一种创新混合架构，将Mamba-2与选择性注意力层（SALs）相结合。这些SALs能识别需要长距离交互的标记，过滤次要特征，并通过注意力模块增强其表征。该方法既保持了Mamba的高效性，又在内存密集型任务中实现了类Transformer的性能。通过约束注意力预算，Taipan在保持计算效率的同时，将准确预测的上下文长度扩展至100万标记。实验表明，Taipan在不同规模和任务中均展现出卓越性能，为高效长上下文语言建模提供了突破性解决方案。
