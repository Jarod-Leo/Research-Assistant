# Large Language Model Enabled Semantic Communication Systems

链接: http://arxiv.org/abs/2407.14112v1

原文摘要:
Large language models (LLMs) have recently demonstrated state-of-the-art
performance across various natural language processing (NLP) tasks, achieving
near-human levels in multiple language understanding challenges and aligning
closely with the core principles of semantic communication. Inspired by LLMs'
advancements in semantic processing, we propose an innovative LLM-enabled
semantic communication system framework, named LLM-SC, that applies LLMs
directly to the physical layer coding and decoding for the first time. By
analyzing the relationship between the training process of LLMs and the
optimization objectives of semantic communication, we propose training a
semantic encoder through LLMs' tokenizer training and establishing a semantic
knowledge base via the LLMs' unsupervised pre-training process. This knowledge
base aids in constructing the optimal decoder by providing the prior
probability of the transmitted language sequence. Based on this foundation, we
derive the optimal decoding criterion for the receiver and introduce the beam
search algorithm to further reduce the complexity. Furthermore, we assert that
existing LLMs can be employed directly for LLM-SC without additional
re-training or fine-tuning. Simulation results demonstrate that LLM-SC
outperforms classical DeepSC at signal-to-noise ratios (SNR) exceeding 3 dB,
enabling error-free transmission of semantic information under high SNR, which
is unattainable by DeepSC. In addition to semantic-level performance, LLM-SC
demonstrates compatibility with technical-level performance, achieving
approximately 8 dB coding gain for a bit error ratio (BER) of $10^{-3}$ without
any channel coding while maintaining the same joint source-channel coding rate
as traditional communication systems.

中文翻译:
大型语言模型（LLM）近期在各类自然语言处理（NLP）任务中展现出最先进的性能，在多项语言理解挑战中达到接近人类水平的表现，与语义通信的核心原理高度契合。受LLM在语义处理领域突破的启发，我们首次提出将LLM直接应用于物理层编解码的创新性语义通信系统框架——LLM-SC。通过分析LLM训练过程与语义通信优化目标之间的关联性，我们提出利用LLM的分词器训练实现语义编码器构建，并借助LLM的无监督预训练过程建立语义知识库。该知识库通过提供传输语言序列的先验概率，辅助构建最优解码器。基于此，我们推导出接收端的最优解码准则，并引入束搜索算法进一步降低复杂度。此外，我们论证现有LLM无需重新训练或微调即可直接应用于LLM-SC系统。仿真结果表明，在信噪比（SNR）超过3 dB时，LLM-SC性能优于经典DeepSC方案，且能在高SNR下实现语义信息的无差错传输，这是DeepSC无法实现的。除语义层面性能外，LLM-SC还展现出与技术层面性能的兼容性：在保持与传统通信系统相同联合信源信道编码速率的前提下，未采用任何信道编码时即可实现误码率（BER）$10^{-3}$条件下约8 dB的编码增益。
