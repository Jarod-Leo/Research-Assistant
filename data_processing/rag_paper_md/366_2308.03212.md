# Average-Hard Attention Transformers are Constant-Depth Uniform Threshold Circuits

链接: http://arxiv.org/abs/2308.03212v1

原文摘要:
Transformers have emerged as a widely used neural network model for various
natural language processing tasks. Previous research explored their
relationship with constant-depth threshold circuits, making two assumptions:
average-hard attention and logarithmic precision for internal computations
relative to input length. Merrill et al. (2022) prove that average-hard
attention transformers recognize languages that fall within the complexity
class TC0, denoting the set of languages that can be recognized by
constant-depth polynomial-size threshold circuits. Likewise, Merrill and
Sabharwal (2023) show that log-precision transformers recognize languages
within the class of uniform TC0. This shows that both transformer models can be
simulated by constant-depth threshold circuits, with the latter being more
robust due to generating a uniform circuit family. Our paper shows that the
first result can be extended to yield uniform circuits as well.

中文翻译:
Transformer已成为广泛应用于各类自然语言处理任务的神经网络模型。先前研究在探讨其与恒定深度阈值电路关系时，提出了两个假设条件：平均硬注意力机制以及与输入长度相关的对数精度内部计算。Merrill等人（2022年）证明，采用平均硬注意力的Transformer可识别的语言属于复杂度类TC0，即能被恒定深度多项式规模阈值电路识别的语言集合。类似地，Merrill与Sabharwal（2023年）研究表明，具备对数精度的Transformer可识别均匀TC0类中的语言。这证实两种Transformer模型均可由恒定深度阈值电路模拟，其中后者因能生成均匀电路族而更具鲁棒性。本文论证了首个结论同样可扩展至生成均匀电路的情形。
