# Text to Band Gap: Pre-trained Language Models as Encoders for Semiconductor Band Gap Prediction

链接: http://arxiv.org/abs/2501.03456v1

原文摘要:
In this study, we explore the use of a transformer-based language model as an
encoder to predict the band gaps of semiconductor materials directly from their
text descriptions. Quantum chemistry simulations, including Density Functional
Theory (DFT), are computationally intensive and time-consuming, which limits
their practicality for high-throughput material screening, particularly for
complex systems. Shallow machine learning (ML) models, while effective, often
require extensive data preprocessing to convert non-numerical material
properties into numerical inputs. In contrast, our approach leverages textual
data directly, bypassing the need for complex feature engineering. We generate
material descriptions in two formats: formatted strings combining features and
natural language text generated using the ChatGPT API. We demonstrate that the
RoBERTa model, pre-trained on natural language processing tasks, performs
effectively as an encoder for prediction tasks. With minimal fine-tuning, it
achieves a mean absolute error (MAE) of approximately 0.33 eV, performing
better than shallow machine learning models such as Support Vector Regression,
Random Forest, and XGBoost. Even when only the linear regression head is
trained while keeping the RoBERTa encoder layers frozen, the accuracy remains
nearly identical to that of the fully trained model. This demonstrates that the
pre-trained RoBERTa encoder is highly adaptable for processing domain-specific
text related to material properties, such as the band gap, significantly
reducing the need for extensive retraining. This study highlights the potential
of transformer-based language models to serve as efficient and versatile
encoders for semiconductor materials property prediction tasks.

中文翻译:
本研究探索了一种基于Transformer架构的语言模型作为编码器，直接从半导体材料的文本描述预测其带隙值。传统量子化学模拟方法（如密度泛函理论）虽精确但计算成本高昂，限制了其在复杂体系高通量材料筛选中的实用性；而浅层机器学习模型虽有效，却需耗费大量精力将非数值型材料特征转化为数值输入。相比之下，我们的方法直接利用文本数据，规避了复杂的特征工程过程。

我们采用两种文本格式生成材料描述：结合特征参数的格式化字符串与通过ChatGPT API生成的自然语言文本。实验表明，经过自然语言任务预训练的RoBERTa模型作为编码器表现出色，仅需微调即可达到约0.33电子伏特的平均绝对误差（MAE），其性能优于支持向量回归、随机森林和XGBoost等浅层机器学习模型。值得注意的是，即使冻结RoBERTa编码器层仅训练线性回归头，模型精度仍与全参数训练相当，这证明预训练编码器对带隙等材料特性相关领域文本具有极强的适应能力，可大幅降低重复训练需求。

本研究表明基于Transformer的语言模型能作为高效通用的编码器，为半导体材料性能预测任务提供新的解决方案。
