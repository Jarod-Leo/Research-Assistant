# Filter-then-Generate: Large Language Models with Structure-Text Adapter for Knowledge Graph Completion

链接: http://arxiv.org/abs/2412.09094v1

原文摘要:
Large Language Models (LLMs) present massive inherent knowledge and superior
semantic comprehension capability, which have revolutionized various tasks in
natural language processing. Despite their success, a critical gap remains in
enabling LLMs to perform knowledge graph completion (KGC). Empirical evidence
suggests that LLMs consistently perform worse than conventional KGC approaches,
even through sophisticated prompt design or tailored instruction-tuning.
Fundamentally, applying LLMs on KGC introduces several critical challenges,
including a vast set of entity candidates, hallucination issue of LLMs, and
under-exploitation of the graph structure. To address these challenges, we
propose a novel instruction-tuning-based method, namely FtG. Specifically, we
present a filter-then-generate paradigm and formulate the KGC task into a
multiple-choice question format. In this way, we can harness the capability of
LLMs while mitigating the issue casused by hallucinations. Moreover, we devise
a flexible ego-graph serialization prompt and employ a structure-text adapter
to couple structure and text information in a contextualized manner.
Experimental results demonstrate that FtG achieves substantial performance gain
compared to existing state-of-the-art methods. The instruction dataset and code
are available at https://github.com/LB0828/FtG.

中文翻译:
大型语言模型（LLMs）具备海量固有知识与卓越语义理解能力，已彻底革新自然语言处理领域的多项任务。然而在知识图谱补全（KGC）任务中，即便通过精心设计的提示或定制化的指令微调，实证研究表明LLMs的表现始终逊色于传统KGC方法。究其本质，将LLMs应用于KGC面临三大核心挑战：庞大的实体候选集、LLMs的幻觉问题，以及图结构利用不足的困境。

为此，我们提出创新性指令微调方法FtG。该方法采用"筛选-生成"双阶段范式，将KGC任务重构为多项选择题形式，既能充分发挥LLMs的潜能，又可有效缓解幻觉问题。此外，我们设计了灵活的自中心图序列化提示模板，并引入结构-文本适配器，实现图结构与文本信息的上下文感知融合。实验结果表明，FtG相较现有最优方法取得显著性能提升。相关指令数据集与代码已开源。
