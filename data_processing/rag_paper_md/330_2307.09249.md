# UniTabE: Pretraining a Unified Tabular Encoder for Heterogeneous Tabular Data

链接: http://arxiv.org/abs/2307.09249v1

原文摘要:
Recent advancements in NLP have witnessed the groundbreaking impact of
pretrained models, yielding impressive outcomes across various tasks. This
study seeks to extend the power of pretraining methodologies to facilitating
the prediction over tables in data science, a domain traditionally overlooked,
yet inherently challenging due to the plethora of table schemas intrinsic to
different tasks. The primary research questions underpinning this work revolve
around the establishment of a universal pretraining protocol for tables with
varied structures, the generalizability and transferability of learned
knowledge across tasks, the adaptation to diverse downstream applications, and
the incorporation of incremental columns over time. In response to these
challenges, we introduce UniTabE, a straightforward yet effective method
designed to process tables in a uniform manner, devoid of constraints imposed
by specific table structures. UniTabE's core concept relies on representing
each basic table element with a module, termed TabUnit. This is subsequently
followed by a Transformer encoder to refine the representation. Moreover, our
model is designed to facilitate pretraining and finetuning through the
utilization of free-form prompts. In order to implement the pretraining phase,
we curated an expansive tabular dataset comprising approximately 13B samples,
meticulously gathered from the Kaggle platform. This research primarily centers
on classification and regression tasks involving tabular data, and conducts
rigorous experimental testing and analyses to validate the effectiveness of our
methodology. The experimental results demonstrate UniTabE's superior
performance against several baselines across massive benchmarks. This,
therefore, underscores UniTabE's potential to significantly enhance the
semantic representation of tabular data, thereby marking a significant stride
for tabular data analysis.

中文翻译:
自然语言处理领域的最新进展见证了预训练模型的突破性影响，其在各类任务中展现出卓越性能。本研究旨在将预训练方法的优势拓展至数据科学中的表格预测领域——这一传统上被忽视却因任务间表格模式差异而极具挑战性的方向。本研究围绕四个核心问题展开：如何建立适用于异构表格的通用预训练框架、所学知识的跨任务迁移能力、对多样化下游应用的适应性，以及随时间推移新增列的处理机制。针对这些挑战，我们提出了UniTabE方法，其通过统一处理机制摆脱特定表格结构的限制。该方法的核心理念在于采用TabUnit模块表示基础表格元素，再通过Transformer编码器优化表征。此外，模型支持基于自由格式提示词的预训练与微调流程。为实现预训练阶段，我们从Kaggle平台精心构建了包含约130亿样本的大规模表格数据集。本研究聚焦表格数据的分类与回归任务，通过系统实验验证了方法的有效性。实验结果表明，UniTabE在多个基准测试中显著优于基线模型，证实了其在提升表格数据语义表征方面的潜力，为表格数据分析领域迈出了重要一步。
