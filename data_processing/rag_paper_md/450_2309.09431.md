# FactoFormer: Factorized Hyperspectral Transformers with Self-Supervised Pre-Training

链接: http://arxiv.org/abs/2309.09431v1

原文摘要:
Hyperspectral images (HSIs) contain rich spectral and spatial information.
Motivated by the success of transformers in the field of natural language
processing and computer vision where they have shown the ability to learn long
range dependencies within input data, recent research has focused on using
transformers for HSIs. However, current state-of-the-art hyperspectral
transformers only tokenize the input HSI sample along the spectral dimension,
resulting in the under-utilization of spatial information. Moreover,
transformers are known to be data-hungry and their performance relies heavily
on large-scale pretraining, which is challenging due to limited annotated
hyperspectral data. Therefore, the full potential of HSI transformers has not
been fully realized. To overcome these limitations, we propose a novel
factorized spectral-spatial transformer that incorporates factorized
self-supervised pretraining procedures, leading to significant improvements in
performance. The factorization of the inputs allows the spectral and spatial
transformers to better capture the interactions within the hyperspectral data
cubes. Inspired by masked image modeling pretraining, we also devise efficient
masking strategies for pretraining each of the spectral and spatial
transformers. We conduct experiments on six publicly available datasets for HSI
classification task and demonstrate that our model achieves state-of-the-art
performance in all the datasets. The code for our model will be made available
at https://github.com/csiro-robotics/factoformer.

中文翻译:
高光谱图像（HSI）蕴含丰富的光谱与空间信息。受Transformer在自然语言处理和计算机视觉领域成功建模输入数据长程依赖关系的启发，近期研究开始探索Transformer在HSI中的应用。然而，当前最先进的高光谱Transformer仅沿光谱维度对输入样本进行标记化处理，导致空间信息利用不足。此外，Transformer模型以数据饥渴著称，其性能高度依赖大规模预训练，而高光谱标注数据的稀缺性使这一要求面临挑战。因此，HSI Transformer的潜力尚未得到充分释放。为突破这些限制，我们提出一种新型因子化光谱-空间Transformer架构，结合因子化自监督预训练策略，显著提升了模型性能。通过输入数据的因子化处理，光谱与空间Transformer能更有效地捕捉高光谱立方体内部的交互关系。受掩码图像建模预训练的启发，我们还为光谱和空间Transformer分别设计了高效的掩码预训练策略。在六个公开高光谱分类数据集上的实验表明，我们的模型在所有数据集上都达到了最先进的性能水平。代码已开源在https://github.com/csiro-robotics/factoformer。
