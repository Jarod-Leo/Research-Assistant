# Knowledge Graphs for Enhancing Large Language Models in Entity Disambiguation

链接: http://arxiv.org/abs/2505.02737v2

原文摘要:
Recent advances in Large Language Models (LLMs) have positioned them as a
prominent solution for Natural Language Processing tasks. Notably, they can
approach these problems in a zero or few-shot manner, thereby eliminating the
need for training or fine-tuning task-specific models. However, LLMs face some
challenges, including hallucination and the presence of outdated knowledge or
missing information from specific domains in the training data. These problems
cannot be easily solved by retraining the models with new data as it is a
time-consuming and expensive process. To mitigate these issues, Knowledge
Graphs (KGs) have been proposed as a structured external source of information
to enrich LLMs. With this idea, in this work we use KGs to enhance LLMs for
zero-shot Entity Disambiguation (ED). For that purpose, we leverage the
hierarchical representation of the entities' classes in a KG to gradually prune
the candidate space as well as the entities' descriptions to enrich the input
prompt with additional factual knowledge. Our evaluation on popular ED datasets
shows that the proposed method outperforms non-enhanced and description-only
enhanced LLMs, and has a higher degree of adaptability than task-specific
models. Furthermore, we conduct an error analysis and discuss the impact of the
leveraged KG's semantic expressivity on the ED performance.

中文翻译:
大型语言模型（LLMs）的最新进展使其成为自然语言处理任务的重要解决方案。尤为突出的是，它们能以零样本或少样本方式处理这些问题，从而无需训练或微调特定任务模型。然而，LLMs仍面临一些挑战，包括幻觉现象、训练数据中存在过时知识或特定领域信息缺失等问题。通过新数据重新训练模型难以有效解决这些问题，因为这一过程耗时且成本高昂。为缓解这些局限，知识图谱（KGs）被提出作为结构化外部信息源来增强LLMs。基于这一思路，本研究利用KGs提升LLMs在零样本实体消歧（ED）任务中的表现。具体而言，我们通过KG中实体类别的层级表示逐步剪枝候选空间，并利用实体描述为输入提示注入额外事实知识。在主流ED数据集上的评估表明，所提方法优于未增强及仅使用描述增强的LLMs，且比专用任务模型具有更高的适应能力。此外，我们进行了错误分析，并探讨了所采用KG的语义表达能力对ED性能的影响。
