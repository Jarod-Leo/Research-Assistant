# Rethinking with Retrieval: Faithful Large Language Model Inference

链接: http://arxiv.org/abs/2301.00303v1

原文摘要:
Despite the success of large language models (LLMs) in various natural
language processing (NLP) tasks, the stored knowledge in these models may
inevitably be incomplete, out-of-date, or incorrect. This motivates the need to
utilize external knowledge to assist LLMs. Unfortunately, current methods for
incorporating external knowledge often require additional training or
fine-tuning, which can be costly and may not be feasible for LLMs. To address
this issue, we propose a novel post-processing approach, rethinking with
retrieval (RR), which retrieves relevant external knowledge based on the
decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting.
This lightweight approach does not require additional training or fine-tuning
and is not limited by the input length of LLMs. We evaluate the effectiveness
of RR through extensive experiments with GPT-3 on three complex reasoning
tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our
results show that RR can produce more faithful explanations and improve the
performance of LLMs.

中文翻译:
尽管大语言模型（LLMs）在各种自然语言处理（NLP）任务中取得了成功，但这些模型中存储的知识难免存在不完整、过时或错误的情况。这促使我们需要利用外部知识来辅助LLMs。然而，当前整合外部知识的方法通常需要额外的训练或微调，这不仅成本高昂，而且对于LLMs来说可能并不可行。为解决这一问题，我们提出了一种新颖的后处理方法——检索式反思（Rethinking with Retrieval, RR），该方法基于从思维链（Chain-of-Thought, CoT）提示中分解出的推理步骤，检索相关的外部知识。这种轻量级方法无需额外训练或微调，且不受LLMs输入长度的限制。我们通过在常识推理、时序推理和表格推理三个复杂任务上对GPT-3进行大量实验，评估了RR的有效性。结果表明，RR能够生成更可信的解释，并提升LLMs的性能。
