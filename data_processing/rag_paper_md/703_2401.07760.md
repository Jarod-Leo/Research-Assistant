# On the importance of Data Scale in Pretraining Arabic Language Models

链接: http://arxiv.org/abs/2401.07760v1

原文摘要:
Pretraining monolingual language models have been proven to be vital for
performance in Arabic Natural Language Processing (NLP) tasks. In this paper,
we conduct a comprehensive study on the role of data in Arabic Pretrained
Language Models (PLMs). More precisely, we reassess the performance of a suite
of state-of-the-art Arabic PLMs by retraining them on massive-scale,
high-quality Arabic corpora. We have significantly improved the performance of
the leading Arabic encoder-only BERT-base and encoder-decoder T5-base models on
the ALUE and ORCA leaderboards, thereby reporting state-of-the-art results in
their respective model categories. In addition, our analysis strongly suggests
that pretraining data by far is the primary contributor to performance,
surpassing other factors. Our models and source code are publicly available at
https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/JABER-PyTorch.

中文翻译:
研究表明，单语预训练语言模型对阿拉伯语自然语言处理任务性能至关重要。本文系统探究了数据在阿拉伯语预训练语言模型中的作用，通过在大规模高质量阿拉伯语语料库上重新训练一系列先进模型，我们对这一关键因素进行了实证分析。实验表明，我们对主流阿拉伯语BERT-base编码器模型和T5-base编码器-解码器模型的重新训练，显著提升了其在ALUE和ORCA评测基准上的表现，分别在相应模型类别中取得了当前最优性能。深入分析证实，预训练数据质量是影响模型性能的首要因素，其重要性远超其他变量因素。相关模型及源代码已开源发布于https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/JABER-PyTorch。
