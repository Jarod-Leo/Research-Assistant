# Traveling Words: A Geometric Interpretation of Transformers

链接: http://arxiv.org/abs/2309.07315v1

原文摘要:
Transformers have significantly advanced the field of natural language
processing, but comprehending their internal mechanisms remains a challenge. In
this paper, we introduce a novel geometric perspective that elucidates the
inner mechanisms of transformer operations. Our primary contribution is
illustrating how layer normalization confines the latent features to a
hyper-sphere, subsequently enabling attention to mold the semantic
representation of words on this surface. This geometric viewpoint seamlessly
connects established properties such as iterative refinement and contextual
embeddings. We validate our insights by probing a pre-trained 124M parameter
GPT-2 model. Our findings reveal clear query-key attention patterns in early
layers and build upon prior observations regarding the subject-specific nature
of attention heads at deeper layers. Harnessing these geometric insights, we
present an intuitive understanding of transformers, depicting them as processes
that model the trajectory of word particles along the hyper-sphere.

中文翻译:
Transformer模型显著推动了自然语言处理领域的发展，但其内部工作机制的理解仍具挑战性。本文提出了一种新颖的几何视角，用以阐释Transformer运算的内在机制。我们的核心贡献在于揭示了层归一化如何将潜在特征约束至超球面，进而使注意力机制能够在此曲面上塑造词语的语义表征。这一几何观点有机整合了迭代优化、上下文嵌入等已知特性。我们通过分析一个预训练的1.24亿参数GPT-2模型验证了该观点，发现浅层网络呈现清晰的查询-键注意力模式，并深化了先前关于深层注意力头具有主题专一性的观察。基于这些几何洞察，我们提出了对Transformer的直观理解——将其视为模拟词语粒子沿超球面运动轨迹的过程。
