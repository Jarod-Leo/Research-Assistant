# Are queries and keys always relevant? A case study on Transformer wave functions

链接: http://arxiv.org/abs/2405.18874v1

原文摘要:
The dot product attention mechanism, originally designed for natural language
processing tasks, is a cornerstone of modern Transformers. It adeptly captures
semantic relationships between word pairs in sentences by computing a
similarity overlap between queries and keys. In this work, we explore the
suitability of Transformers, focusing on their attention mechanisms, in the
specific domain of the parametrization of variational wave functions to
approximate ground states of quantum many-body spin Hamiltonians. Specifically,
we perform numerical simulations on the two-dimensional $J_1$-$J_2$ Heisenberg
model, a common benchmark in the field of quantum many-body systems on lattice.
By comparing the performance of standard attention mechanisms with a simplified
version that excludes queries and keys, relying solely on positions, we achieve
competitive results while reducing computational cost and parameter usage.
Furthermore, through the analysis of the attention maps generated by standard
attention mechanisms, we show that the attention weights become effectively
input-independent at the end of the optimization. We support the numerical
results with analytical calculations, providing physical insights of why
queries and keys should be, in principle, omitted from the attention mechanism
when studying large systems.

中文翻译:
点积注意力机制最初为自然语言处理任务设计，是现代Transformer架构的核心组件。它通过计算查询向量与键向量的相似度重叠，擅长捕捉句子中词对之间的语义关联。本研究聚焦于Transformer的注意力机制，探讨其在变分波函数参数化这一特定领域中的应用潜力，旨在逼近量子多体自旋哈密顿量的基态。我们以二维$J_1$-$J_2$海森堡模型为研究对象（该模型是晶格量子多体系统领域的标准基准），通过数值模拟对比标准注意力机制与简化版本（去除查询-键机制，仅依赖位置信息）的性能表现。结果表明，简化方案在降低计算成本和参数用量的同时仍能保持竞争力。进一步分析标准注意力机制生成的注意力图谱发现，优化过程结束时注意力权重实际上与输入无关。我们通过解析计算验证了数值结果，并从物理角度阐明：在研究大尺度系统时，注意力机制原则上应当省略查询与键向量的理论依据。
