# Visual Question Answering Instruction: Unlocking Multimodal Large Language Model To Domain-Specific Visual Multitasks

链接: http://arxiv.org/abs/2402.08360v1

原文摘要:
Having revolutionized natural language processing (NLP) applications, large
language models (LLMs) are expanding into the realm of multimodal inputs. Owing
to their ability to interpret images, multimodal LLMs (MLLMs) have been
primarily used for vision-language tasks. Currently, MLLMs have not yet been
extended for domain-specific visual tasks, which require a more explicit
understanding of visual information. We developed a method to transform
domain-specific visual and vision-language datasets into a unified question
answering format called Visual Question Answering Instruction (VQA-IN), thereby
extending MLLM to domain-specific tasks. The VQA-IN was applied to train
multiple MLLM architectures using smaller versions of LLMs (sLLMs). The
experimental results indicated that the proposed method achieved a high score
metric on domainspecific visual tasks while also maintaining its performance on
vision-language tasks in a multitask manner.

中文翻译:
大型语言模型（LLMs）已彻底革新自然语言处理（NLP）应用领域，现正逐步拓展至多模态输入范畴。得益于其图像解析能力，多模态大模型（MLLMs）当前主要应用于视觉-语言任务。然而在需要显式理解视觉信息的领域专用视觉任务中，MLLMs尚未得到有效延伸。本研究创新性地提出一种方法，将领域专用视觉及视觉-语言数据集统一转化为"视觉问答指令"（VQA-IN）格式，从而实现了MLLMs向领域专用任务的扩展。通过采用小型化LLMs（sLLMs）架构进行多模态训练，实验结果表明：该方法不仅在领域专用视觉任务上取得高分值评估指标，还能以多任务方式保持模型在视觉-语言任务上的性能表现。
