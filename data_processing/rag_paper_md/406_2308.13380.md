# In-context learning for model-free system identification

链接: http://arxiv.org/abs/2308.13380v1

原文摘要:
Is it possible to understand the intricacies of a dynamical system not solely
from its input/output pattern, but also by observing the behavior of other
systems within the same class? This central question drives the study presented
in this paper.
  In response to this query, we introduce a novel paradigm for system
identification, addressing two primary tasks: one-step-ahead prediction and
multi-step simulation. Unlike conventional methods, we do not directly estimate
a model for the specific system. Instead, we learn a meta model that represents
a class of dynamical systems. This meta model is trained on a potentially
infinite stream of synthetic data, generated by simulators whose settings are
randomly extracted from a probability distribution. When provided with a
context from a new system-specifically, an input/output sequence-the meta model
implicitly discerns its dynamics, enabling predictions of its behavior.
  The proposed approach harnesses the power of Transformers, renowned for their
\emph{in-context learning} capabilities. For one-step prediction, a GPT-like
decoder-only architecture is utilized, whereas the simulation problem employs
an encoder-decoder structure. Initial experimental results affirmatively answer
our foundational question, opening doors to fresh research avenues in system
identification.

中文翻译:
能否不仅通过输入/输出模式，还能通过观察同类系统中其他系统的行为来理解动态系统的复杂性？这一核心问题驱动了本文的研究。

针对这一疑问，我们提出了一种全新的系统辨识范式，重点解决两项核心任务：单步预测与多步仿真。与传统方法不同，我们并不直接为特定系统建立模型，而是学习一个能表征某类动态系统的元模型。该元模型通过潜在无限的合成数据流进行训练，这些数据由参数随机采自概率分布的模拟器生成。当输入新系统的上下文（即特定输入/输出序列）时，元模型能隐式识别其动态特性，从而预测其行为。

本研究利用以"上下文学习"能力著称的Transformer架构：单步预测采用类GPT的纯解码器结构，仿真任务则使用编码器-解码器框架。初步实验结果肯定地回答了我们的基础性问题，为系统辨识领域开辟了新的研究方向。
