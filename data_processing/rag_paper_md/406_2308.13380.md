# In-context learning for model-free system identification

链接: http://arxiv.org/abs/2308.13380v1

原文摘要:
Is it possible to understand the intricacies of a dynamical system not solely
from its input/output pattern, but also by observing the behavior of other
systems within the same class? This central question drives the study presented
in this paper.
  In response to this query, we introduce a novel paradigm for system
identification, addressing two primary tasks: one-step-ahead prediction and
multi-step simulation. Unlike conventional methods, we do not directly estimate
a model for the specific system. Instead, we learn a meta model that represents
a class of dynamical systems. This meta model is trained on a potentially
infinite stream of synthetic data, generated by simulators whose settings are
randomly extracted from a probability distribution. When provided with a
context from a new system-specifically, an input/output sequence-the meta model
implicitly discerns its dynamics, enabling predictions of its behavior.
  The proposed approach harnesses the power of Transformers, renowned for their
\emph{in-context learning} capabilities. For one-step prediction, a GPT-like
decoder-only architecture is utilized, whereas the simulation problem employs
an encoder-decoder structure. Initial experimental results affirmatively answer
our foundational question, opening doors to fresh research avenues in system
identification.

中文翻译:
是否有可能不仅通过动态系统的输入/输出模式，还能通过观察同一类别中其他系统的行为来理解其复杂机制？这一核心问题构成了本文的研究驱动力。

针对这一命题，我们提出了一种全新的系统辨识范式，重点解决两项关键任务：单步预测与多步仿真。与传统方法不同，我们并不直接为特定系统建立模型，而是学习一个能表征某类动态系统的元模型。该元模型通过持续训练于可能无限增长的合成数据流——这些数据由参数设置从概率分布中随机抽取的模拟器生成。当输入新系统的上下文（特指输入/输出序列）时，元模型能隐式识别其动态特性，从而预测其行为。

本方法充分发挥了Transformer架构的优势，该架构以其"上下文学习"能力著称。对于单步预测任务，我们采用类GPT的纯解码器结构；而仿真问题则使用编码器-解码器架构。初步实验结果肯定地回答了我们的基础设问，为系统辨识领域开启了新的研究路径。
