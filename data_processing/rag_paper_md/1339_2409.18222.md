# Trustworthy AI: Securing Sensitive Data in Large Language Models

链接: http://arxiv.org/abs/2409.18222v1

原文摘要:
Large Language Models (LLMs) have transformed natural language processing
(NLP) by enabling robust text generation and understanding. However, their
deployment in sensitive domains like healthcare, finance, and legal services
raises critical concerns about privacy and data security. This paper proposes a
comprehensive framework for embedding trust mechanisms into LLMs to dynamically
control the disclosure of sensitive information. The framework integrates three
core components: User Trust Profiling, Information Sensitivity Detection, and
Adaptive Output Control. By leveraging techniques such as Role-Based Access
Control (RBAC), Attribute-Based Access Control (ABAC), Named Entity Recognition
(NER), contextual analysis, and privacy-preserving methods like differential
privacy, the system ensures that sensitive information is disclosed
appropriately based on the user's trust level. By focusing on balancing data
utility and privacy, the proposed solution offers a novel approach to securely
deploying LLMs in high-risk environments. Future work will focus on testing
this framework across various domains to evaluate its effectiveness in managing
sensitive data while maintaining system efficiency.

中文翻译:
大型语言模型（LLMs）通过强大的文本生成与理解能力革新了自然语言处理领域。然而，其在医疗、金融和法律服务等敏感场景的部署引发了隐私与数据安全的核心关切。本文提出一个将信任机制嵌入LLMs的综合框架，以动态控制敏感信息泄露。该框架整合三大核心组件：用户信任画像、信息敏感度检测和自适应输出控制，通过采用基于角色的访问控制（RBAC）、基于属性的访问控制（ABAC）、命名实体识别（NER）、上下文分析等技术，结合差分隐私等保护方法，确保系统根据用户信任等级恰当地披露敏感信息。该方案聚焦数据效用与隐私保护的平衡，为高风险环境中安全部署LLMs提供了创新路径。后续研究将跨领域测试该框架，评估其在维持系统效率的同时管理敏感数据的实际效果。
