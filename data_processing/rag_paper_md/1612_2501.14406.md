# Adaptive Rank Allocation for Federated Parameter-Efficient Fine-Tuning of Language Models

链接: http://arxiv.org/abs/2501.14406v1

原文摘要:
Pre-trained Language Models (PLMs) have demonstrated their superiority and
versatility in modern Natural Language Processing (NLP), effectively adapting
to various downstream tasks through further fine-tuning. Federated
Parameter-Efficient Fine-Tuning (FedPEFT) has emerged as a promising solution
to address privacy and efficiency challenges in distributed training for PLMs
on resource-constrained local devices. However, our measurements reveal two key
limitations of FedPEFT: heterogeneous data across devices leads to significant
performance degradation, and a fixed parameter configuration results in
communication inefficiency. To overcome these limitations, we propose FedARA, a
novel Adaptive Rank Allocation framework for federated parameter-efficient
fine-tuning of language models. Specifically, FedARA employs truncated Singular
Value Decomposition (SVD) adaptation to enhance similar feature representation
across clients, significantly mitigating the adverse effects of data
heterogeneity. Subsequently, it utilizes dynamic rank allocation to
progressively identify critical ranks, effectively improving communication
efficiency. Lastly, it leverages rank-based module pruning to automatically
remove inactive modules, steadily reducing local computational cost and memory
usage in each federated learning round. Extensive experiments show that FedARA
consistently outperforms baselines by an average of 6.95% to 8.49% across
various datasets and models under heterogeneous data while significantly
improving communication efficiency by 2.40$ \times$. Moreover, experiments on
various edge devices demonstrate substantial decreases in total training time
and energy consumption by up to 48.90% and 46.95%, respectively.

中文翻译:
预训练语言模型（PLMs）在现代自然语言处理（NLP）中展现出卓越的通用性和优势，通过微调能有效适配各类下游任务。联邦参数高效微调（FedPEFT）作为一种新兴解决方案，致力于在资源受限的本地设备上解决PLMs分布式训练中的隐私与效率挑战。然而，我们的实测发现FedPEFT存在两大关键局限：设备间的数据异构性导致性能显著下降，固定参数配置引发通信效率低下。为此，我们提出FedARA——一种面向语言模型联邦参数高效微调的自适应秩分配框架。具体而言，FedARA通过截断奇异值分解（SVD）适配增强客户端间的相似特征表征，显著缓解数据异构的负面影响；继而采用动态秩分配渐进识别关键秩，有效提升通信效率；最后基于秩的模块剪枝自动移除非活跃模块，持续降低每轮联邦学习的本地计算成本和内存占用。大量实验表明，在异构数据场景下，FedARA在多种数据集和模型上平均以6.95%至8.49%的优势持续超越基线方法，同时通信效率提升达2.40倍。此外，各类边缘设备实验证明其总训练时间和能耗分别最高降低48.90%与46.95%。
