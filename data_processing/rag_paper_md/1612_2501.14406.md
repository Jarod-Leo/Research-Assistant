# Adaptive Rank Allocation for Federated Parameter-Efficient Fine-Tuning of Language Models

链接: http://arxiv.org/abs/2501.14406v1

原文摘要:
Pre-trained Language Models (PLMs) have demonstrated their superiority and
versatility in modern Natural Language Processing (NLP), effectively adapting
to various downstream tasks through further fine-tuning. Federated
Parameter-Efficient Fine-Tuning (FedPEFT) has emerged as a promising solution
to address privacy and efficiency challenges in distributed training for PLMs
on resource-constrained local devices. However, our measurements reveal two key
limitations of FedPEFT: heterogeneous data across devices leads to significant
performance degradation, and a fixed parameter configuration results in
communication inefficiency. To overcome these limitations, we propose FedARA, a
novel Adaptive Rank Allocation framework for federated parameter-efficient
fine-tuning of language models. Specifically, FedARA employs truncated Singular
Value Decomposition (SVD) adaptation to enhance similar feature representation
across clients, significantly mitigating the adverse effects of data
heterogeneity. Subsequently, it utilizes dynamic rank allocation to
progressively identify critical ranks, effectively improving communication
efficiency. Lastly, it leverages rank-based module pruning to automatically
remove inactive modules, steadily reducing local computational cost and memory
usage in each federated learning round. Extensive experiments show that FedARA
consistently outperforms baselines by an average of 6.95% to 8.49% across
various datasets and models under heterogeneous data while significantly
improving communication efficiency by 2.40$ \times$. Moreover, experiments on
various edge devices demonstrate substantial decreases in total training time
and energy consumption by up to 48.90% and 46.95%, respectively.

中文翻译:
预训练语言模型（PLMs）在现代自然语言处理（NLP）中展现出卓越的通用性，通过微调可高效适配各类下游任务。联邦参数高效微调（FedPEFT）作为一种新兴解决方案，有效应对了资源受限本地设备上PLMs分布式训练的隐私与效率挑战。然而，我们的实测发现FedPEFT存在两大局限：设备间数据异构导致性能显著下降，固定参数配置引发通信效率低下。为此，我们提出FedARA——一种面向语言模型联邦参数高效微调的自适应秩分配框架。具体而言，FedARA通过截断奇异值分解（SVD）适配增强客户端间相似特征表示，显著缓解数据异构的负面影响；采用动态秩分配逐步识别关键秩，有效提升通信效率；结合基于秩的模块剪枝自动移除非活跃模块，持续降低每轮联邦学习的本地计算开销与内存占用。大量实验表明，在异构数据场景下，FedARA在各类数据集和模型上平均以6.95%至8.49%的优势持续超越基线方法，同时通信效率提升达2.40倍。边缘设备实验进一步验证其训练总时长与能耗最高可降低48.90%和46.95%。
