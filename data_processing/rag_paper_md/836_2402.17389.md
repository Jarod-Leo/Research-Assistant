# FairBelief - Assessing Harmful Beliefs in Language Models

链接: http://arxiv.org/abs/2402.17389v1

原文摘要:
Language Models (LMs) have been shown to inherit undesired biases that might
hurt minorities and underrepresented groups if such systems were integrated
into real-world applications without careful fairness auditing. This paper
proposes FairBelief, an analytical approach to capture and assess beliefs,
i.e., propositions that an LM may embed with different degrees of confidence
and that covertly influence its predictions. With FairBelief, we leverage
prompting to study the behavior of several state-of-the-art LMs across
different previously neglected axes, such as model scale and likelihood,
assessing predictions on a fairness dataset specifically designed to quantify
LMs' outputs' hurtfulness. Finally, we conclude with an in-depth qualitative
assessment of the beliefs emitted by the models. We apply FairBelief to English
LMs, revealing that, although these architectures enable high performances on
diverse natural language processing tasks, they show hurtful beliefs about
specific genders. Interestingly, training procedure and dataset, model scale,
and architecture induce beliefs of different degrees of hurtfulness.

中文翻译:
研究表明，语言模型（LMs）会继承不良偏见，若未经审慎的公平性评估就将此类系统投入实际应用，可能会伤害少数群体和弱势群体。本文提出FairBelief这一分析方法，旨在捕捉和评估语言模型所隐含的信念——即模型可能以不同置信度嵌入、并暗中影响其预测结果的命题。通过FairBelief，我们利用提示技术研究了多个前沿语言模型在以往被忽视的维度（如模型规模与似然度）上的表现，并基于专门设计的公平性数据集量化模型输出的伤害性程度。最终，我们对模型生成的信念进行了深入的定性评估。将FairBelief应用于英语语言模型后发现：尽管这些架构在多样化的自然语言处理任务中表现优异，但它们对特定性别群体存在有害信念。值得注意的是，训练流程与数据集、模型规模及架构会诱发具有不同伤害程度的信念。
