# Multilevel Large Language Models for Everyone

链接: http://arxiv.org/abs/2307.13221v1

原文摘要:
Large language models have made significant progress in the past few years.
However, they are either generic {\it or} field specific, splitting the
community into different groups. In this paper, we unify these large language
models into a larger map, where the generic {\it and} specific models are
linked together and can improve each other, based on the user personal input
and information from the internet. The idea of linking several large language
models together is inspired by the functionality of human brain. The specific
regions on the brain cortex are specific for certain low level functionality.
And these regions can jointly work together to achieve more complex high level
functionality. Such behavior on human brain cortex sheds the light to design
the multilevel large language models that contain global level, field level and
user level models. The user level models run on local machines to achieve
efficient response and protect the user's privacy. Such multilevel models
reduce some redundancy and perform better than the single level models. The
proposed multilevel idea can be applied in various applications, such as
natural language processing, computer vision tasks, professional assistant,
business and healthcare.

中文翻译:
近年来，大型语言模型取得了显著进展。然而，这些模型要么是通用型，要么是领域专用型，导致研究社群分化为不同群体。本文提出将这些大型语言模型整合到一个更宏大的框架中，通过结合用户个性化输入和互联网信息，使通用模型与专用模型相互关联、协同优化。这种联结多个大型语言模型的灵感源自人类大脑运作机制——大脑皮层特定区域负责特定基础功能，这些区域通过协同工作实现更复杂的高级功能。基于这一原理，我们设计了包含全局层、领域层和用户层的多级大型语言模型架构。其中用户层模型运行于本地设备，既能保证响应效率又可保护用户隐私。这种多级架构通过减少冗余设计，在性能上超越了单层模型。所提出的多级模型理念可广泛应用于自然语言处理、计算机视觉任务、专业助手、商业及医疗健康等多个领域。
