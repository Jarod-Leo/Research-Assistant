# Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models

链接: http://arxiv.org/abs/2502.11559v1

原文摘要:
Pre-training large language models (LLMs) on vast text corpora enhances
natural language processing capabilities but risks encoding social biases,
particularly gender bias. While parameter-modification methods like fine-tuning
mitigate bias, they are resource-intensive, unsuitable for closed-source
models, and lack adaptability to evolving societal norms. Instruction-based
approaches offer flexibility but often compromise task performance. To address
these limitations, we propose $\textit{FaIRMaker}$, an automated and
model-independent framework that employs an $\textbf{auto-search and
refinement}$ paradigm to adaptively generate Fairwords, which act as
instructions integrated into input queries to reduce gender bias and enhance
response quality. Extensive experiments demonstrate that $\textit{FaIRMaker}$
automatically searches for and dynamically refines Fairwords, effectively
mitigating gender bias while preserving task integrity and ensuring
compatibility with both API-based and open-source LLMs.

中文翻译:
在大规模文本语料库上预训练大型语言模型（LLM）虽能增强自然语言处理能力，却存在编码社会偏见（尤其是性别偏见）的风险。尽管参数修改方法（如微调）可缓解偏见，但其资源消耗大、不适用于闭源模型，且难以适应动态变化的社会规范。基于指令的方法虽具灵活性，却常以牺牲任务性能为代价。为突破这些局限，我们提出$\textit{FaIRMaker}$框架——采用$\textbf{自动搜索与优化}$范式的模型无关自动化系统，通过自适应生成"公平词"（Fairwords）作为指令嵌入输入查询，在降低性别偏见的同时提升响应质量。大量实验表明，$\textit{FaIRMaker}$能自动搜索并动态优化公平词，既有效消减性别偏见，又保持任务完整性，且兼容基于API和开源的大型语言模型。
