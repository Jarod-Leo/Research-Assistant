# Pensez: Less Data, Better Reasoning -- Rethinking French LLM

链接: http://arxiv.org/abs/2503.13661v1

原文摘要:
Large language models (LLMs) have demonstrated remarkable capabilities in
various natural language processing tasks. However, achieving strong
performance in specialized domains like mathematical reasoning and non-English
languages often requires extensive training on massive datasets. This paper
investigates a contrasting approach: strategic fine-tuning on a small,
high-quality, bilingual (English-French) dataset to enhance both the reasoning
capabilities and French language proficiency of a large language model. Rather
than relying on scale, we explore the hypothesis that targeted data curation
and optimized training can achieve competitive, or even superior, performance.
We demonstrate, through targeted supervised fine-tuning (SFT) on only 2,000
carefully selected samples, significant improvements in mathematical reasoning.
Specifically, Pensez 7B exhibits an increase in accuracy of the base model up
to 20% on the AIME25 and a 12% increase on a French MATH level 5 benchmark.
These results challenge the prevailing assumption that massive datasets are
aprerequisite for strong reasoning performance in LLMs, highlighting the
potential of strategic data curation and optimized fine-tuning for enhancing
both specialized skills and multilingual capabilities. Our findings have
implications for the efficient development of high-performing, multilingual
LLMs, especially in resource-constrained scenarios.

中文翻译:
大型语言模型（LLM）在各类自然语言处理任务中展现出卓越能力，但要在数学推理等专业领域或非英语语言中实现强劲表现，通常需要海量数据的训练。本文研究了一种截然不同的方法：通过对小型高质量英法双语数据集进行策略性微调，同步提升大语言模型的推理能力和法语水平。我们摒弃规模依赖，探索了"针对性数据优化与训练能实现竞争性乃至更优性能"的假设。实验表明，仅用2000个精选样本进行监督微调（SFT），模型在数学推理上就取得显著进步——Pensez 7B基础模型在AIME25测试集准确率最高提升20%，在法语MATH五级基准上提升12%。这些发现颠覆了"大模型强推理必须依赖大数据"的主流认知，揭示了策略性数据筛选与优化微调对提升专业能力和多语言性能的双重潜力。本研究为资源受限环境下高效开发高性能多语言大模型提供了新思路。
