# Recent Advances of Foundation Language Models-based Continual Learning: A Survey

链接: http://arxiv.org/abs/2405.18653v1

原文摘要:
Recently, foundation language models (LMs) have marked significant
achievements in the domains of natural language processing (NLP) and computer
vision (CV). Unlike traditional neural network models, foundation LMs obtain a
great ability for transfer learning by acquiring rich commonsense knowledge
through pre-training on extensive unsupervised datasets with a vast number of
parameters. However, they still can not emulate human-like continuous learning
due to catastrophic forgetting. Consequently, various continual learning
(CL)-based methodologies have been developed to refine LMs, enabling them to
adapt to new tasks without forgetting previous knowledge. However, a systematic
taxonomy of existing approaches and a comparison of their performance are still
lacking, which is the gap that our survey aims to fill. We delve into a
comprehensive review, summarization, and classification of the existing
literature on CL-based approaches applied to foundation language models, such
as pre-trained language models (PLMs), large language models (LLMs) and
vision-language models (VLMs). We divide these studies into offline CL and
online CL, which consist of traditional methods, parameter-efficient-based
methods, instruction tuning-based methods and continual pre-training methods.
Offline CL encompasses domain-incremental learning, task-incremental learning,
and class-incremental learning, while online CL is subdivided into hard task
boundary and blurry task boundary settings. Additionally, we outline the
typical datasets and metrics employed in CL research and provide a detailed
analysis of the challenges and future work for LMs-based continual learning.

中文翻译:
近年来，基础语言模型（LMs）在自然语言处理（NLP）和计算机视觉（CV）领域取得了显著成就。与传统神经网络模型不同，基础语言模型通过在海量无监督数据集上进行参数规模庞大的预训练，获得了丰富的常识知识，从而具备极强的迁移学习能力。然而，由于灾难性遗忘问题，这些模型仍无法实现类人类的持续学习。为此，研究者开发了多种基于持续学习（CL）的方法来优化语言模型，使其在适应新任务的同时保留已有知识。但目前仍缺乏对现有方法的系统性分类框架及性能比较，这正是本综述旨在填补的空白。

我们全面梳理、总结并分类了应用于基础语言模型（如预训练语言模型PLMs、大语言模型LLMs和视觉语言模型VLMs）的持续学习研究文献。将现有方法划分为离线持续学习与在线持续学习两大范式：前者包含传统方法、参数高效方法、指令微调方法和持续预训练方法，涵盖领域增量学习、任务增量学习和类别增量学习三类场景；后者则细分为硬任务边界与模糊任务边界两种设定。此外，我们系统归纳了持续学习研究常用的数据集与评估指标，并对基于语言模型的持续学习所面临的挑战与未来研究方向进行了深度剖析。
