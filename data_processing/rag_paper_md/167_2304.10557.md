# An Introduction to Transformers

链接: http://arxiv.org/abs/2304.10557v1

原文摘要:
The transformer is a neural network component that can be used to learn
useful representations of sequences or sets of data-points. The transformer has
driven recent advances in natural language processing, computer vision, and
spatio-temporal modelling. There are many introductions to transformers, but
most do not contain precise mathematical descriptions of the architecture and
the intuitions behind the design choices are often also missing. Moreover, as
research takes a winding path, the explanations for the components of the
transformer can be idiosyncratic. In this note we aim for a mathematically
precise, intuitive, and clean description of the transformer architecture. We
will not discuss training as this is rather standard. We assume that the reader
is familiar with fundamental topics in machine learning including multi-layer
perceptrons, linear transformations, softmax functions and basic probability.

中文翻译:
Transformer是一种神经网络组件，可用于学习序列或数据点集的有效表征。这一架构近年来推动了自然语言处理、计算机视觉以及时空建模领域的重大进展。虽然已有诸多关于Transformer的入门资料，但多数缺乏对架构的精确数学描述，且往往未阐明设计选择背后的原理。此外，由于研究路径的曲折性，现有对Transformer组件的解释常带有个人化色彩。本文旨在以数学严谨、直观清晰的方式阐述Transformer架构（训练方法因属常规内容将不予讨论），预设读者已掌握机器学习基础知识，包括多层感知机、线性变换、Softmax函数及概率论基础。
