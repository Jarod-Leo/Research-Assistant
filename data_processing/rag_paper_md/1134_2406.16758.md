# Towards Fast Multilingual LLM Inference: Speculative Decoding and Specialized Drafters

链接: http://arxiv.org/abs/2406.16758v1

原文摘要:
Large language models (LLMs) have revolutionized natural language processing
and broadened their applicability across diverse commercial applications.
However, the deployment of these models is constrained by high inference time
in multilingual settings. To mitigate this challenge, this paper explores a
training recipe of an assistant model in speculative decoding, which is
leveraged to draft and-then its future tokens are verified by the target LLM.
We show that language-specific draft models, optimized through a targeted
pretrain-and-finetune strategy, substantially brings a speedup in inference
time compared to the previous methods. We validate these models across various
languages in inference time, out-of-domain speedup, and GPT-4o evaluation.

中文翻译:
大型语言模型（LLM）彻底改变了自然语言处理领域，并拓宽了其在多样化商业应用中的适用性。然而，这些模型在多语言环境下的部署受到高推理时间的限制。为缓解这一挑战，本文探索了一种辅助模型在推测解码中的训练方案——该模型负责生成草稿，随后由目标LLM验证其未来令牌。研究表明，通过针对性预训练与微调策略优化的语言专用草稿模型，相比现有方法能显著提升推理速度。我们在推理时间、跨领域加速效果及GPT-4o评估等多个维度上，验证了这些模型对多种语言的有效性。
