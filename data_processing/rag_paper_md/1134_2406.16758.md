# Towards Fast Multilingual LLM Inference: Speculative Decoding and Specialized Drafters

链接: http://arxiv.org/abs/2406.16758v1

原文摘要:
Large language models (LLMs) have revolutionized natural language processing
and broadened their applicability across diverse commercial applications.
However, the deployment of these models is constrained by high inference time
in multilingual settings. To mitigate this challenge, this paper explores a
training recipe of an assistant model in speculative decoding, which is
leveraged to draft and-then its future tokens are verified by the target LLM.
We show that language-specific draft models, optimized through a targeted
pretrain-and-finetune strategy, substantially brings a speedup in inference
time compared to the previous methods. We validate these models across various
languages in inference time, out-of-domain speedup, and GPT-4o evaluation.

中文翻译:
以下是符合您要求的中文翻译：

大型语言模型（LLMs）彻底改变了自然语言处理领域，并拓展了其在各类商业应用中的适用性。然而在多语言场景下，这些模型的部署受限于高昂的推理时间成本。为应对这一挑战，本文探索了推测解码中辅助模型的训练方案——该模型负责生成草稿文本，随后由目标LLM验证其预测的未来标记。研究表明，通过针对性预训练与微调策略优化的语言专用草稿模型，相较现有方法能显著提升推理速度。我们在推理耗时、跨领域加速效果和GPT-4o评估三个维度上，对多语言场景下的模型性能进行了全面验证。

（翻译说明：
1. 专业术语处理：LLMs保留英文缩写但补充中文全称，"speculative decoding"译为技术界通用译法"推测解码"
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句，如将定语从句转换为分句结构
3. 概念显化："draft models"译为"草稿模型"并添加"文本"以明确指代对象
4. 技术准确性："tokens"根据语境译为"标记"而非字面翻译
5. 数据维度处理：将原文列举的验证指标整合为符合中文学术表达的排比结构）
