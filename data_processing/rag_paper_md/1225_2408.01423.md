# Prompt Recursive Search: A Living Framework with Adaptive Growth in LLM Auto-Prompting

链接: http://arxiv.org/abs/2408.01423v1

原文摘要:
Large Language Models (LLMs) exhibit remarkable proficiency in addressing a
diverse array of tasks within the Natural Language Processing (NLP) domain,
with various prompt design strategies significantly augmenting their
capabilities. However, these prompts, while beneficial, each possess inherent
limitations. The primary prompt design methodologies are twofold: The first,
exemplified by the Chain of Thought (CoT), involves manually crafting prompts
specific to individual datasets, hence termed Expert-Designed Prompts (EDPs).
Once these prompts are established, they are unalterable, and their
effectiveness is capped by the expertise of the human designers. When applied
to LLMs, the static nature of EDPs results in a uniform approach to both simple
and complex problems within the same dataset, leading to the inefficient use of
tokens for straightforward issues. The second method involves prompts
autonomously generated by the LLM, known as LLM-Derived Prompts (LDPs), which
provide tailored solutions to specific problems, mitigating the limitations of
EDPs. However, LDPs may encounter a decline in performance when tackling
complex problems due to the potential for error accumulation during the
solution planning process. To address these challenges, we have conceived a
novel Prompt Recursive Search (PRS) framework that leverages the LLM to
generate solutions specific to the problem, thereby conserving tokens. The
framework incorporates an assessment of problem complexity and an adjustable
structure, ensuring a reduction in the likelihood of errors. We have
substantiated the efficacy of PRS framework through extensive experiments using
LLMs with different numbers of parameters across a spectrum of datasets in
various domains. Compared to the CoT method, the PRS method has increased the
accuracy on the BBH dataset by 8% using Llama3-7B model, achieving a 22%
improvement.

中文翻译:
大语言模型（LLMs）在自然语言处理（NLP）领域展现出卓越的多任务处理能力，而多样化的提示设计策略进一步强化了其性能。然而现有提示方法均存在固有缺陷：主流提示设计可分为两类，第一类以思维链（CoT）为代表，通过人工为特定数据集定制提示模板，称为专家设计提示（EDPs）。这类提示一经确定便无法调整，其效果完全受限于设计者的专业水平。当应用于LLMs时，EDPs的静态性导致模型对同一数据集中简单与复杂问题采用统一处理方式，造成简单问题的令牌资源浪费。第二类是由LLM自主生成的提示，称为LLM派生提示（LDPs），能为具体问题提供定制化解决方案，从而克服EDPs的局限性。但LDPs在解决复杂问题时，由于规划过程中可能产生错误累积，性能会出现下降。针对这些问题，我们提出了创新的提示递归搜索（PRS）框架，通过LLM生成问题专属解决方案以节省令牌消耗。该框架集成问题复杂度评估与可调节结构，有效降低错误发生概率。我们采用不同参数规模的LLMs在跨领域数据集上进行了广泛实验，验证了PRS框架的优越性——相较于CoT方法，使用Llama3-7B模型时PRS在BBH数据集上的准确率提升8%，最高达到22%的显著改进。
