# Breaking Free Transformer Models: Task-specific Context Attribution Promises Improved Generalizability Without Fine-tuning Pre-trained LLMs

链接: http://arxiv.org/abs/2401.16638v1

原文摘要:
Fine-tuning large pre-trained language models (LLMs) on particular datasets
is a commonly employed strategy in Natural Language Processing (NLP)
classification tasks. However, this approach usually results in a loss of
models generalizability. In this paper, we present a framework that allows for
maintaining generalizability, and enhances the performance on the downstream
task by utilizing task-specific context attribution. We show that a linear
transformation of the text representation from any transformer model using the
task-specific concept operator results in a projection onto the latent concept
space, referred to as context attribution in this paper. The specific concept
operator is optimized during the supervised learning stage via novel loss
functions. The proposed framework demonstrates that context attribution of the
text representation for each task objective can improve the capacity of the
discriminator function and thus achieve better performance for the
classification task. Experimental results on three datasets, namely HateXplain,
IMDB reviews, and Social Media Attributions, illustrate that the proposed model
attains superior accuracy and generalizability. Specifically, for the
non-fine-tuned BERT on the HateXplain dataset, we observe 8% improvement in
accuracy and 10% improvement in F1-score. Whereas for the IMDB dataset,
fine-tuned state-of-the-art XLNet is outperformed by 1% for both accuracy and
F1-score. Furthermore, in an out-of-domain cross-dataset test, DistilBERT
fine-tuned on the IMDB dataset in conjunction with the proposed model improves
the F1-score on the HateXplain dataset by 7%. For the Social Media Attributions
dataset of YouTube comments, we observe 5.2% increase in F1-metric. The
proposed framework is implemented with PyTorch and provided open-source on
GitHub.

中文翻译:
在自然语言处理（NLP）分类任务中，针对特定数据集微调大型预训练语言模型（LLM）是一种常见策略。然而，这种方法通常会导致模型泛化能力下降。本文提出一种框架，通过利用任务特定的上下文归因，既能保持泛化性，又能提升下游任务性能。研究表明：使用任务专用概念算子对任何Transformer模型的文本表示进行线性变换后，可将其投影至潜在概念空间（本文称为上下文归因）。该概念算子通过新型损失函数在监督学习阶段进行优化。实验证明，针对每个任务目标进行文本表示的上下文归因，能增强判别函数能力从而提升分类任务表现。在HateXplain、IMDB影评和社交媒体归因三个数据集上的实验表明，该模型在准确率和泛化性方面均表现优异。具体而言：对于未微调的BERT模型，在HateXplain数据集上准确率提升8%，F1值提高10%；在IMDB数据集上，其表现优于当前最优的微调XLNet模型1%（准确率和F1值）；在跨数据集测试中，基于IMDB微调的DistilBERT结合本模型使HateXplain数据集的F1值提升7%；在YouTube评论的社交媒体归因数据集上，F1指标增长5.2%。该框架已通过PyTorch实现并在GitHub开源。
