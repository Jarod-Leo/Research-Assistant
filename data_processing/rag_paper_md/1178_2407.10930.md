# Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together

链接: http://arxiv.org/abs/2407.10930v1

原文摘要:
Natural Language Processing (NLP) systems are increasingly taking the form of
sophisticated modular pipelines, e.g., Retrieval Augmented Generation (RAG),
where each module may involve a distinct Language Model (LM) and an associated
prompt template. These compound systems often lack intermediate labels or
gradient flow to optimize each module, making their end-to-end optimization
challenging. Here we seek strategies to optimize both the module-level LM
weights and the associated prompt templates of such systems to maximize a
downstream task metric. We propose for the first time combining the weight and
prompt optimization strategies to optimize a modular LM pipeline by alternating
between the two to get the same LM to teach itself. In experiments with
multi-hop QA, mathematical reasoning, and feature-based classification using
mistral-7b, llama-2-7b, and llama-3-8b, these BetterTogether strategies
optimizing the weights and prompts of a pipeline together outperform directly
optimizing weights alone and prompts alone by up to 60% and 6%, respectively,
on average across LMs and tasks. BetterTogether optimizer is released in DSPy
at http://dspy.ai

中文翻译:
自然语言处理（NLP）系统正日益发展为复杂的模块化流程，例如检索增强生成（RAG）架构，其中每个模块可能包含不同的语言模型（LM）及对应的提示模板。这类复合系统通常缺乏中间标签或梯度流来优化各模块，导致端到端优化面临挑战。本文旨在探索同时优化模块级语言模型权重与关联提示模板的策略，以最大化下游任务指标。我们首次提出将权重优化与提示优化策略相结合，通过交替执行两种优化方式实现模块化语言模型管道的自我迭代提升。在多跳问答、数学推理和基于特征的分类任务中，使用mistral-7b、llama-2-7b和llama-3-8b模型的实验表明：这种"协同优化"策略在权重与提示联合优化时，平均性能分别比单独优化权重或提示最高提升60%和6%（跨模型与任务均值）。该优化器已集成至DSPy框架并发布于http://dspy.ai。
