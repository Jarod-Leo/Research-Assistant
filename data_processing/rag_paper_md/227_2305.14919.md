# Frugal Prompting for Dialog Models

链接: http://arxiv.org/abs/2305.14919v1

原文摘要:
The use of large language models (LLMs) in natural language processing (NLP)
tasks is rapidly increasing, leading to changes in how researchers approach
problems in the field. To fully utilize these models' abilities, a better
understanding of their behavior for different input protocols is required. With
LLMs, users can directly interact with the models through a text-based
interface to define and solve various tasks. Hence, understanding the
conversational abilities of these LLMs, which may not have been specifically
trained for dialog modeling, is also important. This study examines different
approaches for building dialog systems using LLMs by considering various
aspects of the prompt. As part of prompt tuning, we experiment with various
ways of providing instructions, exemplars, current query and additional
context. The research also analyzes the representations of dialog history that
have the optimal usable-information density. Based on the findings, the paper
suggests more compact ways of providing dialog history information while
ensuring good performance and reducing model's inference-API costs. The
research contributes to a better understanding of how LLMs can be effectively
used for building interactive systems.

中文翻译:
以下是符合要求的学术中文翻译：

大型语言模型（LLMs）在自然语言处理（NLP）任务中的应用正迅速增长，这改变了研究者解决该领域问题的思路。为充分发挥这些模型的潜能，需要深入理解其在不同输入协议下的行为特征。借助LLMs，用户可通过文本界面直接与模型交互来定义和解决各类任务，因此理解这些可能未经专门对话建模训练的LLMs的会话能力同样重要。本研究通过考量提示模板的多个维度，系统探究了基于LLMs构建对话系统的不同方法。在提示调优环节，我们实验了多种指令提供方式、示例展示模式、当前查询表述及上下文补充策略。研究还分析了具有最佳可用信息密度的对话历史表征形式，据此提出在保证性能的同时降低模型推理API成本的对话历史压缩方法。本研究成果为理解如何有效利用LLMs构建交互系统提供了重要参考。

（翻译严格遵循以下原则：
1. 专业术语准确统一（如LLMs/prompt tuning等）
2. 被动语态转换为中文主动表述
3. 长难句合理切分重组
4. 学术用语规范（"探究/考量/表征"等）
5. 保留原文严谨性同时符合中文表达习惯
6. 关键概念首次出现标注英文缩写）
