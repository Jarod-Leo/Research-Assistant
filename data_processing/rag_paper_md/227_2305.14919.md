# Frugal Prompting for Dialog Models

链接: http://arxiv.org/abs/2305.14919v1

原文摘要:
The use of large language models (LLMs) in natural language processing (NLP)
tasks is rapidly increasing, leading to changes in how researchers approach
problems in the field. To fully utilize these models' abilities, a better
understanding of their behavior for different input protocols is required. With
LLMs, users can directly interact with the models through a text-based
interface to define and solve various tasks. Hence, understanding the
conversational abilities of these LLMs, which may not have been specifically
trained for dialog modeling, is also important. This study examines different
approaches for building dialog systems using LLMs by considering various
aspects of the prompt. As part of prompt tuning, we experiment with various
ways of providing instructions, exemplars, current query and additional
context. The research also analyzes the representations of dialog history that
have the optimal usable-information density. Based on the findings, the paper
suggests more compact ways of providing dialog history information while
ensuring good performance and reducing model's inference-API costs. The
research contributes to a better understanding of how LLMs can be effectively
used for building interactive systems.

中文翻译:
大型语言模型（LLMs）在自然语言处理（NLP）任务中的应用正迅速增长，这促使研究者重新审视该领域问题的解决方式。为充分发挥这些模型的潜能，需要深入理解其在不同输入协议下的行为特性。用户可通过基于文本的界面直接与LLMs交互来定义和解决各类任务，因此探究这些并非专为对话建模训练的LLMs的会话能力也至关重要。本研究从提示设计的多个维度出发，系统考察了利用LLMs构建对话系统的不同方法。在提示调优环节，我们实验了指令模板、示例样本、当前查询及附加语境等多种配置方案，并重点分析了对话历史信息的最优表征密度。基于实验结果，本文提出了在保证性能的同时压缩对话历史信息的方法，可有效降低模型推理API成本。该研究为理解如何高效利用LLMs构建交互系统提供了新的见解。
