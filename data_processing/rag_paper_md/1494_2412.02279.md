# A Comprehensive Evaluation of Large Language Models on Aspect-Based Sentiment Analysis

链接: http://arxiv.org/abs/2412.02279v1

原文摘要:
Recently, Large Language Models (LLMs) have garnered increasing attention in
the field of natural language processing, revolutionizing numerous downstream
tasks with powerful reasoning and generation abilities. For example, In-Context
Learning (ICL) introduces a fine-tuning-free paradigm, allowing out-of-the-box
LLMs to execute downstream tasks by analogy learning without any fine-tuning.
Besides, in a fine-tuning-dependent paradigm where substantial training data
exists, Parameter-Efficient Fine-Tuning (PEFT), as the cost-effective methods,
enable LLMs to achieve excellent performance comparable to full fine-tuning.
  However, these fascinating techniques employed by LLMs have not been fully
exploited in the ABSA field. Previous works probe LLMs in ABSA by merely using
randomly selected input-output pairs as demonstrations in ICL, resulting in an
incomplete and superficial evaluation. In this paper, we shed light on a
comprehensive evaluation of LLMs in the ABSA field, involving 13 datasets, 8
ABSA subtasks, and 6 LLMs. Specifically, we design a unified task formulation
to unify ``multiple LLMs for multiple ABSA subtasks in multiple paradigms.''
For the fine-tuning-dependent paradigm, we efficiently fine-tune LLMs using
instruction-based multi-task learning. For the fine-tuning-free paradigm, we
propose 3 demonstration selection strategies to stimulate the few-shot
abilities of LLMs. Our extensive experiments demonstrate that LLMs achieve a
new state-of-the-art performance compared to fine-tuned Small Language Models
(SLMs) in the fine-tuning-dependent paradigm. More importantly, in the
fine-tuning-free paradigm where SLMs are ineffective, LLMs with ICL still
showcase impressive potential and even compete with fine-tuned SLMs on some
ABSA subtasks.

中文翻译:
近年来，大语言模型（LLMs）在自然语言处理领域日益受到关注，其强大的推理与生成能力正重塑众多下游任务。例如，上下文学习（ICL）开创了免微调范式，使开箱即用的LLMs能通过类比学习直接执行下游任务而无需任何参数调整。此外，在拥有充足训练数据的依赖微调场景中，参数高效微调（PEFT）作为高性价比方法，可使LLMs达到与全参数微调相媲美的卓越性能。

然而，这些LLMs的先进技术在方面级情感分析（ABSA）领域尚未得到充分探索。现有研究仅采用随机选取的输入-输出对作为ICL示例，导致评估既不全面也流于表面。本文对LLMs在ABSA领域展开系统性评估，涵盖13个数据集、8项ABSA子任务和6种LLMs。具体而言，我们设计了统一的任务框架，实现"多范式下多LLMs处理多ABSA子任务"的整合。在依赖微调范式中，采用基于指令的多任务学习进行高效参数调优；在免微调范式中，提出三种示例选择策略以激发LLMs的小样本学习能力。

实验结果表明：在依赖微调场景下，LLMs相较微调后的小语言模型（SLMs）取得了新的性能突破；更重要的是，在SLMs失效的免微调场景中，搭载ICL的LLMs仍展现出惊人潜力，部分ABSA子任务表现甚至可与微调SLMs比肩。
