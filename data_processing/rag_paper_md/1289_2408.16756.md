# How Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities of Large Language Models

链接: http://arxiv.org/abs/2408.16756v1

原文摘要:
The rapid evolution of large language models (LLMs) has transformed the
competitive landscape in natural language processing (NLP), particularly for
English and other data-rich languages. However, underrepresented languages like
Cantonese, spoken by over 85 million people, face significant development gaps,
which is particularly concerning given the economic significance of the
Guangdong-Hong Kong-Macau Greater Bay Area, and in substantial
Cantonese-speaking populations in places like Singapore and North America.
Despite its wide use, Cantonese has scant representation in NLP research,
especially compared to other languages from similarly developed regions. To
bridge these gaps, we outline current Cantonese NLP methods and introduce new
benchmarks designed to evaluate LLM performance in factual generation,
mathematical logic, complex reasoning, and general knowledge in Cantonese,
which aim to advance open-source Cantonese LLM technology. We also propose
future research directions and recommended models to enhance Cantonese LLM
development.

中文翻译:
大型语言模型（LLM）的快速发展重塑了自然语言处理（NLP）领域的竞争格局，尤其对英语等数据资源丰富的语言影响显著。然而，以粤语为代表的使用人口超8500万但资源匮乏的语言，其技术发展明显滞后——这在粤港澳大湾区经济地位突出的背景下更显矛盾，且新加坡、北美等地亦存在大量粤语使用群体。尽管应用广泛，粤语在NLP研究中的关注度远低于同等发达地区的其他语言。为弥合这一差距，本文系统梳理了当前粤语NLP技术方法，并创新性地构建了评估LLM粤语事实生成、数理逻辑、复杂推理与常识认知能力的基准测试体系，旨在推动开源粤语大模型技术进步。同时提出了未来重点研究方向及推荐模型架构，为粤语大模型的可持续发展提供路线图。
