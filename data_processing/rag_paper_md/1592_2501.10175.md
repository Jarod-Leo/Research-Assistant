# Multi-stage Training of Bilingual Islamic LLM for Neural Passage Retrieval

链接: http://arxiv.org/abs/2501.10175v1

原文摘要:
This study examines the use of Natural Language Processing (NLP) technology
within the Islamic domain, focusing on developing an Islamic neural retrieval
model. By leveraging the robust XLM-R model, the research employs a language
reduction technique to create a lightweight bilingual large language model
(LLM). Our approach for domain adaptation addresses the unique challenges faced
in the Islamic domain, where substantial in-domain corpora exist only in Arabic
while limited in other languages, including English.
  The work utilizes a multi-stage training process for retrieval models,
incorporating large retrieval datasets, such as MS MARCO, and smaller,
in-domain datasets to improve retrieval performance. Additionally, we have
curated an in-domain retrieval dataset in English by employing data
augmentation techniques and involving a reliable Islamic source. This approach
enhances the domain-specific dataset for retrieval, leading to further
performance gains.
  The findings suggest that combining domain adaptation and a multi-stage
training method for the bilingual Islamic neural retrieval model enables it to
outperform monolingual models on downstream retrieval tasks.

中文翻译:
本研究探讨了自然语言处理（NLP）技术在伊斯兰领域的应用，重点开发了一种伊斯兰神经检索模型。通过采用强大的XLM-R模型，研究运用语言降维技术构建了一个轻量级双语大语言模型（LLM）。我们的领域自适应方法解决了伊斯兰领域特有的挑战——该领域存在大量阿拉伯语语料库，但包括英语在内的其他语言资源却十分有限。

该工作采用多阶段训练流程来优化检索模型：首先利用MS MARCO等大型检索数据集进行训练，再通过较小规模的领域内数据集进行微调以提升检索性能。此外，我们运用数据增强技术并依托权威伊斯兰资料来源，构建了英语领域的专业检索数据集。这种方法有效扩充了领域特定数据集，从而实现了检索效果的进一步提升。

研究结果表明，结合领域自适应策略与多阶段训练方法的双语伊斯兰神经检索模型，在下游检索任务中的表现优于单语模型。
