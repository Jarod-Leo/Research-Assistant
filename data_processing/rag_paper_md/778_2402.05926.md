# On the Convergence of Zeroth-Order Federated Tuning in Large Language Models

链接: http://arxiv.org/abs/2402.05926v1

原文摘要:
The confluence of Federated Learning (FL) and Large Language Models (LLMs) is
ushering in a new era in privacy-preserving natural language processing.
However, the intensive memory requirements for fine-tuning LLMs pose
significant challenges, especially when deploying on clients with limited
computational resources. To circumvent this, we explore the novel integration
of Memory-efficient Zeroth-Order Optimization within a federated setting, a
synergy we term as FedMeZO. Our study is the first to examine the theoretical
underpinnings of FedMeZO in the context of LLMs, tackling key questions
regarding the influence of large parameter spaces on optimization behavior, the
establishment of convergence properties, and the identification of critical
parameters for convergence to inform personalized federated strategies. Our
extensive empirical evidence supports the theory, showing that FedMeZO not only
converges faster than traditional first-order methods such as FedAvg but also
significantly reduces GPU memory usage during training to levels comparable to
those during inference. Moreover, the proposed personalized FL strategy that is
built upon the theoretical insights to customize the client-wise learning rate
can effectively accelerate loss reduction. We hope our work can help to bridge
theoretical and practical aspects of federated fine-tuning for LLMs, thereby
stimulating further advancements and research in this area.

中文翻译:
联邦学习（FL）与大规模语言模型（LLMs）的融合正在开创隐私保护自然语言处理的新纪元。然而，LLMs微调过程中庞大的内存需求带来了严峻挑战，尤其在计算资源有限的客户端设备上部署时更为突出。为此，我们创新性地将内存高效的零阶优化方法引入联邦学习框架，提出名为FedMeZO的新型协同框架。本研究首次系统探讨了FedMeZO在LLMs应用中的理论基础，重点解决了三大核心问题：超大规模参数空间对优化行为的影响机制、收敛特性的理论证明，以及指导个性化联邦策略的关键收敛参数识别。大量实验数据验证了理论分析，表明FedMeZO不仅比FedAvg等传统一阶方法收敛更快，更能将训练时的GPU内存占用降至与推理阶段相当的水平。此外，基于理论洞见构建的个性化联邦策略——通过定制客户端学习率——可有效加速损失函数下降。本研究旨在弥合LLMs联邦微调理论与实践的鸿沟，以期推动该领域的持续创新与深入研究。  

（翻译说明：  
1. 专业术语处理："Zeroth-Order Optimization"译为"零阶优化"，"convergence properties"译为"收敛特性"  
2. 长句拆分：将原文复合句按中文表达习惯分解为多个短句  
3. 被动语态转化："it is shown that"转为主动句式"实验数据表明"  
4. 概念显化："client-wise learning rate"译为"客户端学习率"并补充说明"定制"以明确含义  
5. 学术风格保持：使用"范式""机制""洞见"等符合学术论文表达的词汇  
6. 文化适配："bridge"译为"弥合鸿沟"增强表达效果）
