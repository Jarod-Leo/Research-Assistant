# CAT: Circular-Convolutional Attention for Sub-Quadratic Transformers

链接: http://arxiv.org/abs/2504.06704v1

原文摘要:
Transformers have driven remarkable breakthroughs in natural language
processing and computer vision, yet their standard attention mechanism still
imposes O(N^2) complexity, hindering scalability to longer sequences. We
introduce Circular-convolutional ATtention (CAT), a Fourier-based approach that
efficiently applies circular convolutions to reduce complexity without
sacrificing representational power. CAT achieves O(NlogN) computations,
requires fewer learnable parameters by streamlining fully-connected layers, and
introduces no heavier operations, resulting in consistent accuracy improvements
and about a 10% speedup in naive PyTorch implementations on large-scale
benchmarks such as ImageNet-1k and WikiText-103. Grounded in an
engineering-isomorphism framework, CAT's design not only offers practical
efficiency and ease of implementation but also provides insights to guide the
development of next-generation, high-performance Transformer architectures.
Finally, our ablation studies highlight the key conditions underlying CAT's
success, shedding light on broader principles for scalable attention
mechanisms.

中文翻译:
Transformer模型在自然语言处理和计算机视觉领域取得了突破性进展，但其标准注意力机制仍存在O(N²)复杂度问题，制约了长序列场景下的可扩展性。我们提出基于傅里叶变换的循环卷积注意力机制（CAT），通过高效应用循环卷积在保持表征能力的同时降低计算复杂度。该机制实现O(NlogN)计算复杂度，通过精简全连接层减少可学习参数量，且未引入更重操作，在ImageNet-1k和WikiText-103等大规模基准测试中，不仅保持精度持续提升，还在原生PyTorch实现上获得约10%的加速。基于工程同构框架设计的CAT机制，既提供了实际高效的实现方案，也为开发下一代高性能Transformer架构提供了理论洞见。消融实验揭示了CAT成功的关键条件，为可扩展注意力机制的设计提供了普适性指导原则。
