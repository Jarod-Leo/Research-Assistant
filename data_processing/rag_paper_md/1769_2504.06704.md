# CAT: Circular-Convolutional Attention for Sub-Quadratic Transformers

链接: http://arxiv.org/abs/2504.06704v1

原文摘要:
Transformers have driven remarkable breakthroughs in natural language
processing and computer vision, yet their standard attention mechanism still
imposes O(N^2) complexity, hindering scalability to longer sequences. We
introduce Circular-convolutional ATtention (CAT), a Fourier-based approach that
efficiently applies circular convolutions to reduce complexity without
sacrificing representational power. CAT achieves O(NlogN) computations,
requires fewer learnable parameters by streamlining fully-connected layers, and
introduces no heavier operations, resulting in consistent accuracy improvements
and about a 10% speedup in naive PyTorch implementations on large-scale
benchmarks such as ImageNet-1k and WikiText-103. Grounded in an
engineering-isomorphism framework, CAT's design not only offers practical
efficiency and ease of implementation but also provides insights to guide the
development of next-generation, high-performance Transformer architectures.
Finally, our ablation studies highlight the key conditions underlying CAT's
success, shedding light on broader principles for scalable attention
mechanisms.

中文翻译:
Transformer模型在自然语言处理和计算机视觉领域取得了革命性突破，但其标准注意力机制仍存在O(N²)复杂度问题，制约了长序列场景的可扩展性。本研究提出基于傅里叶变换的循环卷积注意力机制（CAT），通过循环卷积运算在保持表征能力的同时实现计算复杂度降阶。该机制具有三大优势：1）计算复杂度降至O(NlogN)；2）通过精简全连接层减少可学习参数量；3）未引入额外复杂运算。在ImageNet-1k和WikiText-103等大规模基准测试中，原生PyTorch实现即可获得约10%的速度提升及稳定的准确率改进。基于工程同构理论框架设计的CAT机制，不仅具备实践高效性和易实现性，更为新一代高性能Transformer架构的发展提供了理论洞见。消融实验进一步揭示了CAT成功的关键条件，为可扩展注意力机制的设计提供了普适性指导原则。
