# Ranking LLMs by compression

链接: http://arxiv.org/abs/2406.14171v1

原文摘要:
We conceptualize the process of understanding as information compression, and
propose a method for ranking large language models (LLMs) based on lossless
data compression. We demonstrate the equivalence of compression length under
arithmetic coding with cumulative negative log probabilities when using a large
language model as a prior, that is, the pre-training phase of the model is
essentially the process of learning the optimal coding length. At the same
time, the evaluation metric compression ratio can be obtained without actual
compression, which greatly saves overhead. In this paper, we use five large
language models as priors for compression, then compare their performance on
challenging natural language processing tasks, including sentence completion,
question answering, and coreference resolution. Experimental results show that
compression ratio and model performance are positively correlated, so it can be
used as a general metric to evaluate large language models.

中文翻译:
我们将理解过程概念化为信息压缩，并提出了一种基于无损数据压缩的大型语言模型（LLM）排序方法。通过论证算术编码下压缩长度与使用大语言模型作为先验时的累积负对数概率之间的等价性，我们揭示了模型预训练阶段本质上就是学习最优编码长度的过程。同时，评估指标压缩比无需实际压缩即可获得，这极大节省了计算开销。本文选取五种大语言模型作为压缩先验，在句子补全、问答和共指消解等具有挑战性的自然语言处理任务上对比其表现。实验结果表明压缩比与模型性能呈正相关，因此可作为评估大语言模型的通用指标。
