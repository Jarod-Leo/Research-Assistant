# LLM Reasoning Engine: Specialized Training for Enhanced Mathematical Reasoning

链接: http://arxiv.org/abs/2412.20227v1

原文摘要:
Large Language Models (LLMs) have shown remarkable performance in various
natural language processing tasks but face challenges in mathematical
reasoning, where complex problem-solving requires both linguistic understanding
and mathematical reasoning skills. Existing approaches to address this
challenge often rely on ensemble methods and suffer from the problem of data
scarcity in target domains. In this work, we present a novel method to enhance
LLMs' capabilities in mathematical reasoning tasks. Motivated by the need to
bridge this gap, our approach incorporates a question paraphrase strategy,
which aims at diversifying the linguistic forms of mathematical questions to
improve generalization. Additionally, specialized training objectives are
employed to guide the model's learning process, focusing on enhancing its
understanding of mathematical concepts and reasoning processes. We conduct
experiments on four datasets using different LLMs, and demonstrate the
effectiveness of our approach in improving LLMs' performance on mathematical
reasoning tasks. Our findings underscore the significance of our methodology in
the advancement of large language models and its potential implications for
real-world applications that require mathematical reasoning abilities.

中文翻译:
大型语言模型（LLMs）在各类自然语言处理任务中展现出卓越性能，但在数学推理领域仍面临挑战——复杂问题的解决需要同时具备语言理解与数学推理能力。现有解决方案多依赖集成方法，且受限于目标领域的数据稀缺问题。本研究提出了一种增强LLMs数学推理能力的新方法。基于弥合这一能力差距的需求，我们的方法创新性地引入问题复述策略，通过多样化数学问题的语言表达形式来提升模型泛化能力。同时采用专项训练目标引导模型学习过程，重点强化其对数学概念与推理流程的理解。我们在四个数据集上使用不同LLMs进行实验，验证了该方法在提升数学推理任务性能方面的有效性。研究结果不仅凸显了本方法论对推动大语言模型发展的重要意义，更揭示了其在需要数学推理能力的现实应用场景中的潜在价值。
