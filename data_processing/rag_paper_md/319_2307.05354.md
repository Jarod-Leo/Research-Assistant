# GujiBERT and GujiGPT: Construction of Intelligent Information Processing Foundation Language Models for Ancient Texts

链接: http://arxiv.org/abs/2307.05354v1

原文摘要:
In the context of the rapid development of large language models, we have
meticulously trained and introduced the GujiBERT and GujiGPT language models,
which are foundational models specifically designed for intelligent information
processing of ancient texts. These models have been trained on an extensive
dataset that encompasses both simplified and traditional Chinese characters,
allowing them to effectively handle various natural language processing tasks
related to ancient books, including but not limited to automatic sentence
segmentation, punctuation, word segmentation, part-of-speech tagging, entity
recognition, and automatic translation. Notably, these models have exhibited
exceptional performance across a range of validation tasks using publicly
available datasets. Our research findings highlight the efficacy of employing
self-supervised methods to further train the models using classical text
corpora, thus enhancing their capability to tackle downstream tasks. Moreover,
it is worth emphasizing that the choice of font, the scale of the corpus, and
the initial model selection all exert significant influence over the ultimate
experimental outcomes. To cater to the diverse text processing preferences of
researchers in digital humanities and linguistics, we have developed three
distinct categories comprising a total of nine model variations. We believe
that by sharing these foundational language models specialized in the domain of
ancient texts, we can facilitate the intelligent processing and scholarly
exploration of ancient literary works and, consequently, contribute to the
global dissemination of China's rich and esteemed traditional culture in this
new era.

中文翻译:
在大语言模型快速发展的背景下，我们通过精心训练推出了GujiBERT与GujiGPT语言模型——这是专门面向古籍智能化信息处理的基础模型。该系列模型基于涵盖简繁汉字的大规模语料库进行训练，能够有效处理古籍自动断句、标点、分词、词性标注、实体识别及自动翻译等多种自然语言处理任务。值得注意的是，在公开数据集的多项验证任务中，这些模型均展现出卓越性能。我们的研究证实：采用自监督方法结合古典文本语料库进行增量训练，能显著提升模型应对下游任务的能力；同时需要强调，字体选择、语料规模与初始模型筛选都对最终实验效果产生重要影响。为满足数字人文与语言学研究者不同的文本处理需求，我们开发了包含三大类别共九种变体的模型系列。我们相信，通过共享这些古籍领域的专业基础语言模型，将有助于推动古代文学作品的智能化处理与学术研究，从而为新时代中国优秀传统文化的全球传播作出贡献。

（翻译说明：  
1. 专业术语处理："self-supervised methods"译为"自监督方法"，"downstream tasks"译为"下游任务"，符合NLP领域术语规范  
2. 句式重构：将原文复合长句拆分为符合中文表达习惯的短句，如将"Notably..."独立成句处理  
3. 文化适配："China's rich and esteemed traditional culture"译为"中国优秀传统文化"，符合我国官方表述  
4. 逻辑显化：通过增补"同时需要强调"等连接词，使研究发现的并列关系更清晰  
5. 被动语态转换：将"have been trained"等被动式转为中文主动态表达  
6. 术语统一性：全篇保持"语料库"、"模型"等关键术语的一致性）
