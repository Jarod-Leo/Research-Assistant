# A new approach for fine-tuning sentence transformers for intent classification and out-of-scope detection tasks

链接: http://arxiv.org/abs/2410.13649v1

原文摘要:
In virtual assistant (VA) systems it is important to reject or redirect user
queries that fall outside the scope of the system. One of the most accurate
approaches for out-of-scope (OOS) rejection is to combine it with the task of
intent classification on in-scope queries, and to use methods based on the
similarity of embeddings produced by transformer-based sentence encoders.
Typically, such encoders are fine-tuned for the intent-classification task,
using cross-entropy loss. Recent work has shown that while this produces
suitable embeddings for the intent-classification task, it also tends to
disperse in-scope embeddings over the full sentence embedding space. This
causes the in-scope embeddings to potentially overlap with OOS embeddings,
thereby making OOS rejection difficult. This is compounded when OOS data is
unknown. To mitigate this issue our work proposes to regularize the
cross-entropy loss with an in-scope embedding reconstruction loss learned using
an auto-encoder. Our method achieves a 1-4% improvement in the area under the
precision-recall curve for rejecting out-of-sample (OOS) instances, without
compromising intent classification performance.

中文翻译:
在虚拟助手（VA）系统中，准确拒斥或转接超出系统处理范围的用户查询至关重要。当前最精确的越界查询（OOS）拒斥方法之一，是将该任务与界内查询的意图分类任务相结合，并采用基于Transformer句子编码器生成嵌入向量相似度的技术。传统做法是通过交叉熵损失对编码器进行意图分类任务的微调。最新研究表明，虽然这种方法能为意图分类任务生成合适的嵌入向量，但也会导致界内嵌入向量在整个句子嵌入空间中过度分散。这种分散可能造成界内嵌入向量与越界嵌入向量发生重叠，从而增加拒斥难度——当越界数据未知时，问题尤为突出。为解决这一难题，我们提出采用自动编码器学习界内嵌入重构损失，以此正则化交叉熵损失函数。实验表明，该方法在保持意图分类性能不变的前提下，对样本外（OOS）实例拒斥的精确率-召回率曲线下面积实现了1-4%的提升。

（翻译说明：
1. 专业术语处理："out-of-scope"译为"越界查询"，"in-scope"译为"界内"，"auto-encoder"保留技术术语"自动编码器"
2. 长句拆分：将原文复合长句按中文表达习惯分解为多个短句，如将Transformer相关描述单独成句
3. 被动语态转换："it is important to"译为主动式"至关重要"，"are fine-tuned"译为"进行微调"
4. 概念显化："This is compounded"引申译为"问题尤为突出"以明确指代关系
5. 技术指标保留：精确保持"1-4% improvement"等量化表述的准确性
6. 逻辑连接词优化：使用"传统做法""最新研究""为解决"等符合学术文本特征的衔接词）
