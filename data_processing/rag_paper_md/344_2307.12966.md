# Aligning Large Language Models with Human: A Survey

链接: http://arxiv.org/abs/2307.12966v1

原文摘要:
Large Language Models (LLMs) trained on extensive textual corpora have
emerged as leading solutions for a broad array of Natural Language Processing
(NLP) tasks. Despite their notable performance, these models are prone to
certain limitations such as misunderstanding human instructions, generating
potentially biased content, or factually incorrect (hallucinated) information.
Hence, aligning LLMs with human expectations has become an active area of
interest within the research community. This survey presents a comprehensive
overview of these alignment technologies, including the following aspects. (1)
Data collection: the methods for effectively collecting high-quality
instructions for LLM alignment, including the use of NLP benchmarks, human
annotations, and leveraging strong LLMs. (2) Training methodologies: a detailed
review of the prevailing training methods employed for LLM alignment. Our
exploration encompasses Supervised Fine-tuning, both Online and Offline human
preference training, along with parameter-efficient training mechanisms. (3)
Model Evaluation: the methods for evaluating the effectiveness of these
human-aligned LLMs, presenting a multifaceted approach towards their
assessment. In conclusion, we collate and distill our findings, shedding light
on several promising future research avenues in the field. This survey,
therefore, serves as a valuable resource for anyone invested in understanding
and advancing the alignment of LLMs to better suit human-oriented tasks and
expectations. An associated GitHub link collecting the latest papers is
available at https://github.com/GaryYufei/AlignLLMHumanSurvey.

中文翻译:
基于海量文本训练的大语言模型（LLMs）已成为解决各类自然语言处理（NLP）任务的主流方案。尽管表现卓越，这些模型仍存在若干局限：可能误解人类指令、生成带有偏见的內容，或产生与事实不符的虚构信息。因此，如何使大语言模型与人类期望对齐已成为学界研究热点。本综述系统梳理了相关对齐技术，涵盖以下维度：（1）数据收集：高效获取高质量指令数据的方法，包括利用NLP基准数据集、人工标注以及调用强效大语言模型；（2）训练方法：详细解析主流对齐训练策略，包括监督微调、在线/离线人类偏好训练，以及参数高效训练机制；（3）模型评估：多维度评估对齐效果的方法体系。最后，我们整合研究发现，指明该领域未来若干极具潜力的研究方向。本综述旨在为关注大语言模型对齐研究的学者提供系统参考，推动模型更好地适配人类任务与价值期待。相关GitHub链接持续更新最新论文：https://github.com/GaryYufei/AlignLLMHumanSurvey。

（注：译文严格遵循学术规范，采用术语统一原则，如"alignment"译为"对齐"；通过拆分英文长句为中文短句结构（如将"presenting a multifaceted approach..."处理为独立分句）；保留技术概念精确性（如"Supervised Fine-tuning"译为"监督微调"）；对被动语态进行主动化处理（如"are prone to..."译为"仍存在"）；重要概念首次出现保留英文缩写"LLMs"并在括号内标注全称）
