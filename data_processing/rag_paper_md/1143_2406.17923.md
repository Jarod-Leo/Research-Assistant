# PAFT: A Parallel Training Paradigm for Effective LLM Fine-Tuning

链接: http://arxiv.org/abs/2406.17923v1

原文摘要:
Large language models (LLMs) have shown remarkable abilities in diverse
natural language processing (NLP) tasks. The LLMs generally undergo supervised
fine-tuning (SFT) followed by preference alignment to be usable in downstream
applications. However, this sequential training pipeline leads to alignment tax
that degrades the LLM performance.
  This paper introduces PAFT, a new PArallel training paradigm for effective
LLM Fine-Tuning, which independently performs SFT and preference alignment
(e.g., DPO and ORPO, etc.) with the same pre-trained model on respective
datasets. The model produced by SFT and the model from preference alignment are
then merged into a final model by parameter fusing for use in downstream
applications. This work reveals important findings that preference alignment
like DPO naturally results in a sparse model while SFT leads to a natural dense
model which needs to be sparsified for effective model merging. This paper
introduces an effective interference resolution which reduces the redundancy by
sparsifying the delta parameters. The LLM resulted from the new training
paradigm achieved Rank #1 on the HuggingFace Open LLM Leaderboard.
Comprehensive evaluation shows the effectiveness of the parallel training
paradigm.

中文翻译:
大型语言模型（LLMs）在多样化的自然语言处理（NLP）任务中展现出卓越能力。通常，LLMs会先经过监督微调（SFT），再进行偏好对齐以适用于下游应用。然而，这种顺序训练流程会导致"对齐税"，从而降低模型性能。  

本文提出PAFT——一种高效的并行LLM微调新范式，其核心在于使用同一预训练模型，分别在SFT数据集和偏好对齐（如DPO、ORPO等）数据集上独立训练。随后，通过参数融合将SFT产出的模型与偏好对齐模型合并为最终下游应用模型。研究发现：DPO等偏好对齐会自然产生稀疏模型，而SFT则生成天然稠密模型——后者需经稀疏化处理才能有效融合。为此，本文提出通过稀疏化增量参数来消除冗余的干扰消解方法。采用该并行训练范式的新模型在HuggingFace开放LLM排行榜荣登榜首，全面评估验证了该范式的有效性。  

（注：根据学术摘要翻译规范，采用以下处理：  
1. 专业术语保留英文缩写（LLM/NLP/SFT/DPO/ORPO）并首次出现时标注全称  
2. "alignment tax"译为行业通用表述"对齐税"  
3. "parameter fusing"译为"参数融合"以保持技术一致性  
4. 被动语态转换为主动句式（如"are merged"→"合并为"）  
5. 长难句拆分重组，如将定语从句"which independently performs..."处理为分句）
