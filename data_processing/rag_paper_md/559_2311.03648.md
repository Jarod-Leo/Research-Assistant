# Instruct Me More! Random Prompting for Visual In-Context Learning

链接: http://arxiv.org/abs/2311.03648v1

原文摘要:
Large-scale models trained on extensive datasets, have emerged as the
preferred approach due to their high generalizability across various tasks.
In-context learning (ICL), a popular strategy in natural language processing,
uses such models for different tasks by providing instructive prompts but
without updating model parameters. This idea is now being explored in computer
vision, where an input-output image pair (called an in-context pair) is
supplied to the model with a query image as a prompt to exemplify the desired
output. The efficacy of visual ICL often depends on the quality of the prompts.
We thus introduce a method coined Instruct Me More (InMeMo), which augments
in-context pairs with a learnable perturbation (prompt), to explore its
potential. Our experiments on mainstream tasks reveal that InMeMo surpasses the
current state-of-the-art performance. Specifically, compared to the baseline
without learnable prompt, InMeMo boosts mIoU scores by 7.35 and 15.13 for
foreground segmentation and single object detection tasks, respectively. Our
findings suggest that InMeMo offers a versatile and efficient way to enhance
the performance of visual ICL with lightweight training. Code is available at
https://github.com/Jackieam/InMeMo.

中文翻译:
基于海量数据训练的大规模模型因其在各类任务中展现出的卓越泛化能力，已成为当前的主流研究方法。在自然语言处理领域备受关注的上下文学习（ICL）策略，通过提供指令性提示而不更新模型参数的方式，成功将此类模型应用于不同任务。这一理念正被引入计算机视觉领域：研究者通过输入-输出图像对（称为上下文配对样本）与查询图像共同构成提示，以此示范期望的输出效果。视觉ICL的效能往往取决于提示质量，为此我们提出"深度指导"（InMeMo）方法——通过为上下文配对样本添加可学习扰动（提示）来挖掘其潜力。主流任务的实验表明，InMeMo突破了现有技术性能极限：在基线模型未使用可学习提示的情况下，该方法将前景分割和单目标检测任务的mIoU分数分别提升了7.35和15.13。研究表明，InMeMo通过轻量级训练为提升视觉ICL性能提供了通用高效的解决方案。代码已开源：https://github.com/Jackieam/InMeMo。

（翻译说明：
1. 专业术语处理："in-context learning"统一译为"上下文学习"，"in-context pair"译为"上下文配对样本"，保持学术一致性
2. 技术概念显化："learnable perturbation"译为"可学习扰动"并括号标注"提示"，既准确传达技术含义又便于理解
3. 句式重构：将英文长句拆分为符合中文表达习惯的短句，如将实验数据部分重组为冒号引导的列举式表达
4. 动态对等："state-of-the-art"译为"技术性能极限"而非字面直译，更符合中文技术文献表述
5. 品牌名称保留：方法名"InMeMo"采用音意结合译法，既保留原名又通过"深度指导"体现方法特性
6. 被动语态转化：将"are supplied"等被动结构转换为中文主动态，如"研究者通过...共同构成"
7. 数据呈现优化：精确保留数值单位"mIoU分数"，采用中文数字书写规范）
