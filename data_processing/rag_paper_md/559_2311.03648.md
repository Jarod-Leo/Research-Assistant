# Instruct Me More! Random Prompting for Visual In-Context Learning

链接: http://arxiv.org/abs/2311.03648v1

原文摘要:
Large-scale models trained on extensive datasets, have emerged as the
preferred approach due to their high generalizability across various tasks.
In-context learning (ICL), a popular strategy in natural language processing,
uses such models for different tasks by providing instructive prompts but
without updating model parameters. This idea is now being explored in computer
vision, where an input-output image pair (called an in-context pair) is
supplied to the model with a query image as a prompt to exemplify the desired
output. The efficacy of visual ICL often depends on the quality of the prompts.
We thus introduce a method coined Instruct Me More (InMeMo), which augments
in-context pairs with a learnable perturbation (prompt), to explore its
potential. Our experiments on mainstream tasks reveal that InMeMo surpasses the
current state-of-the-art performance. Specifically, compared to the baseline
without learnable prompt, InMeMo boosts mIoU scores by 7.35 and 15.13 for
foreground segmentation and single object detection tasks, respectively. Our
findings suggest that InMeMo offers a versatile and efficient way to enhance
the performance of visual ICL with lightweight training. Code is available at
https://github.com/Jackieam/InMeMo.

中文翻译:
基于海量数据训练的大规模模型，因其在各类任务中展现出的卓越泛化能力，已成为当前的主流选择。自然语言处理领域流行的上下文学习（ICL）策略，通过提供指令性提示而不更新模型参数，使此类模型能够适应不同任务。这一理念正被引入计算机视觉领域，其中输入-输出图像对（称为上下文配对）与查询图像共同作为提示，以示范期望输出效果。视觉ICL的效能往往取决于提示质量，为此我们提出"深度指导"（InMeMo）方法，通过可学习扰动（提示）增强上下文配对，探索其潜力。主流任务实验表明，InMeMo突破了现有最佳性能：在前景分割和单目标检测任务中，相较于无学习提示的基线模型，mIoU分数分别提升7.35和15.13。研究表明，InMeMo通过轻量级训练为提升视觉ICL性能提供了高效通用方案。代码已开源于https://github.com/Jackieam/InMeMo。
