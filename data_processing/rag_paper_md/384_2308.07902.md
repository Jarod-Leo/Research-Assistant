# Through the Lens of Core Competency: Survey on Evaluation of Large Language Models

链接: http://arxiv.org/abs/2308.07902v1

原文摘要:
From pre-trained language model (PLM) to large language model (LLM), the
field of natural language processing (NLP) has witnessed steep performance
gains and wide practical uses. The evaluation of a research field guides its
direction of improvement. However, LLMs are extremely hard to thoroughly
evaluate for two reasons. First of all, traditional NLP tasks become inadequate
due to the excellent performance of LLM. Secondly, existing evaluation tasks
are difficult to keep up with the wide range of applications in real-world
scenarios. To tackle these problems, existing works proposed various benchmarks
to better evaluate LLMs. To clarify the numerous evaluation tasks in both
academia and industry, we investigate multiple papers concerning LLM
evaluations. We summarize 4 core competencies of LLM, including reasoning,
knowledge, reliability, and safety. For every competency, we introduce its
definition, corresponding benchmarks, and metrics. Under this competency
architecture, similar tasks are combined to reflect corresponding ability,
while new tasks can also be easily added into the system. Finally, we give our
suggestions on the future direction of LLM's evaluation.

中文翻译:
从预训练语言模型（PLM）到大规模语言模型（LLM），自然语言处理（NLP）领域经历了性能的飞跃式提升与应用场景的广泛拓展。对研究领域的评估指引着其改进方向，然而LLM的全面评估面临两大挑战：一方面，传统NLP任务因LLM的卓越表现而失去区分度；另一方面，现有评估体系难以覆盖现实场景中多样化的应用需求。为解决这些问题，学界已提出多种基准测试框架。为厘清学术界与工业界纷繁复杂的评估任务，本文系统研究了多篇LLM评估相关文献，提炼出推理、知识、可靠性与安全性四大核心能力维度。针对每个能力维度，我们详细阐释其定义内涵、对应基准测试体系及评估指标。该能力架构既可通过整合相似任务来映射特定能力，又能灵活纳入新兴评估任务。最后，我们对LLM评估的未来发展方向提出了建设性建议。
