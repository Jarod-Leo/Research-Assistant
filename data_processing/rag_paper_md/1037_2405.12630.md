# Exploration of Masked and Causal Language Modelling for Text Generation

链接: http://arxiv.org/abs/2405.12630v1

原文摘要:
Large Language Models (LLMs) have revolutionised the field of Natural
Language Processing (NLP) and have achieved state-of-the-art performance in
practically every task in this field. However, the prevalent approach used in
text generation, Causal Language Modelling (CLM), which generates text
sequentially from left to right, inherently limits the freedom of the model,
which does not decide when and where each token is generated. In contrast,
Masked Language Modelling (MLM), primarily used for language understanding
tasks, can generate tokens anywhere in the text and any order. This paper
conducts an extensive comparison of MLM and CLM approaches for text generation
tasks. To do so, we pre-train several language models of comparable sizes on
three different datasets, namely 1) medical discharge summaries, 2) movie plot
synopses, and 3) authorship verification datasets. To assess the quality of the
generations, we first employ quantitative metrics and then perform a
qualitative human evaluation to analyse coherence and grammatical correctness.
In addition, we evaluate the usefulness of the generated texts by using them in
three different downstream tasks: 1) Entity Recognition, 2) Text
Classification, and 3) Authorship Verification. The results show that MLM
consistently outperforms CLM in text generation across all datasets, with
higher quantitative scores and better coherence in the generated text. The
study also finds \textit{no strong correlation} between the quality of the
generated text and the performance of the models in the downstream tasks. With
this study, we show that MLM for text generation has great potential for future
research and provides direction for future studies in this area.

中文翻译:
大型语言模型（LLM）彻底改变了自然语言处理（NLP）领域，几乎在该领域的每项任务中都实现了最先进的性能。然而，当前文本生成的主流方法——因果语言建模（CLM）采用从左至右的序列生成方式，本质上限制了模型的自由度，使其无法自主决定每个词元的生成时机与位置。相比之下，主要用于语言理解任务的掩码语言建模（MLM）则能在文本任意位置以任何顺序生成词元。本文对文本生成任务中的MLM与CLM方法展开了全面对比研究：我们在三个不同数据集（1）医疗出院摘要，2）电影剧情梗概，3）作者验证数据集上预训练了多个规模相当的语言模型。为评估生成质量，我们首先采用量化指标，继而通过人工评估分析文本连贯性与语法正确性。此外，我们通过三项下游任务验证生成文本的实用性：1）实体识别，2）文本分类，3）作者验证。结果表明，在所有数据集中，MLM在文本生成方面始终优于CLM，其生成文本不仅量化得分更高，且具有更好的连贯性。研究还发现生成文本质量与模型在下游任务中的表现之间不存在强相关性。本研究表明，基于MLM的文本生成具有重要的研究潜力，并为该领域未来研究提供了方向指引。
