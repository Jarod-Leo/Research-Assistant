# AccelTran: A Sparsity-Aware Accelerator for Dynamic Inference with Transformers

链接: http://arxiv.org/abs/2302.14705v1

原文摘要:
Self-attention-based transformer models have achieved tremendous success in
the domain of natural language processing. Despite their efficacy, accelerating
the transformer is challenging due to its quadratic computational complexity
and large activation sizes. Existing transformer accelerators attempt to prune
its tokens to reduce memory access, albeit with high compute overheads.
Moreover, previous works directly operate on large matrices involved in the
attention operation, which limits hardware utilization. In order to address
these challenges, this work proposes a novel dynamic inference scheme,
DynaTran, which prunes activations at runtime with low overhead, substantially
reducing the number of ineffectual operations. This improves the throughput of
transformer inference. We further propose tiling the matrices in transformer
operations along with diverse dataflows to improve data reuse, thus enabling
higher energy efficiency. To effectively implement these methods, we propose
AccelTran, a novel accelerator architecture for transformers. Extensive
experiments with different models and benchmarks demonstrate that DynaTran
achieves higher accuracy than the state-of-the-art top-k hardware-aware pruning
strategy while attaining up to 1.2$\times$ higher sparsity. One of our proposed
accelerators, AccelTran-Edge, achieves 330K$\times$ higher throughput with
93K$\times$ lower energy requirement when compared to a Raspberry Pi device. On
the other hand, AccelTran-Server achieves 5.73$\times$ higher throughput and
3.69$\times$ lower energy consumption compared to the state-of-the-art
transformer co-processor, Energon. The simulation source code is available at
https://github.com/jha-lab/acceltran.

中文翻译:
基于自注意力机制的Transformer模型在自然语言处理领域取得了巨大成功。然而由于其二次方的计算复杂度和庞大的激活规模，加速Transformer模型仍面临挑战。现有加速器尝试通过剪枝令牌来减少内存访问，但计算开销较大。此外，先前研究直接对注意力运算中的大型矩阵进行操作，限制了硬件利用率。

为解决这些问题，本研究提出动态推理框架DynaTran，能以低开销实现运行时激活剪枝，显著减少无效操作，从而提升Transformer推理吞吐量。我们进一步提出矩阵分块策略与多样化数据流方案，通过提升数据复用率实现更高能效。为有效实施这些方法，设计了新型Transformer加速器架构AccelTran。

多模型多基准测试表明，DynaTran在保持优于最先进Top-K硬件感知剪枝策略精度的同时，稀疏度提升达1.2倍。其中边缘计算版本AccelTran-Edge相比树莓派设备实现33万倍吞吐量提升与9.3万倍能效优化；服务器版本AccelTran-Server相较当前最优Transformer协处理器Energon，吞吐量提升5.73倍且能耗降低3.69倍。仿真源代码已开源：https://github.com/jha-lab/acceltran。
