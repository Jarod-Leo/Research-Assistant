# AccelTran: A Sparsity-Aware Accelerator for Dynamic Inference with Transformers

链接: http://arxiv.org/abs/2302.14705v1

原文摘要:
Self-attention-based transformer models have achieved tremendous success in
the domain of natural language processing. Despite their efficacy, accelerating
the transformer is challenging due to its quadratic computational complexity
and large activation sizes. Existing transformer accelerators attempt to prune
its tokens to reduce memory access, albeit with high compute overheads.
Moreover, previous works directly operate on large matrices involved in the
attention operation, which limits hardware utilization. In order to address
these challenges, this work proposes a novel dynamic inference scheme,
DynaTran, which prunes activations at runtime with low overhead, substantially
reducing the number of ineffectual operations. This improves the throughput of
transformer inference. We further propose tiling the matrices in transformer
operations along with diverse dataflows to improve data reuse, thus enabling
higher energy efficiency. To effectively implement these methods, we propose
AccelTran, a novel accelerator architecture for transformers. Extensive
experiments with different models and benchmarks demonstrate that DynaTran
achieves higher accuracy than the state-of-the-art top-k hardware-aware pruning
strategy while attaining up to 1.2$\times$ higher sparsity. One of our proposed
accelerators, AccelTran-Edge, achieves 330K$\times$ higher throughput with
93K$\times$ lower energy requirement when compared to a Raspberry Pi device. On
the other hand, AccelTran-Server achieves 5.73$\times$ higher throughput and
3.69$\times$ lower energy consumption compared to the state-of-the-art
transformer co-processor, Energon. The simulation source code is available at
https://github.com/jha-lab/acceltran.

中文翻译:
基于自注意力机制的Transformer模型在自然语言处理领域取得了巨大成功。然而，由于其二次方的计算复杂度和庞大的激活规模，加速Transformer模型仍面临挑战。现有加速器尝试通过剪枝技术减少内存访问，但计算开销较高。此外，先前研究直接处理注意力运算中的大型矩阵，限制了硬件利用率。为解决这些问题，本研究提出了一种创新的动态推理方案DynaTran，该方案能以低开销实现运行时激活剪枝，显著减少无效操作，从而提升Transformer推理吞吐量。我们进一步提出对Transformer运算矩阵进行分块处理，并采用多样化数据流以提高数据复用率，从而实现更高能效。为有效实施这些方法，我们设计了新型加速器架构AccelTran。通过多种模型和基准测试的广泛实验表明，DynaTran在保持最高1.2倍稀疏度的同时，其准确率优于当前最先进的top-k硬件感知剪枝策略。其中边缘计算版本AccelTran-Edge相比树莓派设备实现了33万倍吞吐量提升和9.3万倍能耗降低；服务器版本AccelTran-Server相比前沿Transformer协处理器Energon，吞吐量提升5.73倍，能耗降低3.69倍。仿真源代码已开源在https://github.com/jha-lab/acceltran。
