# Seeing Eye to AI: Human Alignment via Gaze-Based Response Rewards for Large Language Models

链接: http://arxiv.org/abs/2410.01532v1

原文摘要:
Advancements in Natural Language Processing (NLP), have led to the emergence
of Large Language Models (LLMs) such as GPT, Llama, Claude, and Gemini, which
excel across a range of tasks but require extensive fine-tuning to align their
outputs with human expectations. A widely used method for achieving this
alignment is Reinforcement Learning from Human Feedback (RLHF), which, despite
its success, faces challenges in accurately modelling human preferences. In
this paper, we introduce GazeReward, a novel framework that integrates implicit
feedback -- and specifically eye-tracking (ET) data -- into the Reward Model
(RM). In addition, we explore how ET-based features can provide insights into
user preferences. Through ablation studies we test our framework with different
integration methods, LLMs, and ET generator models, demonstrating that our
approach significantly improves the accuracy of the RM on established human
preference datasets. This work advances the ongoing discussion on optimizing AI
alignment with human values, exploring the potential of cognitive data for
shaping future NLP research.

中文翻译:
自然语言处理（NLP）技术的进步催生了GPT、Llama、Claude和Gemini等大型语言模型（LLMs），这些模型虽在多项任务中表现卓越，但需经过大量微调才能使其输出符合人类预期。目前广泛采用的强化学习人类反馈（RLHF）方法虽取得成效，但在精确建模人类偏好方面仍面临挑战。本文提出GazeReward创新框架，通过整合眼动追踪（ET）数据等隐性反馈来优化奖励模型（RM）。我们系统探究了基于ET的特征如何揭示用户偏好，并通过消融实验验证了不同集成方法、LLMs及ET生成模型的效果，证明该框架能显著提升RM在权威人类偏好数据集上的准确率。这项研究为优化人工智能与人类价值观对齐提供了新思路，揭示了认知数据对未来NLP研究的塑造潜力。
