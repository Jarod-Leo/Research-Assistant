# Small Models, Big Impact: Efficient Corpus and Graph-Based Adaptation of Small Multilingual Language Models for Low-Resource Languages

链接: http://arxiv.org/abs/2502.10140v1

原文摘要:
Low-resource languages (LRLs) face significant challenges in natural language
processing (NLP) due to limited data. While current state-of-the-art large
language models (LLMs) still struggle with LRLs, smaller multilingual models
(mLMs) such as mBERT and XLM-R offer greater promise due to a better fit of
their capacity to low training data sizes. This study systematically
investigates parameter-efficient adapter-based methods for adapting mLMs to
LRLs, evaluating three architectures: Sequential Bottleneck, Invertible
Bottleneck, and Low-Rank Adaptation. Using unstructured text from GlotCC and
structured knowledge from ConceptNet, we show that small adaptation datasets
(e.g., up to 1 GB of free-text or a few MB of knowledge graph data) yield gains
in intrinsic (masked language modeling) and extrinsic tasks (topic
classification, sentiment analysis, and named entity recognition). We find that
Sequential Bottleneck adapters excel in language modeling, while Invertible
Bottleneck adapters slightly outperform other methods on downstream tasks due
to better embedding alignment and larger parameter counts. Adapter-based
methods match or outperform full fine-tuning while using far fewer parameters,
and smaller mLMs prove more effective for LRLs than massive LLMs like LLaMA-3,
GPT-4, and DeepSeek-R1-based distilled models. While adaptation improves
performance, pre-training data size remains the dominant factor, especially for
languages with extensive pre-training coverage.

中文翻译:
低资源语言（LRLs）在自然语言处理（NLP）中因数据有限面临显著挑战。尽管当前最先进的大语言模型（LLMs）仍难以应对LRLs，但mBERT、XLM-R等小型多语言模型（mLMs）因其容量更适配低训练数据量而展现出更大潜力。本研究系统探究了基于适配器的参数高效方法在mLMs适配LRLs中的应用，评估了三种架构：序列瓶颈适配器、可逆瓶颈适配器和低秩自适应。利用GlotCC的非结构化文本和ConceptNet的结构化知识，我们发现小规模适配数据集（如1GB自由文本或几MB知识图谱数据）能在内在任务（掩码语言建模）和外在任务（主题分类、情感分析和命名实体识别）中带来性能提升。研究表明，序列瓶颈适配器在语言建模中表现优异，而可逆瓶颈适配器因更好的嵌入对齐和更多参数量，在下游任务中略胜一筹。基于适配器的方法仅用极少参数即可达到或超越全参数微调效果，且小型mLMs对LRLs的处理能力远超LLaMA-3、GPT-4等巨型LLMs及其蒸馏模型。虽然适配能提升性能，但预训练数据规模仍是主导因素，尤其对预训练覆盖较广的语言而言。
