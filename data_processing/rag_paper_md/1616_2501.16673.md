# LLM-AutoDiff: Auto-Differentiate Any LLM Workflow

链接: http://arxiv.org/abs/2501.16673v2

原文摘要:
Large Language Models (LLMs) have reshaped natural language processing,
powering applications from multi-hop retrieval and question answering to
autonomous agent workflows. Yet, prompt engineering -- the task of crafting
textual inputs to effectively direct LLMs -- remains difficult and
labor-intensive, particularly for complex pipelines that combine multiple LLM
calls with functional operations like retrieval and data formatting. We
introduce LLM-AutoDiff: a novel framework for Automatic Prompt Engineering
(APE) that extends textual gradient-based methods (such as Text-Grad) to
multi-component, potentially cyclic LLM architectures. Implemented within the
AdalFlow library, LLM-AutoDiff treats each textual input as a trainable
parameter and uses a frozen backward engine LLM to generate feedback-akin to
textual gradients -- that guide iterative prompt updates. Unlike prior
single-node approaches, LLM-AutoDiff inherently accommodates functional nodes,
preserves time-sequential behavior in repeated calls (e.g., multi-hop loops),
and combats the "lost-in-the-middle" problem by isolating distinct sub-prompts
(instructions, formats, or few-shot examples). It further boosts training
efficiency by focusing on error-prone samples through selective gradient
computation. Across diverse tasks, including single-step classification,
multi-hop retrieval-based QA, and agent-driven pipelines, LLM-AutoDiff
consistently outperforms existing textual gradient baselines in both accuracy
and training cost. By unifying prompt optimization through a graph-centric
lens, LLM-AutoDiff offers a powerful new paradigm for scaling and automating
LLM workflows - mirroring the transformative role that automatic
differentiation libraries have long played in neural network research.

中文翻译:
大型语言模型（LLMs）重塑了自然语言处理领域，推动了从多跳检索问答到自主智能体工作流的各类应用。然而，提示工程——即设计文本输入以有效引导LLMs的任务——仍然具有挑战性且耗时费力，尤其对于将多个LLM调用与检索、数据格式化等功能操作结合的复杂流程。我们提出LLM-AutoDiff：一种创新的自动提示工程（APE）框架，将基于文本梯度的方法（如Text-Grad）扩展至多组件、可能含循环结构的LLM架构。该框架通过AdalFlow库实现，将每个文本输入视为可训练参数，并利用冻结的反向引擎LLM生成类似文本梯度的反馈，指导提示的迭代优化。

与先前单节点方法不同，LLM-AutoDiff天然支持功能节点，在重复调用（如多跳循环）中保持时序行为，并通过隔离不同子提示（指令、格式或少量示例）缓解"中间迷失"问题。其通过选择性梯度计算聚焦易错样本，进一步提升了训练效率。在单步分类、基于多跳检索的问答及智能体驱动流程等多样化任务中，LLM-AutoDiff在准确率和训练成本上均优于现有文本梯度基线方法。这种以图为中心的统一提示优化范式，为LLM工作流的规模化与自动化提供了强大新范式——正如自动微分库在神经网络研究中长期发挥的变革性作用。
