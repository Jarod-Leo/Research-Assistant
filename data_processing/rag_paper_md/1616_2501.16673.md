# LLM-AutoDiff: Auto-Differentiate Any LLM Workflow

链接: http://arxiv.org/abs/2501.16673v2

原文摘要:
Large Language Models (LLMs) have reshaped natural language processing,
powering applications from multi-hop retrieval and question answering to
autonomous agent workflows. Yet, prompt engineering -- the task of crafting
textual inputs to effectively direct LLMs -- remains difficult and
labor-intensive, particularly for complex pipelines that combine multiple LLM
calls with functional operations like retrieval and data formatting. We
introduce LLM-AutoDiff: a novel framework for Automatic Prompt Engineering
(APE) that extends textual gradient-based methods (such as Text-Grad) to
multi-component, potentially cyclic LLM architectures. Implemented within the
AdalFlow library, LLM-AutoDiff treats each textual input as a trainable
parameter and uses a frozen backward engine LLM to generate feedback-akin to
textual gradients -- that guide iterative prompt updates. Unlike prior
single-node approaches, LLM-AutoDiff inherently accommodates functional nodes,
preserves time-sequential behavior in repeated calls (e.g., multi-hop loops),
and combats the "lost-in-the-middle" problem by isolating distinct sub-prompts
(instructions, formats, or few-shot examples). It further boosts training
efficiency by focusing on error-prone samples through selective gradient
computation. Across diverse tasks, including single-step classification,
multi-hop retrieval-based QA, and agent-driven pipelines, LLM-AutoDiff
consistently outperforms existing textual gradient baselines in both accuracy
and training cost. By unifying prompt optimization through a graph-centric
lens, LLM-AutoDiff offers a powerful new paradigm for scaling and automating
LLM workflows - mirroring the transformative role that automatic
differentiation libraries have long played in neural network research.

中文翻译:
大语言模型（LLMs）重塑了自然语言处理领域，为多跳检索、问答系统到自主智能体工作流等应用提供了强大支持。然而提示工程——即设计文本输入以有效引导大语言模型的任务——仍然具有较高难度且耗时费力，尤其对于将多个LLM调用与检索、数据格式化等功能操作相结合的复杂流程而言。我们提出LLM-AutoDiff：一种创新的自动提示工程（APE）框架，将基于文本梯度的方法（如Text-Grad）扩展至多组件、可能含循环结构的LLM架构。该框架在AdalFlow库中实现，将每个文本输入视为可训练参数，并利用冻结的反向引擎LLM生成类似文本梯度的反馈来指导迭代式提示更新。与先前单节点方法不同，LLM-AutoDiff原生支持功能节点，在重复调用（如多跳循环）中保持时序行为，并通过隔离不同子提示（指令、格式或少量示例）来缓解"中间迷失"问题。该框架还通过选择性梯度计算聚焦易错样本，从而提升训练效率。在单步分类、基于多跳检索的问答以及智能体驱动流程等多样化任务中，LLM-AutoDiff在准确率和训练成本方面均持续超越现有文本梯度基线方法。通过以图为中心的统一视角进行提示优化，LLM-AutoDiff为扩展和自动化LLM工作流提供了强大新范式——这正如同自动微分库在神经网络研究中长期发挥的革命性作用。
