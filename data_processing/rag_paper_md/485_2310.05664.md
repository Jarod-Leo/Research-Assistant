# ViTs are Everywhere: A Comprehensive Study Showcasing Vision Transformers in Different Domain

链接: http://arxiv.org/abs/2310.05664v1

原文摘要:
Transformer design is the de facto standard for natural language processing
tasks. The success of the transformer design in natural language processing has
lately piqued the interest of researchers in the domain of computer vision.
When compared to Convolutional Neural Networks (CNNs), Vision Transformers
(ViTs) are becoming more popular and dominant solutions for many vision
problems. Transformer-based models outperform other types of networks, such as
convolutional and recurrent neural networks, in a range of visual benchmarks.
We evaluate various vision transformer models in this work by dividing them
into distinct jobs and examining their benefits and drawbacks. ViTs can
overcome several possible difficulties with convolutional neural networks
(CNNs). The goal of this survey is to show the first use of ViTs in CV. In the
first phase, we categorize various CV applications where ViTs are appropriate.
Image classification, object identification, image segmentation, video
transformer, image denoising, and NAS are all CV applications. Our next step
will be to analyze the state-of-the-art in each area and identify the models
that are currently available. In addition, we outline numerous open research
difficulties as well as prospective research possibilities.

中文翻译:
Transformer架构已成为自然语言处理任务的事实标准。其在自然语言处理领域的成功应用，近期激发了计算机视觉领域研究者的浓厚兴趣。相较于卷积神经网络（CNNs），视觉Transformer（ViTs）正逐渐成为解决众多视觉问题的主流方案。基于Transformer的模型在一系列视觉基准测试中展现出超越卷积神经网络和循环神经网络等架构的性能优势。本研究通过将各类视觉Transformer模型划分为不同任务场景，系统评估了它们的优势与局限性。ViTs能够有效克服传统卷积神经网络存在的若干潜在缺陷。

本综述旨在呈现ViTs在计算机视觉领域的首次应用全景。研究首先对适合采用ViTs的计算机视觉应用场景进行分类梳理，涵盖图像分类、目标检测、图像分割、视频Transformer、图像去噪以及神经架构搜索（NAS）等方向。继而深入分析各领域最新研究进展，梳理现有模型体系。此外，我们还系统阐述了当前面临的开放性研究挑战，并展望了未来潜在的研究方向。
