# Dialogue for Prompting: a Policy-Gradient-Based Discrete Prompt Optimization for Few-shot Learning

链接: http://arxiv.org/abs/2308.07272v1

原文摘要:
Prompt-based pre-trained language models (PLMs) paradigm have succeeded
substantially in few-shot natural language processing (NLP) tasks. However,
prior discrete prompt optimization methods require expert knowledge to design
the base prompt set and identify high-quality prompts, which is costly,
inefficient, and subjective. Meanwhile, existing continuous prompt optimization
methods improve the performance by learning the ideal prompts through the
gradient information of PLMs, whose high computational cost, and low
readability and generalizability are often concerning. To address the research
gap, we propose a Dialogue-comprised Policy-gradient-based Discrete Prompt
Optimization ($DP_2O$) method. We first design a multi-round dialogue alignment
strategy for readability prompt set generation based on GPT-4. Furthermore, we
propose an efficient prompt screening metric to identify high-quality prompts
with linear complexity. Finally, we construct a reinforcement learning (RL)
framework based on policy gradients to match the prompts to inputs optimally.
By training a policy network with only 0.67% of the PLM parameter size on the
tasks in the few-shot setting, $DP_2O$ outperforms the state-of-the-art (SOTA)
method by 1.52% in accuracy on average on four open-source datasets. Moreover,
subsequent experiments also demonstrate that $DP_2O$ has good universality,
robustness, and generalization ability.

中文翻译:
基于提示词（prompt）的预训练语言模型（PLMs）范式在少样本自然语言处理（NLP）任务中取得了显著成功。然而，现有离散提示词优化方法依赖专家知识设计基础提示集并筛选高质量提示，存在成本高、效率低且主观性强的问题。同时，主流连续提示词优化方法通过PLMs的梯度信息学习理想提示词，虽能提升性能，但其计算成本高昂、可读性与泛化性差的缺陷不容忽视。为此，本文提出融合对话策略梯度的离散提示词优化方法（$DP_2O$）。首先基于GPT-4设计多轮对话对齐策略生成可读性提示集；进而提出线性复杂度的提示词高效筛选指标；最终构建基于策略梯度的强化学习框架实现提示词与输入的最优匹配。在少样本场景下，仅需训练参数量为PLM 0.67%的策略网络，$DP_2O$在四个开源数据集上的准确率平均超越现有最优方法1.52%。后续实验进一步验证了该方法具有良好的普适性、鲁棒性和泛化能力。
