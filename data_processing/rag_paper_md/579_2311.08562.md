# MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration

链接: http://arxiv.org/abs/2311.08562v2

原文摘要:
Large Language Models (LLMs) have significantly advanced natural language
processing, demonstrating exceptional reasoning, tool usage, and memory
capabilities. As their applications expand into multi-agent environments, there
arises a need for a comprehensive evaluation framework that captures LLMs'
reasoning, planning, collaboration, and other social abilities. This work
introduces a novel competition-based benchmark framework specifically designed
to assess LLMs within multi-agent settings, providing quantitative metrics to
evaluate their judgment, reasoning, deception, self-awareness, cooperation,
coordination, and rationality. We utilize two social deduction games alongside
three game-theory scenarios to create diverse environments. Our frame is
fortified with the probabilistic graphic modeling (PGM) method, enhancing the
LLMs' capabilities in navigating complex social and cognitive dimensions. We
evaluate seven LLMs, quantitatively highlighting a significant capability gap
of over threefold between the strongest, GPT o1, and the weakest, Llama-2-70B.
It also confirms that our PGM enhancement boosts the abilities of all selected
models by an average of 37%. Our data and code can be found here
https://github.com/cathyxl/MAgIC.

中文翻译:
大语言模型（LLMs）在自然语言处理领域取得显著突破，展现出卓越的推理能力、工具使用能力和记忆能力。随着其应用场景向多智能体环境扩展，亟需建立一套能全面评估LLMs推理、规划、协作等社会性能力的框架。本研究创新性地提出基于竞技的基准测试框架，专门用于多智能体场景下的LLM评估，通过量化指标衡量模型的判断力、推理能力、欺骗性、自我意识、合作性、协调性和理性程度。我们采用两款社交推理游戏与三种博弈论情境构建多样化测试环境，并引入概率图模型（PGM）方法强化框架，显著提升LLMs应对复杂社会认知维度的能力。对七种LLM的评估数据显示，性能最强的GPT-4与最弱的Llama-2-70B之间存在超过三倍的能力差距。实验同时证实，PGM增强策略使所有测试模型平均能力提升37%。相关数据与代码已开源。
