# Document Structure in Long Document Transformers

链接: http://arxiv.org/abs/2401.17658v1

原文摘要:
Long documents often exhibit structure with hierarchically organized elements
of different functions, such as section headers and paragraphs. Despite the
omnipresence of document structure, its role in natural language processing
(NLP) remains opaque. Do long-document Transformer models acquire an internal
representation of document structure during pre-training? How can structural
information be communicated to a model after pre-training, and how does it
influence downstream performance? To answer these questions, we develop a novel
suite of probing tasks to assess structure-awareness of long-document
Transformers, propose general-purpose structure infusion methods, and evaluate
the effects of structure infusion on QASPER and Evidence Inference, two
challenging long-document NLP tasks. Results on LED and LongT5 suggest that
they acquire implicit understanding of document structure during pre-training,
which can be further enhanced by structure infusion, leading to improved
end-task performance. To foster research on the role of document structure in
NLP modeling, we make our data and code publicly available.

中文翻译:
长文档通常呈现出由不同功能元素按层级组织而成的结构，例如章节标题与段落。尽管文档结构无处不在，但其在自然语言处理（NLP）中的作用仍不明确：长文档Transformer模型是否在预训练过程中习得了对文档结构的内部表征？如何在预训练后向模型传递结构信息？这种信息又如何影响下游任务表现？为解答这些问题，我们开发了一套新颖的探测任务来评估长文档Transformer的结构感知能力，提出了通用型结构注入方法，并在QASPER和Evidence Inference这两个具有挑战性的长文档NLP任务上评估了结构注入的效果。基于LED和LongT5的实验结果表明，这些模型在预训练期间已获得对文档结构的隐式理解，而结构注入能进一步增强这种理解，从而提升终端任务性能。为促进文档结构在NLP建模中作用的研究，我们公开了相关数据与代码。
