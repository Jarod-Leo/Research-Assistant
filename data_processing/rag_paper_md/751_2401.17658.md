# Document Structure in Long Document Transformers

链接: http://arxiv.org/abs/2401.17658v1

原文摘要:
Long documents often exhibit structure with hierarchically organized elements
of different functions, such as section headers and paragraphs. Despite the
omnipresence of document structure, its role in natural language processing
(NLP) remains opaque. Do long-document Transformer models acquire an internal
representation of document structure during pre-training? How can structural
information be communicated to a model after pre-training, and how does it
influence downstream performance? To answer these questions, we develop a novel
suite of probing tasks to assess structure-awareness of long-document
Transformers, propose general-purpose structure infusion methods, and evaluate
the effects of structure infusion on QASPER and Evidence Inference, two
challenging long-document NLP tasks. Results on LED and LongT5 suggest that
they acquire implicit understanding of document structure during pre-training,
which can be further enhanced by structure infusion, leading to improved
end-task performance. To foster research on the role of document structure in
NLP modeling, we make our data and code publicly available.

中文翻译:
以下是符合要求的学术中文翻译：

长文档通常呈现具有层级化功能元素的结构特征，例如章节标题与段落划分。尽管文档结构普遍存在，但其在自然语言处理（NLP）中的作用机制仍不明确。长文档Transformer模型是否在预训练过程中习得了对文档结构的内部表征？如何在预训练后向模型传递结构信息？这种信息又如何影响下游任务表现？为探究这些问题，我们开发了一套新颖的探测任务来评估长文档Transformer的结构感知能力，提出了通用型结构注入方法，并在QASPER和Evidence Inference这两个具有挑战性的长文档NLP任务上评估了结构注入的效果。基于LED和LongT5的实验结果表明：这些模型在预训练期间已获得对文档结构的隐式理解，而结构注入能进一步强化这种理解，从而提升终端任务性能。为促进文档结构在NLP建模中的研究，我们已公开相关数据与代码。

（翻译说明：
1. 专业术语处理：Transformer/LED/LongT5等专业名称保留不译，QASPER和Evidence Inference作为任务名称保留
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句，如将"Despite..."从句转换为转折短句
3. 学术表达："probe tasks"译为"探测任务"，"structure infusion"译为"结构注入"
4. 逻辑显化：将"opaque"意译为"作用机制仍不明确"，更符合中文论文表述
5. 被动语态转换：将英文被动式改为中文主动式，如"can be enhanced"译为"能进一步强化"
6. 术语一致性：全文统一"预训练/下游任务/终端任务"等关键术语译法）
