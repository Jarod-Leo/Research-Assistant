# When Automated Assessment Meets Automated Content Generation: Examining Text Quality in the Era of GPTs

链接: http://arxiv.org/abs/2309.14488v1

原文摘要:
The use of machine learning (ML) models to assess and score textual data has
become increasingly pervasive in an array of contexts including natural
language processing, information retrieval, search and recommendation, and
credibility assessment of online content. A significant disruption at the
intersection of ML and text are text-generating large-language models such as
generative pre-trained transformers (GPTs). We empirically assess the
differences in how ML-based scoring models trained on human content assess the
quality of content generated by humans versus GPTs. To do so, we propose an
analysis framework that encompasses essay scoring ML-models, human and
ML-generated essays, and a statistical model that parsimoniously considers the
impact of type of respondent, prompt genre, and the ML model used for
assessment model. A rich testbed is utilized that encompasses 18,460
human-generated and GPT-based essays. Results of our benchmark analysis reveal
that transformer pretrained language models (PLMs) more accurately score human
essay quality as compared to CNN/RNN and feature-based ML methods.
Interestingly, we find that the transformer PLMs tend to score GPT-generated
text 10-15\% higher on average, relative to human-authored documents.
Conversely, traditional deep learning and feature-based ML models score human
text considerably higher. Further analysis reveals that although the
transformer PLMs are exclusively fine-tuned on human text, they more
prominently attend to certain tokens appearing only in GPT-generated text,
possibly due to familiarity/overlap in pre-training. Our framework and results
have implications for text classification settings where automated scoring of
text is likely to be disrupted by generative AI.

中文翻译:
机器学习（ML）模型在文本数据评估与评分中的应用已日益普及，涵盖自然语言处理、信息检索、搜索推荐、在线内容可信度评估等多个领域。而生成式预训练变换模型（GPTs）等文本生成大语言模型的出现，正在颠覆ML与文本处理的交叉领域。本研究通过实证分析，对比了基于人类内容训练的ML评分模型对人类撰写内容与GPT生成内容的质量评估差异。为此，我们构建了一个分析框架，包含作文评分ML模型、人类与ML生成的文章样本，以及一个统计模型——该模型精要地考量了作答者类型、命题题材和评估模型类型的影响因素。实验采用包含18,460篇人类撰写与GPT生成文章的丰富测试集，基准分析结果表明：相较于CNN/RNN和基于特征的ML方法，预训练变换语言模型（PLMs）能更准确地评估人类文章质量。有趣的是，我们发现变换器PLMs对GPT生成文本的评分平均高出人类撰写文档10-15%。相反，传统深度学习与基于特征的ML模型则对人类文本给出显著更高评分。进一步分析显示，尽管变换器PLMs仅针对人类文本进行微调，它们却会更显著地关注GPT生成文本中的特定标记，这可能是由于预训练阶段的熟悉度/重叠性所致。本研究的框架与结论对可能受生成式AI冲击的文本自动评分场景具有重要启示意义。
