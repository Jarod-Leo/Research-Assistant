# SeedLM: Compressing LLM Weights into Seeds of Pseudo-Random Generators

链接: http://arxiv.org/abs/2410.10714v1

原文摘要:
Large Language Models (LLMs) have transformed natural language processing,
but face significant challenges in widespread deployment due to their high
runtime cost. In this paper, we introduce SeedLM, a novel post-training
compression method that uses seeds of pseudo-random generators to encode and
compress model weights. Specifically, for each block of weights, we find a seed
that is fed into a Linear Feedback Shift Register (LFSR) during inference to
efficiently generate a random matrix. This matrix is then linearly combined
with compressed coefficients to reconstruct the weight block. SeedLM reduces
memory access and leverages idle compute cycles during inference, effectively
speeding up memory-bound tasks by trading compute for fewer memory accesses.
Unlike state-of-the-art compression methods that rely on calibration data, our
approach is data-free and generalizes well across diverse tasks. Our
experiments with Llama 3 70B, which is particularly challenging to compress,
show that SeedLM achieves significantly better zero-shot accuracy retention at
4- and 3-bit than state-of-the-art techniques, while maintaining performance
comparable to FP16 baselines. Additionally, FPGA-based tests demonstrate that
4-bit SeedLM, as model size increases to 70B, approaches a 4x speed-up over an
FP16 Llama 2/3 baseline.

中文翻译:
大型语言模型（LLMs）虽已革新自然语言处理领域，但其高昂的运行成本严重制约了广泛部署。本文提出SeedLM——一种创新的训练后压缩方法，通过伪随机生成器的种子编码并压缩模型权重。具体而言，针对每个权重块，我们寻找一个种子，在推理过程中将其输入线性反馈移位寄存器（LFSR）以高效生成随机矩阵，再与压缩系数线性组合重构权重块。该方法通过以计算资源换取内存访问次数的减少，显著降低了内存访问量并利用推理过程中的闲置计算周期，有效加速了内存受限任务。与依赖校准数据的现有压缩技术不同，我们的方案无需数据支持且能跨任务泛化良好。在压缩难度极高的Llama 3 70B模型实验中，SeedLM在4位和3位量化下比前沿技术实现了更优异的零样本精度保持，同时保持与FP16基线相当的性能。基于FPGA的测试进一步表明，当模型规模扩展至700亿参数时，4位SeedLM相较FP16版Llama 2/3基线可接近实现4倍加速。
