# BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents

链接: http://arxiv.org/abs/2406.03007v1

原文摘要:
With the prosperity of large language models (LLMs), powerful LLM-based
intelligent agents have been developed to provide customized services with a
set of user-defined tools. State-of-the-art methods for constructing LLM agents
adopt trained LLMs and further fine-tune them on data for the agent task.
However, we show that such methods are vulnerable to our proposed backdoor
attacks named BadAgent on various agent tasks, where a backdoor can be embedded
by fine-tuning on the backdoor data. At test time, the attacker can manipulate
the deployed LLM agents to execute harmful operations by showing the trigger in
the agent input or environment. To our surprise, our proposed attack methods
are extremely robust even after fine-tuning on trustworthy data. Though
backdoor attacks have been studied extensively in natural language processing,
to the best of our knowledge, we could be the first to study them on LLM agents
that are more dangerous due to the permission to use external tools. Our work
demonstrates the clear risk of constructing LLM agents based on untrusted LLMs
or data. Our code is public at https://github.com/DPamK/BadAgent

中文翻译:
随着大语言模型（LLMs）的蓬勃发展，基于LLM的智能代理被广泛开发，通过一系列用户定义工具提供定制化服务。当前构建LLM代理的最先进方法采用预训练LLM，并针对代理任务数据进行微调。然而我们发现，这类方法在面对我们提出的名为BadAgent的后门攻击时表现脆弱——通过在微调阶段植入后门数据，攻击者能在测试阶段通过输入或环境中的触发信号，操控已部署的LLM代理执行恶意操作。令人惊讶的是，即使在可信数据上微调后，我们提出的攻击方法仍保持极强的鲁棒性。尽管后门攻击在自然语言处理领域已有广泛研究，但据我们所知，本研究首次揭示了LLM代理场景下此类攻击的更高危险性，因其具备调用外部工具的权限。这项工作清晰表明基于不可信LLM或数据构建代理模型存在的重大风险。代码已开源于https://github.com/DPamK/BadAgent。
