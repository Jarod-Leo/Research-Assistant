# Atleus: Accelerating Transformers on the Edge Enabled by 3D Heterogeneous Manycore Architectures

链接: http://arxiv.org/abs/2501.09588v1

原文摘要:
Transformer architectures have become the standard neural network model for
various machine learning applications including natural language processing and
computer vision. However, the compute and memory requirements introduced by
transformer models make them challenging to adopt for edge applications.
Furthermore, fine-tuning pre-trained transformers (e.g., foundation models) is
a common task to enhance the model's predictive performance on specific
tasks/applications. Existing transformer accelerators are oblivious to
complexities introduced by fine-tuning. In this paper, we propose the design of
a three-dimensional (3D) heterogeneous architecture referred to as Atleus that
incorporates heterogeneous computing resources specifically optimized to
accelerate transformer models for the dual purposes of fine-tuning and
inference. Specifically, Atleus utilizes non-volatile memory and systolic array
for accelerating transformer computational kernels using an integrated 3D
platform. Moreover, we design a suitable NoC to achieve high performance and
energy efficiency. Finally, Atleus adopts an effective quantization scheme to
support model compression. Experimental results demonstrate that Atleus
outperforms existing state-of-the-art by up to 56x and 64.5x in terms of
performance and energy efficiency respectively

中文翻译:
Transformer架构已成为包括自然语言处理和计算机视觉在内的多种机器学习应用的标准神经网络模型。然而，这类模型对计算与内存的高需求使其难以在边缘应用中普及。此外，针对特定任务/应用提升模型预测性能时，对预训练Transformer（如基础模型）进行微调是一项常见操作。现有Transformer加速器尚未考虑微调引入的复杂性。本文提出了一种名为Atleus的三维异构架构设计，该架构整合了专为加速Transformer模型优化的异构计算资源，可同时支持微调与推理双重目标。具体而言，Atleus通过三维集成平台，利用非易失性存储器和脉动阵列加速Transformer计算核心；设计高性能低功耗片上网络；并采用高效量化方案支持模型压缩。实验表明，Atleus在性能和能效方面分别较现有最优方案提升达56倍和64.5倍。
