# Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment

链接: http://arxiv.org/abs/2312.12148v1

原文摘要:
With the continuous growth in the number of parameters of transformer-based
pretrained language models (PLMs), particularly the emergence of large language
models (LLMs) with billions of parameters, many natural language processing
(NLP) tasks have demonstrated remarkable success. However, the enormous size
and computational demands of these models pose significant challenges for
adapting them to specific downstream tasks, especially in environments with
limited computational resources. Parameter Efficient Fine-Tuning (PEFT) offers
an effective solution by reducing the number of fine-tuning parameters and
memory usage while achieving comparable performance to full fine-tuning. The
demands for fine-tuning PLMs, especially LLMs, have led to a surge in the
development of PEFT methods, as depicted in Fig. 1. In this paper, we present a
comprehensive and systematic review of PEFT methods for PLMs. We summarize
these PEFT methods, discuss their applications, and outline future directions.
Furthermore, we conduct experiments using several representative PEFT methods
to better understand their effectiveness in parameter efficiency and memory
efficiency. By offering insights into the latest advancements and practical
applications, this survey serves as an invaluable resource for researchers and
practitioners seeking to navigate the challenges and opportunities presented by
PEFT in the context of PLMs.

中文翻译:
随着基于Transformer架构的预训练语言模型（PLMs）参数量持续增长，尤其是参数量达数十亿的大型语言模型（LLMs）的出现，许多自然语言处理（NLP）任务取得了显著成功。然而，这些模型的庞大规模和计算需求为适应特定下游任务带来了巨大挑战，尤其在计算资源受限的环境中。参数高效微调技术（PEFT）通过减少微调参数量与内存占用，同时保持与全参数微调相当的性能，为此提供了有效解决方案。如图1所示，针对PLMs（尤其是LLMs）的微调需求已推动PEFT方法呈现爆发式发展。本文对PLMs的PEFT方法进行了全面系统的综述：归纳现有方法、探讨应用场景并展望未来方向。此外，我们通过多种代表性PEFT方法的实验研究，深入评估其在参数效率与内存效率方面的表现。本综述通过呈现最新技术进展与实践应用，为研究人员和从业者应对PLMs领域PEFT技术带来的挑战与机遇提供了重要参考。
