# Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment

链接: http://arxiv.org/abs/2312.12148v1

原文摘要:
With the continuous growth in the number of parameters of transformer-based
pretrained language models (PLMs), particularly the emergence of large language
models (LLMs) with billions of parameters, many natural language processing
(NLP) tasks have demonstrated remarkable success. However, the enormous size
and computational demands of these models pose significant challenges for
adapting them to specific downstream tasks, especially in environments with
limited computational resources. Parameter Efficient Fine-Tuning (PEFT) offers
an effective solution by reducing the number of fine-tuning parameters and
memory usage while achieving comparable performance to full fine-tuning. The
demands for fine-tuning PLMs, especially LLMs, have led to a surge in the
development of PEFT methods, as depicted in Fig. 1. In this paper, we present a
comprehensive and systematic review of PEFT methods for PLMs. We summarize
these PEFT methods, discuss their applications, and outline future directions.
Furthermore, we conduct experiments using several representative PEFT methods
to better understand their effectiveness in parameter efficiency and memory
efficiency. By offering insights into the latest advancements and practical
applications, this survey serves as an invaluable resource for researchers and
practitioners seeking to navigate the challenges and opportunities presented by
PEFT in the context of PLMs.

中文翻译:
随着基于Transformer架构的预训练语言模型(PLMs)参数量持续增长，尤其是涌现出数十亿参数规模的大语言模型(LLMs)，众多自然语言处理(NLP)任务展现出卓越性能。然而这些模型的庞大体量和计算需求，使其在适配特定下游任务时面临巨大挑战，尤其在计算资源受限的环境中。参数高效微调技术(PEFT)通过减少可调参数量与内存占用，同时保持与全参数微调相当的性能，提供了有效解决方案。如图1所示，针对PLMs尤其是LLMs的微调需求，直接推动了PEFT方法的蓬勃发展。本文对PLMs参数高效微调方法进行全面系统的梳理，归纳总结现有PEFT方法，探讨其应用场景并展望未来发展方向。此外，我们选取若干代表性PEFT方法进行实验，以更直观地理解其在参数效率与内存效率方面的表现。本综述通过呈现最新技术进展与实践应用，为研究者与实践者应对PLMs参数高效微调领域的挑战与机遇提供了重要参考。
