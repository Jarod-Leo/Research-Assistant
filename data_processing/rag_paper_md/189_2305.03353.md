# MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic

链接: http://arxiv.org/abs/2305.03353v1

原文摘要:
Theory of Mind (ToM) is a critical component of intelligence but its
assessment remains the subject of heated debates. Prior research applied human
ToM assessments to natural language processing models using either
human-created standardized tests or rule-based templates. However, these
methods primarily focus on simplistic reasoning and require further validation.
Here, we leverage dynamic epistemic logic to isolate a particular component of
ToM and to generate controlled problems. We also introduce new verbalization
techniques to express these problems in English natural language. Our findings
indicate that some language model scaling (from 70M to 6B and 350M to 174B)
does not consistently yield results better than random chance. While GPT-4
demonstrates superior epistemic reasoning capabilities, there is still room for
improvement. Our code and datasets are publicly available
(https://huggingface.co/datasets/sileod/mindgames ,
https://github.com/sileod/llm-theory-of-mind )

中文翻译:
心智理论（Theory of Mind, ToM）是智能的核心组成部分，但其评估方法仍存在激烈争议。现有研究主要通过人工设计的标准化测试或基于规则的模板，将人类ToM评估应用于自然语言处理模型。然而这些方法主要关注简单推理，仍需进一步验证。本研究采用动态认知逻辑分离ToM的特定成分并生成受控问题，同时引入新的语言表达技术将这些问题转化为英语自然语言。实验发现，部分语言模型的规模扩展（从7000万到60亿、3.5亿到1740亿参数）并未产生显著优于随机猜测的结果。尽管GPT-4展现出卓越的认知推理能力，但仍有提升空间。相关代码与数据集已开源（参见https://huggingface.co/datasets/sileod/mindgames 及 https://github.com/sileod/llm-theory-of-mind ）。
