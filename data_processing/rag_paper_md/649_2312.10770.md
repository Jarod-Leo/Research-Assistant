# Identification of Knowledge Neurons in Protein Language Models

链接: http://arxiv.org/abs/2312.10770v1

原文摘要:
Neural language models have become powerful tools for learning complex
representations of entities in natural language processing tasks. However,
their interpretability remains a significant challenge, particularly in domains
like computational biology where trust in model predictions is crucial. In this
work, we aim to enhance the interpretability of protein language models,
specifically the state-of-the-art ESM model, by identifying and characterizing
knowledge neurons - components that express understanding of key information.
After fine-tuning the ESM model for the task of enzyme sequence classification,
we compare two knowledge neuron selection methods that preserve a subset of
neurons from the original model. The two methods, activation-based and
integrated gradient-based selection, consistently outperform a random baseline.
In particular, these methods show that there is a high density of knowledge
neurons in the key vector prediction networks of self-attention modules. Given
that key vectors specialize in understanding different features of input
sequences, these knowledge neurons could capture knowledge of different enzyme
sequence motifs. In the future, the types of knowledge captured by each neuron
could be characterized.

中文翻译:
以下是您提供的英文论文摘要的中文翻译：

神经语言模型已成为自然语言处理任务中学习实体复杂表征的强大工具。然而其可解释性仍存在重大挑战，尤其在计算生物学等需要高度信任模型预测的领域。本研究旨在通过识别和表征知识神经元（表达关键信息理解的组件）来增强蛋白质语言模型（特别是最先进的ESM模型）的可解释性。在对ESM模型进行酶序列分类任务的微调后，我们比较了两种保留原始模型部分神经元的知识神经元选择方法。基于激活度和基于积分梯度的选择方法均持续优于随机基线。特别值得注意的是，这些方法揭示了自注意力模块中关键向量预测网络存在高密度的知识神经元。鉴于关键向量专门用于理解输入序列的不同特征，这些知识神经元可能捕获了不同酶序列模式的知识。未来可进一步表征每个神经元所捕获的知识类型。

（翻译说明：根据学术论文摘要的文体特征，译文采用简洁规范的科技汉语表达，专业术语如"self-attention modules"译为"自注意力模块"保持领域一致性；长难句如"methods show that..."通过拆分重组为中文惯用的流水句；被动语态"could be characterized"转化为主动式"可进一步表征"以符合中文表达习惯；关键概念"knowledge neurons"首次出现时添加括号注释确保概念清晰。）
