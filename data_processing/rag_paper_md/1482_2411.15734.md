# Development of Pre-Trained Transformer-based Models for the Nepali Language

链接: http://arxiv.org/abs/2411.15734v1

原文摘要:
Transformer-based pre-trained language models have dominated the field of
Natural Language Processing (NLP) for quite some time now. However, the Nepali
language, spoken by approximately 32 million people worldwide, remains
significantly underrepresented in this domain. This underrepresentation is
primarily attributed to the scarcity of monolingual data corpora and limited
available resources for the Nepali language. While existing efforts have
predominantly concentrated on basic encoder-based models, there is a notable
gap in the exploration of decoder-based architectures. To address this gap, we
have collected 27.5 GB of Nepali text data, approximately 2.4x larger than any
previously available Nepali language corpus. Leveraging this data, we
pre-trained three different models i.e., BERT, RoBERTa, and GPT-2, exclusively
for the Nepali Language. Furthermore, we performed instruction tuning and
explored its potential for monolingual Nepali data, providing a foundation for
future research. Our models outperformed the existing best model by 2 points on
Nep-gLUE benchmark, scoring 95.60 and also outperformed existing models on text
generation tasks, demonstrating improvements in both understanding and
generating Nepali text.

中文翻译:
基于Transformer的预训练语言模型在自然语言处理（NLP）领域占据主导地位已有时日。然而，全球约3200万人使用的尼泊尔语在该领域仍存在显著代表性不足的问题。这种不足主要归因于尼泊尔语单语语料库的匮乏及可用资源的有限性。现有研究多集中于基于编码器的基础模型，而在基于解码器的架构探索方面存在明显空白。为填补这一空白，我们收集了27.5GB尼泊尔语文本数据（规模约为现有最大尼泊尔语料库的2.4倍），并基于该数据预训练了BERT、RoBERTa和GPT-2三种专用于尼泊尔语的模型。此外，我们进行了指令微调并探索了其在单语尼泊尔数据中的应用潜力，为未来研究奠定基础。我们的模型在Nep-gLUE基准测试中以95.60分超越现有最佳模型2个百分点，同时在文本生成任务中表现优异，展现了在尼泊尔语文本理解与生成方面的双重提升。
