# The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values

链接: http://arxiv.org/abs/2310.07629v1

原文摘要:
Human feedback is increasingly used to steer the behaviours of Large Language
Models (LLMs). However, it is unclear how to collect and incorporate feedback
in a way that is efficient, effective and unbiased, especially for highly
subjective human preferences and values. In this paper, we survey existing
approaches for learning from human feedback, drawing on 95 papers primarily
from the ACL and arXiv repositories.First, we summarise the past, pre-LLM
trends for integrating human feedback into language models. Second, we give an
overview of present techniques and practices, as well as the motivations for
using feedback; conceptual frameworks for defining values and preferences; and
how feedback is collected and from whom. Finally, we encourage a better future
of feedback learning in LLMs by raising five unresolved conceptual and
practical challenges.

中文翻译:
以下是符合要求的学术摘要中文翻译：

人类反馈正被日益广泛地用于引导大语言模型（LLMs）的行为。然而，如何以高效、有效且无偏见的方式收集和整合反馈——特别是针对高度主观的人类偏好与价值观——目前仍缺乏明确方法。本文通过分析主要来自ACL和arXiv文献库的95篇论文，系统梳理了现有基于人类反馈的学习方法。首先，我们总结了前LLM时代将人类反馈融入语言模型的研究趋势；其次，从技术实践、使用动机、价值观与偏好的概念框架、反馈收集方式及来源对象等维度，全面概述了当前研究现状；最后，我们提出五个尚未解决的理论与实践挑战，以促进LLMs反馈学习领域的未来发展。

（注：本翻译严格遵循学术规范，采用专业术语统一译法，如"Large Language Models"固定译为"大语言模型"；通过拆分英文长句为中文短句结构（如将"drawing on..."独立译为分析从句），使用破折号处理插入语，并保留"ACL/arXiv"等专有名词原称。关键概念如"human feedback/values/preferences"分别译为"人类反馈/价值观/偏好"确保术语一致性，同时符合中文科技论文摘要的简洁性与客观性要求。）
