# The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values

链接: http://arxiv.org/abs/2310.07629v1

原文摘要:
Human feedback is increasingly used to steer the behaviours of Large Language
Models (LLMs). However, it is unclear how to collect and incorporate feedback
in a way that is efficient, effective and unbiased, especially for highly
subjective human preferences and values. In this paper, we survey existing
approaches for learning from human feedback, drawing on 95 papers primarily
from the ACL and arXiv repositories.First, we summarise the past, pre-LLM
trends for integrating human feedback into language models. Second, we give an
overview of present techniques and practices, as well as the motivations for
using feedback; conceptual frameworks for defining values and preferences; and
how feedback is collected and from whom. Finally, we encourage a better future
of feedback learning in LLMs by raising five unresolved conceptual and
practical challenges.

中文翻译:
人类反馈正日益被用于引导大语言模型（LLM）的行为。然而，如何高效、有效且无偏见地收集并整合反馈——尤其是针对高度主观的人类偏好与价值观——仍缺乏明确方法。本文基于ACL和arXiv文献库中的95篇论文，系统梳理了现有从人类反馈中学习的研究路径。首先，我们总结了前LLM时代将人类反馈融入语言模型的演进脉络；其次，从技术实践、应用动机、价值观定义框架、反馈收集方式及来源对象等维度，全面剖析了当前研究现状；最后，通过提出五个未解决的理论与实践挑战，为构建更完善的LLM反馈学习体系提供了前瞻性思考。
