# Comprehensive Study on German Language Models for Clinical and Biomedical Text Understanding

链接: http://arxiv.org/abs/2404.05694v1

原文摘要:
Recent advances in natural language processing (NLP) can be largely
attributed to the advent of pre-trained language models such as BERT and
RoBERTa. While these models demonstrate remarkable performance on general
datasets, they can struggle in specialized domains such as medicine, where
unique domain-specific terminologies, domain-specific abbreviations, and
varying document structures are common. This paper explores strategies for
adapting these models to domain-specific requirements, primarily through
continuous pre-training on domain-specific data. We pre-trained several German
medical language models on 2.4B tokens derived from translated public English
medical data and 3B tokens of German clinical data. The resulting models were
evaluated on various German downstream tasks, including named entity
recognition (NER), multi-label classification, and extractive question
answering. Our results suggest that models augmented by clinical and
translation-based pre-training typically outperform general domain models in
medical contexts. We conclude that continuous pre-training has demonstrated the
ability to match or even exceed the performance of clinical models trained from
scratch. Furthermore, pre-training on clinical data or leveraging translated
texts have proven to be reliable methods for domain adaptation in medical NLP
tasks.

中文翻译:
自然语言处理（NLP）领域的最新进展很大程度上归功于BERT、RoBERTa等预训练语言模型的出现。尽管这些模型在通用数据集上表现卓越，但在医学等专业领域却可能面临挑战——这些领域普遍存在特有的专业术语、领域缩写及多样化的文档结构。本文探讨了通过领域数据持续预训练等策略，使模型适应专业领域需求的方法。我们基于24亿标记的翻译英文公开医学数据和30亿标记的德文临床数据，预训练了多个德文医学语言模型，并在命名实体识别（NER）、多标签分类和抽取式问答等德文下游任务中进行了评估。实验表明，经过临床数据和翻译数据增强的模型在医学场景中通常优于通用领域模型。研究证实：持续预训练能够达到甚至超越从头训练的临床模型性能；而基于临床数据的预训练或利用翻译文本，均是医学NLP任务中领域适应的有效方法。
