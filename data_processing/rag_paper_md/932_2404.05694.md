# Comprehensive Study on German Language Models for Clinical and Biomedical Text Understanding

链接: http://arxiv.org/abs/2404.05694v1

原文摘要:
Recent advances in natural language processing (NLP) can be largely
attributed to the advent of pre-trained language models such as BERT and
RoBERTa. While these models demonstrate remarkable performance on general
datasets, they can struggle in specialized domains such as medicine, where
unique domain-specific terminologies, domain-specific abbreviations, and
varying document structures are common. This paper explores strategies for
adapting these models to domain-specific requirements, primarily through
continuous pre-training on domain-specific data. We pre-trained several German
medical language models on 2.4B tokens derived from translated public English
medical data and 3B tokens of German clinical data. The resulting models were
evaluated on various German downstream tasks, including named entity
recognition (NER), multi-label classification, and extractive question
answering. Our results suggest that models augmented by clinical and
translation-based pre-training typically outperform general domain models in
medical contexts. We conclude that continuous pre-training has demonstrated the
ability to match or even exceed the performance of clinical models trained from
scratch. Furthermore, pre-training on clinical data or leveraging translated
texts have proven to be reliable methods for domain adaptation in medical NLP
tasks.

中文翻译:
近年来，自然语言处理（NLP）领域的重大进展主要归功于BERT、RoBERTa等预训练语言模型的出现。尽管这些模型在通用数据集上表现卓越，但在医学等专业领域却可能表现欠佳——这类领域通常存在独特的专业术语、领域特定缩写以及多样化的文档结构。本文探讨了通过领域数据持续预训练等方式，使通用模型适应专业领域需求的具体策略。我们基于24亿词符的英译德公开医学数据和30亿词符的德语临床数据，对多个德语医学语言模型进行了预训练，并在命名实体识别（NER）、多标签分类和抽取式问答等德语下游任务上评估了模型性能。实验结果表明：经过临床数据和翻译数据增强的预训练模型，在医学语境下通常优于通用领域模型。研究证实，持续预训练策略能够达到甚至超越从头训练的临床专用模型性能。无论是采用临床数据预训练还是利用翻译文本，都被证明是医学NLP任务中领域适应的有效方法。
