# A Hybrid Transformer Architecture with a Quantized Self-Attention Mechanism Applied to Molecular Generation

链接: http://arxiv.org/abs/2502.19214v1

原文摘要:
The success of the self-attention mechanism in classical machine learning
models has inspired the development of quantum analogs aimed at reducing
computational overhead. Self-attention integrates learnable query and key
matrices to calculate attention scores between all pairs of tokens in a
sequence. These scores are then multiplied by a learnable value matrix to
obtain the output self-attention matrix, enabling the model to effectively
capture long-range dependencies within the input sequence. Here, we propose a
hybrid quantum-classical self-attention mechanism as part of a transformer
decoder, the architecture underlying large language models (LLMs). To
demonstrate its utility in chemistry, we train this model on the QM9 dataset
for conditional generation, using SMILES strings as input, each labeled with a
set of physicochemical properties that serve as conditions during inference.
Our theoretical analysis shows that the time complexity of the query-key dot
product is reduced from $\mathcal{O}(n^2 d)$ in a classical model to
$\mathcal{O}(n^2\log d)$ in our quantum model, where $n$ and $d$ represent the
sequence length and embedding dimension, respectively. We perform simulations
using NVIDIA's CUDA-Q platform, which is designed for efficient GPU
scalability. This work provides a promising avenue for quantum-enhanced natural
language processing (NLP).

中文翻译:
经典机器学习模型中自注意力机制的成功，激发了旨在降低计算开销的量子类比方法的发展。自注意力机制通过可学习的查询矩阵和键矩阵计算序列中所有标记对之间的注意力分数，再将这些分数与可学习的值矩阵相乘，最终输出自注意力矩阵，使模型能有效捕捉输入序列中的长程依赖关系。本文提出了一种混合量子-经典自注意力机制，将其作为大语言模型（LLMs）基础架构——Transformer解码器的组成部分。为验证该机制在化学领域的实用性，我们在QM9数据集上以SMILES字符串为输入进行条件生成训练，每个字符串均标注了在推理时作为条件的物理化学性质集。理论分析表明，查询-键点积运算的时间复杂度从经典模型的$\mathcal{O}(n^2 d)$降至量子模型的$\mathcal{O}(n^2\log d)$，其中$n$和$d$分别表示序列长度和嵌入维度。我们采用专为高效GPU扩展设计的NVIDIA CUDA-Q平台进行模拟实验，这项工作为量子增强的自然语言处理（NLP）提供了可行路径。
