# Transformers are efficient hierarchical chemical graph learners

链接: http://arxiv.org/abs/2310.01704v1

原文摘要:
Transformers, adapted from natural language processing, are emerging as a
leading approach for graph representation learning. Contemporary graph
transformers often treat nodes or edges as separate tokens. This approach leads
to computational challenges for even moderately-sized graphs due to the
quadratic scaling of self-attention complexity with token count. In this paper,
we introduce SubFormer, a graph transformer that operates on subgraphs that
aggregate information by a message-passing mechanism. This approach reduces the
number of tokens and enhances learning long-range interactions. We demonstrate
SubFormer on benchmarks for predicting molecular properties from chemical
structures and show that it is competitive with state-of-the-art graph
transformers at a fraction of the computational cost, with training times on
the order of minutes on a consumer-grade graphics card. We interpret the
attention weights in terms of chemical structures. We show that SubFormer
exhibits limited over-smoothing and avoids over-squashing, which is prevalent
in traditional graph neural networks.

中文翻译:
源自自然语言处理领域的Transformer模型，正逐渐成为图表示学习的主流方法。当前图Transformer通常将节点或边视为独立标记单元，这种处理方式会因自注意力机制复杂度随标记数量呈平方级增长，导致即使中等规模图数据也会面临巨大计算挑战。本文提出的SubFormer创新性地采用基于消息传递机制聚合信息的子图作为处理单元，通过减少标记数量有效提升了长程交互学习能力。我们在化学结构分子性质预测基准测试中验证了该模型，结果表明SubFormer仅需消费级显卡数分钟的训练时长，就能达到当前最优图Transformer模型的性能水平。通过分析注意力权重与化学结构的对应关系，我们发现SubFormer能有效缓解传统图神经网络普遍存在的过度平滑现象，同时避免了信息过度压缩问题。
