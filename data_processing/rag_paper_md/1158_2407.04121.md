# Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models

链接: http://arxiv.org/abs/2407.04121v1

原文摘要:
Large Language Models (LLMs) have gained widespread adoption in various
natural language processing tasks, including question answering and dialogue
systems. However, a major drawback of LLMs is the issue of hallucination, where
they generate unfaithful or inconsistent content that deviates from the input
source, leading to severe consequences. In this paper, we propose a robust
discriminator named RelD to effectively detect hallucination in LLMs' generated
answers. RelD is trained on the constructed RelQA, a bilingual
question-answering dialogue dataset along with answers generated by LLMs and a
comprehensive set of metrics. Our experimental results demonstrate that the
proposed RelD successfully detects hallucination in the answers generated by
diverse LLMs. Moreover, it performs well in distinguishing hallucination in
LLMs' generated answers from both in-distribution and out-of-distribution
datasets. Additionally, we also conduct a thorough analysis of the types of
hallucinations that occur and present valuable insights. This research
significantly contributes to the detection of reliable answers generated by
LLMs and holds noteworthy implications for mitigating hallucination in the
future work.

中文翻译:
大型语言模型（LLMs）已在问答系统、对话系统等多种自然语言处理任务中得到广泛应用。然而，这类模型存在一个显著缺陷——幻觉问题，即生成与输入源不符或自相矛盾的内容，可能引发严重后果。本文提出名为RelD的鲁棒判别器，可有效检测LLM生成答案中的幻觉现象。该模型基于我们构建的双语问答对话数据集RelQA进行训练，该数据集包含LLM生成的答案及一套综合性评估指标。实验结果表明，RelD能成功识别不同LLM生成答案中的幻觉内容，且在分布内和分布外数据集上均表现出优异的幻觉区分能力。此外，我们还对幻觉类型进行了系统分析，并提出了具有实践价值的见解。本研究对提升LLM生成答案的可靠性检测具有重要意义，并为未来工作中缓解幻觉问题提供了重要启示。
