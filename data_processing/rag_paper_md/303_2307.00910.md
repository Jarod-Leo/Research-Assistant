# Contextual Prompt Learning for Vision-Language Understanding

链接: http://arxiv.org/abs/2307.00910v1

原文摘要:
Recent advances in multimodal learning has resulted in powerful
vision-language models, whose representations are generalizable across a
variety of downstream tasks. Recently, their generalization ability has been
further extended by incorporating trainable prompts, borrowed from the natural
language processing literature. While such prompt learning techniques have
shown impressive results, we identify that these prompts are trained based on
global image features which limits itself in two aspects: First, by using
global features, these prompts could be focusing less on the discriminative
foreground image, resulting in poor generalization to various
out-of-distribution test cases. Second, existing work weights all prompts
equally whereas intuitively, prompts should be reweighed according to the
semantics of the image. We address these as part of our proposed Contextual
Prompt Learning (CoPL) framework, capable of aligning the prompts to the
localized features of the image. Our key innovations over earlier works include
using local image features as part of the prompt learning process, and more
crucially, learning to weight these prompts based on local features that are
appropriate for the task at hand. This gives us dynamic prompts that are both
aligned to local image features as well as aware of local contextual
relationships. Our extensive set of experiments on a variety of standard and
few-shot datasets show that our method produces substantially improved
performance when compared to the current state of the art methods. We also
demonstrate both few-shot and out-of-distribution performance to establish the
utility of learning dynamic prompts that are aligned to local image features.

中文翻译:
多模态学习的最新进展催生了强大的视觉-语言模型，其表征能力可泛化至多种下游任务。近期研究通过引入可训练提示词（借鉴自自然语言处理领域），进一步扩展了这类模型的泛化能力。尽管现有提示学习技术已展现出卓越效果，但我们发现这些提示词仅基于全局图像特征进行训练，存在两大局限：其一，依赖全局特征可能导致提示词忽略判别性前景信息，从而削弱模型在分布外测试场景的泛化性能；其二，现有方法对所有提示词等权重处理，而直觉上应根据图像语义动态调整提示词权重。针对这些问题，我们提出上下文提示学习框架（CoPL），通过将提示词与图像局部特征对齐实现优化。相较于前人工作，本研究的核心创新在于：将局部图像特征纳入提示学习过程，更重要的是，基于任务相关局部特征学习动态权重分配机制。由此生成的动态提示词既能与局部图像特征对齐，又能感知局部上下文关联。我们在多个标准数据集和小样本数据集上的大量实验表明，该方法相较当前最优技术实现了显著性能提升。通过小样本学习和分布外泛化测试，我们验证了局部特征对齐的动态提示词学习机制的有效性。
