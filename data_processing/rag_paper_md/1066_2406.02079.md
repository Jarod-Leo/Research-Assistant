# Assessing the Performance of Chinese Open Source Large Language Models in Information Extraction Tasks

链接: http://arxiv.org/abs/2406.02079v1

原文摘要:
Information Extraction (IE) plays a crucial role in Natural Language
Processing (NLP) by extracting structured information from unstructured text,
thereby facilitating seamless integration with various real-world applications
that rely on structured data. Despite its significance, recent experiments
focusing on English IE tasks have shed light on the challenges faced by Large
Language Models (LLMs) in achieving optimal performance, particularly in
sub-tasks like Named Entity Recognition (NER). In this paper, we delve into a
comprehensive investigation of the performance of mainstream Chinese
open-source LLMs in tackling IE tasks, specifically under zero-shot conditions
where the models are not fine-tuned for specific tasks. Additionally, we
present the outcomes of several few-shot experiments to further gauge the
capability of these models. Moreover, our study includes a comparative analysis
between these open-source LLMs and ChatGPT, a widely recognized language model,
on IE performance. Through meticulous experimentation and analysis, we aim to
provide insights into the strengths, limitations, and potential enhancements of
existing Chinese open-source LLMs in the domain of Information Extraction
within the context of NLP.

中文翻译:
信息抽取（IE）作为自然语言处理（NLP）的核心技术，通过从非结构化文本中提取结构化信息，为依赖结构化数据的现实应用提供了无缝衔接的桥梁。尽管其重要性不言而喻，但近期针对英语IE任务的实验揭示了大语言模型（LLMs）在实现最优性能时面临的挑战，特别是在命名实体识别（NER）等子任务中。本文深入探究了主流中文开源大语言模型在零样本条件下（即未经特定任务微调时）处理IE任务的综合表现，并通过若干少样本实验进一步评估这些模型的潜力。此外，研究还对比分析了这些开源模型与业界公认的ChatGPT在信息抽取性能上的差异。通过严谨的实验设计与分析，我们旨在揭示现有中文开源大语言模型在NLP信息抽取领域的优势、局限性及可能的改进方向。
