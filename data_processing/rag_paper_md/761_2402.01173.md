# Efficient Prompt Caching via Embedding Similarity

链接: http://arxiv.org/abs/2402.01173v1

原文摘要:
Large language models (LLMs) have achieved huge success in numerous natural
language process (NLP) tasks. However, it faces the challenge of significant
resource consumption during inference. In this paper, we aim to improve the
inference efficiency of LLMs by prompt caching, i.e., if the current prompt can
be answered by the same response of a previous prompt, one can directly utilize
that previous response without calling the LLM. Specifically, we focus on the
prediction accuracy of prompt caching for single-round question-answering tasks
via embedding similarity. The existing embeddings of prompts mostly focus on
whether two prompts are semantically similar, which is not necessarily
equivalent to whether the same response can answer them. Therefore, we propose
a distillation-based method to fine-tune the existing embeddings for better
caching prediction. Theoretically, we provide finite-sample guarantees for the
convergence of our method under different types of loss functions. Empirically,
we carefully construct a hard dataset based on Kwiatkowski et al. (2019) where
the existing embedding model (Wang et al., 2022) only achieves an AUC of 0.51.
We then fine-tune the above embedding model, which significantly improves the
AUC of caching prediction from 0.51 to 0.81. We also conduct simulations
demonstrating that our trained models achieve better caching efficiency than
the previous embedding model.

中文翻译:
大型语言模型（LLMs）在众多自然语言处理（NLP）任务中取得了巨大成功，但其推理过程中面临显著的资源消耗挑战。本文旨在通过提示缓存技术提升LLMs的推理效率——若当前提示可由先前某提示的相同响应回答，则无需调用LLM直接复用该响应。具体而言，我们聚焦于基于嵌入相似度的单轮问答任务提示缓存预测准确性。现有提示嵌入多关注语义相似性，但这与"能否用相同响应回答"并不等价。为此，我们提出基于蒸馏的方法对现有嵌入进行微调以优化缓存预测。理论上，我们为不同损失函数下方法的收敛性提供了有限样本保证。实证方面，我们基于Kwiatkowski等人（2019）构建了一个高难度数据集（现有嵌入模型AUC仅0.51），经微调后将缓存预测AUC从0.51显著提升至0.81。模拟实验进一步表明，经训练的模型比原嵌入模型实现了更优的缓存效率。
