# Efficient Prompt Caching via Embedding Similarity

链接: http://arxiv.org/abs/2402.01173v1

原文摘要:
Large language models (LLMs) have achieved huge success in numerous natural
language process (NLP) tasks. However, it faces the challenge of significant
resource consumption during inference. In this paper, we aim to improve the
inference efficiency of LLMs by prompt caching, i.e., if the current prompt can
be answered by the same response of a previous prompt, one can directly utilize
that previous response without calling the LLM. Specifically, we focus on the
prediction accuracy of prompt caching for single-round question-answering tasks
via embedding similarity. The existing embeddings of prompts mostly focus on
whether two prompts are semantically similar, which is not necessarily
equivalent to whether the same response can answer them. Therefore, we propose
a distillation-based method to fine-tune the existing embeddings for better
caching prediction. Theoretically, we provide finite-sample guarantees for the
convergence of our method under different types of loss functions. Empirically,
we carefully construct a hard dataset based on Kwiatkowski et al. (2019) where
the existing embedding model (Wang et al., 2022) only achieves an AUC of 0.51.
We then fine-tune the above embedding model, which significantly improves the
AUC of caching prediction from 0.51 to 0.81. We also conduct simulations
demonstrating that our trained models achieve better caching efficiency than
the previous embedding model.

中文翻译:
以下是符合要求的学术中文翻译：

大语言模型（LLMs）在众多自然语言处理（NLP）任务中取得了显著成功，但其推理过程面临资源消耗巨大的挑战。本文旨在通过提示缓存技术提升LLMs的推理效率——若当前提示可通过历史提示的相同答案响应，则无需调用LLM即可直接复用历史响应。具体而言，我们聚焦于基于嵌入相似度的单轮问答任务中提示缓存的预测准确性。现有提示嵌入方法主要关注语义相似性判断，但这与"是否适用相同响应"并不等价。为此，我们提出基于蒸馏的方法对现有嵌入进行微调以优化缓存预测。理论上，我们为不同损失函数下该方法的收敛性提供了有限样本保证。实证方面，我们基于Kwiatkowski等人（2019）构建了一个高难度数据集（现有嵌入模型Wang等人（2022）仅获得0.51的AUC值），经微调后将缓存预测AUC从0.51显著提升至0.81。模拟实验进一步表明，经训练的模型比原嵌入模型具有更优的缓存效率。

（翻译严格遵循以下原则：
1. 专业术语统一（如LLMs/NLP保持中文全称+括号标注英文缩写）
2. 被动语态转换（"it faces"→"面临"）
3. 长句拆分重组（将原文复合句分解为符合中文表达习惯的短句）
4. 学术用语准确（"finite-sample guarantees"→"有限样本保证"）
5. 数据呈现规范（AUC值/引用格式严格对应原文）
6. 逻辑连接清晰（"Specifically"→"具体而言"等衔接词处理））
