# Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models

链接: http://arxiv.org/abs/2312.07887v1

原文摘要:
Incremental Learning (IL) has been a long-standing problem in both vision and
Natural Language Processing (NLP) communities. In recent years, as Pre-trained
Language Models (PLMs) have achieved remarkable progress in various NLP
downstream tasks, utilizing PLMs as backbones has become a common practice in
recent research of IL in NLP. Most assume that catastrophic forgetting is the
biggest obstacle to achieving superior IL performance and propose various
techniques to overcome this issue. However, we find that this assumption is
problematic. Specifically, we revisit more than 20 methods on four
classification tasks (Text Classification, Intent Classification, Relation
Extraction, and Named Entity Recognition) under the two most popular IL
settings (Class-Incremental and Task-Incremental) and reveal that most of them
severely underestimate the inherent anti-forgetting ability of PLMs. Based on
the observation, we propose a frustratingly easy method called SEQ* for IL with
PLMs. The results show that SEQ* has competitive or superior performance
compared to state-of-the-art (SOTA) IL methods and requires considerably less
trainable parameters and training time. These findings urge us to revisit the
IL with PLMs and encourage future studies to have a fundamental understanding
of the catastrophic forgetting in PLMs. The data, code and scripts are publicly
available at
https://github.com/zzz47zzz/codebase-for-incremental-learning-with-llm.

中文翻译:
增量学习（Incremental Learning, IL）长期以来一直是视觉与自然语言处理（NLP）领域的重要课题。近年来，随着预训练语言模型（Pre-trained Language Models, PLMs）在各种NLP下游任务中取得显著进展，采用PLMs作为模型主干已成为NLP增量学习研究的主流做法。多数研究认为灾难性遗忘是阻碍IL性能提升的首要障碍，并提出了多种技术来缓解该问题。然而，我们发现这一假设存在根本性缺陷。

通过系统评估20余种方法在文本分类、意图识别、关系抽取和命名实体识别四类任务上的表现（涵盖类增量与任务增量两种主流IL场景），我们揭示出当前方法严重低估了PLMs与生俱来的抗遗忘能力。基于这一发现，我们提出了一种极其简单的PLMs增量学习方法SEQ*。实验表明，该方法在性能上媲美甚至超越现有最优IL方案，同时所需可训练参数量和训练时间显著降低。这些发现促使我们重新审视PLMs的增量学习机制，并呼吁未来研究应更深入地理解PLMs中的灾难性遗忘本质。相关数据、代码及脚本已开源发布于https://github.com/zzz47zzz/codebase-for-incremental-learning-with-llm。
