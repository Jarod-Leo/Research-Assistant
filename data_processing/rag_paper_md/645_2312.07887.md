# Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models

链接: http://arxiv.org/abs/2312.07887v1

原文摘要:
Incremental Learning (IL) has been a long-standing problem in both vision and
Natural Language Processing (NLP) communities. In recent years, as Pre-trained
Language Models (PLMs) have achieved remarkable progress in various NLP
downstream tasks, utilizing PLMs as backbones has become a common practice in
recent research of IL in NLP. Most assume that catastrophic forgetting is the
biggest obstacle to achieving superior IL performance and propose various
techniques to overcome this issue. However, we find that this assumption is
problematic. Specifically, we revisit more than 20 methods on four
classification tasks (Text Classification, Intent Classification, Relation
Extraction, and Named Entity Recognition) under the two most popular IL
settings (Class-Incremental and Task-Incremental) and reveal that most of them
severely underestimate the inherent anti-forgetting ability of PLMs. Based on
the observation, we propose a frustratingly easy method called SEQ* for IL with
PLMs. The results show that SEQ* has competitive or superior performance
compared to state-of-the-art (SOTA) IL methods and requires considerably less
trainable parameters and training time. These findings urge us to revisit the
IL with PLMs and encourage future studies to have a fundamental understanding
of the catastrophic forgetting in PLMs. The data, code and scripts are publicly
available at
https://github.com/zzz47zzz/codebase-for-incremental-learning-with-llm.

中文翻译:
以下是符合您要求的中文翻译：

增量学习（Incremental Learning, IL）长期以来一直是计算机视觉和自然语言处理（NLP）领域的关键难题。近年来，随着预训练语言模型（PLMs）在各种NLP下游任务中取得显著进展，采用PLMs作为主干网络已成为NLP增量学习研究的主流做法。现有研究大多将灾难性遗忘视为阻碍IL性能提升的首要障碍，并提出了多种应对技术。然而，我们发现这一前提假设存在根本性问题。

具体而言，我们通过在文本分类、意图识别、关系抽取和命名实体识别四大分类任务上，对两种主流IL设置（类增量与任务增量）下的20余种方法进行系统性重评估，发现现有方法严重低估了PLMs与生俱来的抗遗忘能力。基于这一发现，我们提出了一种极其简单的PLM增量学习方法SEQ*。实验表明：SEQ*在性能上媲美甚至超越当前最先进的IL方法，同时所需可训练参数量和训练时间显著减少。这些发现促使我们重新审视基于PLMs的增量学习，并呼吁未来研究应深入理解PLMs中的灾难性遗忘本质。相关数据、代码及脚本已开源：https://github.com/zzz47zzz/codebase-for-incremental-learning-with-llm。

（注：根据学术规范要求，专业术语首次出现时保留英文缩写，如"Incremental Learning (IL)"译为"增量学习（Incremental Learning, IL）"；技术术语如"Class-Incremental"采用领域通用译法"类增量"；长难句按中文表达习惯拆分重组；保持被动语态与主动语态的合理转换）
