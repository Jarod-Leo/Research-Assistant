# Towards Pareto Optimal Throughput in Small Language Model Serving

链接: http://arxiv.org/abs/2404.03353v1

原文摘要:
Large language models (LLMs) have revolutionized the state-of-the-art of many
different natural language processing tasks. Although serving LLMs is
computationally and memory demanding, the rise of Small Language Models (SLMs)
offers new opportunities for resource-constrained users, who now are able to
serve small models with cutting-edge performance. In this paper, we present a
set of experiments designed to benchmark SLM inference at performance and
energy levels. Our analysis provides a new perspective in serving, highlighting
that the small memory footprint of SLMs allows for reaching the Pareto-optimal
throughput within the resource capacity of a single accelerator. In this
regard, we present an initial set of findings demonstrating how model
replication can effectively improve resource utilization for serving SLMs.

中文翻译:
大型语言模型（LLMs）已彻底革新了众多自然语言处理任务的技术前沿。尽管运行LLM对计算和内存资源要求极高，但小型语言模型（SLMs）的兴起为资源受限用户提供了新机遇——他们如今也能部署具备尖端性能的小型模型。本文通过一系列实验对SLM推理在性能和能耗层面进行基准测试，分析结果为模型服务提供了新视角：SLMs的小内存占用使其能在单加速器资源限制内实现帕累托最优吞吐量。基于此，我们展示了初步研究成果，证明模型复制策略可有效提升SLM服务的资源利用率。
