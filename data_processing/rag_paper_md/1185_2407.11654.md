# R-SFLLM: Jamming Resilient Framework for Split Federated Learning with Large Language Models

链接: http://arxiv.org/abs/2407.11654v1

原文摘要:
Split federated learning (SFL) is a compute-efficient paradigm in distributed
machine learning (ML), where components of large ML models are outsourced to
remote servers. A significant challenge in SFL, particularly when deployed over
wireless channels, is the susceptibility of transmitted model parameters to
adversarial jamming that could jeopardize the learning process. This is
particularly pronounced for word embedding parameters in large language models
(LLMs), which are crucial for language understanding. In this paper, rigorous
insights are provided into the influence of jamming LLM word embeddings in SFL
by deriving an expression for the ML training loss divergence and showing that
it is upper-bounded by the mean squared error (MSE). Based on this analysis, a
physical layer framework is developed for resilient SFL with LLMs (R-SFLLM)
over wireless networks. R-SFLLM leverages wireless sensing data to gather
information on the jamming directions-of-arrival (DoAs) for the purpose of
devising a novel, sensing-assisted anti-jamming strategy while jointly
optimizing beamforming, user scheduling, and resource allocation. Extensive
experiments using BERT and RoBERTa models demonstrate R-SFLLM's effectiveness,
achieving close-to-baseline performance across various natural language
processing (NLP) tasks and datasets. The proposed methodology further
introduces an adversarial training component, where controlled noise exposure
significantly enhances the LLM's resilience to perturbed parameters during
training. The results show that more noise-sensitive models, such as RoBERTa,
benefit from this feature, especially when resource allocation is unfair. It is
also shown that worst-case jamming in particular translates into worst-case
model outcomes, thereby necessitating the need for jamming-resilient SFL
protocols.

中文翻译:
分割联邦学习（Split Federated Learning, SFL）作为分布式机器学习（ML）中的一种计算高效范式，其核心在于将大型ML模型的部分组件外包至远程服务器执行。然而，SFL在无线信道部署时面临的关键挑战在于：传输的模型参数极易受到对抗性干扰攻击，可能危及整个学习过程。这一现象在大型语言模型（LLMs）的词嵌入参数上尤为突出，因其对语言理解具有决定性作用。本文通过推导ML训练损失散度的数学表达式，并证明其上限受均方误差（MSE）约束，首次为干扰对SFL中LLM词嵌入的影响提供了严格的理论洞察。

基于此分析，我们开发了面向无线网络的抗干扰LLM分割联邦学习物理层框架（R-SFLLM）。该框架创新性地利用无线传感数据获取干扰到达方向（DoAs）信息，设计出传感辅助的抗干扰策略，同时联合优化波束成形、用户调度和资源分配。通过BERT和RoBERTa模型的广泛实验表明，R-SFLLM在多种自然语言处理（NLP）任务和数据集上均能接近基线性能。本方法还引入了对抗训练组件——通过受控噪声暴露显著增强LLM对训练期间参数扰动的鲁棒性。结果显示，RoBERTa等对噪声敏感的模型尤其受益于此特性，特别是在资源分配不公的情况下。研究同时证实，最恶劣的干扰场景直接导致最差的模型表现，这凸显了开发抗干扰SFL协议的迫切必要性。
