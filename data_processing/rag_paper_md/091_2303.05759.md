# An Overview on Language Models: Recent Developments and Outlook

链接: http://arxiv.org/abs/2303.05759v1

原文摘要:
Language modeling studies the probability distributions over strings of
texts. It is one of the most fundamental tasks in natural language processing
(NLP). It has been widely used in text generation, speech recognition, machine
translation, etc. Conventional language models (CLMs) aim to predict the
probability of linguistic sequences in a causal manner, while pre-trained
language models (PLMs) cover broader concepts and can be used in both causal
sequential modeling and fine-tuning for downstream applications. PLMs have
their own training paradigms (usually self-supervised) and serve as foundation
models in modern NLP systems. This overview paper provides an introduction to
both CLMs and PLMs from five aspects, i.e., linguistic units, architectures,
training methods, evaluation methods, and applications. Furthermore, we discuss
the relationship between CLMs and PLMs and shed light on the future directions
of language modeling in the pre-trained era.

中文翻译:
语言建模研究文本序列的概率分布，是自然语言处理（NLP）中最基础的任务之一，被广泛应用于文本生成、语音识别、机器翻译等领域。传统语言模型（CLM）以因果方式预测语言序列概率，而预训练语言模型（PLM）涵盖更广泛概念，既可用于因果序列建模，也可通过微调应用于下游任务。PLM具有独特的训练范式（通常采用自监督学习），已成为现代NLP系统的基础模型。本文从语言单元、模型架构、训练方法、评估方法和应用场景五个维度，系统梳理了CLM与PLM的研究进展，阐明两者间的关联关系，并对预训练时代语言建模的未来发展方向进行了展望。
