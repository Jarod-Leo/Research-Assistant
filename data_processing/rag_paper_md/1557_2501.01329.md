# The Prompt Alchemist: Automated LLM-Tailored Prompt Optimization for Test Case Generation

链接: http://arxiv.org/abs/2501.01329v1

原文摘要:
Test cases are essential for validating the reliability and quality of
software applications. Recent studies have demonstrated the capability of Large
Language Models (LLMs) to generate useful test cases for given source code.
However, the existing work primarily relies on human-written plain prompts,
which often leads to suboptimal results since the performance of LLMs can be
highly influenced by the prompts. Moreover, these approaches use the same
prompt for all LLMs, overlooking the fact that different LLMs might be best
suited to different prompts. Given the wide variety of possible prompt
formulations, automatically discovering the optimal prompt for each LLM
presents a significant challenge. Although there are methods on automated
prompt optimization in the natural language processing field, they are hard to
produce effective prompts for the test case generation task. First, the methods
iteratively optimize prompts by simply combining and mutating existing ones
without proper guidance, resulting in prompts that lack diversity and tend to
repeat the same errors in the generated test cases. Second, the prompts are
generally lack of domain contextual knowledge, limiting LLMs' performance in
the task.

中文翻译:
测试用例对于验证软件应用的可靠性与质量至关重要。近期研究表明，大型语言模型（LLMs）能够为给定源代码生成有效的测试用例。然而现有研究主要依赖人工编写的简单提示，由于模型性能易受提示内容影响，这种方式往往难以获得最优结果。此外，这些方法对所有LLM使用统一提示模板，忽视了不同模型可能适配不同提示的特点。面对海量可能的提示表述方式，如何为每个LLM自动发现最优提示成为重大挑战。尽管自然语言处理领域已有自动提示优化方法，但这些方法难以生成适用于测试用例生成任务的有效提示：首先，现有方法仅通过简单组合和变异现有提示进行迭代优化，缺乏有效指导机制，导致生成的提示多样性不足且会在测试用例中重复相同错误；其次，现有提示普遍缺乏领域上下文知识，限制了LLM在任务中的表现。
