# SELFormer: Molecular Representation Learning via SELFIES Language Models

链接: http://arxiv.org/abs/2304.04662v1

原文摘要:
Automated computational analysis of the vast chemical space is critical for
numerous fields of research such as drug discovery and material science.
Representation learning techniques have recently been employed with the primary
objective of generating compact and informative numerical expressions of
complex data. One approach to efficiently learn molecular representations is
processing string-based notations of chemicals via natural language processing
(NLP) algorithms. Majority of the methods proposed so far utilize SMILES
notations for this purpose; however, SMILES is associated with numerous
problems related to validity and robustness, which may prevent the model from
effectively uncovering the knowledge hidden in the data. In this study, we
propose SELFormer, a transformer architecture-based chemical language model
that utilizes a 100% valid, compact and expressive notation, SELFIES, as input,
in order to learn flexible and high-quality molecular representations.
SELFormer is pre-trained on two million drug-like compounds and fine-tuned for
diverse molecular property prediction tasks. Our performance evaluation has
revealed that, SELFormer outperforms all competing methods, including graph
learning-based approaches and SMILES-based chemical language models, on
predicting aqueous solubility of molecules and adverse drug reactions. We also
visualized molecular representations learned by SELFormer via dimensionality
reduction, which indicated that even the pre-trained model can discriminate
molecules with differing structural properties. We shared SELFormer as a
programmatic tool, together with its datasets and pre-trained models. Overall,
our research demonstrates the benefit of using the SELFIES notations in the
context of chemical language modeling and opens up new possibilities for the
design and discovery of novel drug candidates with desired features.

中文翻译:
自动化计算分析广阔的化学空间对于药物发现与材料科学等诸多研究领域至关重要。近年来，表征学习技术被广泛应用，其核心目标是生成复杂数据的紧凑且信息丰富的数值表达。其中一种高效学习分子表征的方法，是通过自然语言处理（NLP）算法处理基于字符串的化学表达式。当前大多数方法采用SMILES符号实现这一目的，但SMILES存在诸多涉及有效性与鲁棒性的问题，可能阻碍模型有效挖掘数据中的隐藏知识。本研究提出SELFormer——一种基于Transformer架构的化学语言模型，该模型采用100%有效、紧凑且富有表现力的SELFIES符号作为输入，以学习灵活且高质量的分子表征。SELFormer在两百万类药化合物上进行预训练，并针对多种分子性质预测任务进行微调。性能评估表明，在预测分子水溶性和药物不良反应任务上，SELFormer优于所有竞争方法（包括基于图学习的方法和基于SMILES的化学语言模型）。通过降维可视化SELFormer学习的分子表征发现，即使预训练模型也能区分具有不同结构特性的分子。我们已将SELFormer作为编程工具与其数据集、预训练模型一并开源。总体而言，本研究证明了在化学语言建模中使用SELFIES符号的优势，并为设计发现具有特定功能的新药候选分子开辟了新途径。
