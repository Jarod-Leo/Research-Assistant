# Benchmarking LLMs via Uncertainty Quantification

链接: http://arxiv.org/abs/2401.12794v1

原文摘要:
The proliferation of open-source Large Language Models (LLMs) from various
institutions has highlighted the urgent need for comprehensive evaluation
methods. However, current evaluation platforms, such as the widely recognized
HuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty,
which is vital for thoroughly assessing LLMs. To bridge this gap, we introduce
a new benchmarking approach for LLMs that integrates uncertainty
quantification. Our examination involves nine LLMs (LLM series) spanning five
representative natural language processing tasks. Our findings reveal that: I)
LLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs
may display greater uncertainty compared to their smaller counterparts; and
III) Instruction-finetuning tends to increase the uncertainty of LLMs. These
results underscore the significance of incorporating uncertainty in the
evaluation of LLMs.

中文翻译:
随着各机构开源大语言模型（LLMs）的激增，全面评估方法的迫切性日益凸显。然而当前主流评估平台（如广受认可的HuggingFace开放LLM排行榜）忽视了一个关键维度——不确定性，而这对于深入评估LLMs至关重要。为填补这一空白，我们提出了一种融合不确定性量化的新型LLM基准测试方法。通过对涵盖五大代表性自然语言处理任务的九款LLM（LLM系列）进行系统检验，研究发现：一）准确率更高的LLM可能表现出更低确定性；二）较大规模的LLM相较于小型版本可能具有更强不确定性；三）指令微调往往会增加LLM的不确定性。这些发现有力论证了在LLM评估中纳入不确定性考量具有重要意义。
