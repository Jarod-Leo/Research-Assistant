# Benchmarking LLMs via Uncertainty Quantification

链接: http://arxiv.org/abs/2401.12794v1

原文摘要:
The proliferation of open-source Large Language Models (LLMs) from various
institutions has highlighted the urgent need for comprehensive evaluation
methods. However, current evaluation platforms, such as the widely recognized
HuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty,
which is vital for thoroughly assessing LLMs. To bridge this gap, we introduce
a new benchmarking approach for LLMs that integrates uncertainty
quantification. Our examination involves nine LLMs (LLM series) spanning five
representative natural language processing tasks. Our findings reveal that: I)
LLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs
may display greater uncertainty compared to their smaller counterparts; and
III) Instruction-finetuning tends to increase the uncertainty of LLMs. These
results underscore the significance of incorporating uncertainty in the
evaluation of LLMs.

中文翻译:
以下是符合要求的学术中文翻译：

【译文】
随着各机构开源大语言模型（LLMs）的激增，建立全面评估体系的迫切性日益凸显。然而当前主流评估平台（如广受认可的HuggingFace开放LLM排行榜）存在关键维度缺失——不确定性评估，这对LLMs的完整性能评估至关重要。为填补这一空白，我们提出了一种融合不确定性量化的新型LLM基准测试方法。通过对涵盖五大代表性自然语言处理任务的九个LLM（LLM系列）进行系统检验，我们获得以下发现：一）高准确率的LLM可能伴随较低确定性；二）较大规模的LLM可能比较小模型表现出更强不确定性；三）指令微调通常会增大LLM的不确定性。这些结论证实了不确定性评估在LLM性能体系中的核心价值。

【翻译要点说明】
1. 专业术语处理：
- "proliferation"译为"激增"符合学术语境
- "uncertainty quantification"统一译为"不确定性量化"（学科标准译法）
- "instruction-finetuning"译为"指令微调"（NLP领域通用译法）

2. 句式重构：
- 将原文三个主要发现转换为中文惯用的"一/二/三"分项陈述
- "spanning five..."处理为前置定语"涵盖...的"，符合中文前置修饰特点

3. 学术风格保持：
- 使用"凸显"、"维度缺失"等正式学术用语
- "benchmarking approach"译为"基准测试方法"（计算机领域标准译法）
- 结论部分使用"证实了...核心价值"的学术评价句式

4. 文化适应性调整：
- "HuggingFace"保留英文原名（学术惯例）
- "leaderboard"意译为"排行榜"而非直译"领导板"

5. 技术准确性：
- 严格区分"certainty"（确定性）与"uncertainty"（不确定性）的对应译法
- "larger-scale"译为"较大规模"准确反映模型参数量的比较
