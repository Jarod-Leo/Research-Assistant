# Sign2GPT: Leveraging Large Language Models for Gloss-Free Sign Language Translation

链接: http://arxiv.org/abs/2405.04164v1

原文摘要:
Automatic Sign Language Translation requires the integration of both computer
vision and natural language processing to effectively bridge the communication
gap between sign and spoken languages. However, the deficiency in large-scale
training data to support sign language translation means we need to leverage
resources from spoken language. We introduce, Sign2GPT, a novel framework for
sign language translation that utilizes large-scale pretrained vision and
language models via lightweight adapters for gloss-free sign language
translation. The lightweight adapters are crucial for sign language
translation, due to the constraints imposed by limited dataset sizes and the
computational requirements when training with long sign videos. We also propose
a novel pretraining strategy that directs our encoder to learn sign
representations from automatically extracted pseudo-glosses without requiring
gloss order information or annotations. We evaluate our approach on two public
benchmark sign language translation datasets, namely RWTH-PHOENIX-Weather 2014T
and CSL-Daily, and improve on state-of-the-art gloss-free translation
performance with a significant margin.

中文翻译:
自动手语翻译需要结合计算机视觉与自然语言处理技术，以有效弥合手语与口语之间的沟通鸿沟。然而，由于缺乏支持手语翻译的大规模训练数据，我们必须充分利用口语资源。本文提出Sign2GPT——一种创新的手语翻译框架，该框架通过轻量级适配器利用大规模预训练的视觉与语言模型，实现无需手语词汇注释的翻译。鉴于有限数据集规模的约束以及长手语视频训练时的计算需求，轻量级适配器对手语翻译至关重要。我们还提出了一种新颖的预训练策略，引导编码器从自动提取的伪词汇标签中学习手语表征，且无需词汇顺序信息或标注支持。在RWTH-PHOENIX-Weather 2014T和CSL-Daily两个公开手语翻译基准数据集上的实验表明，我们的方法显著提升了当前最优的无词汇注释翻译性能。
