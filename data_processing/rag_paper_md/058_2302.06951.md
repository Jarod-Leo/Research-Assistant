# Few-shot learning approaches for classifying low resource domain specific software requirements

链接: http://arxiv.org/abs/2302.06951v1

原文摘要:
With the advent of strong pre-trained natural language processing models like
BERT, DeBERTa, MiniLM, T5, the data requirement for industries to fine-tune
these models to their niche use cases has drastically reduced (typically to a
few hundred annotated samples for achieving a reasonable performance). However,
the availability of even a few hundred annotated samples may not always be
guaranteed in low resource domains like automotive, which often limits the
usage of such deep learning models in an industrial setting. In this paper we
aim to address the challenge of fine-tuning such pre-trained models with only a
few annotated samples, also known as Few-shot learning. Our experiments focus
on evaluating the performance of a diverse set of algorithms and methodologies
to achieve the task of classifying BOSCH automotive domain textual software
requirements into 3 categories, while utilizing only 15 annotated samples per
category for fine-tuning. We find that while SciBERT and DeBERTa based models
tend to be the most accurate at 15 training samples, their performance
improvement scales minimally as the number of annotated samples is increased to
50 in comparison to Siamese and T5 based models.

中文翻译:
随着BERT、DeBERTa、MiniLM、T5等强大预训练自然语言处理模型的出现，行业针对特定场景微调这些模型所需的数据量已大幅减少（通常仅需数百个标注样本即可达到合理性能）。然而在汽车等资源匮乏领域，即使获取数百个标注样本也并非总能得到保证，这往往限制了此类深度学习模型在工业环境中的应用。本文旨在解决仅用少量标注样本（即小样本学习）微调预训练模型的挑战。我们通过实验评估了多种算法与方法，仅使用每类别15个标注样本进行微调，成功将博世汽车领域文本软件需求分类为3个类别。研究发现：基于SciBERT和DeBERTa的模型在15个训练样本时准确率最高，但当标注样本增至50个时，其性能提升幅度明显小于基于孪生网络和T5的模型。
