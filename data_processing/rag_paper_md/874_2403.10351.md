# TriSum: Learning Summarization Ability from Large Language Models with Structured Rationale

链接: http://arxiv.org/abs/2403.10351v1

原文摘要:
The advent of large language models (LLMs) has significantly advanced natural
language processing tasks like text summarization. However, their large size
and computational demands, coupled with privacy concerns in data transmission,
limit their use in resource-constrained and privacy-centric settings. To
overcome this, we introduce TriSum, a framework for distilling LLMs' text
summarization abilities into a compact, local model. Initially, LLMs extract a
set of aspect-triple rationales and summaries, which are refined using a
dual-scoring method for quality. Next, a smaller local model is trained with
these tasks, employing a curriculum learning strategy that evolves from simple
to complex tasks. Our method enhances local model performance on various
benchmarks (CNN/DailyMail, XSum, and ClinicalTrial), outperforming baselines by
4.5%, 8.5%, and 7.4%, respectively. It also improves interpretability by
providing insights into the summarization rationale.

中文翻译:
大型语言模型（LLM）的出现显著推动了文本摘要等自然语言处理任务的进展。然而，其庞大的体量、高计算需求以及数据传输中的隐私问题，限制了在资源受限和注重隐私场景下的应用。为此，我们提出TriSum框架，旨在将LLM的文本摘要能力蒸馏至一个轻量化的本地模型中。该框架首先通过LLM提取一组"方面-三元组"依据及摘要，并采用双评分机制优化内容质量；随后利用课程学习策略，由简至繁训练小型本地模型完成这些任务。实验表明，我们的方法在CNN/DailyMail、XSum和ClinicalTrial多个基准测试中分别以4.5%、8.5%和7.4%的优势超越基线模型，同时通过揭示摘要生成依据增强了模型可解释性。
