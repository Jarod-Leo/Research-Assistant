# TriSum: Learning Summarization Ability from Large Language Models with Structured Rationale

链接: http://arxiv.org/abs/2403.10351v1

原文摘要:
The advent of large language models (LLMs) has significantly advanced natural
language processing tasks like text summarization. However, their large size
and computational demands, coupled with privacy concerns in data transmission,
limit their use in resource-constrained and privacy-centric settings. To
overcome this, we introduce TriSum, a framework for distilling LLMs' text
summarization abilities into a compact, local model. Initially, LLMs extract a
set of aspect-triple rationales and summaries, which are refined using a
dual-scoring method for quality. Next, a smaller local model is trained with
these tasks, employing a curriculum learning strategy that evolves from simple
to complex tasks. Our method enhances local model performance on various
benchmarks (CNN/DailyMail, XSum, and ClinicalTrial), outperforming baselines by
4.5%, 8.5%, and 7.4%, respectively. It also improves interpretability by
providing insights into the summarization rationale.

中文翻译:
以下是符合要求的学术摘要翻译：

大型语言模型（LLMs）的出现显著推动了文本摘要等自然语言处理任务的发展。然而，其庞大的参数量与计算需求，加之数据传输中的隐私隐患，导致其在资源受限和注重隐私的场景中应用受限。为此，我们提出TriSum框架，通过知识蒸馏将LLMs的文本摘要能力迁移至轻量级本地模型。该框架首先利用LLMs提取包含多维属性三元组的摘要依据及初始摘要，并通过双评分机制进行质量优化；随后采用课程学习策略，使本地模型从简单到复杂逐步掌握这些任务。实验表明，本方法在CNN/DailyMail、XSum和ClinicalTrial三大基准测试中分别以4.5%、8.5%和7.4%的优势超越基线模型，同时通过揭示摘要生成依据增强了模型可解释性。

（翻译说明：
1. 专业术语处理："aspect-triple rationales"译为"多维属性三元组依据"，"dual-scoring method"译为"双评分机制"，保持学术精确性
2. 句式重构：将原文复合句拆分为符合中文表达习惯的短句，如将"which are refined..."独立译为分句
3. 被动语态转换："a smaller local model is trained"主动化为"使本地模型...掌握"
4. 数据呈现：百分比差异保留原文精确数值，增加"三大"作为范畴提示
5. 概念显化："interpretability"译为"可解释性"符合机器学习领域术语规范）
