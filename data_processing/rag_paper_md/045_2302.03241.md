# Continual Learning of Language Models

链接: http://arxiv.org/abs/2302.03241v1

原文摘要:
Language models (LMs) have been instrumental for the rapid advance of natural
language processing. This paper studies continual pre-training of LMs, in
particular, continual domain-adaptive pre-training (or continual DAP-training).
Existing research has shown that further pre-training an LM using a domain
corpus to adapt the LM to the domain can improve the end-task performance in
the domain. This paper proposes a novel method to continually DAP-train an LM
with a sequence of unlabeled domain corpora to adapt the LM to these domains to
improve their end-task performances. The key novelty of our method is a
soft-masking mechanism that directly controls the update to the LM. A novel
proxy is also proposed to preserve the general knowledge in the original LM.
Additionally, it contrasts the representations of the previously learned domain
knowledge (including the general knowledge in the pre-trained LM) and the
knowledge from the current full network to achieve knowledge integration. The
method not only overcomes catastrophic forgetting, but also achieves knowledge
transfer to improve end-task performances. Empirical evaluation demonstrates
the effectiveness of the proposed method.

中文翻译:
语言模型（LMs）对自然语言处理的快速发展起到了关键作用。本文研究语言模型的持续预训练，特别是持续领域自适应预训练（或称持续DAP训练）。现有研究表明，利用领域语料库对LM进行进一步预训练以使其适应特定领域，能够提升该领域下游任务的性能。本文提出一种创新方法，通过一系列未标注的领域语料库持续进行DAP训练，使LM逐步适应多个领域，从而提升各领域下游任务表现。该方法的核心创新在于采用软掩蔽机制直接控制LM的参数更新，并引入新型代理机制来保留原始LM中的通用知识。此外，该方法通过对比先前学习到的领域知识（包括预训练LM中的通用知识）与当前完整网络生成的知识表征，实现知识整合。该方法不仅有效克服灾难性遗忘问题，还能促进知识迁移以提升下游任务性能。实证评估结果验证了所提方法的有效性。
