# Continual Learning of Language Models

链接: http://arxiv.org/abs/2302.03241v1

原文摘要:
Language models (LMs) have been instrumental for the rapid advance of natural
language processing. This paper studies continual pre-training of LMs, in
particular, continual domain-adaptive pre-training (or continual DAP-training).
Existing research has shown that further pre-training an LM using a domain
corpus to adapt the LM to the domain can improve the end-task performance in
the domain. This paper proposes a novel method to continually DAP-train an LM
with a sequence of unlabeled domain corpora to adapt the LM to these domains to
improve their end-task performances. The key novelty of our method is a
soft-masking mechanism that directly controls the update to the LM. A novel
proxy is also proposed to preserve the general knowledge in the original LM.
Additionally, it contrasts the representations of the previously learned domain
knowledge (including the general knowledge in the pre-trained LM) and the
knowledge from the current full network to achieve knowledge integration. The
method not only overcomes catastrophic forgetting, but also achieves knowledge
transfer to improve end-task performances. Empirical evaluation demonstrates
the effectiveness of the proposed method.

中文翻译:
语言模型（LMs）对自然语言处理的快速发展起到了关键作用。本文研究语言模型的持续预训练，特别是持续领域自适应预训练（或称持续DAP训练）。现有研究表明，使用领域语料库对语言模型进行进一步预训练，使其适应特定领域，能够提升该领域下游任务的性能。本文提出了一种创新方法，通过一系列无标注领域语料库持续进行DAP训练，使语言模型逐步适应多个领域，从而提升其下游任务表现。

本方法的核心创新在于采用软掩蔽机制直接控制语言模型的参数更新。同时提出了一种新型代理机制来保留原始语言模型中的通用知识。此外，该方法通过对比已习得领域知识（包括预训练语言模型中的通用知识）与当前完整网络表征的方式实现知识融合。该方法不仅克服了灾难性遗忘问题，还实现了知识迁移以提升下游任务性能。实证评估验证了所提方法的有效性。

（注：根据学术论文翻译规范，对部分术语进行了标准化处理：
1. "continual DAP-training"译为"持续DAP训练"并保留英文缩写
2. "soft-masking mechanism"译为"软掩蔽机制"符合计算机领域术语
3. "proxy"在上下文语境中译为"代理机制"而非直译"代理"
4. 长难句进行了符合中文表达习惯的拆分重组
5. 保持被动语态与主动语态的合理转换）
