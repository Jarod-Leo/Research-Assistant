# On the Applicability of Language Models to Block-Based Programs

链接: http://arxiv.org/abs/2302.03927v1

原文摘要:
Block-based programming languages like Scratch are increasingly popular for
programming education and end-user programming. Recent program analyses build
on the insight that source code can be modelled using techniques from natural
language processing. Many of the regularities of source code that support this
approach are due to the syntactic overhead imposed by textual programming
languages. This syntactic overhead, however, is precisely what block-based
languages remove in order to simplify programming. Consequently, it is unclear
how well this modelling approach performs on block-based programming languages.
In this paper, we investigate the applicability of language models for the
popular block-based programming language Scratch. We model Scratch programs
using n-gram models, the most essential type of language model, and
transformers, a popular deep learning model. Evaluation on the example tasks of
code completion and bug finding confirm that blocks inhibit predictability, but
the use of language models is nevertheless feasible. Our findings serve as
foundation for improving tooling and analyses for block-based languages.

中文翻译:
诸如Scratch这类基于块的编程语言在编程教育和终端用户编程领域日益流行。近期研究提出，源代码可借鉴自然语言处理技术进行建模分析，其有效性很大程度上源于文本编程语言固有的语法冗余特征。然而，基于块的语言正是通过消除这些语法冗余来简化编程过程，这使得传统建模方法在该领域的适用性存疑。本文针对流行的Scratch语言，系统评估了语言模型的适用性：采用最基础的n-gram模型和主流深度学习模型Transformer分别建模，通过代码补全和错误检测两个典型任务验证发现，虽然块结构会降低程序可预测性，但语言模型仍具可行性。该研究为改进基于块语言的工具链和分析方法奠定了理论基础。
