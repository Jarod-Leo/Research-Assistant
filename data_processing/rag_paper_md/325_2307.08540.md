# Utilization of Pre-trained Language Model for Adapter-based Knowledge Transfer in Software Engineering

链接: http://arxiv.org/abs/2307.08540v1

原文摘要:
Software Engineering (SE) Pre-trained Language Models (PLMs), such as
CodeBERT, are pre-trained on large code corpora, and their learned knowledge
has shown success in transferring into downstream tasks (e.g., code clone
detection) through the fine-tuning of PLMs. In Natural Language Processing
(NLP), an alternative in transferring the knowledge of PLMs is explored through
the use of adapter, a compact and parameter efficient module that is inserted
into a PLM. Although the use of adapters has shown promising results in many
NLP-based downstream tasks, their application and exploration in SE-based
downstream tasks are limited.
  Here, we study the knowledge transfer using adapters on multiple down-stream
tasks including cloze test, code clone detection, and code summarization. These
adapters are trained on code corpora and are inserted into a PLM that is
pre-trained on English corpora or code corpora. We called these PLMs as NL-PLM
and C-PLM, respectively. We observed an improvement in results using NL-PLM
over a PLM that does not have adapters, and this suggested that adapters can
transfer and utilize useful knowledge from NL-PLM to SE tasks. The results are
sometimes on par with or exceed the results of C-PLM; while being more
efficient in terms of the number of parameters and training time.
Interestingly, adapters inserted into a C-PLM generally yield better results
than a traditional fine-tuned C-PLM. Our results open new directions to build
more compact models for SE tasks.

中文翻译:
软件工程（SE）领域的预训练语言模型（PLMs），例如CodeBERT，通过在大规模代码语料库上进行预训练，其习得的知识已证明能够通过微调有效迁移至下游任务（如代码克隆检测）。在自然语言处理（NLP）领域，研究者探索了另一种迁移PLM知识的方式——适配器（Adapter），这是一种插入PLM中的紧凑且参数高效的模块。尽管适配器在众多NLP下游任务中展现出优异性能，但其在SE下游任务中的应用与研究仍较为有限。

本文研究了适配器在填空测试、代码克隆检测和代码摘要等多个下游任务中的知识迁移效果。这些适配器在代码语料库上训练后，被嵌入至基于英语语料库（NL-PLM）或代码语料库（C-PLM）预训练的PLM中。实验表明：使用NL-PLM的适配器模型相较无适配器的PLM性能有所提升，说明适配器能够将NL-PLM的有用知识迁移至SE任务，其效果有时与C-PLM相当甚至更优，同时在参数量和训练效率上更具优势。值得注意的是，插入C-PLM的适配器通常比传统微调的C-PLM表现更好。这一发现为构建更轻量化的SE任务模型开辟了新方向。
