# How does a Language-Specific Tokenizer affect LLMs?

链接: http://arxiv.org/abs/2502.12560v1

原文摘要:
The necessity of language-specific tokenizers intuitively appears crucial for
effective natural language processing, yet empirical analyses on their
significance and underlying reasons are lacking. This study explores how
language-specific tokenizers influence the behavior of Large Language Models
predominantly trained with English text data, through the case study of Korean.
The research unfolds in two main stages: (1) the development of a
Korean-specific extended tokenizer and (2) experiments to compare models with
the basic tokenizer and the extended tokenizer through various Next Token
Prediction tasks. Our in-depth analysis reveals that the extended tokenizer
decreases confidence in incorrect predictions during generation and reduces
cross-entropy in complex tasks, indicating a tendency to produce less
nonsensical outputs. Consequently, the extended tokenizer provides stability
during generation, potentially leading to higher performance in downstream
tasks.

中文翻译:
语言专用分词器对于高效自然语言处理的必要性在直觉上似乎不言而喻，但关于其重要性及深层原因的实证分析却鲜见探讨。本研究以韩语为案例，探究语言专用分词器如何影响主要基于英语文本数据训练的大语言模型行为。研究分为两个核心阶段：(1)开发韩语专用扩展分词器；(2)通过多项"下一词预测"任务对比基础分词器与扩展分词器模型的性能。深度分析表明：扩展分词器能降低生成过程中错误预测的置信度，并在复杂任务中减少交叉熵，这表明其倾向于产生更少无意义的输出。因此，扩展分词器为生成过程提供了稳定性，有望在下游任务中实现更优表现。
