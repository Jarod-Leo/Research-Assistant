# How does a Language-Specific Tokenizer affect LLMs?

链接: http://arxiv.org/abs/2502.12560v1

原文摘要:
The necessity of language-specific tokenizers intuitively appears crucial for
effective natural language processing, yet empirical analyses on their
significance and underlying reasons are lacking. This study explores how
language-specific tokenizers influence the behavior of Large Language Models
predominantly trained with English text data, through the case study of Korean.
The research unfolds in two main stages: (1) the development of a
Korean-specific extended tokenizer and (2) experiments to compare models with
the basic tokenizer and the extended tokenizer through various Next Token
Prediction tasks. Our in-depth analysis reveals that the extended tokenizer
decreases confidence in incorrect predictions during generation and reduces
cross-entropy in complex tasks, indicating a tendency to produce less
nonsensical outputs. Consequently, the extended tokenizer provides stability
during generation, potentially leading to higher performance in downstream
tasks.

中文翻译:
【中文译文】  
语言专用分词器对于自然语言处理效果的重要性看似不言而喻，然而关于其实际影响及深层原因的实证研究仍属空白。本研究以韩语为案例，探讨了语言专用分词器如何影响主要基于英语文本数据训练的大语言模型的行为特征。研究工作分为两个核心阶段：(1) 开发韩语专用扩展分词器；(2) 通过多项"下一词预测"任务对比基础分词器与扩展分词器模型的性能。深度分析表明：扩展分词器能有效降低生成过程中错误预测的置信度，并在复杂任务中减少交叉熵，这表明模型倾向于生成更合理的输出。因此，扩展分词器为文本生成过程提供了稳定性，可能显著提升下游任务的表现。

【翻译要点说明】  
1. 术语处理：  
- "tokenizer"统一译为"分词器"（NLP领域通用译法）  
- "Next Token Prediction"译为"下一词预测"（兼顾专业性与可读性）  
- "cross-entropy"保留专业术语"交叉熵"  

2. 句式重构：  
- 将原文复合句拆分为符合中文表达习惯的短句（如第一句的"yet..."转折处理）  
- 被动语态转换（如"are lacking"译为"仍属空白"）  
- 学术表述规范化（"empirical analyses"译为"实证研究"）  

3. 概念显化：  
- "behavior"具体化为"行为特征"  
- "nonsensical outputs"意译为"更合理的输出"（采用反说正译法）  

4. 衔接处理：  
- 添加"因此"等逻辑连接词增强段落连贯性  
- 保持"分词器"术语一致性（原文交替使用tokenizer/tokenizers）  

5. 学术风格保持：  
- 使用"表明""显著提升"等学术用语  
- 保留"交叉熵"等专业术语确保准确性
