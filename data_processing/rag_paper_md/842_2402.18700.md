# Learning to Compress Prompt in Natural Language Formats

链接: http://arxiv.org/abs/2402.18700v1

原文摘要:
Large language models (LLMs) are great at processing multiple natural
language processing tasks, but their abilities are constrained by inferior
performance with long context, slow inference speed, and the high cost of
computing the results. Deploying LLMs with precise and informative context
helps users process large-scale datasets more effectively and cost-efficiently.
Existing works rely on compressing long prompt contexts into soft prompts.
However, soft prompt compression encounters limitations in transferability
across different LLMs, especially API-based LLMs. To this end, this work aims
to compress lengthy prompts in the form of natural language with LLM
transferability. This poses two challenges: (i) Natural Language (NL) prompts
are incompatible with back-propagation, and (ii) NL prompts lack flexibility in
imposing length constraints. In this work, we propose a Natural Language Prompt
Encapsulation (Nano-Capsulator) framework compressing original prompts into NL
formatted Capsule Prompt while maintaining the prompt utility and
transferability. Specifically, to tackle the first challenge, the
Nano-Capsulator is optimized by a reward function that interacts with the
proposed semantics preserving loss. To address the second question, the
Nano-Capsulator is optimized by a reward function featuring length constraints.
Experimental results demonstrate that the Capsule Prompt can reduce 81.4% of
the original length, decrease inference latency up to 4.5x, and save 80.1% of
budget overheads while providing transferability across diverse LLMs and
different datasets.

中文翻译:
大型语言模型（LLMs）在处理多种自然语言处理任务方面表现出色，但其能力受限于长上下文性能不足、推理速度慢以及计算成本高昂等问题。通过部署具有精准且信息丰富的上下文LLMs，能帮助用户更高效且经济地处理大规模数据集。现有研究多依赖将长提示上下文压缩为软提示，然而软提示压缩在不同LLMs（尤其是基于API的LLMs）间的可迁移性存在局限。为此，本研究旨在以具备LLM可迁移性的自然语言形式压缩冗长提示，这面临两大挑战：（i）自然语言（NL）提示无法与反向传播兼容；（ii）NL提示在施加长度约束时缺乏灵活性。本文提出自然语言提示封装框架（Nano-Capsulator），将原始提示压缩为自然语言格式的胶囊提示（Capsule Prompt），同时保持提示效用与可迁移性。具体而言，针对第一个挑战，Nano-Capsulator通过结合语义保留损失函数的奖励机制进行优化；针对第二个问题，采用具备长度约束的奖励函数进行优化。实验结果表明，胶囊提示能将原始长度缩减81.4%，推理延迟降低最高达4.5倍，节省80.1%的预算开销，并在不同LLMs和数据集间展现出良好的可迁移性。
