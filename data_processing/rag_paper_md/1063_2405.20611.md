# Bi-Directional Transformers vs. word2vec: Discovering Vulnerabilities in Lifted Compiled Code

链接: http://arxiv.org/abs/2405.20611v1

原文摘要:
Detecting vulnerabilities within compiled binaries is challenging due to lost
high-level code structures and other factors such as architectural
dependencies, compilers, and optimization options. To address these obstacles,
this research explores vulnerability detection using natural language
processing (NLP) embedding techniques with word2vec, BERT, and RoBERTa to learn
semantics from intermediate representation (LLVM IR) code. Long short-term
memory (LSTM) neural networks were trained on embeddings from encoders created
using approximately 48k LLVM functions from the Juliet dataset. This study is
pioneering in its comparison of word2vec models with multiple bidirectional
transformers (BERT, RoBERTa) embeddings built using LLVM code to train neural
networks to detect vulnerabilities in compiled binaries. Word2vec Skip-Gram
models achieved 92% validation accuracy in detecting vulnerabilities,
outperforming word2vec Continuous Bag of Words (CBOW), BERT, and RoBERTa. This
suggests that complex contextual embeddings may not provide advantages over
simpler word2vec models for this task when a limited number (e.g. 48K) of data
samples are used to train the bidirectional transformer-based models. The
comparative results provide novel insights into selecting optimal embeddings
for learning compiler-independent semantic code representations to advance
machine learning detection of vulnerabilities in compiled binaries.

中文翻译:
由于高层代码结构的丢失以及架构依赖性、编译器和优化选项等因素的影响，检测编译后二进制文件中的漏洞具有挑战性。为应对这些难题，本研究探索利用自然语言处理（NLP）嵌入技术（包括word2vec、BERT和RoBERTa）从中间表示（LLVM IR）代码中学习语义特征。基于Juliet数据集中约4.8万个LLVM函数生成的编码器嵌入向量，我们训练了长短期记忆（LSTM）神经网络。该研究开创性地比较了word2vec模型与多种双向Transformer（BERT、RoBERTa）嵌入方法在训练神经网络检测二进制文件漏洞时的表现，其中word2vec Skip-Gram模型以92%的验证准确率优于word2vec连续词袋（CBOW）、BERT和RoBERTa模型。这表明当训练双向Transformer模型的数据样本量有限（如4.8万条）时，复杂的上下文嵌入方法可能不会比简单的word2vec模型更具优势。这些对比结果为选择最优嵌入方法来学习编译器无关的语义代码表示提供了新见解，有助于推进机器学习在二进制文件漏洞检测领域的发展。
