# A review on the use of large language models as virtual tutors

链接: http://arxiv.org/abs/2405.11983v1

原文摘要:
Transformer architectures contribute to managing long-term dependencies for
Natural Language Processing, representing one of the most recent changes in the
field. These architectures are the basis of the innovative, cutting-edge Large
Language Models (LLMs) that have produced a huge buzz in several fields and
industrial sectors, among the ones education stands out. Accordingly, these
generative Artificial Intelligence-based solutions have directed the change in
techniques and the evolution in educational methods and contents, along with
network infrastructure, towards high-quality learning. Given the popularity of
LLMs, this review seeks to provide a comprehensive overview of those solutions
designed specifically to generate and evaluate educational materials and which
involve students and teachers in their design or experimental plan. To the best
of our knowledge, this is the first review of educational applications (e.g.,
student assessment) of LLMs. As expected, the most common role of these systems
is as virtual tutors for automatic question generation. Moreover, the most
popular models are GTP-3 and BERT. However, due to the continuous launch of new
generative models, new works are expected to be published shortly.

中文翻译:
Transformer架构在自然语言处理领域有效解决了长程依赖问题，成为该领域最具突破性的技术革新之一。作为前沿大型语言模型（LLMs）的核心基础，这些架构已在多个行业引发巨大反响，其中教育领域表现尤为突出。基于生成式人工智能的解决方案正引领着教学技术、教育方法、内容体系及网络基础设施的变革，推动高质量学习的发展。鉴于LLMs的广泛影响，本文综述了专门用于生成和评估教学材料、并让师生参与设计或实验方案的相关研究。据我们所知，这是首篇针对LLMs教育应用（如学生评估）的系统综述。研究表明，这些系统最常见的功能是作为自动出题的虚拟导师，其中GPT-3和BERT模型应用最为广泛。值得注意的是，随着新型生成模型的持续涌现，预计短期内将有更多研究成果发布。
