# Block-wise Bit-Compression of Transformer-based Models

链接: http://arxiv.org/abs/2303.09184v1

原文摘要:
With the popularity of the recent Transformer-based models represented by
BERT, GPT-3 and ChatGPT, there has been state-of-the-art performance in a range
of natural language processing tasks. However, the massive computations, huge
memory footprint, and thus high latency of Transformer-based models is an
inevitable challenge for the cloud with high real-time requirement. To tackle
the issue, we propose BBCT, a method of block-wise bit-compression for
transformer without retraining. Our method achieves more fine-grained
compression of the whole transformer, including embedding, matrix
multiplication, GELU, softmax, layer normalization, and all the intermediate
results. As a case, we compress an efficient BERT with the method of BBCT. Our
benchmark test results on General Language Understanding Evaluation (GLUE) show
that BBCT can achieve less than 1% accuracy drop in most tasks.

中文翻译:
随着以BERT、GPT-3和ChatGPT为代表的Transformer模型广泛应用，其在多项自然语言处理任务中展现出顶尖性能。然而，这类模型庞大的计算量、显存占用以及由此产生的高延迟，对实时性要求严苛的云端部署构成了不可避免的挑战。为此，我们提出BBCT（分块位宽压缩技术），一种无需重新训练的Transformer全模型压缩方法。该方法实现了对嵌入层、矩阵乘法、GELU激活函数、Softmax归一化、层归一化及所有中间结果的细粒度压缩。以高效版BERT为实例，我们在通用语言理解评估基准（GLUE）上的测试表明：BBCT能在绝大多数任务中保持精度损失低于1%。
