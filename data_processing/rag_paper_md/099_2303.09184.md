# Block-wise Bit-Compression of Transformer-based Models

链接: http://arxiv.org/abs/2303.09184v1

原文摘要:
With the popularity of the recent Transformer-based models represented by
BERT, GPT-3 and ChatGPT, there has been state-of-the-art performance in a range
of natural language processing tasks. However, the massive computations, huge
memory footprint, and thus high latency of Transformer-based models is an
inevitable challenge for the cloud with high real-time requirement. To tackle
the issue, we propose BBCT, a method of block-wise bit-compression for
transformer without retraining. Our method achieves more fine-grained
compression of the whole transformer, including embedding, matrix
multiplication, GELU, softmax, layer normalization, and all the intermediate
results. As a case, we compress an efficient BERT with the method of BBCT. Our
benchmark test results on General Language Understanding Evaluation (GLUE) show
that BBCT can achieve less than 1% accuracy drop in most tasks.

中文翻译:
随着以BERT、GPT-3和ChatGPT为代表的Transformer模型近期广受欢迎，该架构在一系列自然语言处理任务中展现出最先进的性能。然而基于Transformer模型的海量计算、巨大内存占用及由此产生的高延迟，对于具有高实时性要求的云端部署而言仍是不可避免的挑战。为解决这一问题，我们提出BBCT（分块位压缩Transformer）方法，该方法无需重新训练即可实现Transformer模型的压缩。我们的技术实现了对整个Transformer模型更细粒度的压缩，涵盖词嵌入层、矩阵乘法、GELU激活函数、softmax归一化、层归一化以及所有中间计算结果。以高效版BERT模型为例，我们应用BBCT方法进行压缩。在通用语言理解评估基准（GLUE）上的测试结果表明，该方法在多数任务中能实现准确率下降不足1%的压缩效果。

（注：根据学术文献翻译规范，对部分术语进行了标准化处理：
1. "state-of-the-art performance"译为"最先进的性能"而非字面的"艺术级表现"
2. "block-wise bit-compression"采用"分块位压缩"的译法并保留英文缩写BBCT
3. 技术术语如GELU、softmax等保留英文原名
4. 机构名称GLUE按标准译法处理为"通用语言理解评估基准"）
