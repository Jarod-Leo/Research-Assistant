# From Words to Molecules: A Survey of Large Language Models in Chemistry

链接: http://arxiv.org/abs/2402.01439v1

原文摘要:
In recent years, Large Language Models (LLMs) have achieved significant
success in natural language processing (NLP) and various interdisciplinary
areas. However, applying LLMs to chemistry is a complex task that requires
specialized domain knowledge. This paper provides a thorough exploration of the
nuanced methodologies employed in integrating LLMs into the field of chemistry,
delving into the complexities and innovations at this interdisciplinary
juncture. Specifically, our analysis begins with examining how molecular
information is fed into LLMs through various representation and tokenization
methods. We then categorize chemical LLMs into three distinct groups based on
the domain and modality of their input data, and discuss approaches for
integrating these inputs for LLMs. Furthermore, this paper delves into the
pretraining objectives with adaptations to chemical LLMs. After that, we
explore the diverse applications of LLMs in chemistry, including novel
paradigms for their application in chemistry tasks. Finally, we identify
promising research directions, including further integration with chemical
knowledge, advancements in continual learning, and improvements in model
interpretability, paving the way for groundbreaking developments in the field.

中文翻译:
近年来，大型语言模型（LLMs）在自然语言处理（NLP）及众多交叉学科领域取得了显著成就。然而将LLMs应用于化学领域是一项复杂任务，需要专业化的领域知识。本文深入探讨了将LLMs融入化学领域的精细化方法，剖析这一学科交叉地带的复杂性与创新实践。具体而言，我们首先分析如何通过不同表征与标记化方法将分子信息输入LLMs；继而根据输入数据的领域与模态特征，将化学LLMs划分为三大类型，并探讨其输入整合策略；随后详细阐释了针对化学LLMs的预训练目标适配方案。本文还系统梳理了LLMs在化学中的多元化应用场景，包括创新性的任务解决范式。最后，我们指出了该领域极具前景的研究方向：深化化学知识整合、推进持续学习机制、增强模型可解释性等，这些突破将为化学领域带来革命性进展。

（译文特点说明：
1. 专业术语精准对应："tokenization"译为"标记化"，"pretraining objectives"译为"预训练目标"
2. 长句拆分重构：将原文复合句按中文表达习惯分解为多个短句，如将"delving into..."独立为分句
3. 逻辑显化处理：通过"具体而言"、"继而"等连接词强化段落衔接
4. 学术风格保持：使用"阐释"、"范式"等符合学术论文语境的词汇
5. 动态对等翻译："groundbreaking developments"译为"革命性进展"而非字面直译
6. 术语统一性：全篇保持"LLMs"缩写与"大型语言模型"的混用规范）
