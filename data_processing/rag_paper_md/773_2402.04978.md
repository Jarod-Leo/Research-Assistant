# An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration

链接: http://arxiv.org/abs/2402.04978v1

原文摘要:
While Large Language Models (LLMs) demonstrate exceptional performance in a
multitude of Natural Language Processing (NLP) tasks, they encounter challenges
in practical applications, including issues with hallucinations, inadequate
knowledge updating, and limited transparency in the reasoning process. To
overcome these limitations, this study innovatively proposes a collaborative
training-free reasoning scheme involving tight cooperation between Knowledge
Graph (KG) and LLMs. This scheme first involves using LLMs to iteratively
explore KG, selectively retrieving a task-relevant knowledge subgraph to
support reasoning. The LLMs are then guided to further combine inherent
implicit knowledge to reason on the subgraph while explicitly elucidating the
reasoning process. Through such a cooperative approach, our scheme achieves
more reliable knowledge-based reasoning and facilitates the tracing of the
reasoning results. Experimental results show that our scheme significantly
progressed across multiple datasets, notably achieving over a 10% improvement
on the QALD10 dataset compared to the best baseline and the fine-tuned
state-of-the-art (SOTA) work. Building on this success, this study hopes to
offer a valuable reference for future research in the fusion of KG and LLMs,
thereby enhancing LLMs' proficiency in solving complex issues.

中文翻译:
尽管大语言模型（LLM）在众多自然语言处理（NLP）任务中展现出卓越性能，但其在实际应用中仍面临挑战，包括幻觉问题、知识更新不足以及推理过程透明度有限等。为突破这些局限，本研究创新性地提出了一种知识图谱（KG）与LLM紧密协作的无训练推理方案。该方案首先利用LLM迭代探索知识图谱，选择性检索出与任务相关的知识子图以支撑推理；随后引导LLM进一步结合自身隐含知识在子图上进行推理，同时显式阐明推理过程。通过这种协同机制，我们的方案实现了更可靠的基于知识的推理，并支持对推理结果的溯源。实验结果表明，该方案在多个数据集上取得显著进展，尤其在QALD10数据集上相较最佳基线方法和微调后的前沿（SOTA）工作提升超过10%。基于这一成果，本研究期望为未来知识图谱与大语言模型融合的研究提供有益参考，从而提升LLM解决复杂问题的能力。
