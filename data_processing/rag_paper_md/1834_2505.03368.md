# Geospatial Mechanistic Interpretability of Large Language Models

链接: http://arxiv.org/abs/2505.03368v1

原文摘要:
Large Language Models (LLMs) have demonstrated unprecedented capabilities
across various natural language processing tasks. Their ability to process and
generate viable text and code has made them ubiquitous in many fields, while
their deployment as knowledge bases and "reasoning" tools remains an area of
ongoing research. In geography, a growing body of literature has been focusing
on evaluating LLMs' geographical knowledge and their ability to perform spatial
reasoning. However, very little is still known about the internal functioning
of these models, especially about how they process geographical information.
  In this chapter, we establish a novel framework for the study of geospatial
mechanistic interpretability - using spatial analysis to reverse engineer how
LLMs handle geographical information. Our aim is to advance our understanding
of the internal representations that these complex models generate while
processing geographical information - what one might call "how LLMs think about
geographic information" if such phrasing was not an undue anthropomorphism.
  We first outline the use of probing in revealing internal structures within
LLMs. We then introduce the field of mechanistic interpretability, discussing
the superposition hypothesis and the role of sparse autoencoders in
disentangling polysemantic internal representations of LLMs into more
interpretable, monosemantic features. In our experiments, we use spatial
autocorrelation to show how features obtained for placenames display spatial
patterns related to their geographic location and can thus be interpreted
geospatially, providing insights into how these models process geographical
information. We conclude by discussing how our framework can help shape the
study and use of foundation models in geography.

中文翻译:
大型语言模型（LLMs）已在各类自然语言处理任务中展现出前所未有的能力。其处理与生成有效文本及代码的普适性，使其广泛应用于多领域，而作为知识库与"推理"工具的部署仍属持续探索的前沿。地理学界日益关注评估LLMs的地理知识储备与空间推理能力，但对其内部工作机制——尤其是地理信息处理方式——的认知仍极为有限。

本章构建了一个研究地理空间机制可解释性的创新框架，通过空间分析方法逆向解析LLMs处理地理信息的模式。我们旨在深化理解这些复杂模型处理地理信息时生成的内部表征（若不以拟人化表述为前提，或可称之为"LLMs对地理信息的认知方式"）。

首先系统阐述探针技术在揭示LLMs内部结构中的应用；继而引入机制可解释性研究领域，探讨叠加假说及稀疏自编码器在解构LLMs多义性内部表征为可解释单义特征中的作用。实验环节通过空间自相关分析，揭示地名特征呈现与其地理位置相关的空间模式，从而实现对模型地理信息处理机制的空间化解读。最后探讨本框架如何推动地理学基础模型的研究与应用范式革新。

（注：译文严格遵循学术文本规范，采用术语统一、被动语态转换、长句拆分等策略，确保专业性与可读性平衡。关键概念如"mechanistic interpretability"译为"机制可解释性"符合计算机领域共识，"polysemantic/monosemantic features"处理为"多义性/单义特征"准确传达技术内涵。）
