# Compositional Translation: A Novel LLM-based Approach for Low-resource Machine Translation

链接: http://arxiv.org/abs/2503.04554v1

原文摘要:
The ability of generative large language models (LLMs) to perform in-context
learning has given rise to a large body of research into how best to prompt
models for various natural language processing tasks. Machine Translation (MT)
has been shown to benefit from in-context examples, in particular when they are
semantically similar to the sentence to translate. In this paper, we propose a
new LLM-based translation paradigm, compositional translation, to replace naive
few-shot MT with similarity-based demonstrations. An LLM is used to decompose a
sentence into simpler phrases, and then to translate each phrase with the help
of retrieved demonstrations. Finally, the LLM is prompted to translate the
initial sentence with the help of the self-generated phrase-translation pairs.
Our intuition is that this approach should improve translation because these
shorter phrases should be intrinsically easier to translate and easier to match
with relevant examples. This is especially beneficial in low-resource
scenarios, and more generally whenever the selection pool is small or out of
domain. We show that compositional translation boosts LLM translation
performance on a wide range of popular MT benchmarks, including FLORES 200,
NTREX 128 and TICO-19. Code and outputs are available at
https://github.com/ArmelRandy/compositional-translation

中文翻译:
生成式大语言模型（LLM）的上下文学习能力引发了大量关于如何为各类自然语言处理任务设计最佳提示的研究。机器翻译（MT）已被证明能受益于上下文示例，尤其是当这些示例在语义上与待翻译句子相似时。本文提出了一种基于LLM的新型翻译范式——组合式翻译，以取代基于相似性示例的简单少样本MT。该方法利用LLM将句子分解为更简单的短语，然后借助检索到的示例翻译每个短语，最后提示LLM在自生成的短语-翻译对辅助下完成整句翻译。我们的核心观点是：由于这些较短短语本质上更易翻译且更易匹配相关示例，该方法应能提升翻译质量。这在低资源场景下尤为有利，更广泛适用于候选池较小或领域外的情况。实验表明，组合式翻译显著提升了LLM在FLORES 200、NTREX 128和TICO-19等多个主流MT基准上的性能。代码与输出结果详见https://github.com/ArmelRandy/compositional-translation。
