# Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective

链接: http://arxiv.org/abs/2405.16747v1

原文摘要:
The two-stage fine-tuning (FT) method, linear probing (LP) then fine-tuning
(LP-FT), outperforms linear probing and FT alone. This holds true for both
in-distribution (ID) and out-of-distribution (OOD) data. One key reason for its
success is the preservation of pre-trained features, achieved by obtaining a
near-optimal linear head during LP. However, despite the widespread use of
large language models, there has been limited exploration of more complex
architectures such as Transformers. In this paper, we analyze the training
dynamics of LP-FT for classification tasks on the basis of the neural tangent
kernel (NTK) theory. Our analysis decomposes the NTK matrix into two
components. This decomposition highlights the importance of the linear head
norm alongside the prediction accuracy at the start of the FT stage. We also
observe a significant increase in the linear head norm during LP, which stems
from training with the cross-entropy (CE) loss. This increase in the linear
head norm effectively reduces changes in learned features. Furthermore, we find
that this increased norm can adversely affect model calibration, which can be
corrected using temperature scaling. Additionally, we extend our analysis with
the NTK to the low-rank adaptation (LoRA) method and validate its
effectiveness. Our experiments using a Transformer-based model on multiple
natural language processing datasets confirm our theoretical analysis. Our
study demonstrates the effectiveness of LP-FT for fine-tuning language models.
Code is available at https://github.com/tom4649/lp-ft_ntk.

中文翻译:
两阶段微调（FT）方法——先线性探测（LP）再微调（LP-FT）——在分布内（ID）和分布外（OOD）数据上均优于单独使用线性探测或微调。其成功的关键在于通过LP阶段获得接近最优的线性头，从而保留预训练特征。然而，尽管大语言模型广泛应用，针对Transformer等复杂架构的深入探索仍显不足。本文基于神经正切核（NTK）理论，系统分析了分类任务中LP-FT的训练动态。通过将NTK矩阵分解为两个组分，我们揭示了FT阶段开始时线性头范数与预测精度同等重要。实验发现：使用交叉熵（CE）损失进行LP训练会显著增大线性头范数，从而有效抑制特征学习的变化；但过大的范数会损害模型校准性，可通过温度缩放进行修正。此外，我们将NTK理论扩展至低秩适应（LoRA）方法并验证其有效性。基于Transformer模型在多个自然语言处理数据集上的实验完全支持理论分析，证实LP-FT在语言模型微调中的优越性。代码已开源：https://github.com/tom4649/lp-ft_ntk。
