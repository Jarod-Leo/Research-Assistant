# SoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership Inference Attacks

链接: http://arxiv.org/abs/2403.08481v1

原文摘要:
Natural language processing models have experienced a significant upsurge in
recent years, with numerous applications being built upon them. Many of these
applications require fine-tuning generic base models on customized, proprietary
datasets. This fine-tuning data is especially likely to contain personal or
sensitive information about individuals, resulting in increased privacy risk.
Membership inference attacks are the most commonly employed attack to assess
the privacy leakage of a machine learning model. However, limited research is
available on the factors that affect the vulnerability of language models to
this kind of attack, or on the applicability of different defense strategies in
the language domain. We provide the first systematic review of the
vulnerability of fine-tuned large language models to membership inference
attacks, the various factors that come into play, and the effectiveness of
different defense strategies. We find that some training methods provide
significantly reduced privacy risk, with the combination of differential
privacy and low-rank adaptors achieving the best privacy protection against
these attacks.

中文翻译:
近年来，自然语言处理模型呈现显著增长态势，大量应用基于此类模型构建。其中许多应用需要在定制化专有数据集上对通用基础模型进行微调，这类微调数据尤其可能包含个人隐私或敏感信息，从而导致隐私风险加剧。成员推理攻击作为评估机器学习模型隐私泄露的主要手段，目前关于语言模型易受此类攻击的影响因素，以及不同防御策略在语言领域的适用性研究仍较为有限。本文首次系统性地研究了微调后大语言模型对成员推理攻击的脆弱性、相关影响因素及各类防御策略的有效性。研究发现某些训练方法能显著降低隐私风险，其中差分隐私与低秩适配器相结合的策略可提供最优的隐私保护效果。
