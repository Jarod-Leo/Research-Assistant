# Automatically Extracting Numerical Results from Randomized Controlled Trials with Large Language Models

链接: http://arxiv.org/abs/2405.01686v1

原文摘要:
Meta-analyses statistically aggregate the findings of different randomized
controlled trials (RCTs) to assess treatment effectiveness. Because this yields
robust estimates of treatment effectiveness, results from meta-analyses are
considered the strongest form of evidence. However, rigorous evidence syntheses
are time-consuming and labor-intensive, requiring manual extraction of data
from individual trials to be synthesized. Ideally, language technologies would
permit fully automatic meta-analysis, on demand. This requires accurately
extracting numerical results from individual trials, which has been beyond the
capabilities of natural language processing (NLP) models to date. In this work,
we evaluate whether modern large language models (LLMs) can reliably perform
this task. We annotate (and release) a modest but granular evaluation dataset
of clinical trial reports with numerical findings attached to interventions,
comparators, and outcomes. Using this dataset, we evaluate the performance of
seven LLMs applied zero-shot for the task of conditionally extracting numerical
findings from trial reports. We find that massive LLMs that can accommodate
lengthy inputs are tantalizingly close to realizing fully automatic
meta-analysis, especially for dichotomous (binary) outcomes (e.g., mortality).
However, LLMs -- including ones trained on biomedical texts -- perform poorly
when the outcome measures are complex and tallying the results requires
inference. This work charts a path toward fully automatic meta-analysis of RCTs
via LLMs, while also highlighting the limitations of existing models for this
aim.

中文翻译:
元分析通过统计学方法整合不同随机对照试验（RCT）的研究结果以评估治疗效果。由于这种方法能提供稳健的治疗效果估计，元分析结果被视为最有力的证据形式。然而，严谨的证据合成过程耗时且劳动密集，需要人工从单个试验中提取数据进行整合。理想情况下，语言技术应能实现按需全自动元分析，这要求从单个试验报告中准确提取数值结果——这一任务迄今为止仍超出自然语言处理（NLP）模型的能力范围。本研究评估了现代大语言模型（LLM）能否可靠完成该任务。

我们构建并发布了一个规模适中但粒度精细的临床试验报告评估数据集，其中标注了与干预措施、对照组和结局指标相关联的数值结果。基于该数据集，我们测试了七种零样本学习的LLM在条件性提取试验报告数值结果任务中的表现。研究发现：对于需要处理长文本输入的大规模LLM而言，实现全自动元分析已近在咫尺——特别是针对二分类结局（如死亡率）的提取；但当结局指标较为复杂且需要推理计算时（包括经过生物医学文本训练的模型在内），LLM的表现明显欠佳。

本研究为通过LLM实现RCT全自动元分析指明了技术路径，同时也揭示了现有模型在该目标上的局限性。
