# Paired Completion: Flexible Quantification of Issue-framing at Scale with LLMs

链接: http://arxiv.org/abs/2408.09742v1

原文摘要:
Detecting and quantifying issue framing in textual discourse - the
perspective one takes to a given topic (e.g. climate science vs. denialism,
misogyny vs. gender equality) - is highly valuable to a range of end-users from
social and political scientists to program evaluators and policy analysts.
However, conceptual framing is notoriously challenging for automated natural
language processing (NLP) methods since the words and phrases used by either
`side' of an issue are often held in common, with only subtle stylistic
flourishes separating their use. Here we develop and rigorously evaluate new
detection methods for issue framing and narrative analysis within large text
datasets. By introducing a novel application of next-token log probabilities
derived from generative large language models (LLMs) we show that issue framing
can be reliably and efficiently detected in large corpora with only a few
examples of either perspective on a given issue, a method we call `paired
completion'. Through 192 independent experiments over three novel, synthetic
datasets, we evaluate paired completion against prompt-based LLM methods and
labelled methods using traditional NLP and recent LLM contextual embeddings. We
additionally conduct a cost-based analysis to mark out the feasible set of
performant methods at production-level scales, and a model bias analysis.
Together, our work demonstrates a feasible path to scalable, accurate and
low-bias issue-framing in large corpora.

中文翻译:
检测与量化文本话语中的议题框架——即个体对特定主题所采取的视角（如气候科学vs.否定论、厌女现象vs.性别平等）——对于从社会政治学家到项目评估者及政策分析师等终端用户群体具有极高价值。然而概念性框架构建对自动化自然语言处理（NLP）方法而言存在公认挑战，因为争议双方常使用相同词汇，仅通过微妙风格差异区分立场。本研究开发并严格评估了针对大规模文本数据集中议题框架与叙事分析的新型检测方法。通过创新性应用生成式大语言模型（LLMs）的下一词元对数概率，我们证明仅需给定议题任一视角的少量示例即可在大规模语料库中实现可靠高效的框架检测，该方法被命名为"配对补全"。基于三个新型合成数据集的192项独立实验，我们将配对补全与基于提示的LLM方法、使用传统NLP及最新LLM上下文嵌入的标注方法进行对比评估。研究还包含基于成本的生产级规模可行性分析及模型偏差分析。综合而言，本研究为大规模语料库中实现可扩展、高精度、低偏差的议题框架分析提供了可行路径。
