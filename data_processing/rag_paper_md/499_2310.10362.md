# Prompt Tuning for Multi-View Graph Contrastive Learning

链接: http://arxiv.org/abs/2310.10362v1

原文摘要:
Graphs have become an important modeling tool for web applications, and Graph
Neural Networks (GNNs) have achieved great success in graph representation
learning. However, the performance of traditional GNNs heavily relies on a
large amount of supervision. Recently, ``pre-train, fine-tune'' has become the
paradigm to address the issues of label dependency and poor generalization.
However, the pre-training strategies vary for graphs with homophily and
heterophily, and the objectives for various downstream tasks also differ. This
leads to a gap between pretexts and downstream tasks, resulting in ``negative
transfer'' and poor performance. Inspired by prompt learning in Natural
Language Processing (NLP), many studies turn to bridge the gap and fully
leverage the pre-trained model. However, existing methods for graph prompting
are tailored to homophily, neglecting inherent heterophily on graphs.
Meanwhile, most of them rely on the randomly initialized prompts, which
negatively impact on the stability. Therefore, we propose Self-Prompt, a
prompting framework for graphs based on the model and data itself. We first
introduce asymmetric graph contrastive learning for pretext to address
heterophily and align the objectives of pretext and downstream tasks. Then we
reuse the component from pre-training phase as the self adapter and introduce
self-prompts based on graph itself for task adaptation. Finally, we conduct
extensive experiments on 11 benchmark datasets to demonstrate its superiority.
We provide our codes at https://github.com/gongchenghua/Self-Pro.

中文翻译:
图已成为网络应用中的重要建模工具，图神经网络（GNNs）在图表示学习领域取得了显著成功。然而，传统GNN的性能高度依赖大量监督信息。近年来，"预训练-微调"范式成为解决标签依赖性和泛化能力不足的主流方案。但同质图与异质图的预训练策略存在差异，且不同下游任务的目标函数也各不相同，这导致预训练任务与下游任务间存在鸿沟，引发"负迁移"和性能下降问题。受自然语言处理中提示学习的启发，许多研究试图弥合这一差距以充分利用预训练模型。但现有图提示方法均针对同质图设计，忽视了图的固有异质性；同时大多依赖随机初始化的提示向量，影响了模型稳定性。为此，我们提出基于模型与数据自身的图提示框架Self-Prompt：首先采用非对称图对比学习作为预训练任务以兼容异质性，并实现预训练与下游任务的目标对齐；随后复用预训练阶段的组件作为自适应器，基于图数据自身生成提示向量进行任务适配。最终在11个基准数据集上的实验验证了方法的优越性，代码已开源。
