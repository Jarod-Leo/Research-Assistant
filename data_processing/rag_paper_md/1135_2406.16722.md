# Venturing into Uncharted Waters: The Navigation Compass from Transformer to Mamba

链接: http://arxiv.org/abs/2406.16722v1

原文摘要:
Transformer, a deep neural network architecture, has long dominated the field
of natural language processing and beyond. Nevertheless, the recent
introduction of Mamba challenges its supremacy, sparks considerable interest
among researchers, and gives rise to a series of Mamba-based models that have
exhibited notable potential. This survey paper orchestrates a comprehensive
discussion, diving into essential research dimensions, covering: (i) the
functioning of the Mamba mechanism and its foundation on the principles of
structured state space models; (ii) the proposed improvements and the
integration of Mamba with various networks, exploring its potential as a
substitute for Transformers; (iii) the combination of Transformers and Mamba to
compensate for each other's shortcomings. We have also made efforts to
interpret Mamba and Transformer in the framework of kernel functions, allowing
for a comparison of their mathematical nature within a unified context. Our
paper encompasses the vast majority of improvements related to Mamba to date.

中文翻译:
Transformer作为一种深度神经网络架构，长期主导着自然语言处理及其他领域的发展。然而，近期Mamba架构的提出对其统治地位发起了挑战，引发了研究者的广泛关注，并催生出一系列展现出显著潜力的Mamba衍生模型。本综述论文系统性地组织了全面讨论，深入探究了以下核心研究维度：(i) Mamba机制的工作原理及其基于结构化状态空间模型的理论基础；(ii) 现有改进方案及Mamba与各类网络的融合研究，探索其替代Transformer的潜力；(iii) Transformer与Mamba的互补结合以弥补彼此缺陷。我们还尝试在核函数框架下对Mamba和Transformer进行数学阐释，使其能在统一语境中进行本质性比较。本文涵盖了迄今为止绝大多数与Mamba相关的改进研究。
