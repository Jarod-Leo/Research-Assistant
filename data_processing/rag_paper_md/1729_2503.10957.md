# Predicting Stock Movement with BERTweet and Transformers

链接: http://arxiv.org/abs/2503.10957v1

原文摘要:
Applying deep learning and computational intelligence to finance has been a
popular area of applied research, both within academia and industry, and
continues to attract active attention. The inherently high volatility and
non-stationary of the data pose substantial challenges to machine learning
models, especially so for today's expressive and highly-parameterized deep
learning models. Recent work has combined natural language processing on data
from social media to augment models based purely on historic price data to
improve performance has received particular attention. Previous work has
achieved state-of-the-art performance on this task by combining techniques such
as bidirectional GRUs, variational autoencoders, word and document embeddings,
self-attention, graph attention, and adversarial training. In this paper, we
demonstrated the efficacy of BERTweet, a variant of BERT pre-trained
specifically on a Twitter corpus, and the transformer architecture by achieving
competitive performance with the existing literature and setting a new baseline
for Matthews Correlation Coefficient on the Stocknet dataset without auxiliary
data sources.

中文翻译:
将深度学习与计算智能应用于金融领域已成为学术界和工业界共同关注的热点研究方向，这一趋势至今仍保持着旺盛的活力。金融数据本身具有的高波动性与非平稳性特征对机器学习模型构成了重大挑战，尤其对当今参数庞大、表现力强的深度学习模型更是如此。近期研究通过结合社交媒体数据的自然语言处理技术，对仅依赖历史价格数据的传统模型进行增强，显著提升了模型性能，这一方向获得了学界特别关注。已有研究通过融合双向门控循环单元、变分自编码器、词向量与文档向量嵌入、自注意力机制、图注意力网络及对抗训练等技术，在该任务上实现了最先进的性能表现。本文通过实验证明：基于推特语料库预训练的BERTweet模型（BERT的变体）结合Transformer架构，在不使用辅助数据源的情况下，不仅在Stocknet数据集上取得了与现有文献相当的竞争性表现，更为马修斯相关系数指标建立了新的基准。
