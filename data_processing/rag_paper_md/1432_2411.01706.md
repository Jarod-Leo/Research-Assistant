# Investigating Large Language Models for Complex Word Identification in Multilingual and Multidomain Setups

链接: http://arxiv.org/abs/2411.01706v1

原文摘要:
Complex Word Identification (CWI) is an essential step in the lexical
simplification task and has recently become a task on its own. Some variations
of this binary classification task have emerged, such as lexical complexity
prediction (LCP) and complexity evaluation of multi-word expressions (MWE).
Large language models (LLMs) recently became popular in the Natural Language
Processing community because of their versatility and capability to solve
unseen tasks in zero/few-shot settings. Our work investigates LLM usage,
specifically open-source models such as Llama 2, Llama 3, and Vicuna v1.5, and
closed-source, such as ChatGPT-3.5-turbo and GPT-4o, in the CWI, LCP, and MWE
settings. We evaluate zero-shot, few-shot, and fine-tuning settings and show
that LLMs struggle in certain conditions or achieve comparable results against
existing methods. In addition, we provide some views on meta-learning combined
with prompt learning. In the end, we conclude that the current state of LLMs
cannot or barely outperform existing methods, which are usually much smaller.

中文翻译:
复杂词识别（CWI）是词汇简化任务中的关键环节，近年来已发展为独立研究课题。该二分类任务衍生出若干变体，如词汇复杂度预测（LCP）和多词表达式（MWE）复杂度评估。大型语言模型（LLM）凭借其多功能性及在零样本/少样本场景下处理未知任务的能力，近期在自然语言处理领域广受关注。本研究系统考察了开源模型（Llama 2/3、Vicuna v1.5）与闭源模型（ChatGPT-3.5-turbo、GPT-4o）在CWI、LCP及MWE任务中的表现，通过零样本、少样本和微调实验发现：LLMs在某些条件下表现欠佳，或仅能达到与传统方法相当的水平——尽管后者模型规模通常小得多。此外，我们对元学习与提示学习的结合应用提出了见解。最终得出结论：当前LLMs尚无法显著超越现有方法，其优势表现十分有限。

（翻译说明：
1. 专业术语采用学界通用译法，如"zero-shot"译为"零样本"
2. 长句拆分重组，如将原文条件状语从句转化为破折号补充说明
3. 被动语态转为主动表述，如"are evaluated"译为"通过...实验发现"
4. 学术用语规范化处理，如"versatility"译为"多功能性"而非字面直译
5. 保持逻辑连接词完整，如"in addition/In the end"对应"此外/最终"
6. 技术概念保留英文缩写同时补充中文全称，确保专业性）
