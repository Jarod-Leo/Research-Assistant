# Investigating Large Language Models for Complex Word Identification in Multilingual and Multidomain Setups

链接: http://arxiv.org/abs/2411.01706v1

原文摘要:
Complex Word Identification (CWI) is an essential step in the lexical
simplification task and has recently become a task on its own. Some variations
of this binary classification task have emerged, such as lexical complexity
prediction (LCP) and complexity evaluation of multi-word expressions (MWE).
Large language models (LLMs) recently became popular in the Natural Language
Processing community because of their versatility and capability to solve
unseen tasks in zero/few-shot settings. Our work investigates LLM usage,
specifically open-source models such as Llama 2, Llama 3, and Vicuna v1.5, and
closed-source, such as ChatGPT-3.5-turbo and GPT-4o, in the CWI, LCP, and MWE
settings. We evaluate zero-shot, few-shot, and fine-tuning settings and show
that LLMs struggle in certain conditions or achieve comparable results against
existing methods. In addition, we provide some views on meta-learning combined
with prompt learning. In the end, we conclude that the current state of LLMs
cannot or barely outperform existing methods, which are usually much smaller.

中文翻译:
复杂词识别（CWI）是词汇简化任务中的关键步骤，近年来已发展为独立研究课题。该二分类任务衍生出若干变体，如词汇复杂度预测（LCP）和多词表达式（MWE）复杂度评估。大型语言模型（LLM）凭借其多功能性及零样本/少样本场景下的任务处理能力，近期在自然语言处理领域广受关注。本研究系统考察了开源模型（Llama 2/3、Vicuna v1.5）与闭源模型（ChatGPT-3.5-turbo、GPT-4o）在CWI、LCP及MWE任务中的表现，通过零样本、少样本及微调实验发现：LLM在特定条件下表现欠佳，或仅能达到与传统方法相当的水平。研究同时探讨了元学习与提示学习结合的可行性，最终得出结论：当前LLM性能尚无法超越（或仅勉强持平）通常规模更小的现有方法。
