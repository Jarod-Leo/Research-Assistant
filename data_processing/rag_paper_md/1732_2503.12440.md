# HKCanto-Eval: A Benchmark for Evaluating Cantonese Language Understanding and Cultural Comprehension in LLMs

链接: http://arxiv.org/abs/2503.12440v1

原文摘要:
The ability of language models to comprehend and interact in diverse
linguistic and cultural landscapes is crucial. The Cantonese language used in
Hong Kong presents unique challenges for natural language processing due to its
rich cultural nuances and lack of dedicated evaluation datasets. The
HKCanto-Eval benchmark addresses this gap by evaluating the performance of
large language models (LLMs) on Cantonese language understanding tasks,
extending to English and Written Chinese for cross-lingual evaluation.
HKCanto-Eval integrates cultural and linguistic nuances intrinsic to Hong Kong,
providing a robust framework for assessing language models in realistic
scenarios. Additionally, the benchmark includes questions designed to tap into
the underlying linguistic metaknowledge of the models. Our findings indicate
that while proprietary models generally outperform open-weight models,
significant limitations remain in handling Cantonese-specific linguistic and
cultural knowledge, highlighting the need for more targeted training data and
evaluation methods. The code can be accessed at
https://github.com/hon9kon9ize/hkeval2025

中文翻译:
语言模型在多元语言文化环境中理解和交互的能力至关重要。香港使用的粤语因其丰富的文化内涵及缺乏专门评估数据集，为自然语言处理带来独特挑战。HKCanto-Eval基准通过评估大语言模型（LLMs）在粤语理解任务上的表现填补了这一空白，并延伸至英语和书面中文以进行跨语言评估。该基准整合了香港特有的文化与语言细微差异，为现实场景中的语言模型评估提供了稳健框架。此外，基准还包含旨在探测模型底层语言元知识的问题。研究发现，尽管专有模型普遍优于开源权重模型，但在处理粤语特有语言文化知识方面仍存在显著局限，凸显出需要更具针对性的训练数据和评估方法。相关代码可通过https://github.com/hon9kon9ize/hkeval2025获取。
