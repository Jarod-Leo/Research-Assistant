# LLMCad: Fast and Scalable On-device Large Language Model Inference

链接: http://arxiv.org/abs/2309.04255v1

原文摘要:
Generative tasks, such as text generation and question answering, hold a
crucial position in the realm of mobile applications. Due to their sensitivity
to privacy concerns, there is a growing demand for their execution directly on
mobile devices. Currently, the execution of these generative tasks heavily
depends on Large Language Models (LLMs). Nevertheless, the limited memory
capacity of these devices presents a formidable challenge to the scalability of
such models.
  In our research, we introduce LLMCad, an innovative on-device inference
engine specifically designed for efficient generative Natural Language
Processing (NLP) tasks. The core idea behind LLMCad revolves around model
collaboration: a compact LLM, residing in memory, takes charge of generating
the most straightforward tokens, while a high-precision LLM steps in to
validate these tokens and rectify any identified errors. LLMCad incorporates
three novel techniques: (1) Instead of generating candidate tokens in a
sequential manner, LLMCad employs the smaller LLM to construct a token tree,
encompassing a wider range of plausible token pathways. Subsequently, the
larger LLM can efficiently validate all of these pathways simultaneously. (2)
It employs a self-adjusting fallback strategy, swiftly initiating the
verification process whenever the smaller LLM generates an erroneous token. (3)
To ensure a continuous flow of token generation, LLMCad speculatively generates
tokens during the verification process by implementing a compute-IO pipeline.
Through an extensive series of experiments, LLMCad showcases an impressive
token generation speed, achieving rates up to 9.3x faster than existing
inference engines.

中文翻译:
在移动应用领域，文本生成与问答等生成式任务占据着核心地位。由于这类任务对隐私问题高度敏感，直接在移动设备端执行的需求日益增长。当前，这类生成任务的执行主要依赖于大语言模型（LLMs）。然而，移动设备有限的内存容量对这些模型的扩展性构成了严峻挑战。

本研究提出了LLMCad——一个专为高效生成式自然语言处理（NLP）任务设计的创新性设备端推理引擎。其核心思想在于模型协同机制：一个常驻内存的轻量级LLM负责生成最直接的词元，而高精度LLM则介入验证这些词元并修正识别到的错误。LLMCad融合了三大创新技术：（1）摒弃传统顺序生成候选词元的方式，通过小型LLM构建覆盖多路径可能性的词元树，使大型LLM能同步高效验证所有路径；（2）采用自调节回退策略，当小型LLM产生错误词元时立即触发验证流程；（3）通过构建计算-IO流水线，在验证过程中预生成词元以保障输出流畅性。经过大量实验验证，LLMCad展现出卓越的词元生成速度，较现有推理引擎最高提升达9.3倍。
