# Trust at Your Own Peril: A Mixed Methods Exploration of the Ability of Large Language Models to Generate Expert-Like Systems Engineering Artifacts and a Characterization of Failure Modes

链接: http://arxiv.org/abs/2502.09690v1

原文摘要:
Multi-purpose Large Language Models (LLMs), a subset of generative Artificial
Intelligence (AI), have recently made significant progress. While expectations
for LLMs to assist systems engineering (SE) tasks are paramount; the
interdisciplinary and complex nature of systems, along with the need to
synthesize deep-domain knowledge and operational context, raise questions
regarding the efficacy of LLMs to generate SE artifacts, particularly given
that they are trained using data that is broadly available on the internet. To
that end, we present results from an empirical exploration, where a human
expert-generated SE artifact was taken as a benchmark, parsed, and fed into
various LLMs through prompt engineering to generate segments of typical SE
artifacts. This procedure was applied without any fine-tuning or calibration to
document baseline LLM performance. We then adopted a two-fold mixed-methods
approach to compare AI generated artifacts against the benchmark. First, we
quantitatively compare the artifacts using natural language processing
algorithms and find that when prompted carefully, the state-of-the-art
algorithms cannot differentiate AI-generated artifacts from the human-expert
benchmark. Second, we conduct a qualitative deep dive to investigate how they
differ in terms of quality. We document that while the two-material appear very
similar, AI generated artifacts exhibit serious failure modes that could be
difficult to detect. We characterize these as: premature requirements
definition, unsubstantiated numerical estimates, and propensity to overspecify.
We contend that this study tells a cautionary tale about why the SE community
must be more cautious adopting AI suggested feedback, at least when generated
by multi-purpose LLMs.

中文翻译:
作为生成式人工智能（AI）的重要分支，多用途大语言模型（LLMs）近期取得了突破性进展。尽管人们普遍期待LLMs能辅助系统工程（SE）任务，但系统的跨学科复杂性、深度领域知识与运行场景的融合需求，引发了关于LLMs生成SE产物的有效性质疑——尤其是考虑到其训练数据仅来自公开网络资源。为此，我们通过实证研究，以专家人工生成的SE产物为基准，经提示工程输入不同LLMs来生成典型SE文档片段。该过程未进行任何模型微调或校准，旨在记录基线性能。

研究采用混合方法对AI生成物与基准进行双重对比：首先通过自然语言处理算法定量分析发现，当提示设计得当时，最先进算法无法区分AI生成物与专家基准；其次开展定性深度剖析，揭示两者在质量上的本质差异。研究发现，尽管表面高度相似，但AI生成物存在三大潜在风险模式：需求定义过早、数值估算缺乏依据、以及过度细化倾向。这些缺陷往往具有隐蔽性。本研究警示系统工程界必须审慎对待AI（至少是多用途LLMs）提供的建议反馈，其结论具有重要实践意义。
