# Trust at Your Own Peril: A Mixed Methods Exploration of the Ability of Large Language Models to Generate Expert-Like Systems Engineering Artifacts and a Characterization of Failure Modes

链接: http://arxiv.org/abs/2502.09690v1

原文摘要:
Multi-purpose Large Language Models (LLMs), a subset of generative Artificial
Intelligence (AI), have recently made significant progress. While expectations
for LLMs to assist systems engineering (SE) tasks are paramount; the
interdisciplinary and complex nature of systems, along with the need to
synthesize deep-domain knowledge and operational context, raise questions
regarding the efficacy of LLMs to generate SE artifacts, particularly given
that they are trained using data that is broadly available on the internet. To
that end, we present results from an empirical exploration, where a human
expert-generated SE artifact was taken as a benchmark, parsed, and fed into
various LLMs through prompt engineering to generate segments of typical SE
artifacts. This procedure was applied without any fine-tuning or calibration to
document baseline LLM performance. We then adopted a two-fold mixed-methods
approach to compare AI generated artifacts against the benchmark. First, we
quantitatively compare the artifacts using natural language processing
algorithms and find that when prompted carefully, the state-of-the-art
algorithms cannot differentiate AI-generated artifacts from the human-expert
benchmark. Second, we conduct a qualitative deep dive to investigate how they
differ in terms of quality. We document that while the two-material appear very
similar, AI generated artifacts exhibit serious failure modes that could be
difficult to detect. We characterize these as: premature requirements
definition, unsubstantiated numerical estimates, and propensity to overspecify.
We contend that this study tells a cautionary tale about why the SE community
must be more cautious adopting AI suggested feedback, at least when generated
by multi-purpose LLMs.

中文翻译:
多用途大语言模型（LLMs）作为生成式人工智能（AI）的子领域，近期取得显著进展。尽管人们高度期待LLMs辅助系统工程（SE）任务，但系统的跨学科复杂特性，以及需要融合深度领域知识与运行情境的要求，引发了关于LLMs生成SE产物的有效性疑问——尤其是考虑到这些模型的训练数据仅来自互联网公开信息。为此，我们通过实证研究呈现以下发现：以人类专家生成的SE产物为基准，经解析后通过提示工程输入不同LLMs以生成典型SE产物片段。该过程未进行任何微调或校准，旨在记录LLMs的基线表现。随后采用混合方法双重验证：首先通过自然语言处理算法定量比对，发现当提示设计得当时，最先进算法无法区分AI生成产物与人类专家基准；其次通过定性深度分析揭示二者质量差异。研究发现，虽然两类材料表面高度相似，但AI生成产物存在难以察觉的严重缺陷模式，具体表现为：需求定义过早、数值估算缺乏依据、以及过度细化倾向。本研究警示SE领域在采纳AI建议反馈时需保持审慎态度——至少对于多用途LLMs生成的内容更应如此。
