# ProLLaMA: A Protein Large Language Model for Multi-Task Protein Language Processing

链接: http://arxiv.org/abs/2402.16445v1

原文摘要:
Large Language Models (LLMs) have achieved remarkable performance in multiple
Natural Language Processing (NLP) tasks. Under the premise that protein
sequences constitute the protein language, Protein Language Models(PLMs) have
advanced the field of protein engineering. However, as of now, unlike LLMs in
NLP, PLMs cannot handle the protein understanding task and the protein
generation task simultaneously in the Protein Language Processing (PLP) field.
This prompts us to delineate the inherent limitations in current PLMs: (i) the
lack of natural language capabilities, (ii) insufficient instruction
understanding, and (iii) high training resource demands. To address these
challenges, we introduce a training framework to transform any general LLM into
a PLM capable of handling multiple PLP tasks. To improve training efficiency,
we propose Protein Vocabulary Pruning (PVP) for general LLMs. We construct a
multi-task instruction dataset containing 13 million samples with superfamily
information, facilitating better modeling of protein sequence-function
landscapes. Through these methods, we develop the ProLLaMA model, the first
known PLM to handle multiple PLP tasks simultaneously. Experiments show that
ProLLaMA achieves state-of-the-art results in the unconditional protein
sequence generation task. In the controllable protein sequence generation task,
ProLLaMA can design novel proteins with desired functionalities. As for the
protein understanding task, ProLLaMA achieves a 62\% exact match rate in
superfamily prediction. Codes, model weights, and datasets are available at
\url{https://github.com/PKU-YuanGroup/ProLLaMA} and
\url{https://huggingface.co/GreatCaptainNemo}.

中文翻译:
大型语言模型（LLM）在多项自然语言处理（NLP）任务中展现出卓越性能。基于蛋白质序列构成"蛋白质语言"的前提，蛋白质语言模型（PLM）推动了蛋白质工程领域的发展。然而目前，与NLP领域的LLM不同，PLM尚无法在蛋白质语言处理（PLP）领域同时胜任蛋白质理解与生成任务。这促使我们揭示当前PLM的固有局限：（1）缺乏自然语言处理能力；（2）指令理解不足；（3）训练资源需求过高。为应对这些挑战，我们提出一个训练框架，可将通用LLM转化为能处理多任务PLP的PLM。通过蛋白质词汇剪枝（PVP）技术提升训练效率，并构建包含1300万条超家族信息样本的多任务指令数据集，以优化蛋白质序列-功能关系的建模。基于此，我们开发了首个能同步处理多PLP任务的ProLLaMA模型。实验表明：在无条件蛋白质序列生成任务中，ProLLaMA达到最先进水平；在可控生成任务中可设计具有特定功能的新蛋白质；在蛋白质理解任务中，超家族预测准确率达62%。相关代码、模型权重及数据集已开源。
