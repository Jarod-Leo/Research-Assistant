# Unmasking Transformers: A Theoretical Approach to Data Recovery via Attention Weights

链接: http://arxiv.org/abs/2310.12462v1

原文摘要:
In the realm of deep learning, transformers have emerged as a dominant
architecture, particularly in natural language processing tasks. However, with
their widespread adoption, concerns regarding the security and privacy of the
data processed by these models have arisen. In this paper, we address a pivotal
question: Can the data fed into transformers be recovered using their attention
weights and outputs? We introduce a theoretical framework to tackle this
problem. Specifically, we present an algorithm that aims to recover the input
data $X \in \mathbb{R}^{d \times n}$ from given attention weights $W = QK^\top
\in \mathbb{R}^{d \times d}$ and output $B \in \mathbb{R}^{n \times n}$ by
minimizing the loss function $L(X)$. This loss function captures the
discrepancy between the expected output and the actual output of the
transformer. Our findings have significant implications for the Localized
Layer-wise Mechanism (LLM), suggesting potential vulnerabilities in the model's
design from a security and privacy perspective. This work underscores the
importance of understanding and safeguarding the internal workings of
transformers to ensure the confidentiality of processed data.

中文翻译:
在深度学习领域，Transformer架构已成为自然语言处理任务的主导模型。然而随着其广泛应用，这类模型所处理数据的安全性与隐私性问题逐渐引发关注。本文致力于解决一个关键问题：能否通过注意力权重和输出来重构Transformer的输入数据？我们提出了解决该问题的理论框架，具体设计了一种算法，旨在通过最小化损失函数\( L(X) \)来从给定的注意力权重\( W = QK^\top \in \mathbb{R}^{d \times d} \)和输出\( B \in \mathbb{R}^{n \times n} \)中恢复输入数据\( X \in \mathbb{R}^{d \times n} \)。该损失函数量化了Transformer预期输出与实际输出之间的差异。我们的研究对局部分层机制（LLM）具有重要启示，从安全与隐私角度揭示了模型设计中可能存在的脆弱性。这项工作强调了理解并保护Transformer内部运作机制对确保数据处理机密性的重要意义。
