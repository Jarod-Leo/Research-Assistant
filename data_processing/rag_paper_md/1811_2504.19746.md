# FineQ: Software-Hardware Co-Design for Low-Bit Fine-Grained Mixed-Precision Quantization of LLMs

链接: http://arxiv.org/abs/2504.19746v1

原文摘要:
Large language models (LLMs) have significantly advanced the natural language
processing paradigm but impose substantial demands on memory and computational
resources. Quantization is one of the most effective ways to reduce memory
consumption of LLMs. However, advanced single-precision quantization methods
experience significant accuracy degradation when quantizing to ultra-low bits.
Existing mixed-precision quantization methods are quantized by groups with
coarse granularity. Employing high precision for group data leads to
substantial memory overhead, whereas low precision severely impacts model
accuracy. To address this issue, we propose FineQ, software-hardware co-design
for low-bit fine-grained mixed-precision quantization of LLMs. First, FineQ
partitions the weights into finer-grained clusters and considers the
distribution of outliers within these clusters, thus achieving a balance
between model accuracy and memory overhead. Then, we propose an outlier
protection mechanism within clusters that uses 3 bits to represent outliers and
introduce an encoding scheme for index and data concatenation to enable aligned
memory access. Finally, we introduce an accelerator utilizing temporal coding
that effectively supports the quantization algorithm while simplifying the
multipliers in the systolic array. FineQ achieves higher model accuracy
compared to the SOTA mixed-precision quantization algorithm at a close average
bit-width. Meanwhile, the accelerator achieves up to 1.79x energy efficiency
and reduces the area of the systolic array by 61.2%.

中文翻译:
大语言模型（LLMs）显著推动了自然语言处理范式的发展，但对内存和计算资源提出了极高要求。量化是降低LLMs内存消耗最有效的方法之一。然而，现有单精度量化方法在超低位宽下会出现显著精度损失，而混合精度量化方法通常采用粗粒度的分组量化策略：组内数据采用高精度会带来巨大内存开销，采用低精度则会严重影响模型精度。针对这一问题，我们提出FineQ——一种面向LLMs低位宽细粒度混合精度量化的软硬件协同设计方案。首先，FineQ将权重划分为更细粒度的聚类单元，并考虑异常值在聚类中的分布特征，从而在模型精度与内存开销间取得平衡；其次，提出聚类内异常值保护机制，采用3比特表示异常值，并引入索引与数据拼接的编码方案以实现对齐内存访问；最后，设计基于时域编码的加速器架构，在有效支持量化算法的同时简化脉动阵列中的乘法器单元。实验表明，在相近平均位宽下，FineQ相比最先进的混合精度量化算法实现了更高的模型精度；所设计的加速器能效最高提升1.79倍，脉动阵列面积减少61.2%。
