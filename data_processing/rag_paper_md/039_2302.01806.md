# Mitigating Data Scarcity for Large Language Models

链接: http://arxiv.org/abs/2302.01806v1

原文摘要:
In recent years, pretrained neural language models (PNLMs) have taken the
field of natural language processing by storm, achieving new benchmarks and
state-of-the-art performances. These models often rely heavily on annotated
data, which may not always be available. Data scarcity are commonly found in
specialized domains, such as medical, or in low-resource languages that are
underexplored by AI research. In this dissertation, we focus on mitigating data
scarcity using data augmentation and neural ensemble learning techniques for
neural language models. In both research directions, we implement neural
network algorithms and evaluate their impact on assisting neural language
models in downstream NLP tasks. Specifically, for data augmentation, we explore
two techniques: 1) creating positive training data by moving an answer span
around its original context and 2) using text simplification techniques to
introduce a variety of writing styles to the original training data. Our
results indicate that these simple and effective solutions improve the
performance of neural language models considerably in low-resource NLP domains
and tasks. For neural ensemble learning, we use a multilabel neural classifier
to select the best prediction outcome from a variety of individual pretrained
neural language models trained for a low-resource medical text simplification
task.

中文翻译:
近年来，预训练神经语言模型（PNLMs）以迅猛之势席卷自然语言处理领域，不断刷新基准测试并实现最先进的性能表现。这类模型通常高度依赖标注数据，然而此类数据并非总能获取。数据稀缺现象在医疗等专业领域以及人工智能研究尚未充分探索的低资源语言中尤为普遍。本论文聚焦于通过数据增强与神经集成学习技术来缓解神经语言模型面临的数据稀缺问题。在这两个研究方向中，我们实现了神经网络算法，并评估了其对辅助神经语言模型完成下游NLP任务的影响。具体而言，在数据增强方面，我们探索了两种技术：1）通过将答案片段在其原始上下文范围内移动来生成正向训练数据；2）运用文本简化技术为原始训练数据引入多样化的文本风格。研究结果表明，这些简洁高效的解决方案显著提升了神经语言模型在低资源NLP领域及任务中的表现。在神经集成学习方面，我们采用多标签神经分类器，从多个针对低资源医疗文本简化任务单独训练的预训练神经语言模型中筛选最优预测结果。
