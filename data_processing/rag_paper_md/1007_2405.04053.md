# Evaluating Text Summaries Generated by Large Language Models Using OpenAI's GPT

链接: http://arxiv.org/abs/2405.04053v1

原文摘要:
This research examines the effectiveness of OpenAI's GPT models as
independent evaluators of text summaries generated by six transformer-based
models from Hugging Face: DistilBART, BERT, ProphetNet, T5, BART, and PEGASUS.
We evaluated these summaries based on essential properties of high-quality
summary - conciseness, relevance, coherence, and readability - using
traditional metrics such as ROUGE and Latent Semantic Analysis (LSA). Uniquely,
we also employed GPT not as a summarizer but as an evaluator, allowing it to
independently assess summary quality without predefined metrics. Our analysis
revealed significant correlations between GPT evaluations and traditional
metrics, particularly in assessing relevance and coherence. The results
demonstrate GPT's potential as a robust tool for evaluating text summaries,
offering insights that complement established metrics and providing a basis for
comparative analysis of transformer-based models in natural language processing
tasks.

中文翻译:
本研究探讨了OpenAI的GPT模型作为独立评估工具的有效性，用于评价六种基于Transformer的Hugging Face模型（DistilBART、BERT、ProphetNet、T5、BART和PEGASUS）生成的文本摘要。我们采用ROUGE和潜在语义分析（LSA）等传统指标，从高质量摘要的核心特性——简洁性、相关性、连贯性和可读性——对这些摘要进行了评估。创新性地，我们还利用GPT模型不作为摘要生成器，而是作为评估者，使其能够不依赖预设指标独立评判摘要质量。分析结果显示，GPT的评估结果与传统指标间存在显著相关性，尤其在评价相关性和连贯性方面。研究结果证明了GPT作为文本摘要评估工具的潜力，其提供的见解既是对现有评估指标的有力补充，也为基于Transformer的自然语言处理模型比较分析提供了新依据。
