# Minimal Time Series Transformer

链接: http://arxiv.org/abs/2503.09791v1

原文摘要:
Transformer is the state-of-the-art model for many natural language
processing, computer vision, and audio analysis problems. Transformer
effectively combines information from the past input and output samples in
auto-regressive manner so that each sample becomes aware of all inputs and
outputs. In sequence-to-sequence (Seq2Seq) modeling, the transformer processed
samples become effective in predicting the next output. Time series forecasting
is a Seq2Seq problem. The original architecture is defined for discrete input
and output sequence tokens, but to adopt it for time series, the model must be
adapted for continuous data. This work introduces minimal adaptations to make
the original transformer architecture suitable for continuous value time series
data.

中文翻译:
Transformer已成为众多自然语言处理、计算机视觉及音频分析任务中的前沿模型。该模型通过自回归方式高效整合历史输入与输出样本信息，使每个样本能感知全部输入输出内容。在序列到序列（Seq2Seq）建模中，经Transformer处理的样本能有效预测后续输出。时间序列预测本质上属于Seq2Seq问题。原始架构虽针对离散输入输出序列标记设计，但为适应时间序列建模需求，必须对模型进行连续数据适配。本研究通过最小化结构调整，成功将原始Transformer架构应用于连续值时间序列数据。
