# Can persistent homology whiten Transformer-based black-box models? A case study on BERT compression

链接: http://arxiv.org/abs/2312.10702v1

原文摘要:
Large Language Models (LLMs) like BERT have gained significant prominence due
to their remarkable performance in various natural language processing tasks.
However, they come with substantial computational and memory costs.
Additionally, they are essentially black-box models, challenging to explain and
interpret. In this article, we propose Optimus BERT Compression and
Explainability (OBCE), a methodology to bring explainability to BERT models
using persistent homology, aiming to measure the importance of each neuron by
studying the topological characteristics of their outputs. As a result, we can
compress BERT significantly by reducing the number of parameters (58.47% of the
original parameters for BERT Base, 52.3% for BERT Large). We evaluated our
methodology on the standard GLUE Benchmark, comparing the results with
state-of-the-art techniques and achieving outstanding results. Consequently,
our methodology can "whiten" BERT models by providing explainability to its
neurons and reducing the model's size, making it more suitable for deployment
on resource-constrained devices.

中文翻译:
诸如BERT之类的大型语言模型(LLM)凭借其在各类自然语言处理任务中的卓越表现而备受瞩目。然而这些模型存在显著的计算与内存开销，且本质上属于黑箱模型，难以进行解释和解读。本文提出Optimus BERT压缩与可解释性框架(OBCE)，通过持续同调技术为BERT模型赋予可解释性，旨在通过分析神经元输出的拓扑特征来评估各神经元重要性。基于此方法，我们能够大幅压缩BERT模型参数量(BERT Base保留58.47%参数，BERT Large保留52.3%)。我们在标准GLUE基准测试中评估了该方法的性能，与当前最先进技术对比取得了优异成果。该框架通过提供神经元可解释性并缩减模型体积，实现了BERT模型的"白盒化"，使其更适合部署在资源受限设备上。
