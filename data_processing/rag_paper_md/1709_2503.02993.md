# Zero-Shot Multi-Label Classification of Bangla Documents: Large Decoders Vs. Classic Encoders

链接: http://arxiv.org/abs/2503.02993v1

原文摘要:
Bangla, a language spoken by over 300 million native speakers and ranked as
the sixth most spoken language worldwide, presents unique challenges in natural
language processing (NLP) due to its complex morphological characteristics and
limited resources. While recent Large Decoder Based models (LLMs), such as GPT,
LLaMA, and DeepSeek, have demonstrated excellent performance across many NLP
tasks, their effectiveness in Bangla remains largely unexplored. In this paper,
we establish the first benchmark comparing decoder-based LLMs with classic
encoder-based models for Zero-Shot Multi-Label Classification (Zero-Shot-MLC)
task in Bangla. Our evaluation of 32 state-of-the-art models reveals that,
existing so-called powerful encoders and decoders still struggle to achieve
high accuracy on the Bangla Zero-Shot-MLC task, suggesting a need for more
research and resources for Bangla NLP.

中文翻译:
孟加拉语作为全球第六大语言，拥有超过3亿母语使用者，其复杂的形态学特征和有限的资源为自然语言处理（NLP）带来了独特挑战。尽管当前基于解码器的大型语言模型（如GPT、LLaMA和DeepSeek）在多项NLP任务中展现出卓越性能，但其对孟加拉语的处理效能仍缺乏系统研究。本文首次建立基准测试，对比基于解码器的大型语言模型与经典编码器模型在孟加拉语零样本多标签分类任务中的表现。通过对32个前沿模型的评估发现，现有所谓强大的编码器和解码器在孟加拉语零样本多标签分类任务中仍难以实现高准确率，这表明孟加拉语NLP领域亟需更多研究投入与资源支持。
