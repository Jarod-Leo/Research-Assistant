# Can Model Fusing Help Transformers in Long Document Classification? An Empirical Study

链接: http://arxiv.org/abs/2307.09532v1

原文摘要:
Text classification is an area of research which has been studied over the
years in Natural Language Processing (NLP). Adapting NLP to multiple domains
has introduced many new challenges for text classification and one of them is
long document classification. While state-of-the-art transformer models provide
excellent results in text classification, most of them have limitations in the
maximum sequence length of the input sequence. The majority of the transformer
models are limited to 512 tokens, and therefore, they struggle with long
document classification problems. In this research, we explore on employing
Model Fusing for long document classification while comparing the results with
well-known BERT and Longformer architectures.

中文翻译:
文本分类是自然语言处理（NLP）领域长期研究的课题。随着NLP技术向多领域拓展，文本分类面临诸多新挑战，其中长文档分类尤为突出。尽管当前最先进的Transformer模型在文本分类任务中表现卓越，但多数模型对输入序列的最大长度存在限制。主流Transformer模型通常仅支持512个标记的输入，这导致其在处理长文档分类问题时面临显著困难。本研究探索了模型融合方法在长文档分类中的应用，并将其与广为人知的BERT和Longformer架构进行了性能对比。
