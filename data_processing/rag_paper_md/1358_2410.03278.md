# What do Large Language Models Need for Machine Translation Evaluation?

链接: http://arxiv.org/abs/2410.03278v1

原文摘要:
Leveraging large language models (LLMs) for various natural language
processing tasks has led to superlative claims about their performance. For the
evaluation of machine translation (MT), existing research shows that LLMs are
able to achieve results comparable to fine-tuned multilingual pre-trained
language models. In this paper, we explore what translation information, such
as the source, reference, translation errors and annotation guidelines, is
needed for LLMs to evaluate MT quality. In addition, we investigate prompting
techniques such as zero-shot, Chain of Thought (CoT) and few-shot prompting for
eight language pairs covering high-, medium- and low-resource languages,
leveraging varying LLM variants. Our findings indicate the importance of
reference translations for an LLM-based evaluation. While larger models do not
necessarily fare better, they tend to benefit more from CoT prompting, than
smaller models. We also observe that LLMs do not always provide a numerical
score when generating evaluations, which poses a question on their reliability
for the task. Our work presents a comprehensive analysis for
resource-constrained and training-less LLM-based evaluation of machine
translation. We release the accrued prompt templates, code and data publicly
for reproducibility.

中文翻译:
以下为英文论文摘要的中文翻译：

利用大语言模型（LLMs）处理各类自然语言处理任务时，学界常对其性能做出极高评价。在机器翻译（MT）评估领域，现有研究表明LLMs能够达到与经过微调的多语言预训练语言模型相媲美的效果。本文系统探究了LLMs评估机器翻译质量所需的信息要素（如源文本、参考译文、翻译错误及标注准则），并针对涵盖高、中、低资源水平的八组语言对，结合不同规模的LLM变体，深入分析了零样本提示、思维链（CoT）提示和少样本提示等技术的应用效果。研究发现：参考译文对基于LLM的评估至关重要；更大规模的模型未必表现更优，但往往能从CoT提示中获得比小模型更显著的提升；同时我们注意到LLMs在生成评估时并不总是输出数值化评分，这对其任务可靠性提出了质疑。本研究为资源受限且无需训练的LLM机器翻译评估提供了全面分析，并公开了积累的提示模板、代码与数据以确保可复现性。

（翻译说明：
1. 专业术语处理：LLMs/Machine Translation等术语保留英文缩写并首次出现时标注全称
2. 长句拆分：将原文复合长句按中文表达习惯分解为多个短句
3. 被动语态转换："it is observed that"等结构转化为主动句式
4. 概念显化："resource-constrained and training-less"译为"资源受限且无需训练"以明确技术特征
5. 学术风格保持：使用"探究""系统分析""显著提升"等符合学术论文表达的措辞
6. 逻辑衔接：通过分号、冒号等标点保持原文论证逻辑的连贯性）
