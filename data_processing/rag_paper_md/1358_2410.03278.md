# What do Large Language Models Need for Machine Translation Evaluation?

链接: http://arxiv.org/abs/2410.03278v1

原文摘要:
Leveraging large language models (LLMs) for various natural language
processing tasks has led to superlative claims about their performance. For the
evaluation of machine translation (MT), existing research shows that LLMs are
able to achieve results comparable to fine-tuned multilingual pre-trained
language models. In this paper, we explore what translation information, such
as the source, reference, translation errors and annotation guidelines, is
needed for LLMs to evaluate MT quality. In addition, we investigate prompting
techniques such as zero-shot, Chain of Thought (CoT) and few-shot prompting for
eight language pairs covering high-, medium- and low-resource languages,
leveraging varying LLM variants. Our findings indicate the importance of
reference translations for an LLM-based evaluation. While larger models do not
necessarily fare better, they tend to benefit more from CoT prompting, than
smaller models. We also observe that LLMs do not always provide a numerical
score when generating evaluations, which poses a question on their reliability
for the task. Our work presents a comprehensive analysis for
resource-constrained and training-less LLM-based evaluation of machine
translation. We release the accrued prompt templates, code and data publicly
for reproducibility.

中文翻译:
利用大型语言模型（LLM）处理各类自然语言处理任务时，其性能表现常被冠以"卓越"的评价。在机器翻译（MT）评估领域，现有研究表明LLM能达到与经过微调的多语言预训练语言模型相当的效果。本文系统探究了LLM评估机器翻译质量所需的信息要素——包括源文本、参考译文、翻译错误及标注规范，并针对涵盖高、中、低资源水平的八种语言对，结合不同规模的LLM变体，深入分析了零样本提示、思维链（CoT）提示和少样本提示等技术的应用效果。研究发现：参考译文对基于LLM的评估至关重要；更大规模的模型未必表现更优，但往往能从CoT提示中获得更大收益；同时观察到LLM在生成评估时并不总是提供量化评分，这对评估可靠性提出了质疑。本研究为资源受限且无需训练的LLM机器翻译评估提供了全面分析，并公开了积累的提示模板、代码及数据以确保可复现性。
