# Large Language Models for Knowledge Graph Embedding Techniques, Methods, and Challenges: A Survey

链接: http://arxiv.org/abs/2501.07766v1

原文摘要:
Large language models (LLMs) have garnered significant attention for their
superior performance in many knowledge-driven applications on the world wide
web.These models are designed to train hundreds of millions or more parameters
on large amounts of text data, enabling them to understand and generate
naturallanguage effectively. As the superior performance of LLMs becomes
apparent,they are increasingly being applied to knowledge graph embedding (KGE)
related tasks to improve the processing results. Traditional KGE representation
learning methods map entities and relations into a low-dimensional vector
space, enablingthe triples in the knowledge graph to satisfy a specific scoring
function in thevector space. However, based on the powerful language
understanding and seman-tic modeling capabilities of LLMs, that have recently
been invoked to varying degrees in different types of KGE related scenarios
such as multi-modal KGE andopen KGE according to their task characteristics. In
this paper, we investigate awide range of approaches for performing
LLMs-related tasks in different types of KGE scenarios. To better compare the
various approaches, we summarize each KGE scenario in a classification.
Finally, we discuss the applications in which the methods are mainly used and
suggest several forward-looking directions for the development of this new
research area.

中文翻译:
大型语言模型（LLMs）因其在全球互联网众多知识驱动应用中的卓越表现而备受关注。这些模型通过海量文本数据训练数亿乃至更多参数，使其能够高效理解与生成自然语言。随着LLMs性能优势的日益凸显，它们正被广泛应用于知识图谱嵌入（KGE）相关任务以提升处理效果。传统KGE表示学习方法将实体与关系映射至低维向量空间，使知识图谱中的三元组在向量空间中满足特定评分函数。而基于LLMs强大的语言理解与语义建模能力，近期研究已根据任务特性，在多模态KGE、开放KGE等不同类型的KGE场景中实现了不同程度的融合应用。本文系统考察了各类KGE场景下LLMs相关任务的实现方法，通过分类框架对各场景研究进行归纳比较，最后探讨了这些方法的主要应用领域，并针对这一新兴研究方向提出了若干前瞻性发展建议。
