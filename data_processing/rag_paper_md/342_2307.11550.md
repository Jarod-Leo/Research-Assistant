# YOLOPose V2: Understanding and Improving Transformer-based 6D Pose Estimation

链接: http://arxiv.org/abs/2307.11550v1

原文摘要:
6D object pose estimation is a crucial prerequisite for autonomous robot
manipulation applications. The state-of-the-art models for pose estimation are
convolutional neural network (CNN)-based. Lately, Transformers, an architecture
originally proposed for natural language processing, is achieving
state-of-the-art results in many computer vision tasks as well. Equipped with
the multi-head self-attention mechanism, Transformers enable simple
single-stage end-to-end architectures for learning object detection and 6D
object pose estimation jointly. In this work, we propose YOLOPose (short form
for You Only Look Once Pose estimation), a Transformer-based multi-object 6D
pose estimation method based on keypoint regression and an improved variant of
the YOLOPose model. In contrast to the standard heatmaps for predicting
keypoints in an image, we directly regress the keypoints. Additionally, we
employ a learnable orientation estimation module to predict the orientation
from the keypoints. Along with a separate translation estimation module, our
model is end-to-end differentiable. Our method is suitable for real-time
applications and achieves results comparable to state-of-the-art methods. We
analyze the role of object queries in our architecture and reveal that the
object queries specialize in detecting objects in specific image regions.
Furthermore, we quantify the accuracy trade-off of using datasets of smaller
sizes to train our model.

中文翻译:
六维物体姿态估计是自主机器人操控应用的关键前提。当前最先进的姿态估计模型基于卷积神经网络（CNN）。近年来，最初为自然语言处理提出的Transformer架构，在众多计算机视觉任务中也取得了领先性能。凭借多头自注意力机制，Transformer能够构建简单的单阶段端到端架构，实现物体检测与六维姿态估计的联合学习。本文提出YOLOPose（You Only Look Once Pose estimation的简称），这是一种基于Transformer的多目标六维姿态估计方法，通过关键点回归实现，并改进了YOLOPose模型的变体。与传统的热力图预测图像关键点不同，我们直接回归关键点坐标。此外，我们采用可学习的朝向估计模块从关键点预测物体朝向，配合独立的平移估计模块，使模型具备端到端可微分特性。该方法适用于实时应用场景，性能与当前最优方法相当。我们分析了物体查询（object queries）在架构中的作用，发现其专门用于检测图像特定区域的物体。同时量化了使用较小规模数据集训练模型时的精度权衡。
