# GET: A Generative EEG Transformer for continuous context-based neural

链接: http://arxiv.org/abs/2406.03115v1

原文摘要:
Generating continuous electroencephalography (EEG) signals through advanced
artificial neural networks presents a novel opportunity to enhance
brain-computer interface (BCI) technology. This capability has the potential to
significantly enhance applications ranging from simulating dynamic brain
activity and data augmentation to improving real-time epilepsy detection and
BCI inference. By harnessing generative transformer neural networks,
specifically designed for EEG signal generation, we can revolutionize the
interpretation and interaction with neural data. Generative AI has demonstrated
significant success across various domains, from natural language processing
(NLP) and computer vision to content creation in visual arts and music. It
distinguishes itself by using large-scale datasets to construct context windows
during pre-training, a technique that has proven particularly effective in NLP,
where models are fine-tuned for specific downstream tasks after extensive
foundational training. However, the application of generative AI in the field
of BCIs, particularly through the development of continuous, context-rich
neural signal generators, has been limited. To address this, we introduce the
Generative EEG Transformer (GET), a model leveraging transformer architecture
tailored for EEG data. The GET model is pre-trained on diverse EEG datasets,
including motor imagery and alpha wave datasets, enabling it to produce
high-fidelity neural signals that maintain contextual integrity. Our empirical
findings indicate that GET not only faithfully reproduces the frequency
spectrum of the training data and input prompts but also robustly generates
continuous neural signals. By adopting the successful training strategies of
the NLP domain for BCIs, the GET sets a new standard for the development and
application of neural signal generation technologies.

中文翻译:
通过先进的人工神经网络生成连续脑电图（EEG）信号，为提升脑机接口（BCI）技术开辟了新途径。这一能力有望显著增强从动态脑活动模拟、数据增强到实时癫痫检测及BCI推理等各类应用。借助专为EEG信号生成设计的生成式Transformer神经网络，我们能够彻底革新神经数据的解读与交互方式。生成式AI已在自然语言处理（NLP）、计算机视觉乃至视觉艺术与音乐内容创作等多个领域取得重大突破，其核心优势在于预训练阶段利用大规模数据集构建上下文窗口——这一技术在NLP领域尤为成功，模型经过广泛基础训练后可针对特定下游任务进行微调。然而，生成式AI在BCI领域的应用，特别是开发具有连续性和丰富上下文特征的神经信号生成器方面仍存在局限。为此，我们提出生成式EEG Transformer（GET）模型，该模型采用专为EEG数据优化的Transformer架构，在运动想象和α波等多个EEG数据集上进行预训练，能够生成保持上下文完整性的高保真神经信号。实验结果表明，GET不仅能精确复现训练数据与输入提示的频谱特征，还能稳定生成连续神经信号。通过将NLP领域的成功训练策略迁移至BCI领域，GET为神经信号生成技术的开发与应用树立了新标杆。
