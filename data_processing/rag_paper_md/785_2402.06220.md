# A Unified Causal View of Instruction Tuning

链接: http://arxiv.org/abs/2402.06220v1

原文摘要:
Instruction tuning on a mixture of tasks has improved zero-shot capabilities
in natural language processing (NLP). Nevertheless, existing methods often
learn features that exhibit correlations between instruction-formatted samples
and target labels, rather than causal relationships. Termed as ``spurious
correlation'' in statistics, such a correlation may change drastically in a new
task, making the effect from the learned features to be misleading. To this
end, we develop a meta Structural Causal Model (meta-SCM) to integrate
different NLP tasks under a single causal structure of the data. Specifically,
the meta-SCM introduces multiple latent factors that represent properties of
source context, only some of which causally influence the target labels for a
specific task. The key idea is to learn task-required causal factors and only
use those to make predictions for a given task. Theoretically, we prove the
causal factor can be identified without mixing information from others. Guided
by the identifiability, we propose a Structural Instruction Tuning (SIT) method
to learn the task-required causal representations that can mimic the causal
factors for each task. The utility of our approach is verified by improvements
of zero-shot ability on a range of unseen datasets and tasks.

中文翻译:
在多任务混合上进行指令调优已提升了自然语言处理（NLP）中的零样本能力。然而，现有方法通常学习到的是指令格式样本与目标标签之间的相关性特征，而非因果关系。这种统计学上称为“伪相关”的关联在新任务中可能发生剧变，导致所学特征产生误导性影响。为此，我们构建了一个元结构因果模型（meta-SCM），将不同NLP任务统一于数据的因果结构框架下。该模型通过引入多个潜在因子来表征源上下文属性，其中仅部分因子对特定任务的目标标签具有因果影响。其核心思想是识别任务所需的因果因子，并仅基于这些因子进行预测。理论上，我们证明了因果因子可在不混杂其他信息的情况下被识别。基于此可识别性，我们提出结构指令调优（SIT）方法，通过学习任务所需的因果表征来模拟各任务的因果因子。实验表明，该方法在一系列未见数据集和任务上的零样本能力提升验证了其有效性。
