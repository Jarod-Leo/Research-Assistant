# ULLME: A Unified Framework for Large Language Model Embeddings with Generation-Augmented Learning

链接: http://arxiv.org/abs/2408.03402v1

原文摘要:
Large Language Models (LLMs) excel in various natural language processing
tasks, but leveraging them for dense passage embedding remains challenging.
This is due to their causal attention mechanism and the misalignment between
their pre-training objectives and the text ranking tasks. Despite some recent
efforts to address these issues, existing frameworks for LLM-based text
embeddings have been limited by their support for only a limited range of LLM
architectures and fine-tuning strategies, limiting their practical application
and versatility. In this work, we introduce the Unified framework for Large
Language Model Embedding (ULLME), a flexible, plug-and-play implementation that
enables bidirectional attention across various LLMs and supports a range of
fine-tuning strategies. We also propose Generation-augmented Representation
Learning (GRL), a novel fine-tuning method to boost LLMs for text embedding
tasks. GRL enforces consistency between representation-based and
generation-based relevance scores, leveraging LLMs' powerful generative
abilities for learning passage embeddings. To showcase our framework's
flexibility and effectiveness, we release three pre-trained models from ULLME
with different backbone architectures, ranging from 1.5B to 8B parameters, all
of which demonstrate strong performance on the Massive Text Embedding
Benchmark. Our framework is publicly available at:
https://github.com/nlp-uoregon/ullme. A demo video for ULLME can also be found
at https://rb.gy/ws1ile.

中文翻译:
大型语言模型（LLMs）在各类自然语言处理任务中表现卓越，但将其应用于密集段落嵌入仍面临挑战。这主要源于其因果注意力机制以及预训练目标与文本排序任务之间的不匹配。尽管近期已有研究尝试解决这些问题，但现有基于LLM的文本嵌入框架因仅支持有限范围的LLM架构和微调策略，制约了其实用性和多功能性。本文提出统一的大型语言模型嵌入框架（ULLME），该即插即用式灵活实现方案支持多种LLM的双向注意力机制，并兼容各类微调策略。我们同时提出生成增强表示学习（GRL）这一创新微调方法，通过强制表示相关性与生成相关性分数的一致性，利用LLM强大的生成能力来优化段落嵌入学习。为展示框架的灵活性与有效性，我们发布了基于不同主干架构（参数量1.5B至8B）的三个ULLME预训练模型，这些模型在Massive文本嵌入基准测试中均展现出强劲性能。本框架已开源于：https://github.com/nlp-uoregon/ullme，演示视频详见：https://rb.gy/ws1ile。
