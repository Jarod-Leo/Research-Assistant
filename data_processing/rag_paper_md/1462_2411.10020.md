# Information Extraction from Clinical Notes: Are We Ready to Switch to Large Language Models?

链接: http://arxiv.org/abs/2411.10020v1

原文摘要:
Backgrounds: Information extraction (IE) is critical in clinical natural
language processing (NLP). While large language models (LLMs) excel on
generative tasks, their performance on extractive tasks remains debated.
Methods: We investigated Named Entity Recognition (NER) and Relation Extraction
(RE) using 1,588 clinical notes from four sources (UT Physicians, MTSamples,
MIMIC-III, and i2b2). We developed an annotated corpus covering 4 clinical
entities and 16 modifiers, and compared instruction-tuned LLaMA-2 and LLaMA-3
against BERT in terms of performance, generalizability, computational
resources, and throughput to BERT. Results: LLaMA models outperformed BERT
across datasets. With sufficient training data, LLaMA showed modest
improvements (1% on NER, 1.5-3.7% on RE); improvements were larger with limited
training data. On unseen i2b2 data, LLaMA-3-70B outperformed BERT by 7% (F1) on
NER and 4% on RE. However, LLaMA models required more computing resources and
ran up to 28 times slower. We implemented "Kiwi," a clinical IE package
featuring both models, available at https://kiwi.clinicalnlp.org/. Conclusion:
This study is among the first to develop and evaluate a comprehensive clinical
IE system using open-source LLMs. Results indicate that LLaMA models outperform
BERT for clinical NER and RE but with higher computational costs and lower
throughputs. These findings highlight that choosing between LLMs and
traditional deep learning methods for clinical IE applications should remain
task-specific, taking into account both performance metrics and practical
considerations such as available computing resources and the intended use case
scenarios.

中文翻译:
背景：信息抽取（IE）在临床自然语言处理（NLP）中至关重要。尽管大语言模型（LLMs）在生成任务上表现出色，但其在抽取任务中的性能仍存在争议。  
方法：我们利用来自四个来源（UT Physicians、MTSamples、MIMIC-III和i2b2）的1,588份临床记录，研究了命名实体识别（NER）和关系抽取（RE）。我们构建了一个标注语料库，涵盖4类临床实体和16种修饰词，并对比了指令调优的LLaMA-2、LLaMA-3与BERT在性能、泛化能力、计算资源消耗及吞吐量方面的表现。  
结果：LLaMA模型在各数据集上均优于BERT。当训练数据充足时，LLaMA的改进幅度较小（NER提升1%，RE提升1.5-3.7%）；而在训练数据有限时改进更显著。在未见过的i2b2数据上，LLaMA-3-70B的NER F1值比BERT高7%，RE高4%。但LLaMA模型需要更多计算资源，运行速度最多慢28倍。我们开发了集成两种模型的临床IE工具包"Kiwi"，发布于https://kiwi.clinicalnlp.org/。  
结论：本研究是首批利用开源LLMs开发并评估综合性临床IE系统的尝试之一。结果表明，LLaMA模型在临床NER和RE任务上优于BERT，但计算成本更高、吞吐量更低。这些发现强调，临床IE应用中LLMs与传统深度学习方法的选择应基于具体任务需求，综合考虑性能指标及计算资源可用性、应用场景等实际因素。
