# TernaryLLM: Ternarized Large Language Model

链接: http://arxiv.org/abs/2406.07177v1

原文摘要:
Large language models (LLMs) have achieved remarkable performance on Natural
Language Processing (NLP) tasks, but they are hindered by high computational
costs and memory requirements. Ternarization, an extreme form of quantization,
offers a solution by reducing memory usage and enabling energy-efficient
floating-point additions. However, applying ternarization to LLMs faces
challenges stemming from outliers in both weights and activations. In this
work, observing asymmetric outliers and non-zero means in weights, we introduce
Dual Learnable Ternarization (DLT), which enables both scales and shifts to be
learnable. We also propose Outlier-Friendly Feature Knowledge Distillation
(OFF) to recover the information lost in extremely low-bit quantization. The
proposed OFF can incorporate semantic information and is insensitive to
outliers. At the core of OFF is maximizing the mutual information between
features in ternarized and floating-point models using cosine similarity.
Extensive experiments demonstrate that our TernaryLLM surpasses previous
low-bit quantization methods on the standard text generation and zero-shot
benchmarks for different LLM families. Specifically, for one of the most
powerful open-source models, LLaMA-3, our approach (W1.58A16) outperforms the
previous state-of-the-art method (W2A16) by 5.8 in terms of perplexity on C4
and by 8.2% in terms of average accuracy on zero-shot tasks.

中文翻译:
大语言模型（LLM）在自然语言处理（NLP）任务中展现出卓越性能，但其高昂的计算成本与内存需求制约了实际应用。三值化作为极端的量化形式，通过降低内存占用并支持高能效浮点加法，为解决这一问题提供了可能。然而，权重与激活值中的异常值导致LLM三值化面临严峻挑战。本研究通过观察权重中非对称异常值与非零均值现象，提出双可学习三值化（DLT）方法，使缩放因子与偏移量均具备可学习性。同时，我们设计了异常值友好的特征知识蒸馏（OFF）框架，以恢复极低位宽量化丢失的信息。该框架能融合语义特征且对异常值不敏感，其核心在于利用余弦相似度最大化三值化模型与浮点模型特征间的互信息。大量实验表明，针对不同系列的LLM模型，TernaryLLM在标准文本生成与零样本评测基准上均超越以往低位宽量化方法。以最强开源模型LLaMA-3为例，我们的方法（W1.58A16）在C4数据集困惑度指标上较此前最优方法（W2A16）提升5.8，在零样本任务平均准确率上提升8.2%。
