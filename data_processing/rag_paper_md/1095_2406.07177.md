# TernaryLLM: Ternarized Large Language Model

链接: http://arxiv.org/abs/2406.07177v1

原文摘要:
Large language models (LLMs) have achieved remarkable performance on Natural
Language Processing (NLP) tasks, but they are hindered by high computational
costs and memory requirements. Ternarization, an extreme form of quantization,
offers a solution by reducing memory usage and enabling energy-efficient
floating-point additions. However, applying ternarization to LLMs faces
challenges stemming from outliers in both weights and activations. In this
work, observing asymmetric outliers and non-zero means in weights, we introduce
Dual Learnable Ternarization (DLT), which enables both scales and shifts to be
learnable. We also propose Outlier-Friendly Feature Knowledge Distillation
(OFF) to recover the information lost in extremely low-bit quantization. The
proposed OFF can incorporate semantic information and is insensitive to
outliers. At the core of OFF is maximizing the mutual information between
features in ternarized and floating-point models using cosine similarity.
Extensive experiments demonstrate that our TernaryLLM surpasses previous
low-bit quantization methods on the standard text generation and zero-shot
benchmarks for different LLM families. Specifically, for one of the most
powerful open-source models, LLaMA-3, our approach (W1.58A16) outperforms the
previous state-of-the-art method (W2A16) by 5.8 in terms of perplexity on C4
and by 8.2% in terms of average accuracy on zero-shot tasks.

中文翻译:
以下是符合要求的学术论文摘要中文翻译：

大语言模型（LLMs）在自然语言处理任务中展现出卓越性能，但其发展受限于高昂的计算成本与内存需求。三值化作为极端的量化形式，能有效降低内存占用并实现高能效的浮点加法运算。然而，权重和激活值中的异常值使得三值化在LLMs中的应用面临挑战。本研究通过观察权重中存在的非对称异常值和非零均值现象，提出双可学习三值化（DLT）方法，使缩放因子和偏移量均具备可学习性。同时，我们设计了异常值友好的特征知识蒸馏（OFF）框架，通过最大化三值化模型与浮点模型特征间的余弦相似度来恢复极低位量化丢失的信息。该框架能有效融合语义特征且对异常值不敏感。大量实验表明，针对不同系列的LLM模型，我们的TernaryLLM方法在标准文本生成和零样本基准测试中均超越以往低位量化方法。特别地，对于当前最强大的开源模型LLaMA-3，我们的方法（W1.58A16）在C4数据集上的困惑度比先前最优方法（W2A16）降低5.8，在零样本任务平均准确率上提升8.2%。

（注：W1.58A16/W2A16等专业符号保留原格式，符合学术规范；采用"三值化""困惑度"等标准术语；通过拆分英文长句为中文短句结构；保持被动语态与客观表述；精确传达技术细节）
