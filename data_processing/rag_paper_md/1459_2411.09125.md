# DROJ: A Prompt-Driven Attack against Large Language Models

链接: http://arxiv.org/abs/2411.09125v1

原文摘要:
Large Language Models (LLMs) have demonstrated exceptional capabilities
across various natural language processing tasks. Due to their training on
internet-sourced datasets, LLMs can sometimes generate objectionable content,
necessitating extensive alignment with human feedback to avoid such outputs.
Despite massive alignment efforts, LLMs remain susceptible to adversarial
jailbreak attacks, which usually are manipulated prompts designed to circumvent
safety mechanisms and elicit harmful responses. Here, we introduce a novel
approach, Directed Rrepresentation Optimization Jailbreak (DROJ), which
optimizes jailbreak prompts at the embedding level to shift the hidden
representations of harmful queries towards directions that are more likely to
elicit affirmative responses from the model. Our evaluations on LLaMA-2-7b-chat
model show that DROJ achieves a 100\% keyword-based Attack Success Rate (ASR),
effectively preventing direct refusals. However, the model occasionally
produces repetitive and non-informative responses. To mitigate this, we
introduce a helpfulness system prompt that enhances the utility of the model's
responses. Our code is available at
https://github.com/Leon-Leyang/LLM-Safeguard.

中文翻译:
大型语言模型（LLMs）在各类自然语言处理任务中展现出卓越能力。由于训练数据源自互联网，这些模型偶尔会生成不当内容，因此需通过大量人类反馈进行对齐以避免此类输出。尽管已投入巨大对齐努力，LLMs仍易受对抗性越狱攻击影响——这类攻击通常通过精心设计的提示词绕过安全机制，诱导有害响应。本文提出创新方法"定向表征优化越狱"（DROJ），在嵌入层面对越狱提示进行优化，将有害查询的隐藏表征向更易获得模型肯定回应的方向偏移。在LLaMA-2-7b-chat模型上的评估显示，DROJ实现了100%基于关键词的攻击成功率，有效避免了直接拒绝响应。但模型有时会产生重复且无实质内容的回答，为此我们引入辅助性系统提示来提升响应实用性。相关代码已开源。
