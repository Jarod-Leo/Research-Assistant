# Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning

链接: http://arxiv.org/abs/2402.06619v1

原文摘要:
Datasets are foundational to many breakthroughs in modern artificial
intelligence. Many recent achievements in the space of natural language
processing (NLP) can be attributed to the finetuning of pre-trained models on a
diverse set of tasks that enables a large language model (LLM) to respond to
instructions. Instruction fine-tuning (IFT) requires specifically constructed
and annotated datasets. However, existing datasets are almost all in the
English language. In this work, our primary goal is to bridge the language gap
by building a human-curated instruction-following dataset spanning 65
languages. We worked with fluent speakers of languages from around the world to
collect natural instances of instructions and completions. Furthermore, we
create the most extensive multilingual collection to date, comprising 513
million instances through templating and translating existing datasets across
114 languages. In total, we contribute four key resources: we develop and
open-source the Aya Annotation Platform, the Aya Dataset, the Aya Collection,
and the Aya Evaluation Suite. The Aya initiative also serves as a valuable case
study in participatory research, involving collaborators from 119 countries. We
see this as a valuable framework for future research collaborations that aim to
bridge gaps in resources.

中文翻译:
数据集是现代人工智能众多突破性进展的基石。自然语言处理（NLP）领域近年来的许多成就，可归因于通过多样化任务对预训练模型进行微调，使大语言模型（LLM）能够响应指令。指令微调（IFT）需要专门构建和标注的数据集，但现有数据集几乎全部为英文。本研究的主要目标是通过构建一个涵盖65种语言的人工标注指令跟随数据集来弥合语言鸿沟。我们与全球各地语言流利者合作，收集了自然场景下的指令及完成实例。此外，通过模板化和翻译现有数据集，我们创建了迄今最广泛的多语言集合，覆盖114种语言共5.13亿条实例。我们贡献了四大核心资源：开发并开源了Aya标注平台、Aya数据集、Aya集合和Aya评估套件。Aya项目还作为参与式研究的典型案例，汇集了来自119个国家的研究者协作，为未来旨在填补资源缺口的研究合作提供了宝贵框架。
