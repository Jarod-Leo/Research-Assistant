# Analyzing the Role of Semantic Representations in the Era of Large Language Models

链接: http://arxiv.org/abs/2405.01502v1

原文摘要:
Traditionally, natural language processing (NLP) models often use a rich set
of features created by linguistic expertise, such as semantic representations.
However, in the era of large language models (LLMs), more and more tasks are
turned into generic, end-to-end sequence generation problems. In this paper, we
investigate the question: what is the role of semantic representations in the
era of LLMs? Specifically, we investigate the effect of Abstract Meaning
Representation (AMR) across five diverse NLP tasks. We propose an AMR-driven
chain-of-thought prompting method, which we call AMRCoT, and find that it
generally hurts performance more than it helps. To investigate what AMR may
have to offer on these tasks, we conduct a series of analysis experiments. We
find that it is difficult to predict which input examples AMR may help or hurt
on, but errors tend to arise with multi-word expressions, named entities, and
in the final inference step where the LLM must connect its reasoning over the
AMR to its prediction. We recommend focusing on these areas for future work in
semantic representations for LLMs. Our code:
https://github.com/causalNLP/amr_llm.

中文翻译:
传统上，自然语言处理（NLP）模型常依赖语言学专家构建的丰富特征集，例如语义表示。然而，在大语言模型（LLM）时代，越来越多的任务被转化为通用的端到端序列生成问题。本文探讨了一个核心问题：语义表示在LLM时代扮演何种角色？我们以抽象意义表示（AMR）为研究对象，在五项多样化NLP任务中展开实验。通过提出AMR驱动的思维链提示方法AMRCoT，发现其带来的性能损害普遍多于提升。为探究AMR的潜在价值，我们进行了一系列分析实验，发现难以预判AMR对具体输入样例的助益或损害，但错误多集中于多词表达式、命名实体，以及LLM需将AMR推理与最终预测相衔接的关键推断环节。建议未来语义表示研究重点关注这些方向。代码已开源：https://github.com/causalNLP/amr_llm。
