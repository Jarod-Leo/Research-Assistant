# Knowledge Transfer from Pre-trained Language Models to Cif-based Speech Recognizers via Hierarchical Distillation

链接: http://arxiv.org/abs/2301.13003v1

原文摘要:
Large-scale pre-trained language models (PLMs) have shown great potential in
natural language processing tasks. Leveraging the capabilities of PLMs to
enhance automatic speech recognition (ASR) systems has also emerged as a
promising research direction. However, previous works may be limited by the
inflexible structures of PLMs and the insufficient utilization of PLMs. To
alleviate these problems, we propose the hierarchical knowledge distillation
(HKD) on the continuous integrate-and-fire (CIF) based ASR models. To transfer
knowledge from PLMs to the ASR models, HKD employs cross-modal knowledge
distillation with contrastive loss at the acoustic level and knowledge
distillation with regression loss at the linguistic level. Compared with the
original CIF-based model, our method achieves 15% and 9% relative error rate
reduction on the AISHELL-1 and LibriSpeech datasets, respectively.

中文翻译:
大规模预训练语言模型（PLMs）在自然语言处理任务中展现出巨大潜力。利用PLMs的能力来增强自动语音识别（ASR）系统，已成为一个颇具前景的研究方向。然而，先前的研究可能受限于PLMs结构灵活性不足及其潜能未充分挖掘的问题。为缓解这些局限性，我们提出在基于连续积分触发（CIF）的ASR模型上实施分层知识蒸馏（HKD）。该方法通过声学层面的对比损失跨模态知识蒸馏和语言层面的回归损失知识蒸馏，将PLMs的知识迁移至ASR模型。实验表明，相较于原始CIF模型，我们的方法在AISHELL-1和LibriSpeech数据集上分别实现了15%和9%的相对错误率下降。

（翻译说明：
1. 专业术语采用学术界通用译法，如"knowledge distillation"译为"知识蒸馏"
2. 长句按中文习惯切分为短句，如原文第二句拆分为两个分句
3. 被动语态转换为主动表达，如"has been limited"译为"受限于"
4. 技术概念保持准确性与可读性平衡，如"CIF"首次出现时保留英文缩写并标注全称
5. 数据呈现方式符合中文科技论文规范，使用"分别"引导并列数据
6. 保持学术文本的客观严谨风格，避免口语化表达）
