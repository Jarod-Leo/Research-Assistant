# Knowledge Transfer from Pre-trained Language Models to Cif-based Speech Recognizers via Hierarchical Distillation

链接: http://arxiv.org/abs/2301.13003v1

原文摘要:
Large-scale pre-trained language models (PLMs) have shown great potential in
natural language processing tasks. Leveraging the capabilities of PLMs to
enhance automatic speech recognition (ASR) systems has also emerged as a
promising research direction. However, previous works may be limited by the
inflexible structures of PLMs and the insufficient utilization of PLMs. To
alleviate these problems, we propose the hierarchical knowledge distillation
(HKD) on the continuous integrate-and-fire (CIF) based ASR models. To transfer
knowledge from PLMs to the ASR models, HKD employs cross-modal knowledge
distillation with contrastive loss at the acoustic level and knowledge
distillation with regression loss at the linguistic level. Compared with the
original CIF-based model, our method achieves 15% and 9% relative error rate
reduction on the AISHELL-1 and LibriSpeech datasets, respectively.

中文翻译:
大规模预训练语言模型（PLMs）在自然语言处理任务中展现出巨大潜力。利用PLMs的能力提升自动语音识别（ASR）系统已成为一个颇具前景的研究方向。然而，先前工作可能受限于PLMs结构灵活性不足及模型利用不充分的问题。为此，我们提出基于连续积分触发（CIF）的ASR模型分层知识蒸馏（HKD）方法。该方法通过声学层面的对比损失跨模态知识蒸馏和语言层面的回归损失知识蒸馏，实现PLMs向ASR模型的知识迁移。实验表明，相较于原始CIF模型，我们的方法在AISHELL-1和LibriSpeech数据集上分别实现15%和9%的相对错误率下降。
