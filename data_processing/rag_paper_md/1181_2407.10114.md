# TokenSHAP: Interpreting Large Language Models with Monte Carlo Shapley Value Estimation

链接: http://arxiv.org/abs/2407.10114v1

原文摘要:
As large language models (LLMs) become increasingly prevalent in critical
applications, the need for interpretable AI has grown. We introduce TokenSHAP,
a novel method for interpreting LLMs by attributing importance to individual
tokens or substrings within input prompts. This approach adapts Shapley values
from cooperative game theory to natural language processing, offering a
rigorous framework for understanding how different parts of an input contribute
to a model's response. TokenSHAP leverages Monte Carlo sampling for
computational efficiency, providing interpretable, quantitative measures of
token importance. We demonstrate its efficacy across diverse prompts and LLM
architectures, showing consistent improvements over existing baselines in
alignment with human judgments, faithfulness to model behavior, and
consistency.
  Our method's ability to capture nuanced interactions between tokens provides
valuable insights into LLM behavior, enhancing model transparency, improving
prompt engineering, and aiding in the development of more reliable AI systems.
TokenSHAP represents a significant step towards the necessary interpretability
for responsible AI deployment, contributing to the broader goal of creating
more transparent, accountable, and trustworthy AI systems.

中文翻译:
随着大语言模型（LLMs）在关键应用中的日益普及，对可解释人工智能的需求不断增长。我们提出TokenSHAP这一创新方法，通过量化输入提示中单个词元或子字符串的重要性来解释LLMs。该方法将合作博弈论中的沙普利值适配至自然语言处理领域，为理解输入各部分如何影响模型响应提供了严谨框架。TokenSHAP采用蒙特卡洛采样提升计算效率，生成可解释的、定量化的词元重要性度量。我们通过多样化提示和不同架构的LLM验证其有效性，结果显示该方法在人类判断一致性、模型行为忠实度和结果稳定性方面均优于现有基线方法。

该方法能捕捉词元间微妙交互作用的特点，为理解LLM行为提供了宝贵洞见，有助于增强模型透明度、优化提示工程，并推动开发更可靠的AI系统。TokenSHAP标志着向负责任AI部署所需可解释性迈出的重要一步，对实现更透明、可问责且可信赖的AI系统这一宏大目标具有积极贡献。
