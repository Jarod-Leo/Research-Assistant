# Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL through Workflow Paradigm

链接: http://arxiv.org/abs/2402.10671v1

原文摘要:
In-context learning of large-language models (LLMs) has achieved remarkable
success in the field of natural language processing, while extensive case
studies reveal that the single-step chain-of-thought prompting approach faces
challenges such as attention diffusion and inadequate performance in complex
tasks like text-to-SQL. To improve the contextual learning capabilities of LLMs
in text-to-SQL, a workflow paradigm method is proposed, aiming to enhance the
attention and problem-solving scope of LLMs through decomposition.
Specifically, the information determination module for eliminating redundant
information and the brand-new prompt structure based on problem classification
greatly enhance the model's attention. Additionally, the inclusion of
self-correction and active learning modules greatly expands the problem-solving
scope of LLMs, hence improving the upper limit of LLM-based approaches.
Extensive experiments conducted on three datasets demonstrate that our approach
outperforms other methods by a significant margin. About 2-3 percentage point
improvements compared to the existing baseline on the Spider Dev,
Spider-Realistic, and Bird Dev datasets and new SOTA results on the Spider Test
dataset are achieved. Our code is available on GitHub:
\url{https://github.com/FlyingFeather/DEA-SQL}.

中文翻译:
大型语言模型（LLM）的上下文学习在自然语言处理领域取得了显著成功，然而大量案例研究表明，单步思维链提示方法面临注意力分散及在文本到SQL等复杂任务中表现不足的挑战。为提升LLM在文本到SQL任务中的上下文学习能力，本文提出一种工作流范式方法，旨在通过任务分解增强模型的注意力与问题解决范围。具体而言，用于消除冗余信息的信息判定模块和基于问题分类的全新提示结构显著提升了模型注意力；同时，自校正与主动学习模块的引入极大拓展了LLM的问题解决边界，从而提升了基于LLM方法的上限性能。在三个数据集上的大量实验表明，本方法以显著优势超越其他方法：在Spider Dev、Spider-Realistic和Bird Dev数据集上较现有基线提升约2-3个百分点，并在Spider Test数据集上创造了新的SOTA结果。代码已开源：\url{https://github.com/FlyingFeather/DEA-SQL}。
