# Reconsidering Token Embeddings with the Definitions for Pre-trained Language Models

链接: http://arxiv.org/abs/2408.01308v1

原文摘要:
Learning token embeddings based on token co-occurrence statistics has proven
effective for both pre-training and fine-tuning in natural language processing.
However, recent studies have pointed out that the distribution of learned
embeddings degenerates into anisotropy (i.e., non-uniform distribution), and
even pre-trained language models (PLMs) suffer from a loss of semantics-related
information in embeddings for low-frequency tokens. This study first analyzes
the fine-tuning dynamics of encoder-based PLMs and demonstrates their
robustness against degeneration. On the basis of this analysis, we propose
DefinitionEMB, a method that utilizes definitions to re-construct isotropically
distributed and semantics-related token embeddings for encoder-based PLMs while
maintaining original robustness during fine-tuning. Our experiments demonstrate
the effectiveness of leveraging definitions from Wiktionary to re-construct
such embeddings for two encoder-based PLMs: RoBERTa-base and BART-large.
Furthermore, the re-constructed embeddings for low-frequency tokens improve the
performance of these models across various GLUE and four text summarization
datasets.

中文翻译:
基于词元共现统计学习词元嵌入的方法，在自然语言处理的预训练与微调阶段均被证实有效。然而，近期研究指出，习得嵌入的分布会退化为各向异性（即非均匀分布），且即使是预训练语言模型（PLMs），低频词元的嵌入也会丢失语义相关信息。本研究首先分析了基于编码器的PLMs在微调过程中的动态特性，验证了其抗退化鲁棒性。基于此分析，我们提出DefinitionEMB方法，通过利用词典定义重构具有各向同性分布且保留语义相关性的词元嵌入，同时保持微调过程中原有的鲁棒性。实验表明，利用维基词典定义重构的嵌入对RoBERTa-base和BART-large两种基于编码器的PLMs具有显著效果。重构后的低频词元嵌入进一步提升了这些模型在GLUE基准测试和四个文本摘要数据集上的性能表现。
