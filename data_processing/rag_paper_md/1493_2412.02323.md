# Pay Attention to the Robustness of Chinese Minority Language Models! Syllable-level Textual Adversarial Attack on Tibetan Script

链接: http://arxiv.org/abs/2412.02323v1

原文摘要:
The textual adversarial attack refers to an attack method in which the
attacker adds imperceptible perturbations to the original texts by elaborate
design so that the NLP (natural language processing) model produces false
judgments. This method is also used to evaluate the robustness of NLP models.
Currently, most of the research in this field focuses on English, and there is
also a certain amount of research on Chinese. However, to the best of our
knowledge, there is little research targeting Chinese minority languages.
Textual adversarial attacks are a new challenge for the information processing
of Chinese minority languages. In response to this situation, we propose a
Tibetan syllable-level black-box textual adversarial attack called TSAttacker
based on syllable cosine distance and scoring mechanism. And then, we conduct
TSAttacker on six models generated by fine-tuning two PLMs (pre-trained
language models) for three downstream tasks. The experiment results show that
TSAttacker is effective and generates high-quality adversarial samples. In
addition, the robustness of the involved models still has much room for
improvement.

中文翻译:
文本对抗攻击是指攻击者通过精心设计在原始文本中添加不易察觉的扰动，使NLP（自然语言处理）模型产生错误判断的攻击方法，该方法也用于评估NLP模型的鲁棒性。目前该领域的研究大多针对英语，对汉语也有一定研究，但据我们所知，针对中国少数民族语言的研究较少。文本对抗攻击是中国少数民族语言信息处理面临的新挑战。针对这一现状，我们提出了一种基于音节余弦距离和评分机制的藏语音节级黑盒文本对抗攻击方法TSAttacker。然后，我们在两个PLM（预训练语言模型）针对三个下游任务微调生成的六个模型上进行了TSAttacker实验。实验结果表明，TSAttacker是有效的，并且生成了高质量的对抗样本。此外，所涉及模型的鲁棒性仍有很大的提升空间。
