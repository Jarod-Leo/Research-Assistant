# Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks

链接: http://arxiv.org/abs/2401.02731v1

原文摘要:
Large language models (LLMs) have demonstrated considerable proficiency in
general natural language processing (NLP) tasks. Instruction tuning, a
successful paradigm, enhances the ability of LLMs to follow natural language
instructions and exhibit robust generalization across general tasks. However,
these models often encounter performance limitations across multiple tasks due
to constrained model capacity. Expanding this capacity during the instruction
tuning phase poses significant challenges. To address this issue, we introduce
parameter-efficient sparsity crafting (PESC), which crafts dense models into
sparse models using the mixture-of-experts (MoE) architecture. PESC integrates
adapters into the MoE layers of sparse models, differentiating experts without
altering the individual weights within these layers. This method significantly
reduces computational costs and GPU memory requirements, facilitating model
capacity expansion through a minimal parameter increase when guaranteeing the
quality of approximation in function space compared to original sparse
upcycling. Our empirical evaluation demonstrates the effectiveness of the PESC
method. Using PESC during instruction tuning, our best sparse model outperforms
other sparse and dense models and exhibits superior general capabilities
compared to GPT-3.5. Our code is available at
https://github.com/wuhy68/Parameter-Efficient-MoE.

中文翻译:
以下是符合学术规范的中文翻译：

大型语言模型（LLMs）在通用自然语言处理（NLP）任务中已展现出显著能力。指令微调作为一种成功范式，可增强LLMs遵循自然语言指令的能力，并在通用任务中表现出强大的泛化性。然而，由于模型容量受限，这些模型在多任务场景下常面临性能瓶颈。在指令微调阶段直接扩展模型容量存在重大挑战。为此，我们提出参数高效稀疏化构建方法（PESC），该方法基于混合专家（MoE）架构将稠密模型转化为稀疏模型。PESC将适配器集成到稀疏模型的MoE层中，在不改变专家层内部权重的前提下实现专家差异化。相较于原始稀疏升级方法，该方法在保证函数空间近似质量的同时，通过最小化参数增长显著降低计算成本和GPU内存需求，从而有效扩展模型容量。实证评估表明PESC方法具有显著优势：采用PESC进行指令微调时，我们的最优稀疏模型不仅超越其他稀疏与稠密模型，其综合能力更优于GPT-3.5。代码已开源：https://github.com/wuhy68/Parameter-Efficient-MoE。

（翻译严格遵循以下原则：
1. 专业术语准确统一："instruction tuning"译为"指令微调"、"Mixture-of-Experts"保留专业缩写"MoE"并首次出现标注全称
2. 被动语态转化：将英文被动结构转换为中文主动表述（如"are integrated"译为"集成"）
3. 长句拆分重组：将原文复合句按中文表达习惯分解为多个短句
4. 逻辑显化：通过"为此"、"从而"等连接词明确行文逻辑
5. 学术规范：保留技术术语缩写格式（如PESC首次出现标注全称）和代码库链接格式）
