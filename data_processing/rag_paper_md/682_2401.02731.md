# Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks

链接: http://arxiv.org/abs/2401.02731v1

原文摘要:
Large language models (LLMs) have demonstrated considerable proficiency in
general natural language processing (NLP) tasks. Instruction tuning, a
successful paradigm, enhances the ability of LLMs to follow natural language
instructions and exhibit robust generalization across general tasks. However,
these models often encounter performance limitations across multiple tasks due
to constrained model capacity. Expanding this capacity during the instruction
tuning phase poses significant challenges. To address this issue, we introduce
parameter-efficient sparsity crafting (PESC), which crafts dense models into
sparse models using the mixture-of-experts (MoE) architecture. PESC integrates
adapters into the MoE layers of sparse models, differentiating experts without
altering the individual weights within these layers. This method significantly
reduces computational costs and GPU memory requirements, facilitating model
capacity expansion through a minimal parameter increase when guaranteeing the
quality of approximation in function space compared to original sparse
upcycling. Our empirical evaluation demonstrates the effectiveness of the PESC
method. Using PESC during instruction tuning, our best sparse model outperforms
other sparse and dense models and exhibits superior general capabilities
compared to GPT-3.5. Our code is available at
https://github.com/wuhy68/Parameter-Efficient-MoE.

中文翻译:
大型语言模型（LLMs）在通用自然语言处理（NLP）任务中展现出卓越能力。指令调优作为一种成功范式，显著提升了LLMs遵循自然语言指令的能力，并在通用任务中表现出强大的泛化性。然而，受限于模型容量，这些模型在多任务场景下常面临性能瓶颈。在指令调优阶段直接扩展模型容量存在重大挑战。为此，我们提出参数高效稀疏构建方法（PESC），通过混合专家（MoE）架构将稠密模型转化为稀疏模型。PESC在稀疏模型的MoE层中集成适配器，在不改变专家层内部权重的前提下实现专家差异化。相较于原始稀疏升级方法，该方法在保证函数空间近似质量的同时，大幅降低计算成本和GPU内存需求，仅需极小参数量增长即可实现模型容量扩展。实证评估表明，PESC方法在指令调优阶段表现优异：我们构建的最佳稀疏模型不仅超越其他稀疏与稠密模型，其通用能力更优于GPT-3.5。代码已开源于https://github.com/wuhy68/Parameter-Efficient-MoE。
