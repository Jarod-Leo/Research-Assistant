# Dynamic data sampler for cross-language transfer learning in large language models

链接: http://arxiv.org/abs/2405.10626v1

原文摘要:
Large Language Models (LLMs) have gained significant attention in the field
of natural language processing (NLP) due to their wide range of applications.
However, training LLMs for languages other than English poses significant
challenges, due to the difficulty in acquiring large-scale corpus and the
requisite computing resources. In this paper, we propose ChatFlow, a
cross-language transfer-based LLM, to address these challenges and train large
Chinese language models in a cost-effective manner. We employ a mix of Chinese,
English, and parallel corpus to continuously train the LLaMA2 model, aiming to
align cross-language representations and facilitate the knowledge transfer
specifically to the Chinese language model. In addition, we use a dynamic data
sampler to progressively transition the model from unsupervised pre-training to
supervised fine-tuning. Experimental results demonstrate that our approach
accelerates model convergence and achieves superior performance. We evaluate
ChatFlow on popular Chinese and English benchmarks, the results indicate that
it outperforms other Chinese models post-trained on LLaMA-2-7B.

中文翻译:
大语言模型（LLMs）因其广泛的应用在自然语言处理（NLP）领域备受关注。然而，针对非英语语言训练LLMs面临重大挑战，主要源于大规模语料库和必要计算资源的获取难度。本文提出ChatFlow——一种基于跨语言迁移的LLM解决方案，以经济高效的方式训练大型中文语言模型。我们采用中文、英文及平行语料混合训练策略，对LLaMA2模型进行持续训练，旨在实现跨语言表征对齐并促进知识向中文模型的定向迁移。此外，通过动态数据采样器逐步将模型从无监督预训练过渡到有监督微调。实验结果表明，该方法能加速模型收敛并取得更优性能。我们在主流中英文基准测试上评估ChatFlow，结果显示其性能优于其他基于LLaMA-2-7B进行后训练的中文模型。
