# Research on Key Technologies for Cross-Cloud Federated Training of Large Language Models

链接: http://arxiv.org/abs/2410.19130v1

原文摘要:
With the rapid development of natural language processing technology, large
language models have demonstrated exceptional performance in various
application scenarios. However, training these models requires significant
computational resources and data processing capabilities. Cross-cloud federated
training offers a new approach to addressing the resource bottlenecks of a
single cloud platform, allowing the computational resources of multiple clouds
to collaboratively complete the training tasks of large models. This study
analyzes the key technologies of cross-cloud federated training, including data
partitioning and distribution, communication optimization, model aggregation
algorithms, and the compatibility of heterogeneous cloud platforms.
Additionally, the study examines data security and privacy protection
strategies in cross-cloud training, particularly the application of data
encryption and differential privacy techniques. Through experimental
validation, the proposed technical framework demonstrates enhanced training
efficiency, ensured data security, and reduced training costs, highlighting the
broad application prospects of cross-cloud federated training.

中文翻译:
随着自然语言处理技术的快速发展，大语言模型在各类应用场景中展现出卓越性能。然而训练此类模型需要消耗大量计算资源与数据处理能力，跨云联邦训练为解决单一云平台资源瓶颈提供了新思路，通过多云协同的计算资源共同完成大模型训练任务。本研究分析了跨云联邦训练中的关键技术，包括数据分片与分发、通信优化、模型聚合算法以及异构云平台兼容性等问题。同时探讨了跨云训练中的数据安全与隐私保护策略，特别是数据加密与差分隐私技术的应用。通过实验验证，所提技术框架能有效提升训练效率、保障数据安全并降低训练成本，展现了跨云联邦训练的广阔应用前景。
