# IAA: Inner-Adaptor Architecture Empowers Frozen Large Language Model with Multimodal Capabilities

链接: http://arxiv.org/abs/2408.12902v1

原文摘要:
In the field of multimodal large language models (MLLMs), common methods
typically involve unfreezing the language model during training to foster
profound visual understanding. However, the fine-tuning of such models with
vision-language data often leads to a diminution of their natural language
processing (NLP) capabilities. To avoid this performance degradation, a
straightforward solution is to freeze the language model while developing
multimodal competencies. Unfortunately, previous works have not attained
satisfactory outcomes. Building on the strategy of freezing the language model,
we conduct thorough structural exploration and introduce the Inner-Adaptor
Architecture (IAA). Specifically, the architecture incorporates multiple
multimodal adaptors at varying depths within the large language model to
facilitate direct interaction with the inherently text-oriented transformer
layers, thereby enabling the frozen language model to acquire multimodal
capabilities. Unlike previous approaches of freezing language models that
require large-scale aligned data, our proposed architecture is able to achieve
superior performance on small-scale datasets. We conduct extensive experiments
to improve the general multimodal capabilities and visual grounding abilities
of the MLLM. Our approach remarkably outperforms previous state-of-the-art
methods across various vision-language benchmarks without sacrificing
performance on NLP tasks. Code and models are available at
https://github.com/360CVGroup/Inner-Adaptor-Architecture.

中文翻译:
在多模态大语言模型（MLLMs）研究领域，现有方法通常会在训练过程中解冻语言模型以促进深层次的视觉理解。然而，使用视觉-语言数据对此类模型进行微调时，往往会导致其自然语言处理（NLP）能力下降。为避免这种性能衰退，最直接的解决方案是在开发多模态能力时冻结语言模型参数。遗憾的是，先前研究尚未取得令人满意的成果。基于冻结语言模型的策略，我们通过深入的结构探索提出了内部适配器架构（Inner-Adaptor Architecture, IAA）。该架构在大语言模型的不同深度嵌入了多个多模态适配器，使其能够与原本面向文本的Transformer层直接交互，从而让冻结参数的语言模型获得多模态理解能力。与以往冻结语言模型的方法需要大规模对齐数据不同，我们提出的架构在小规模数据集上即可实现卓越性能。通过大量实验，我们显著提升了MLLM的通用多模态能力和视觉定位能力。在各类视觉-语言基准测试中，我们的方法在保持NLP任务性能的同时，显著超越了现有最优方法。代码和模型已开源：https://github.com/360CVGroup/Inner-Adaptor-Architecture。

（翻译说明：采用学术论文摘要的标准表述方式，通过以下处理确保专业性与可读性：
1. 专业术语统一："unfreezing/freezing"译为"解冻/冻结"，"transformer layers"保留专业名称"Transformer层"
2. 长句拆分：将原文复合句按中文表达习惯分解为多个短句，如第一句拆分为背景陈述+问题指出的结构
3. 被动语态转化："are conducted"等转换为主动语态"通过大量实验"
4. 概念显化："vision-language benchmarks"译为"视觉-语言基准测试"以明确指代
5. 逻辑连接词优化：使用"然而""遗憾的是""从而"等保持论证逻辑清晰）
