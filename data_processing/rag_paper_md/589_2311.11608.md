# Taiyi: A Bilingual Fine-Tuned Large Language Model for Diverse Biomedical Tasks

链接: http://arxiv.org/abs/2311.11608v1

原文摘要:
Objective: Most existing fine-tuned biomedical large language models (LLMs)
focus on enhancing performance in monolingual biomedical question answering and
conversation tasks. To investigate the effectiveness of the fine-tuned LLMs on
diverse biomedical NLP tasks in different languages, We present Taiyi, a
bilingual fine-tuned LLM for diverse biomedical tasks. Materials and Methods:
We first curated a comprehensive collection of 140 existing biomedical text
mining datasets (102 English and 38 Chinese datasets) across over 10 task
types. Subsequently, a two-stage strategy is proposed for supervised
fine-tuning to optimize the model performance across varied tasks. Results:
Experimental results on 13 test sets covering named entity recognition,
relation extraction, text classification, question answering tasks demonstrate
that Taiyi achieves superior performance compared to general LLMs. The case
study involving additional biomedical NLP tasks further shows Taiyi's
considerable potential for bilingual biomedical multi-tasking. Conclusion:
Leveraging rich high-quality biomedical corpora and developing effective
fine-tuning strategies can significantly improve the performance of LLMs within
the biomedical domain. Taiyi shows the bilingual multi-tasking capability
through supervised fine-tuning. However, those tasks such as information
extraction that are not generation tasks in nature remain challenging for
LLM-based generative approaches, and they still underperform the conventional
discriminative approaches of smaller language models.

中文翻译:
目的：现有大多数经过微调的生物医学大语言模型（LLM）主要聚焦于提升单语种生物医学问答与会话任务的表现。为探究微调后LLM在多语言、多样化生物医学自然语言处理任务中的有效性，本研究推出双语生物医学多任务微调模型——泰医（Taiyi）。材料与方法：我们首先系统整合了涵盖10余种任务类型的140个现有生物医学文本挖掘数据集（英文102个，中文38个），继而提出两阶段监督微调策略以优化模型跨任务性能。结果：在涉及命名实体识别、关系抽取、文本分类和问答等任务的13个测试集上，实验表明泰医模型较通用LLM具有显著性能优势。针对其他生物医学NLP任务的案例研究进一步揭示了该模型在双语多任务处理方面的巨大潜力。结论：利用高质量生物医学语料库资源并开发有效微调策略，可显著提升LLM在生物医学领域的表现。泰医模型通过监督微调展现出双语多任务处理能力，但需指出的是，对于信息抽取等本质上非生成型的任务，基于LLM的生成式方法仍面临挑战，其性能尚不及传统判别式小规模语言模型。
