# Diffusion Models for Non-autoregressive Text Generation: A Survey

链接: http://arxiv.org/abs/2303.06574v1

原文摘要:
Non-autoregressive (NAR) text generation has attracted much attention in the
field of natural language processing, which greatly reduces the inference
latency but has to sacrifice the generation accuracy. Recently, diffusion
models, a class of latent variable generative models, have been introduced into
NAR text generation, showing an improved text generation quality. In this
survey, we review the recent progress in diffusion models for NAR text
generation. As the background, we first present the general definition of
diffusion models and the text diffusion models, and then discuss their merits
for NAR generation. As the core content, we further introduce two mainstream
diffusion models in existing work of text diffusion, and review the key designs
of the diffusion process. Moreover, we discuss the utilization of pre-trained
language models (PLMs) for text diffusion models and introduce optimization
techniques for text data. Finally, we discuss several promising directions and
conclude this paper. Our survey aims to provide researchers with a systematic
reference of related research on text diffusion models for NAR generation. We
present our collection of text diffusion models at
https://github.com/RUCAIBox/Awesome-Text-Diffusion-Models.

中文翻译:
非自回归（NAR）文本生成在自然语言处理领域备受关注，该方法虽能显著降低推理延迟，却不得不牺牲生成准确性。近年来，作为一类潜变量生成模型的扩散模型被引入NAR文本生成领域，展现出提升文本生成质量的潜力。本文系统综述了扩散模型在NAR文本生成中的最新进展：首先作为背景知识，阐述扩散模型与文本扩散模型的一般定义，并探讨其适用于NAR生成的优势；作为核心内容，详细分析现有文本扩散研究中的两类主流模型框架，梳理扩散过程的关键设计；进而讨论预训练语言模型（PLMs）在文本扩散中的运用方式，并介绍面向文本数据的优化技术；最后展望若干具有前景的研究方向。本综述旨在为研究者提供NAR生成领域文本扩散模型的系统性研究参考，相关模型资源集合详见https://github.com/RUCAIBox/Awesome-Text-Diffusion-Models。

（注：根据学术摘要的文体特征，翻译时进行了以下处理：
1. 将原文长句拆分为符合中文表达习惯的短句结构
2. "survey"译为"综述"以符合国内学术惯例
3. 技术术语如"latent variable generative models"采用"潜变量生成模型"的规范译法
4. 被动语态转换为主动表述（如"are introduced"处理为"被引入"）
5. 保留技术缩写（NAR/PLMs）并确保首次出现时带全称
6. 链接地址严格保留原始格式
