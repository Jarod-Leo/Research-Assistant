# Diffusion Models for Non-autoregressive Text Generation: A Survey

链接: http://arxiv.org/abs/2303.06574v1

原文摘要:
Non-autoregressive (NAR) text generation has attracted much attention in the
field of natural language processing, which greatly reduces the inference
latency but has to sacrifice the generation accuracy. Recently, diffusion
models, a class of latent variable generative models, have been introduced into
NAR text generation, showing an improved text generation quality. In this
survey, we review the recent progress in diffusion models for NAR text
generation. As the background, we first present the general definition of
diffusion models and the text diffusion models, and then discuss their merits
for NAR generation. As the core content, we further introduce two mainstream
diffusion models in existing work of text diffusion, and review the key designs
of the diffusion process. Moreover, we discuss the utilization of pre-trained
language models (PLMs) for text diffusion models and introduce optimization
techniques for text data. Finally, we discuss several promising directions and
conclude this paper. Our survey aims to provide researchers with a systematic
reference of related research on text diffusion models for NAR generation. We
present our collection of text diffusion models at
https://github.com/RUCAIBox/Awesome-Text-Diffusion-Models.

中文翻译:
非自回归（NAR）文本生成在自然语言处理领域备受关注，它虽能大幅降低推理延迟，却不得不牺牲生成准确性。近年来，潜变量生成模型中的扩散模型被引入NAR文本生成领域，展现出提升文本生成质量的潜力。本文系统梳理了扩散模型在NAR文本生成中的最新进展：作为背景知识，我们首先阐述扩散模型与文本扩散模型的一般定义，进而分析其在NAR生成中的优势；作为核心内容，我们详细介绍了现有文本扩散研究中的两类主流模型框架，并剖析扩散过程的关键设计；此外，我们探讨了预训练语言模型（PLMs）在文本扩散中的应用，以及针对文本数据的优化技术；最后展望了若干发展方向并总结全文。本综述旨在为研究者提供NAR生成领域文本扩散模型的系统性研究参考，相关模型资源已整理于https://github.com/RUCAIBox/Awesome-Text-Diffusion-Models。
