# Explaining Text Similarity in Transformer Models

链接: http://arxiv.org/abs/2405.06604v1

原文摘要:
As Transformers have become state-of-the-art models for natural language
processing (NLP) tasks, the need to understand and explain their predictions is
increasingly apparent. Especially in unsupervised applications, such as
information retrieval tasks, similarity models built on top of foundation model
representations have been widely applied. However, their inner prediction
mechanisms have mostly remained opaque. Recent advances in explainable AI have
made it possible to mitigate these limitations by leveraging improved
explanations for Transformers through layer-wise relevance propagation (LRP).
Using BiLRP, an extension developed for computing second-order explanations in
bilinear similarity models, we investigate which feature interactions drive
similarity in NLP models. We validate the resulting explanations and
demonstrate their utility in three corpus-level use cases, analyzing
grammatical interactions, multilingual semantics, and biomedical text
retrieval. Our findings contribute to a deeper understanding of different
semantic similarity tasks and models, highlighting how novel explainable AI
methods enable in-depth analyses and corpus-level insights.

中文翻译:
随着Transformer模型成为自然语言处理（NLP）任务的最先进技术，理解和解释其预测机制的需求日益凸显。特别是在无监督应用场景中，如基于基础模型表征构建的相似性模型已广泛应用于信息检索任务，但其内部预测机制大多仍不透明。可解释人工智能领域的最新进展通过分层相关性传播（LRP）技术为Transformer模型提供改进的解释方案，从而缓解了这一局限性。本文利用专为双线性相似模型设计的二阶解释扩展方法BiLRP，探究了NLP模型中驱动相似性计算的特征交互机制。我们通过三个语料库级别的应用案例（语法交互分析、多语言语义研究及生物医学文本检索）验证了所生成解释的有效性，并证明了其实用价值。研究成果深化了对不同语义相似性任务及模型的理解，同时揭示了新型可解释AI方法如何实现深度分析和语料库层面的洞察。
