# Uni-Mol2: Exploring Molecular Pretraining Model at Scale

链接: http://arxiv.org/abs/2406.14969v1

原文摘要:
In recent years, pretraining models have made significant advancements in the
fields of natural language processing (NLP), computer vision (CV), and life
sciences. The significant advancements in NLP and CV are predominantly driven
by the expansion of model parameters and data size, a phenomenon now recognized
as the scaling laws. However, research exploring scaling law in molecular
pretraining models remains unexplored. In this work, we present Uni-Mol2 , an
innovative molecular pretraining model that leverages a two-track transformer
to effectively integrate features at the atomic level, graph level, and
geometry structure level. Along with this, we systematically investigate the
scaling law within molecular pretraining models, characterizing the power-law
correlations between validation loss and model size, dataset size, and
computational resources. Consequently, we successfully scale Uni-Mol2 to 1.1
billion parameters through pretraining on 800 million conformations, making it
the largest molecular pretraining model to date. Extensive experiments show
consistent improvement in the downstream tasks as the model size grows. The
Uni-Mol2 with 1.1B parameters also outperforms existing methods, achieving an
average 27% improvement on the QM9 and 14% on COMPAS-1D dataset.

中文翻译:
近年来，预训练模型在自然语言处理（NLP）、计算机视觉（CV）和生命科学领域取得了显著进展。NLP与CV领域的重大突破主要源于模型参数量和数据规模的扩大，这一现象现被归纳为缩放定律。然而，针对分子预训练模型缩放规律的研究仍属空白。本文提出Uni-Mol2这一创新性分子预训练模型，通过双轨Transformer架构有效整合原子级、图级和几何结构级特征。基于此，我们系统探究了分子预训练中的缩放规律，揭示了验证损失与模型规模、数据集大小及计算资源之间的幂律关系。通过预训练8亿个分子构象，我们成功将Uni-Mol2扩展至11亿参数，成为当前最大规模的分子预训练模型。大量实验表明，随着模型规模增长，下游任务性能持续提升。11亿参数的Uni-Mol2在QM9和COMPAS-1D数据集上分别实现平均27%和14%的性能提升，显著超越现有方法。
