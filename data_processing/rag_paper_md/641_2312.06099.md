# Generative Large Language Models Are All-purpose Text Analytics Engines: Text-to-text Learning Is All Your Need

链接: http://arxiv.org/abs/2312.06099v1

原文摘要:
Objective To solve major clinical natural language processing (NLP) tasks
using a unified text-to-text learning architecture based on a generative large
language model (LLM) via prompt tuning. Methods We formulated 7 key clinical
NLP tasks as text-to-text learning and solved them using one unified generative
clinical LLM, GatorTronGPT, developed using GPT-3 architecture and trained with
up to 20 billion parameters. We adopted soft prompts (i.e., trainable vectors)
with frozen LLM, where the LLM parameters were not updated (i.e., frozen) and
only the vectors of soft prompts were updated, known as prompt tuning. We added
additional soft prompts as a prefix to the input layer, which were optimized
during the prompt tuning. We evaluated the proposed method using 7 clinical NLP
tasks and compared them with previous task-specific solutions based on
Transformer models. Results and Conclusion The proposed approach achieved
state-of-the-art performance for 5 out of 7 major clinical NLP tasks using one
unified generative LLM. Our approach outperformed previous task-specific
transformer models by ~3% for concept extraction and 7% for relation extraction
applied to social determinants of health, 3.4% for clinical concept
normalization, 3.4~10% for clinical abbreviation disambiguation, and 5.5~9% for
natural language inference. Our approach also outperformed a previously
developed prompt-based machine reading comprehension (MRC) model,
GatorTron-MRC, for clinical concept and relation extraction. The proposed
approach can deliver the ``one model for all`` promise from training to
deployment using a unified generative LLM.

中文翻译:
目的 通过提示调优，基于生成式大语言模型（LLM）的统一文本到文本学习架构，解决重大临床自然语言处理（NLP）任务。方法 我们将7项关键临床NLP任务构建为文本到文本学习框架，采用GPT-3架构开发、参数量高达200亿的统一生成式临床LLM——GatorTronGPT进行求解。采用冻结LLM参数的软提示（即可训练向量）策略（仅优化提示向量，不更新LLM参数，称为提示调优），在输入层前缀添加可优化的附加软提示。通过7项临床NLP任务评估该方法，并与基于Transformer模型的既往任务专用方案进行对比。结果与结论 该方案在7项主要临床NLP任务中有5项达到最先进性能：在健康社会决定因素的概念抽取和关系抽取任务中分别超越既往专用Transformer模型约3%和7%；临床概念标准化任务提升3.4%；临床缩写消歧任务提升3.4%~10%；自然语言推理任务提升5.5%~9%。在临床概念和关系抽取方面，其表现也优于此前开发的基于提示的机器阅读理解模型GatorTron-MRC。该方案通过统一生成式LLM实现了从训练到部署的"一模型通解"目标。
