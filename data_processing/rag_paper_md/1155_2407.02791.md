# Model-Enhanced LLM-Driven VUI Testing of VPA Apps

链接: http://arxiv.org/abs/2407.02791v1

原文摘要:
The flourishing ecosystem centered around voice personal assistants (VPA),
such as Amazon Alexa, has led to the booming of VPA apps. The largest app
market Amazon skills store, for example, hosts over 200,000 apps. Despite their
popularity, the open nature of app release and the easy accessibility of apps
also raise significant concerns regarding security, privacy and quality.
Consequently, various testing approaches have been proposed to systematically
examine VPA app behaviors. To tackle the inherent lack of a visible user
interface in the VPA app, two strategies are employed during testing, i.e.,
chatbot-style testing and model-based testing. The former often lacks effective
guidance for expanding its search space, while the latter falls short in
interpreting the semantics of conversations to construct precise and
comprehensive behavior models for apps. In this work, we introduce Elevate, a
model-enhanced large language model (LLM)-driven VUI testing framework. Elevate
leverages LLMs' strong capability in natural language processing to compensate
for semantic information loss during model-based VUI testing. It operates by
prompting LLMs to extract states from VPA apps' outputs and generate
context-related inputs. During the automatic interactions with the app, it
incrementally constructs the behavior model, which facilitates the LLM in
generating inputs that are highly likely to discover new states. Elevate
bridges the LLM and the behavior model with innovative techniques such as
encoding behavior model into prompts and selecting LLM-generated inputs based
on the context relevance. Elevate is benchmarked on 4,000 real-world Alexa
skills, against the state-of-the-art tester Vitas. It achieves 15% higher state
space coverage compared to Vitas on all types of apps, and exhibits significant
advancement in efficiency.

中文翻译:
以亚马逊Alexa等语音个人助手(VPA)为核心的生态系统蓬勃发展，推动了VPA应用程序的爆发式增长。例如最大的应用市场Amazon技能商店已托管超过20万个应用。尽管这些应用广受欢迎，但开放的发布机制和便捷的访问方式也引发了人们对安全性、隐私性和质量的重大担忧。为此，研究者们提出了多种测试方法来系统检测VPA应用行为。

针对VPA应用缺乏可视化用户界面的固有特性，当前测试主要采用两种策略：聊天机器人式测试和基于模型的测试。前者往往缺乏有效指导来扩展搜索空间，后者则在解析对话语义以构建精确全面的应用行为模型方面存在不足。本研究提出Elevate框架——一种基于模型增强的大型语言模型(LLM)驱动的语音用户界面测试方案。该框架利用LLM强大的自然语言处理能力，弥补基于模型的VUI测试中语义信息丢失的问题。其工作原理是通过提示LLM从VPA应用输出中提取状态，并生成上下文相关的输入内容。在与应用程序的自动交互过程中，该框架逐步构建行为模型，帮助LLM生成极有可能发现新状态的输入。

Elevate通过创新技术将LLM与行为模型相连接，包括将行为模型编码为提示词、基于上下文相关性筛选LLM生成的输入等。该框架在4,000个真实Alexa技能上进行了基准测试，相较于最先进的测试工具Vitas，在所有类型应用上实现了15%的状态空间覆盖率提升，并展现出显著的效率优势。
