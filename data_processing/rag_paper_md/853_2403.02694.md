# Privacy-Aware Semantic Cache for Large Language Models

链接: http://arxiv.org/abs/2403.02694v1

原文摘要:
Large Language Models (LLMs) like ChatGPT and Llama have revolutionized
natural language processing and search engine dynamics. However, these models
incur exceptionally high computational costs. For instance, GPT-3 consists of
175 billion parameters, where inference demands billions of floating-point
operations. Caching is a natural solution to reduce LLM inference costs on
repeated queries, which constitute about 31% of the total queries. However,
existing caching methods are incapable of finding semantic similarities among
LLM queries nor do they operate on contextual queries, leading to unacceptable
false hit-and-miss rates. This paper introduces MeanCache, a user-centric
semantic cache for LLM-based services that identifies semantically similar
queries to determine cache hit or miss. Using MeanCache, the response to a
user's semantically similar query can be retrieved from a local cache rather
than re-querying the LLM, thus reducing costs, service provider load, and
environmental impact. MeanCache leverages Federated Learning (FL) to
collaboratively train a query similarity model without violating user privacy.
By placing a local cache in each user's device and using FL, MeanCache reduces
the latency and costs and enhances model performance, resulting in lower false
hit rates. MeanCache also encodes context chains for every cached query,
offering a simple yet highly effective mechanism to discern contextual query
responses from standalone. Our experiments benchmarked against the
state-of-the-art caching method, reveal that MeanCache attains an approximately
17% higher F-score and a 20% increase in precision during semantic cache
hit-and-miss decisions while performing even better on contextual queries. It
also reduces the storage requirement by 83% and accelerates semantic cache
hit-and-miss decisions by 11%.

中文翻译:
以ChatGPT和Llama为代表的大型语言模型（LLM）已彻底革新了自然语言处理与搜索引擎的运行机制。然而，这些模型需要承担极高的计算成本。例如，GPT-3拥有1750亿参数，单次推理就需执行数十亿次浮点运算。针对重复查询（约占总查询量的31%），缓存技术成为降低LLM推理成本的天然解决方案。但现有缓存方法既无法识别查询间的语义相似性，也无法处理上下文关联查询，导致误判率居高不下。

本文提出MeanCache——一种以用户为中心的LLM服务语义缓存系统，通过识别语义相似查询来决定缓存命中与否。借助MeanCache，用户语义相近的查询可直接从本地缓存获取响应，无需重复调用LLM，从而降低成本、减轻服务商负载并减少环境影响。该系统采用联邦学习（FL）协作训练查询相似性模型，在保护用户隐私的前提下实现模型优化。通过在用户设备部署本地缓存并应用FL技术，MeanCache显著降低了延迟与成本，同时将误判率压缩至更低水平。

MeanCache创新性地为每个缓存查询构建上下文链，以简洁高效的机制区分上下文关联查询与独立查询的响应。实验数据显示，相较于最先进的缓存方案，MeanCache在语义缓存命中决策中的F值提升约17%，精确度提高20%，在上下文查询场景表现更为优异。此外，其存储需求降低83%，语义缓存决策速度加快11%。
