# Automatic Construction of a Korean Toxic Instruction Dataset for Ethical Tuning of Large Language Models

链接: http://arxiv.org/abs/2311.18215v1

原文摘要:
Caution: this paper may include material that could be offensive or
distressing.
  The advent of Large Language Models (LLMs) necessitates the development of
training approaches that mitigate the generation of unethical language and
aptly manage toxic user queries. Given the challenges related to human labor
and the scarcity of data, we present KoTox, comprising 39K unethical
instruction-output pairs. This collection of automatically generated toxic
instructions refines the training of LLMs and establishes a foundational
framework for improving LLMs' ethical awareness and response to various toxic
inputs, promoting more secure and responsible interactions in Natural Language
Processing (NLP) applications.

中文翻译:
注意：本文可能包含令人不适或不安的内容。  
随着大语言模型（LLM）的出现，亟需开发能够减少不道德语言生成并妥善处理有害用户查询的训练方法。鉴于人工标注的挑战及数据稀缺性，我们提出了KoTox数据集，包含3.9万条不道德指令-输出对。这套自动生成的有害指令集可优化LLM训练，为提升模型伦理意识及应对各类毒性输入建立基础框架，从而推动自然语言处理（NLP）应用实现更安全、负责任的交互。
