# Conversations in Galician: a Large Language Model for an Underrepresented Language

链接: http://arxiv.org/abs/2311.03812v1

原文摘要:
The recent proliferation of Large Conversation Language Models has
highlighted the economic significance of widespread access to this type of AI
technologies in the current information age. Nevertheless, prevailing models
have primarily been trained on corpora consisting of documents written in
popular languages. The dearth of such cutting-edge tools for low-resource
languages further exacerbates their underrepresentation in the current economic
landscape, thereby impacting their native speakers. This paper introduces two
novel resources designed to enhance Natural Language Processing (NLP) for the
Galician language. We present a Galician adaptation of the Alpaca dataset,
comprising 52,000 instructions and demonstrations. This dataset proves
invaluable for enhancing language models by fine-tuning them to more accurately
adhere to provided instructions. Additionally, as a demonstration of the
dataset utility, we fine-tuned LLaMA-7B to comprehend and respond in Galician,
a language not originally supported by the model, by following the Alpaca
format. This work contributes to the research on multilingual models tailored
for low-resource settings, a crucial endeavor in ensuring the inclusion of all
linguistic communities in the development of Large Language Models. Another
noteworthy aspect of this research is the exploration of how knowledge of a
closely related language, in this case, Portuguese, can assist in generating
coherent text when training resources are scarce. Both the Galician Alpaca
dataset and Cabuxa-7B are publicly accessible on our Huggingface Hub, and we
have made the source code available to facilitate replication of this
experiment and encourage further advancements for underrepresented languages.

中文翻译:
近年来，大型对话语言模型的激增凸显了在当前信息时代普及此类人工智能技术的经济价值。然而，主流模型主要基于流行语言书写的文本语料库进行训练。针对资源匮乏语言的前沿工具短缺，进一步加剧了这些语言在当前经济格局中的边缘化地位，从而影响了其母语使用群体。本文介绍了两项旨在提升加利西亚语自然语言处理能力的新型资源：我们发布了基于Alpaca数据集改造的加利西亚语版本，包含52,000条指令与示例，该数据集通过微调使语言模型更精准遵循指令，对模型优化具有重要价值；同时作为数据集应用示范，我们采用Alpaca格式对LLaMA-7B模型进行微调，使其能够理解并输出该模型原本不支持的加利西亚语。本研究为资源受限环境下的多语言模型研发提供了新思路，这对确保所有语言社群都能参与大型语言模型发展至关重要。另一创新点在于探索了亲属语言（本研究以葡萄牙语为例）知识在训练资源匮乏时辅助生成连贯文本的机制。加利西亚语Alpaca数据集与Cabuxa-7B模型已公开于Huggingface平台，相关源代码亦已开源，旨在促进实验复现并推动弱势语言技术发展。
