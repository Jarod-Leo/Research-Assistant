# PETAH: Parameter Efficient Task Adaptation for Hybrid Transformers in a resource-limited Context

链接: http://arxiv.org/abs/2410.17661v1

原文摘要:
Following their success in natural language processing (NLP), there has been
a shift towards transformer models in computer vision. While transformers
perform well and offer promising multi-tasking performance, due to their high
compute requirements, many resource-constrained applications still rely on
convolutional or hybrid models that combine the benefits of convolution and
attention layers and achieve the best results in the sub 100M parameter range.
Simultaneously, task adaptation techniques that allow for the use of one shared
transformer backbone for multiple downstream tasks, resulting in great storage
savings at negligible cost in performance, have not yet been adopted for hybrid
transformers. In this work, we investigate how to achieve the best
task-adaptation performance and introduce PETAH: Parameter Efficient Task
Adaptation for Hybrid Transformers. We further combine PETAH adaptation with
pruning to achieve highly performant and storage friendly models for
multi-tasking. In our extensive evaluation on classification and other vision
tasks, we demonstrate that our PETAH-adapted hybrid models outperform
established task-adaptation techniques for ViTs while requiring fewer
parameters and being more efficient on mobile hardware.

中文翻译:
在自然语言处理（NLP）领域取得成功后，计算机视觉领域正逐渐转向Transformer模型。尽管Transformer表现优异且具备多任务处理潜力，但由于其高计算需求，许多资源受限的应用仍依赖于卷积或混合模型——这类模型结合了卷积层与注意力层的优势，在参数规模小于1亿的范围内能取得最佳效果。与此同时，适用于多下游任务的共享Transformer主干网络的任务适应技术虽能显著节省存储空间且性能损失可忽略，却尚未在混合Transformer中得到应用。本研究探索如何实现最优任务适应性能，提出了PETAH（混合Transformer的参数高效任务适应方法）。我们进一步将PETAH适应与剪枝技术结合，构建出高性能且存储友好的多任务模型。通过对分类及其他视觉任务的广泛评估，我们证明经PETAH调整的混合模型在性能上超越了ViTs的现有任务适应技术，同时所需参数更少，在移动硬件上运行效率更高。
