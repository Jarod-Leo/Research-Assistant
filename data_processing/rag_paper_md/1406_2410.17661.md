# PETAH: Parameter Efficient Task Adaptation for Hybrid Transformers in a resource-limited Context

链接: http://arxiv.org/abs/2410.17661v1

原文摘要:
Following their success in natural language processing (NLP), there has been
a shift towards transformer models in computer vision. While transformers
perform well and offer promising multi-tasking performance, due to their high
compute requirements, many resource-constrained applications still rely on
convolutional or hybrid models that combine the benefits of convolution and
attention layers and achieve the best results in the sub 100M parameter range.
Simultaneously, task adaptation techniques that allow for the use of one shared
transformer backbone for multiple downstream tasks, resulting in great storage
savings at negligible cost in performance, have not yet been adopted for hybrid
transformers. In this work, we investigate how to achieve the best
task-adaptation performance and introduce PETAH: Parameter Efficient Task
Adaptation for Hybrid Transformers. We further combine PETAH adaptation with
pruning to achieve highly performant and storage friendly models for
multi-tasking. In our extensive evaluation on classification and other vision
tasks, we demonstrate that our PETAH-adapted hybrid models outperform
established task-adaptation techniques for ViTs while requiring fewer
parameters and being more efficient on mobile hardware.

中文翻译:
在自然语言处理（NLP）领域取得成功后，Transformer模型正逐渐向计算机视觉领域迁移。尽管Transformer表现优异且具备多任务处理潜力，但由于其高昂的计算需求，许多资源受限的应用仍依赖于卷积或混合模型——这类模型结合了卷积层与注意力层的优势，在参数量小于1亿的范围内能取得最佳效果。与此同时，目前尚未有研究将任务适配技术应用于混合Transformer架构。这类技术通过共享单个Transformer主干网络处理多个下游任务，能以可忽略的性能代价实现显著的存储节省。

本研究中，我们探索了如何实现最优任务适配性能，并提出PETAH（混合Transformer的参数高效任务适配方法）。我们进一步将PETAH适配与模型剪枝相结合，构建出高性能且存储友好的多任务模型。在分类及其他视觉任务的广泛评估中，经PETAH适配的混合模型在移动硬件上展现出比ViT传统适配技术更优的性能，同时所需参数量更少、运行效率更高。
