# T-former: An Efficient Transformer for Image Inpainting

链接: http://arxiv.org/abs/2305.07239v1

原文摘要:
Benefiting from powerful convolutional neural networks (CNNs), learning-based
image inpainting methods have made significant breakthroughs over the years.
However, some nature of CNNs (e.g. local prior, spatially shared parameters)
limit the performance in the face of broken images with diverse and complex
forms. Recently, a class of attention-based network architectures, called
transformer, has shown significant performance on natural language processing
fields and high-level vision tasks. Compared with CNNs, attention operators are
better at long-range modeling and have dynamic weights, but their computational
complexity is quadratic in spatial resolution, and thus less suitable for
applications involving higher resolution images, such as image inpainting. In
this paper, we design a novel attention linearly related to the resolution
according to Taylor expansion. And based on this attention, a network called
$T$-former is designed for image inpainting. Experiments on several benchmark
datasets demonstrate that our proposed method achieves state-of-the-art
accuracy while maintaining a relatively low number of parameters and
computational complexity. The code can be found at
\href{https://github.com/dengyecode/T-former_image_inpainting}{github.com/dengyecode/T-former\_image\_inpainting}

中文翻译:
得益于强大的卷积神经网络（CNN），基于学习的图像修复方法近年来取得了显著突破。然而CNN的固有特性（如局部先验、空间共享参数）限制了其在处理形态多样且复杂的破损图像时的表现。近期，一类基于注意力机制的网络架构——Transformer在自然语言处理和高层次视觉任务中展现出卓越性能。与CNN相比，注意力算子更擅长长程建模并具有动态权重特性，但其计算复杂度与空间分辨率呈平方关系，因此难以直接适用于图像修复等高分辨率场景。本文根据泰勒展开设计了一种与分辨率呈线性关系的新型注意力机制，并基于此构建了名为$T$-former的图像修复网络。在多个基准数据集上的实验表明，所提方法在保持较低参数量和计算复杂度的同时，达到了最先进的修复精度。代码已开源于\href{https://github.com/dengyecode/T-former_image_inpainting}{github.com/dengyecode/T-former\_image\_inpainting}。
