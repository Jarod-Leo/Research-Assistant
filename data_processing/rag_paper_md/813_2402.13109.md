# CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the Generalizability of Large Language Models

链接: http://arxiv.org/abs/2402.13109v1

原文摘要:
The advancement of large language models (LLMs) has enhanced the ability to
generalize across a wide range of unseen natural language processing (NLP)
tasks through instruction-following. Yet, their effectiveness often diminishes
in low-resource languages like Chinese, exacerbated by biased evaluations from
data leakage, casting doubt on their true generalizability to new linguistic
territories. In response, we introduce the Chinese Instruction-Following
Benchmark (CIF-Bench), designed to evaluate the zero-shot generalizability of
LLMs to the Chinese language. CIF-Bench comprises 150 tasks and 15,000
input-output pairs, developed by native speakers to test complex reasoning and
Chinese cultural nuances across 20 categories. To mitigate data contamination,
we release only half of the dataset publicly, with the remainder kept private,
and introduce diversified instructions to minimize score variance, totaling
45,000 data instances. Our evaluation of 28 selected LLMs reveals a noticeable
performance gap, with the best model scoring only 52.9%, highlighting the
limitations of LLMs in less familiar language and task contexts. This work not
only uncovers the current limitations of LLMs in handling Chinese language
tasks but also sets a new standard for future LLM generalizability research,
pushing towards the development of more adaptable, culturally informed, and
linguistically diverse models.

中文翻译:
大型语言模型（LLM）的发展通过指令跟随能力，显著提升了在未见自然语言处理（NLP）任务上的泛化性能。然而，这类模型在中文等低资源语言中的表现往往不尽如人意，加之数据泄露导致的评估偏差，其对新语言领域的真实泛化能力备受质疑。为此，我们推出中文指令跟随基准（CIF-Bench），旨在系统评估LLM对中文任务的零样本泛化能力。该基准包含由母语者设计的150项任务和15,000组输入输出对，覆盖20个类别，重点考察复杂推理能力与中华文化语境理解。为降低数据污染风险，我们仅公开半数数据集，其余保持非公开状态，并通过引入45,000条多样化指令以减小评分波动。对28个精选LLM的评估显示，最佳模型得分仅52.9%，暴露出模型在陌生语言与任务场景中的明显局限。本研究不仅揭示了当前LLM处理中文任务的瓶颈，更为未来模型泛化能力研究设立了新标准，推动开发更具适应性、文化感知力及语言多样性的新一代模型。
