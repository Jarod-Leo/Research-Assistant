# Multilingual Prompts in LLM-Based Recommenders: Performance Across Languages

链接: http://arxiv.org/abs/2409.07604v1

原文摘要:
Large language models (LLMs) are increasingly used in natural language
processing tasks. Recommender systems traditionally use methods such as
collaborative filtering and matrix factorization, as well as advanced
techniques like deep learning and reinforcement learning. Although language
models have been applied in recommendation, the recent trend have focused on
leveraging the generative capabilities of LLMs for more personalized
suggestions. While current research focuses on English due to its resource
richness, this work explores the impact of non-English prompts on
recommendation performance. Using OpenP5, a platform for developing and
evaluating LLM-based recommendations, we expanded its English prompt templates
to include Spanish and Turkish. Evaluation on three real-world datasets, namely
ML1M, LastFM, and Amazon-Beauty, showed that usage of non-English prompts
generally reduce performance, especially in less-resourced languages like
Turkish. We also retrained an LLM-based recommender model with multilingual
prompts to analyze performance variations. Retraining with multilingual prompts
resulted in more balanced performance across languages, but slightly reduced
English performance. This work highlights the need for diverse language support
in LLM-based recommenders and suggests future research on creating evaluation
datasets, using newer models and additional languages.

中文翻译:
大型语言模型（LLM）在自然语言处理任务中的应用日益广泛。传统推荐系统通常采用协同过滤、矩阵分解等方法，以及深度学习和强化学习等先进技术。尽管语言模型已被应用于推荐领域，但当前趋势更侧重于利用LLM的生成能力来实现更个性化的推荐。由于英语资源丰富，现有研究多聚焦于英语场景，本研究则探讨了非英语提示对推荐性能的影响。基于OpenP5这一LLM推荐系统开发与评估平台，我们将其英文提示模板扩展至西班牙语和土耳其语。通过在ML1M、LastFM和Amazon-Beauty三个真实数据集上的测试发现，使用非英语提示通常会导致性能下降，尤其在土耳其语等资源较少的语言中表现更为明显。我们还采用多语言提示对基于LLM的推荐模型进行了重新训练以分析性能变化。实验表明，多语言提示重训练能实现更均衡的跨语言性能，但会轻微降低英语场景的表现。这项工作揭示了LLM推荐系统需要加强多语言支持，并建议未来研究应关注评估数据集构建、采用更新模型及扩展更多语种。
