# Low Resource Summarization using Pre-trained Language Models

链接: http://arxiv.org/abs/2310.02790v1

原文摘要:
With the advent of Deep Learning based Artificial Neural Networks models,
Natural Language Processing (NLP) has witnessed significant improvements in
textual data processing in terms of its efficiency and accuracy. However, the
research is mostly restricted to high-resource languages such as English and
low-resource languages still suffer from a lack of available resources in terms
of training datasets as well as models with even baseline evaluation results.
Considering the limited availability of resources for low-resource languages,
we propose a methodology for adapting self-attentive transformer-based
architecture models (mBERT, mT5) for low-resource summarization, supplemented
by the construction of a new baseline dataset (76.5k article, summary pairs) in
a low-resource language Urdu. Choosing news (a publicly available source) as
the application domain has the potential to make the proposed methodology
useful for reproducing in other languages with limited resources. Our adapted
summarization model \textit{urT5} with up to 44.78\% reduction in size as
compared to \textit{mT5} can capture contextual information of low resource
language effectively with evaluation score (up to 46.35 ROUGE-1, 77 BERTScore)
at par with state-of-the-art models in high resource language English
\textit{(PEGASUS: 47.21, BART: 45.14 on XSUM Dataset)}. The proposed method
provided a baseline approach towards extractive as well as abstractive
summarization with competitive evaluation results in a limited resource setup.

中文翻译:
随着基于深度学习的人工神经网络模型的出现，自然语言处理（NLP）在文本数据处理效率和准确性方面取得了显著进步。然而，相关研究主要集中于英语等高资源语言，而低资源语言仍面临训练数据集匮乏及缺乏基准评估模型的问题。针对低资源语言资源有限的现状，本文提出一种适配自注意力Transformer架构模型（mBERT、mT5）的低资源摘要生成方法，并构建了乌尔都语（低资源语言）的新基准数据集（7.65万篇新闻与摘要对）。选择新闻（公开可用资源）作为应用领域，使得该方法有望推广至其他资源有限的语言。我们改进的摘要模型\textit{urT5}相较于\textit{mT5}体积最大缩减44.78%，在低资源语言中能有效捕捉上下文信息，其评估分数（最高达46.35 ROUGE-1、77 BERTScore）与英语高资源语言的先进模型\textit{（PEGASUS：47.21，BART：45.14，基于XSUM数据集）}表现相当。该方法为抽取式和生成式摘要提供了基线方案，在有限资源条件下获得了具有竞争力的评估结果。
