# VPA: Fully Test-Time Visual Prompt Adaptation

链接: http://arxiv.org/abs/2309.15251v1

原文摘要:
Textual prompt tuning has demonstrated significant performance improvements
in adapting natural language processing models to a variety of downstream tasks
by treating hand-engineered prompts as trainable parameters. Inspired by the
success of textual prompting, several studies have investigated the efficacy of
visual prompt tuning. In this work, we present Visual Prompt Adaptation (VPA),
the first framework that generalizes visual prompting with test-time
adaptation. VPA introduces a small number of learnable tokens, enabling fully
test-time and storage-efficient adaptation without necessitating source-domain
information. We examine our VPA design under diverse adaptation settings,
encompassing single-image, batched-image, and pseudo-label adaptation. We
evaluate VPA on multiple tasks, including out-of-distribution (OOD)
generalization, corruption robustness, and domain adaptation. Experimental
results reveal that VPA effectively enhances OOD generalization by 3.3% across
various models, surpassing previous test-time approaches. Furthermore, we show
that VPA improves corruption robustness by 6.5% compared to strong baselines.
Finally, we demonstrate that VPA also boosts domain adaptation performance by
relatively 5.2%. Our VPA also exhibits marked effectiveness in improving the
robustness of zero-shot recognition for vision-language models.

中文翻译:
文本提示调优通过将手工设计的提示视为可训练参数，在使自然语言处理模型适应多种下游任务方面展现出显著的性能提升。受文本提示成功的启发，多项研究探索了视觉提示调优的有效性。本研究提出视觉提示自适应框架（VPA），这是首个将视觉提示与测试时自适应相结合的通用化框架。VPA通过引入少量可学习标记，实现了完全测试时且存储高效的自适应，无需依赖源域信息。我们在单图像、批图像及伪标签自适应等多种设置下验证VPA设计，并在分布外泛化、抗干扰鲁棒性和领域自适应等任务中进行评估。实验结果表明，VPA将各类模型的分布外泛化能力平均提升3.3%，优于现有测试时方法；相比强基线模型，其抗干扰鲁棒性提高6.5%；领域自适应性能相对提升5.2%。此外，VPA在增强视觉语言模型零样本识别鲁棒性方面也表现出显著效果。
