# Perspective Transition of Large Language Models for Solving Subjective Tasks

链接: http://arxiv.org/abs/2501.09265v1

原文摘要:
Large language models (LLMs) have revolutionized the field of natural
language processing, enabling remarkable progress in various tasks. Different
from objective tasks such as commonsense reasoning and arithmetic
question-answering, the performance of LLMs on subjective tasks is still
limited, where the perspective on the specific problem plays crucial roles for
better interpreting the context and giving proper response. For example, in
certain scenarios, LLMs may perform better when answering from an expert role
perspective, potentially eliciting their relevant domain knowledge. In
contrast, in some scenarios, LLMs may provide more accurate responses when
answering from a third-person standpoint, enabling a more comprehensive
understanding of the problem and potentially mitigating inherent biases. In
this paper, we propose Reasoning through Perspective Transition (RPT), a method
based on in-context learning that enables LLMs to dynamically select among
direct, role, and third-person perspectives for the best way to solve
corresponding subjective problem. Through extensive experiments on totally 12
subjective tasks by using both closed-source and open-source LLMs including
GPT-4, GPT-3.5, Llama-3, and Qwen-2, our method outperforms widely used single
fixed perspective based methods such as chain-of-thought prompting and expert
prompting, highlights the intricate ways that LLMs can adapt their perspectives
to provide nuanced and contextually appropriate responses for different
problems.

中文翻译:
以下是符合您要求的中文翻译：

大型语言模型（LLMs）已经彻底改变了自然语言处理领域，在各种任务中实现了显著进展。与常识推理和算术问答等客观任务不同，LLMs在主观任务上的表现仍存在局限——这类任务中，对特定问题的视角选择往往对准确理解语境和给出恰当回应至关重要。例如在某些场景下，当LLMs以专家角色视角回答时，可能更易激发其相关领域知识从而表现更优；而在另一些场景中，采用第三人称立场则能帮助模型更全面地理解问题，可能有效缓解固有偏见，从而提供更准确的回答。本文提出"视角转换推理法"（RPT），这是一种基于上下文学习的方法，使LLMs能动态选择直接视角、角色视角或第三人称视角来最优解决相应主观问题。通过对GPT-4、GPT-3.5、Llama-3和Qwen-2等闭源/开源模型在12项主观任务上的大量实验表明，我们的方法优于思维链提示、专家提示等广泛使用的单一固定视角方法，凸显了LLMs通过动态调整视角来为不同问题提供细致入微且符合语境的响应这一复杂机制。

翻译说明：
1. 专业术语处理：LLMs统一译为"大型语言模型"并保留英文缩写，RPT采用中文译名+英文缩写格式
2. 长句拆分：将原文复合长句按中文表达习惯分解为多个短句，如将"where the perspective..."独立成句并添加破折号引导
3. 逻辑显化：通过"例如""而"等连接词明确原文隐含的对比关系
4. 被动语态转换："are still limited"译为主动式"仍存在局限"
5. 概念对等："nuanced responses"译为"细致入微的响应"而非字面直译
6. 技术术语统一："chain-of-thought prompting"采用学界通用译法"思维链提示"
7. 文化适配："inherent biases"译为"固有偏见"符合中文认知心理学表述习惯
