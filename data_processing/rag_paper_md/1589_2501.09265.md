# Perspective Transition of Large Language Models for Solving Subjective Tasks

链接: http://arxiv.org/abs/2501.09265v1

原文摘要:
Large language models (LLMs) have revolutionized the field of natural
language processing, enabling remarkable progress in various tasks. Different
from objective tasks such as commonsense reasoning and arithmetic
question-answering, the performance of LLMs on subjective tasks is still
limited, where the perspective on the specific problem plays crucial roles for
better interpreting the context and giving proper response. For example, in
certain scenarios, LLMs may perform better when answering from an expert role
perspective, potentially eliciting their relevant domain knowledge. In
contrast, in some scenarios, LLMs may provide more accurate responses when
answering from a third-person standpoint, enabling a more comprehensive
understanding of the problem and potentially mitigating inherent biases. In
this paper, we propose Reasoning through Perspective Transition (RPT), a method
based on in-context learning that enables LLMs to dynamically select among
direct, role, and third-person perspectives for the best way to solve
corresponding subjective problem. Through extensive experiments on totally 12
subjective tasks by using both closed-source and open-source LLMs including
GPT-4, GPT-3.5, Llama-3, and Qwen-2, our method outperforms widely used single
fixed perspective based methods such as chain-of-thought prompting and expert
prompting, highlights the intricate ways that LLMs can adapt their perspectives
to provide nuanced and contextually appropriate responses for different
problems.

中文翻译:
大语言模型（LLMs）彻底改变了自然语言处理领域，推动各项任务取得显著进展。相较于常识推理、算术问答等客观任务，LLMs在主观任务上的表现仍存在局限——这类任务中，对特定问题的视角选择往往对准确理解语境和给出恰当回应具有决定性影响。例如在某些场景下，LLMs以专家角色视角作答时能更好调动其领域知识；而在另一些情境中，采用第三人称立场则可能获得更客观的答案，既有助于全面把握问题，又能有效规避模型固有偏见。本文提出"视角转换推理法"（RPT），该方法基于上下文学习机制，使LLMs能动态选择直接视角、角色视角或第三人称视角来最优解决对应主观问题。通过对GPT-4、GPT-3.5、Llama-3和Qwen-2等闭源/开源模型在12类主观任务上的大量实验表明，本方法优于思维链提示、专家提示等广泛使用的单一固定视角方法，揭示了LLMs通过灵活调整视角来为不同问题提供精准、情境化响应的复杂机制。
