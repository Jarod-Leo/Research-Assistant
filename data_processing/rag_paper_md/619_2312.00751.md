# Mitigating Over-smoothing in Transformers via Regularized Nonlocal Functionals

链接: http://arxiv.org/abs/2312.00751v1

原文摘要:
Transformers have achieved remarkable success in a wide range of natural
language processing and computer vision applications. However, the
representation capacity of a deep transformer model is degraded due to the
over-smoothing issue in which the token representations become identical when
the model's depth grows. In this work, we show that self-attention layers in
transformers minimize a functional which promotes smoothness, thereby causing
token uniformity. We then propose a novel regularizer that penalizes the norm
of the difference between the smooth output tokens from self-attention and the
input tokens to preserve the fidelity of the tokens. Minimizing the resulting
regularized energy functional, we derive the Neural Transformer with a
Regularized Nonlocal Functional (NeuTRENO), a novel class of transformer models
that can mitigate the over-smoothing issue. We empirically demonstrate the
advantages of NeuTRENO over the baseline transformers and state-of-the-art
methods in reducing the over-smoothing of token representations on various
practical tasks, including object classification, image segmentation, and
language modeling.

中文翻译:
Transformer模型在自然语言处理和计算机视觉的广泛应用中取得了显著成功。然而，随着模型深度增加，由于过度平滑问题导致标记表征趋于同质化，深度Transformer模型的表征能力会随之下降。本研究首先证明Transformer中的自注意力层通过最小化一个促进平滑性的泛函，从而引发标记统一性问题。为此，我们提出了一种创新正则化项，通过惩罚自注意力平滑输出标记与输入标记间的差异范数来保持标记的保真度。通过最小化这个正则化能量泛函，我们推导出基于正则化非局部泛函的神经Transformer（NeuTRENO）——这是一个能有效缓解过度平滑问题的新型Transformer模型类别。在物体分类、图像分割和语言建模等实际任务中，我们通过实证研究验证了NeuTRENO在减轻标记表征过度平滑方面优于基线Transformer模型和当前最先进方法。
