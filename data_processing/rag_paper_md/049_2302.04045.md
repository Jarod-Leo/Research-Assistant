# Revisiting Offline Compression: Going Beyond Factorization-based Methods for Transformer Language Models

链接: http://arxiv.org/abs/2302.04045v1

原文摘要:
Recent transformer language models achieve outstanding results in many
natural language processing (NLP) tasks. However, their enormous size often
makes them impractical on memory-constrained devices, requiring practitioners
to compress them to smaller networks. In this paper, we explore offline
compression methods, meaning computationally-cheap approaches that do not
require further fine-tuning of the compressed model. We challenge the classical
matrix factorization methods by proposing a novel, better-performing
autoencoder-based framework. We perform a comprehensive ablation study of our
approach, examining its different aspects over a diverse set of evaluation
settings. Moreover, we show that enabling collaboration between modules across
layers by compressing certain modules together positively impacts the final
model performance. Experiments on various NLP tasks demonstrate that our
approach significantly outperforms commonly used factorization-based offline
compression methods.

中文翻译:
近期，Transformer语言模型在众多自然语言处理（NLP）任务中表现卓越。然而，其庞大的规模往往使其难以在内存受限的设备上实际应用，这促使研究者需将其压缩为更小的网络。本文重点探索离线压缩方法，即无需对压缩后模型进行额外微调且计算成本低廉的技术路径。我们通过提出一种性能更优的新型自编码器框架，对传统矩阵分解方法发起挑战。研究中对所提方法进行了全面消融实验，从多维度评估场景剖析其各项特性。此外，我们发现通过联合压缩跨层模块以促进层间协作，能显著提升最终模型性能。在多样化NLP任务上的实验表明，该方法明显优于常用的基于分解的离线压缩技术。
