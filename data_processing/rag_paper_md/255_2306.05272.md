# Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models

链接: http://arxiv.org/abs/2306.05272v2

原文摘要:
The advent of large pre-trained models has brought about a paradigm shift in
both visual representation learning and natural language processing. However,
clustering unlabeled images, as a fundamental and classic machine learning
problem, still lacks an effective solution, particularly for large-scale
datasets. In this paper, we propose a novel image clustering pipeline that
leverages the powerful feature representation of large pre-trained models such
as CLIP and cluster images effectively and efficiently at scale. We first
developed a novel algorithm to estimate the number of clusters in a given
dataset. We then show that the pre-trained features are significantly more
structured by further optimizing the rate reduction objective. The resulting
features may significantly improve the clustering accuracy, e.g., from 57\% to
66\% on ImageNet-1k. Furthermore, by leveraging CLIP's multimodality bridge
between image and text, we develop a simple yet effective self-labeling
algorithm that produces meaningful captions for the clusters. Through extensive
experiments, we show that our pipeline works well on standard datasets such as
CIFAR-10, CIFAR-100, and ImageNet-1k. It also extends to datasets that are not
curated for clustering, such as LAION-Aesthetics and WikiArts. We released the
code in https://github.com/LeslieTrue/CPP.

中文翻译:
大规模预训练模型的出现为视觉表征学习与自然语言处理领域带来了范式变革。然而，作为机器学习基础而经典的课题，无标注图像聚类仍缺乏高效解决方案，尤其在大规模数据集上。本文提出一种新颖的图像聚类流程，通过利用CLIP等大型预训练模型的强大特征表示能力，实现高效的大规模图像聚类。我们首先开发了一种创新算法来估计数据集的聚类数量，随后证明通过进一步优化率降低目标函数，预训练特征会呈现更显著的结构化特性。由此获得的特征可大幅提升聚类准确率，例如在ImageNet-1k数据集上从57%提升至66%。此外，借助CLIP跨模态的图像-文本关联特性，我们设计了一种简洁高效的自标注算法，能为聚类生成语义明确的描述标签。大量实验表明，该流程在CIFAR-10、CIFAR-100和ImageNet-1k等标准数据集上表现优异，并适用于LAION-Aesthetics和WikiArts等非聚类专用数据集。相关代码已开源在https://github.com/LeslieTrue/CPP。
