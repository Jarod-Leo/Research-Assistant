# Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models

链接: http://arxiv.org/abs/2306.05272v2

原文摘要:
The advent of large pre-trained models has brought about a paradigm shift in
both visual representation learning and natural language processing. However,
clustering unlabeled images, as a fundamental and classic machine learning
problem, still lacks an effective solution, particularly for large-scale
datasets. In this paper, we propose a novel image clustering pipeline that
leverages the powerful feature representation of large pre-trained models such
as CLIP and cluster images effectively and efficiently at scale. We first
developed a novel algorithm to estimate the number of clusters in a given
dataset. We then show that the pre-trained features are significantly more
structured by further optimizing the rate reduction objective. The resulting
features may significantly improve the clustering accuracy, e.g., from 57\% to
66\% on ImageNet-1k. Furthermore, by leveraging CLIP's multimodality bridge
between image and text, we develop a simple yet effective self-labeling
algorithm that produces meaningful captions for the clusters. Through extensive
experiments, we show that our pipeline works well on standard datasets such as
CIFAR-10, CIFAR-100, and ImageNet-1k. It also extends to datasets that are not
curated for clustering, such as LAION-Aesthetics and WikiArts. We released the
code in https://github.com/LeslieTrue/CPP.

中文翻译:
以下为符合学术规范的中文翻译：

大规模预训练模型的出现为视觉表征学习和自然语言处理领域带来了范式变革。然而，作为机器学习基础经典任务的未标注图像聚类，至今仍缺乏有效的解决方案——尤其针对大规模数据集。本文提出一种创新的图像聚类流程，通过利用CLIP等大型预训练模型的强大特征表征能力，实现高效的大规模图像聚类。我们首先开发了一种新型算法来估计给定数据集的聚类数量，随后证明通过进一步优化率降低目标函数，预训练特征会呈现更显著的结构化特性。由此获得的特征可大幅提升聚类准确率（例如在ImageNet-1k数据集上从57%提升至66%）。此外，借助CLIP跨模态模型中图像与文本的关联特性，我们设计了一种简洁高效的自标注算法，可为聚类生成语义明确的描述标签。大量实验表明，该流程在CIFAR-10、CIFAR-100和ImageNet-1k等标准数据集上表现优异，并能有效扩展到非聚类专用数据集（如LAION-Aesthetics和WikiArts）。相关代码已开源：https://github.com/LeslieTrue/CPP。

（翻译说明：
1. 专业术语处理："pre-trained models"译为"预训练模型"，"rate reduction objective"保留专业表述"率降低目标函数"
2. 长句拆分：将原文复合长句按中文表达习惯分解为多个短句
3. 被动语态转换："it is shown that"转为主动式"证明"
4. 数据呈现规范：保留准确率数值的英文百分号格式（66%）
5. 学术用语："paradigm shift"译为"范式变革"，"self-labeling"译为"自标注"
6. 链接处理：完整保留原始URL格式）
