# Transformer Based Implementation for Automatic Book Summarization

链接: http://arxiv.org/abs/2301.07057v1

原文摘要:
Document Summarization is the procedure of generating a meaningful and
concise summary of a given document with the inclusion of relevant and
topic-important points. There are two approaches: one is picking up the most
relevant statements from the document itself and adding it to the Summary known
as Extractive and the other is generating sentences for the Summary known as
Abstractive Summarization. Training a machine learning model to perform tasks
that are time-consuming or very difficult for humans to evaluate is a major
challenge. Book Abstract generation is one of such complex tasks. Traditional
machine learning models are getting modified with pre-trained transformers.
Transformer based language models trained in a self-supervised fashion are
gaining a lot of attention; when fine-tuned for Natural Language
Processing(NLP) downstream task like text summarization. This work is an
attempt to use Transformer based techniques for Abstract generation.

中文翻译:
文档摘要是指通过提取与主题相关的关键信息，生成一份既简洁又能准确反映原文内容的概要。目前主要存在两种方法：一种是直接从原文中选取最具代表性的语句组合成摘要（称为抽取式摘要），另一种则是通过重新组织语言生成摘要语句（称为生成式摘要）。  

在机器学习领域，训练模型执行那些耗时或对人类评估极具挑战性的任务是一项重大难题，书籍摘要生成便是此类复杂任务之一。随着预训练Transformer模型的兴起，传统机器学习方法正经历革新。基于Transformer架构、采用自监督方式训练的语言模型，在针对文本摘要等自然语言处理（NLP）下游任务进行微调时，正受到广泛关注。本研究旨在探索基于Transformer的技术在书籍摘要生成中的应用。
