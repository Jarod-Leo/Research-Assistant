# Investigating the Role of Prompting and External Tools in Hallucination Rates of Large Language Models

链接: http://arxiv.org/abs/2410.19385v1

原文摘要:
Large Language Models (LLMs) are powerful computational models trained on
extensive corpora of human-readable text, enabling them to perform
general-purpose language understanding and generation. LLMs have garnered
significant attention in both industry and academia due to their exceptional
performance across various natural language processing (NLP) tasks. Despite
these successes, LLMs often produce inaccuracies, commonly referred to as
hallucinations. Prompt engineering, the process of designing and formulating
instructions for LLMs to perform specific tasks, has emerged as a key approach
to mitigating hallucinations. This paper provides a comprehensive empirical
evaluation of different prompting strategies and frameworks aimed at reducing
hallucinations in LLMs. Various prompting techniques are applied to a broad set
of benchmark datasets to assess the accuracy and hallucination rate of each
method. Additionally, the paper investigates the influence of tool-calling
agents (LLMs augmented with external tools to enhance their capabilities beyond
language generation) on hallucination rates in the same benchmarks. The
findings demonstrate that the optimal prompting technique depends on the type
of problem, and that simpler techniques often outperform more complex methods
in reducing hallucinations. Furthermore, it is shown that LLM agents can
exhibit significantly higher hallucination rates due to the added complexity of
external tool usage.

中文翻译:
大语言模型（LLMs）是基于海量人类可读文本训练的强大计算模型，能够执行通用语言理解与生成任务。凭借在各类自然语言处理（NLP）任务中的卓越表现，LLMs已引起工业界和学术界的广泛关注。然而这些模型仍频繁产生被称为"幻觉"的谬误。提示工程——即通过设计指令引导LLMs执行特定任务的方法——已成为缓解幻觉现象的关键途径。本文对不同提示策略与框架在降低LLMs幻觉方面的效果进行了全面实证评估：通过将多种提示技术应用于多样化基准数据集，量化分析了各方法的准确率与幻觉率；同时探究了工具调用型智能体（通过外部工具扩展语言生成能力的LLMs）对相同基准中幻觉率的影响。研究结果表明：最优提示技术因问题类型而异，且简单方法在降低幻觉方面常优于复杂策略；而由于外部工具使用带来的复杂性，LLM智能体可能表现出显著更高的幻觉率。
