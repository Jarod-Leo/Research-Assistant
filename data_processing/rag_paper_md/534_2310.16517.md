# OccuQuest: Mitigating Occupational Bias for Inclusive Large Language Models

链接: http://arxiv.org/abs/2310.16517v1

原文摘要:
The emergence of large language models (LLMs) has revolutionized natural
language processing tasks. However, existing instruction-tuning datasets suffer
from occupational bias: the majority of data relates to only a few occupations,
which hampers the instruction-tuned LLMs to generate helpful responses to
professional queries from practitioners in specific fields. To mitigate this
issue and promote occupation-inclusive LLMs, we create an instruction-tuning
dataset named \emph{OccuQuest}, which contains 110,000+ prompt-completion pairs
and 30,000+ dialogues covering over 1,000 occupations in 26 occupational
categories. We systematically request ChatGPT, organizing queries
hierarchically based on Occupation, Responsibility, Topic, and Question, to
ensure a comprehensive coverage of occupational specialty inquiries. By
comparing with three commonly used datasets (Dolly, ShareGPT, and WizardLM), we
observe that OccuQuest exhibits a more balanced distribution across
occupations. Furthermore, we assemble three test sets for comprehensive
evaluation, an occu-test set covering 25 occupational categories, an estate set
focusing on real estate, and an occu-quora set containing real-world questions
from Quora. We then fine-tune LLaMA on OccuQuest to obtain OccuLLaMA, which
significantly outperforms state-of-the-art LLaMA variants (Vicuna, Tulu, and
WizardLM) on professional questions in GPT-4 and human evaluations. Notably, on
the occu-quora set, OccuLLaMA reaches a high win rate of 86.4\% against
WizardLM.

中文翻译:
大型语言模型（LLM）的出现彻底改变了自然语言处理任务。然而，现有指令微调数据集普遍存在职业偏见：大部分数据仅涉及少数职业，这阻碍了指令微调后的LLM对特定领域从业者专业咨询生成有效回应。为缓解该问题并推动职业包容性LLM发展，我们构建了名为\emph{OccuQuest}的指令微调数据集，包含11万+提示-补全对和3万+对话，涵盖26个职业大类下的1000余种职业。我们系统性地调用ChatGPT，按照"职业-职责-主题-问题"的层级结构组织查询，确保全面覆盖职业专业性咨询。通过与三个常用数据集（Dolly、ShareGPT和WizardLM）对比，发现OccuQuest在职业分布上更为均衡。此外，我们构建了三个测试集用于综合评估：覆盖25个职业大类的occu-test集、聚焦房地产的estate集，以及包含Quora真实问题的occu-quora集。基于OccuQuest微调LLaMA得到的OccuLLaMA，在GPT-4和人工评估中均显著优于当前最先进的LLaMA变体（Vicuna、Tulu和WizardLM）。值得注意的是，在occu-quora测试集上，OccuLLaMA对WizardLM的胜率高达86.4\%。
