# OccuQuest: Mitigating Occupational Bias for Inclusive Large Language Models

链接: http://arxiv.org/abs/2310.16517v1

原文摘要:
The emergence of large language models (LLMs) has revolutionized natural
language processing tasks. However, existing instruction-tuning datasets suffer
from occupational bias: the majority of data relates to only a few occupations,
which hampers the instruction-tuned LLMs to generate helpful responses to
professional queries from practitioners in specific fields. To mitigate this
issue and promote occupation-inclusive LLMs, we create an instruction-tuning
dataset named \emph{OccuQuest}, which contains 110,000+ prompt-completion pairs
and 30,000+ dialogues covering over 1,000 occupations in 26 occupational
categories. We systematically request ChatGPT, organizing queries
hierarchically based on Occupation, Responsibility, Topic, and Question, to
ensure a comprehensive coverage of occupational specialty inquiries. By
comparing with three commonly used datasets (Dolly, ShareGPT, and WizardLM), we
observe that OccuQuest exhibits a more balanced distribution across
occupations. Furthermore, we assemble three test sets for comprehensive
evaluation, an occu-test set covering 25 occupational categories, an estate set
focusing on real estate, and an occu-quora set containing real-world questions
from Quora. We then fine-tune LLaMA on OccuQuest to obtain OccuLLaMA, which
significantly outperforms state-of-the-art LLaMA variants (Vicuna, Tulu, and
WizardLM) on professional questions in GPT-4 and human evaluations. Notably, on
the occu-quora set, OccuLLaMA reaches a high win rate of 86.4\% against
WizardLM.

中文翻译:
以下是符合学术规范的中文翻译：

大型语言模型（LLMs）的出现为自然语言处理任务带来了革命性变革。然而现有指令微调数据集普遍存在职业偏见：大部分数据仅涉及少数职业，这导致经指令微调的LLMs难以为特定领域从业者的专业查询生成有效回应。为缓解该问题并促进职业包容性LLMs的发展，我们构建了名为\emph{OccuQuest}的指令微调数据集，包含110,000+提示-补全对和30,000+对话，涵盖26个职业大类下的1,000余种职业。我们通过系统化调用ChatGPT，依据"职业-职责-主题-问题"的层级结构组织查询，确保对职业专业性询问的全面覆盖。与三种常用数据集（Dolly、ShareGPT和WizardLM）对比显示，OccuQuest呈现出更均衡的职业分布。此外，我们构建了三个测试集进行全面评估：覆盖25个职业大类的occu-test集、聚焦房地产的estate集，以及包含Quora真实问题的occu-quora集。基于OccuQuest对LLaMA微调得到的OccuLLaMA，在GPT-4和人工评估中针对专业问题的表现显著优于当前最先进的LLaMA变体（Vicuna、Tulu和WizardLM）。值得注意的是，在occu-quora集上，OccuLLaMA相较WizardLM获得86.4%的高胜率。

（翻译说明：
1. 专业术语统一处理："instruction-tuning"译为"指令微调"，"prompt-completion pairs"译为"提示-补全对"
2. 长句拆分重构：将原文复合句按中文表达习惯分解为多个短句
3. 被动语态转化："is requested"等被动结构转换为主动语态
4. 数据呈现方式：保留英文数据集名称及技术术语首字母大写
5. 数值表达：严格对应原文的"110,000+"等量化表述
6. 学术规范：使用"经""构建""呈现"等正式学术用语）
