# Zero-shot LLM-guided Counterfactual Generation for Text

链接: http://arxiv.org/abs/2405.04793v1

原文摘要:
With the development and proliferation of large, complex, black-box models
for solving many natural language processing (NLP) tasks, there is also an
increasing necessity of methods to stress-test these models and provide some
degree of interpretability or explainability. While counterfactual examples are
useful in this regard, automated generation of counterfactuals is a data and
resource intensive process. such methods depend on models such as pre-trained
language models that are then fine-tuned on auxiliary, often task-specific
datasets, that may be infeasible to build in practice, especially for new tasks
and data domains. Therefore, in this work we explore the possibility of
leveraging large language models (LLMs) for zero-shot counterfactual generation
in order to stress-test NLP models. We propose a structured pipeline to
facilitate this generation, and we hypothesize that the instruction-following
and textual understanding capabilities of recent LLMs can be effectively
leveraged for generating high quality counterfactuals in a zero-shot manner,
without requiring any training or fine-tuning. Through comprehensive
experiments on a variety of propreitary and open-source LLMs, along with
various downstream tasks in NLP, we explore the efficacy of LLMs as zero-shot
counterfactual generators in evaluating and explaining black-box NLP models.

中文翻译:
随着大型、复杂且不透明的模型在解决众多自然语言处理（NLP）任务中的广泛应用，对这些模型进行压力测试并提供一定程度的可解释性或可理解性方法的需求也日益增长。尽管反事实示例在此方面具有实用价值，但其自动化生成过程却需要大量数据和资源投入。此类方法通常依赖于预训练语言模型，并需针对辅助性（常为任务特定）数据集进行微调，而这类数据集在实际中可能难以构建，尤其对于新任务和数据领域而言。因此，本研究探索了利用大语言模型（LLMs）进行零样本反事实生成以测试NLP模型的可行性。我们提出了一种结构化流程来促进此类生成，并假设当前LLMs的指令遵循与文本理解能力可被有效用于零样本方式下生成高质量反事实，无需任何训练或微调。通过对多种专有及开源LLMs的全面实验，结合NLP领域各类下游任务，我们探究了LLMs作为零样本反事实生成器在评估和解释黑盒NLP模型方面的效能。
