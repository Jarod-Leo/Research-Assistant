# Pie: Pooling CPU Memory for LLM Inference

链接: http://arxiv.org/abs/2411.09317v1

原文摘要:
The rapid growth of LLMs has revolutionized natural language processing and
AI analysis, but their increasing size and memory demands present significant
challenges. A common solution is to spill over to CPU memory; however,
traditional GPU-CPU memory swapping often results in higher latency and lower
throughput.
  This paper introduces Pie, an LLM inference framework that addresses these
challenges with performance-transparent swapping and adaptive expansion. By
leveraging predictable memory access patterns and the high bandwidth of modern
hardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent
data swapping without affecting foreground computation, expanding effective
memory without added latency. Adaptive expansion dynamically adjusts CPU memory
allocation based on real-time information, optimizing memory usage and
performance under varying conditions.
  Pie maintains low computation latency, high throughput, and high elasticity.
Our experimental evaluation demonstrates that Pie achieves optimal swapping
policy during cache warmup and effectively balances increased memory capacity
with negligible impact on computation. With its extended capacity, Pie
outperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,
Pie can reduce GPU memory usage by up to 1.67X while maintaining the same
performance. Compared to FlexGen, an offline profiling-based swapping solution,
Pie achieves magnitudes lower latency and 9.4X higher throughput.

中文翻译:
以下是符合要求的学术化中文翻译：

大型语言模型（LLM）的快速发展为自然语言处理和人工智能分析带来了革命性变革，但其日益增长的模型规模与内存需求也带来了重大挑战。传统解决方案通常采用CPU内存溢出策略，然而常规的GPU-CPU内存交换往往导致延迟升高和吞吐量下降。

本文提出Pie框架，该LLM推理框架通过性能透明的交换机制与自适应扩展技术有效应对上述挑战。通过利用可预测的内存访问模式，结合NVIDIA GH200 Grace Hopper超级芯片等现代硬件的高带宽特性，Pie实现了不影响前台计算的并发数据交换，在零额外延迟条件下扩展有效内存容量。其自适应扩展机制能根据实时信息动态调整CPU内存分配，在不同运行条件下实现内存使用与性能的最优化。

Pie框架始终保持低计算延迟、高吞吐量和高弹性等优势。实验评估表明：Pie在缓存预热阶段能实现最优交换策略，在计算影响可忽略不计的前提下有效平衡内存容量扩展；在扩展容量场景下，其吞吐量较vLLM提升最高达1.9倍，延迟降低2倍；在保持同等性能时，可减少GPU内存使用达1.67倍。相较于基于离线分析的FlexGen交换方案，Pie实现了数量级更低的延迟和9.4倍的吞吐量提升。

（注：严格遵循学术翻译规范，专业术语保持统一，技术指标数据精确对应，被动语态转换为中文主动表达，长难句进行合理切分，同时保留"throughput/吞吐量"、"latency/延迟"等核心概念的标准译法）
