# Pie: Pooling CPU Memory for LLM Inference

链接: http://arxiv.org/abs/2411.09317v1

原文摘要:
The rapid growth of LLMs has revolutionized natural language processing and
AI analysis, but their increasing size and memory demands present significant
challenges. A common solution is to spill over to CPU memory; however,
traditional GPU-CPU memory swapping often results in higher latency and lower
throughput.
  This paper introduces Pie, an LLM inference framework that addresses these
challenges with performance-transparent swapping and adaptive expansion. By
leveraging predictable memory access patterns and the high bandwidth of modern
hardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent
data swapping without affecting foreground computation, expanding effective
memory without added latency. Adaptive expansion dynamically adjusts CPU memory
allocation based on real-time information, optimizing memory usage and
performance under varying conditions.
  Pie maintains low computation latency, high throughput, and high elasticity.
Our experimental evaluation demonstrates that Pie achieves optimal swapping
policy during cache warmup and effectively balances increased memory capacity
with negligible impact on computation. With its extended capacity, Pie
outperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,
Pie can reduce GPU memory usage by up to 1.67X while maintaining the same
performance. Compared to FlexGen, an offline profiling-based swapping solution,
Pie achieves magnitudes lower latency and 9.4X higher throughput.

中文翻译:
大型语言模型（LLM）的快速发展彻底改变了自然语言处理与人工智能分析领域，但其日益增长的规模与内存需求也带来了严峻挑战。当前主流解决方案是将数据溢出至CPU内存，然而传统的GPU-CPU内存交换往往导致延迟升高和吞吐量下降。

本文提出Pie框架，通过性能透明的交换机制与自适应扩展技术应对这些挑战。该框架利用可预测的内存访问模式，结合NVIDIA GH200 Grace Hopper超级芯片等现代硬件的高带宽特性，实现了不影响前台计算的并发数据交换，在零延迟代价下扩展有效内存容量。自适应扩展机制能根据实时信息动态调整CPU内存分配，在不同工况下优化内存使用与性能表现。

Pie框架始终保持低计算延迟、高吞吐量和高弹性特征。实验评估表明：Pie在缓存预热阶段实现了最优交换策略，有效平衡了内存容量提升与计算性能的关系，其影响可忽略不计。在扩展容量场景下，Pie的吞吐量较vLLM提升1.9倍，延迟降低2倍；在保持同等性能时，GPU内存占用最高减少1.67倍。相较于基于离线分析的FlexGen交换方案，Pie实现了数量级更低的延迟和9.4倍的吞吐量提升。
