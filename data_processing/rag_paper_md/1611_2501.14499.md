# Automated Assignment Grading with Large Language Models: Insights From a Bioinformatics Course

链接: http://arxiv.org/abs/2501.14499v1

原文摘要:
Providing students with individualized feedback through assignments is a
cornerstone of education that supports their learning and development. Studies
have shown that timely, high-quality feedback plays a critical role in
improving learning outcomes. However, providing personalized feedback on a
large scale in classes with large numbers of students is often impractical due
to the significant time and effort required. Recent advances in natural
language processing and large language models (LLMs) offer a promising solution
by enabling the efficient delivery of personalized feedback. These technologies
can reduce the workload of course staff while improving student satisfaction
and learning outcomes. Their successful implementation, however, requires
thorough evaluation and validation in real classrooms. We present the results
of a practical evaluation of LLM-based graders for written assignments in the
2024/25 iteration of the Introduction to Bioinformatics course at the
University of Ljubljana. Over the course of the semester, more than 100
students answered 36 text-based questions, most of which were automatically
graded using LLMs. In a blind study, students received feedback from both LLMs
and human teaching assistants without knowing the source, and later rated the
quality of the feedback. We conducted a systematic evaluation of six commercial
and open-source LLMs and compared their grading performance with human teaching
assistants. Our results show that with well-designed prompts, LLMs can achieve
grading accuracy and feedback quality comparable to human graders. Our results
also suggest that open-source LLMs perform as well as commercial LLMs, allowing
schools to implement their own grading systems while maintaining privacy.

中文翻译:
通过作业为学生提供个性化反馈是支撑其学习与发展的教育基石。研究表明，及时、高质量的反馈对提升学习成效具有关键作用。然而在大规模班级中，由于需要投入大量时间和精力，实施个性化反馈往往难以实现。自然语言处理与大语言模型（LLMs）的最新进展为高效提供个性化反馈带来了突破性解决方案，既能减轻课程人员的工作负担，又可提升学生满意度与学习效果。但成功应用这些技术仍需在实际教学场景中进行全面验证。本文呈现了卢布尔雅那大学生物信息学导论课程（2024/25学年）基于LLM的书面作业评分系统实践评估结果：学期期间，100余名学生完成了36道文本型题目，其中大部分由LLM自动评阅。通过双盲实验，学生在不知来源的情况下同时接收LLM与助教提供的反馈，并对反馈质量进行评分。我们系统评估了六种商业与开源LLM，将其评分表现与人类助教进行对比。结果表明：通过精心设计的提示词，LLM能达到与人类评阅者相当的评分准确性与反馈质量；开源LLM表现不逊于商业模型，使学校在保障数据隐私的前提下自主部署评分系统成为可能。
