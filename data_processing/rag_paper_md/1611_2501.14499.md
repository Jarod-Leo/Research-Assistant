# Automated Assignment Grading with Large Language Models: Insights From a Bioinformatics Course

链接: http://arxiv.org/abs/2501.14499v1

原文摘要:
Providing students with individualized feedback through assignments is a
cornerstone of education that supports their learning and development. Studies
have shown that timely, high-quality feedback plays a critical role in
improving learning outcomes. However, providing personalized feedback on a
large scale in classes with large numbers of students is often impractical due
to the significant time and effort required. Recent advances in natural
language processing and large language models (LLMs) offer a promising solution
by enabling the efficient delivery of personalized feedback. These technologies
can reduce the workload of course staff while improving student satisfaction
and learning outcomes. Their successful implementation, however, requires
thorough evaluation and validation in real classrooms. We present the results
of a practical evaluation of LLM-based graders for written assignments in the
2024/25 iteration of the Introduction to Bioinformatics course at the
University of Ljubljana. Over the course of the semester, more than 100
students answered 36 text-based questions, most of which were automatically
graded using LLMs. In a blind study, students received feedback from both LLMs
and human teaching assistants without knowing the source, and later rated the
quality of the feedback. We conducted a systematic evaluation of six commercial
and open-source LLMs and compared their grading performance with human teaching
assistants. Our results show that with well-designed prompts, LLMs can achieve
grading accuracy and feedback quality comparable to human graders. Our results
also suggest that open-source LLMs perform as well as commercial LLMs, allowing
schools to implement their own grading systems while maintaining privacy.

中文翻译:
通过作业为学生提供个性化反馈是支持其学习与发展的教育基石。研究表明，及时、高质量的反馈对提升学习成效具有关键作用。然而，在人数众多的课堂中大规模提供个性化反馈往往因需耗费大量时间精力而难以实现。自然语言处理与大型语言模型（LLMs）的最新进展为高效提供个性化反馈带来了突破性解决方案，这些技术既能减轻课程人员的工作负担，又能提升学生满意度与学习效果。但其成功应用仍需在实际课堂中进行全面评估验证。

本文展示了卢布尔雅那大学生物信息学导论课程2024/25学年对基于LLM的书面作业评分系统进行的实践评估结果。学期中，100余名学生回答了36个文本型问题，其中大部分由LLM自动评分。在盲测研究中，学生同时收到LLM与人类助教的匿名反馈，随后对反馈质量进行评分。我们系统评估了六种商业与开源LLM，并将其评分表现与人类助教进行对比。结果表明：通过精心设计的提示词，LLM能达到与人类评分者相当的评分准确性与反馈质量；开源LLM表现与商业LLM相当，使学校在保障隐私的前提下可自主部署评分系统。
