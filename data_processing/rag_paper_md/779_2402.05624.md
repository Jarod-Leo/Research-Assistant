# Efficient Models for the Detection of Hate, Abuse and Profanity

链接: http://arxiv.org/abs/2402.05624v1

原文摘要:
Large Language Models (LLMs) are the cornerstone for many Natural Language
Processing (NLP) tasks like sentiment analysis, document classification, named
entity recognition, question answering, summarization, etc. LLMs are often
trained on data which originates from the web. This data is prone to having
content with Hate, Abuse and Profanity (HAP). For a detailed definition of HAP,
please refer to the Appendix. Due to the LLMs being exposed to HAP content
during training, the models learn it and may then generate hateful or profane
content. For example, when the open-source RoBERTa model (specifically, the
RoBERTA base model) from the HuggingFace (HF) Transformers library is prompted
to replace the mask token in `I do not know that Persian people are that MASK`
it returns the word `stupid` with the highest score. This is unacceptable in
civil discourse.The detection of Hate, Abuse and Profanity in text is a vital
component of creating civil and unbiased LLMs, which is needed not only for
English, but for all languages. In this article, we briefly describe the
creation of HAP detectors and various ways of using them to make models civil
and acceptable in the output they generate.

中文翻译:
大型语言模型（LLM）已成为情感分析、文档分类、命名实体识别、问答系统及文本摘要等自然语言处理（NLP）任务的核心技术。这类模型通常基于网络数据进行训练，而此类数据常包含仇恨言论、侮辱性内容与污秽用语（HAP，具体定义参见附录）。由于训练过程中接触HAP内容，模型可能习得并生成具有攻击性或低俗的文本。例如，当调用HuggingFace（HF）Transformers库中的开源RoBERTa模型（基础版）对句子"I do not know that Persian people are that MASK"进行掩码预测时，模型会以最高置信度返回"stupid"一词——这在文明对话中是不可接受的。  

构建文本中HAP内容的检测机制，是打造文明、无偏见大型语言模型的关键环节，且该需求不仅限于英语，适用于所有语言。本文简要阐述了HAP检测器的构建方法，以及如何通过多种技术手段确保模型输出符合文明规范与社会接受度。
