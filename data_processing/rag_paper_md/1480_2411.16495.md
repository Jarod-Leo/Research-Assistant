# AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous Knowledge Reasoning

链接: http://arxiv.org/abs/2411.16495v1

原文摘要:
Despite the outstanding capabilities of large language models (LLMs),
knowledge-intensive reasoning still remains a challenging task due to LLMs'
limitations in compositional reasoning and the hallucination problem. A
prevalent solution is to employ chain-of-thought (CoT) with retrieval-augmented
generation (RAG), which first formulates a reasoning plan by decomposing
complex questions into simpler sub-questions, and then applies iterative RAG at
each sub-question. However, prior works exhibit two crucial problems:
inadequate reasoning planning and poor incorporation of heterogeneous
knowledge. In this paper, we introduce AtomR, a framework for LLMs to conduct
accurate heterogeneous knowledge reasoning at the atomic level. Inspired by how
knowledge graph query languages model compositional reasoning through combining
predefined operations, we propose three atomic knowledge operators, a unified
set of operators for LLMs to retrieve and manipulate knowledge from
heterogeneous sources. First, in the reasoning planning stage, AtomR decomposes
a complex question into a reasoning tree where each leaf node corresponds to an
atomic knowledge operator, achieving question decomposition that is highly
fine-grained and orthogonal. Subsequently, in the reasoning execution stage,
AtomR executes each atomic knowledge operator, which flexibly selects,
retrieves, and operates atomic level knowledge from heterogeneous sources. We
also introduce BlendQA, a challenging benchmark specially tailored for
heterogeneous knowledge reasoning. Experiments on three single-source and two
multi-source datasets show that AtomR outperforms state-of-the-art baselines by
a large margin, with F1 score improvements of 9.4% on 2WikiMultihop and 9.5% on
BlendQA. We release our code and datasets.

中文翻译:
尽管大语言模型（LLM）展现出卓越的能力，但在知识密集型推理任务中仍面临两大挑战：组合推理能力的不足与幻觉问题。当前主流解决方案是结合思维链（CoT）与检索增强生成（RAG），通过将复杂问题分解为子问题并迭代执行RAG来实现推理。然而现有方法存在两个关键缺陷：推理规划不充分与异构知识融合能力弱。本文提出AtomR框架，使LLM能在原子级别执行精准的异构知识推理。受知识图谱查询语言通过预定义操作实现组合推理的启发，我们设计了三类原子知识操作符作为统一接口，支持LLM从异构源检索和操作知识。在推理规划阶段，AtomR将复杂问题分解为以原子操作符为叶节点的推理树，实现高度细粒度且正交的问题拆解；在推理执行阶段，则灵活选择、检索和操作异构源的原子级知识。我们还构建了专为异构知识推理设计的BlendQA基准测试。在三个单源和两个多源数据集上的实验表明，AtomR显著优于现有最优基线，在2WikiMultihop和BlendQA上分别实现9.4%和9.5%的F1值提升。相关代码与数据集已开源。
