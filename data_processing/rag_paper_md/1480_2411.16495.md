# AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous Knowledge Reasoning

链接: http://arxiv.org/abs/2411.16495v1

原文摘要:
Despite the outstanding capabilities of large language models (LLMs),
knowledge-intensive reasoning still remains a challenging task due to LLMs'
limitations in compositional reasoning and the hallucination problem. A
prevalent solution is to employ chain-of-thought (CoT) with retrieval-augmented
generation (RAG), which first formulates a reasoning plan by decomposing
complex questions into simpler sub-questions, and then applies iterative RAG at
each sub-question. However, prior works exhibit two crucial problems:
inadequate reasoning planning and poor incorporation of heterogeneous
knowledge. In this paper, we introduce AtomR, a framework for LLMs to conduct
accurate heterogeneous knowledge reasoning at the atomic level. Inspired by how
knowledge graph query languages model compositional reasoning through combining
predefined operations, we propose three atomic knowledge operators, a unified
set of operators for LLMs to retrieve and manipulate knowledge from
heterogeneous sources. First, in the reasoning planning stage, AtomR decomposes
a complex question into a reasoning tree where each leaf node corresponds to an
atomic knowledge operator, achieving question decomposition that is highly
fine-grained and orthogonal. Subsequently, in the reasoning execution stage,
AtomR executes each atomic knowledge operator, which flexibly selects,
retrieves, and operates atomic level knowledge from heterogeneous sources. We
also introduce BlendQA, a challenging benchmark specially tailored for
heterogeneous knowledge reasoning. Experiments on three single-source and two
multi-source datasets show that AtomR outperforms state-of-the-art baselines by
a large margin, with F1 score improvements of 9.4% on 2WikiMultihop and 9.5% on
BlendQA. We release our code and datasets.

中文翻译:
尽管大语言模型（LLM）展现出卓越的能力，但在知识密集型推理任务中仍面临两大挑战：组合推理能力的局限与幻觉问题。当前主流解决方案是结合思维链（CoT）与检索增强生成（RAG），即先将复杂问题分解为简单子问题制定推理计划，再对每个子问题迭代执行RAG。然而现有方法存在两个关键缺陷：推理规划不充分与异构知识融合能力薄弱。本文提出AtomR框架，使LLM能在原子级别执行精准的异构知识推理。受知识图谱查询语言通过预定义操作组合实现推理的启发，我们设计了三类原子知识操作符——这是LLM从异构源检索与操作知识的统一算子集。在推理规划阶段，AtomR将复杂问题分解为推理树，其每个叶节点对应一个原子操作符，实现高度细粒度且正交的问题分解；在推理执行阶段，AtomR通过原子操作符灵活选择、检索并操作异构源的原子级知识。我们还构建了BlendQA基准测试，专门针对异构知识推理的挑战性场景。在三个单源与两个多源数据集上的实验表明，AtomR显著优于现有最优基线，在2WikiMultihop和BlendQA上分别实现9.4%和9.5%的F1值提升。我们公开了代码与数据集。

（翻译说明：1. 专业术语如"LLM/RAG/CoT"保留英文缩写并添加中文注释 2. "atomic level"译为"原子级别"突出最小操作单元概念 3. "heterogeneous sources"统一处理为"异构源"保持计算机领域术语一致性 4. 长难句拆分重组，如将"which first formulates..."处理为"即先...再..."的汉语流水句式 5. 被动语态转换，如"are decomposed"译为主动式"将...分解为" 6. 补充"预定义操作组合"等解释性成分确保技术准确性）
