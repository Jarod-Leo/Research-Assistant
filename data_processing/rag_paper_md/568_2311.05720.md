# Long-Horizon Dialogue Understanding for Role Identification in the Game of Avalon with Large Language Models

链接: http://arxiv.org/abs/2311.05720v1

原文摘要:
Deception and persuasion play a critical role in long-horizon dialogues
between multiple parties, especially when the interests, goals, and motivations
of the participants are not aligned. Such complex tasks pose challenges for
current Large Language Models (LLM) as deception and persuasion can easily
mislead them, especially in long-horizon multi-party dialogues. To this end, we
explore the game of Avalon: The Resistance, a social deduction game in which
players must determine each other's hidden identities to complete their team's
objective. We introduce an online testbed and a dataset containing 20 carefully
collected and labeled games among human players that exhibit long-horizon
deception in a cooperative-competitive setting. We discuss the capabilities of
LLMs to utilize deceptive long-horizon conversations between six human players
to determine each player's goal and motivation. Particularly, we discuss the
multimodal integration of the chat between the players and the game's state
that grounds the conversation, providing further insights into the true player
identities. We find that even current state-of-the-art LLMs do not reach human
performance, making our dataset a compelling benchmark to investigate the
decision-making and language-processing capabilities of LLMs. Our dataset and
online testbed can be found at our project website:
https://sstepput.github.io/Avalon-NLU/

中文翻译:
在多方的长期对话中，欺骗与说服发挥着关键作用，尤其当参与者的利益、目标和动机不一致时。这类复杂任务对当前大型语言模型（LLM）提出了挑战，因为欺骗和说服极易误导模型，特别是在长期多方对话场景中。为此，我们以社交推理游戏《阿瓦隆：抵抗组织》为研究对象——该游戏中玩家需通过识别彼此隐藏身份来完成团队目标。我们构建了一个在线测试平台，并发布包含20场精心采集和标注的人类玩家对局数据集，这些数据展现了合作-竞争情境下的长期欺骗行为。我们探讨了LLM如何利用六名人类玩家间具有欺骗性的长期对话，来推断每位玩家的目标与动机。特别关注了玩家聊天内容与游戏状态的多模态整合，这种整合能锚定对话语境，为识别真实玩家身份提供更深层线索。研究发现，即使当前最先进的LLM也未能达到人类水平，这使得我们的数据集成为研究LLM决策与语言处理能力的理想基准。数据集与在线测试平台详见项目网站：https://sstepput.github.io/Avalon-NLU/
