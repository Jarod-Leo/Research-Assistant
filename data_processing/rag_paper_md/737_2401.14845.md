# Adaptive Point Transformer

链接: http://arxiv.org/abs/2401.14845v1

原文摘要:
The recent surge in 3D data acquisition has spurred the development of
geometric deep learning models for point cloud processing, boosted by the
remarkable success of transformers in natural language processing. While point
cloud transformers (PTs) have achieved impressive results recently, their
quadratic scaling with respect to the point cloud size poses a significant
scalability challenge for real-world applications. To address this issue, we
propose the Adaptive Point Cloud Transformer (AdaPT), a standard PT model
augmented by an adaptive token selection mechanism. AdaPT dynamically reduces
the number of tokens during inference, enabling efficient processing of large
point clouds. Furthermore, we introduce a budget mechanism to flexibly adjust
the computational cost of the model at inference time without the need for
retraining or fine-tuning separate models. Our extensive experimental
evaluation on point cloud classification tasks demonstrates that AdaPT
significantly reduces computational complexity while maintaining competitive
accuracy compared to standard PTs. The code for AdaPT is made publicly
available.

中文翻译:
近期三维数据采集技术的迅猛发展，推动了点云处理领域几何深度学习模型的进步，其中自然语言处理中Transformer架构的巨大成功更起到了关键助推作用。尽管点云Transformer（PT）近期取得了显著成果，但其计算复杂度随点云规模呈平方级增长的特性，在实际应用中面临严峻的可扩展性挑战。为此，我们提出自适应点云Transformer（AdaPT），通过在标准PT模型中集成自适应令牌选择机制，动态缩减推理过程中的令牌数量，从而实现对大规模点云的高效处理。此外，我们创新性地引入预算调控机制，无需重新训练或微调独立模型，即可在推理阶段灵活调整计算资源消耗。在点云分类任务上的大量实验表明，AdaPT在保持与标准PT相当精度的同时，显著降低了计算复杂度。本研究成果的代码已开源发布。
