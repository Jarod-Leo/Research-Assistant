# Optimizing Vision Transformers with Data-Free Knowledge Transfer

链接: http://arxiv.org/abs/2408.05952v1

原文摘要:
The groundbreaking performance of transformers in Natural Language Processing
(NLP) tasks has led to their replacement of traditional Convolutional Neural
Networks (CNNs), owing to the efficiency and accuracy achieved through the
self-attention mechanism. This success has inspired researchers to explore the
use of transformers in computer vision tasks to attain enhanced long-term
semantic awareness. Vision transformers (ViTs) have excelled in various
computer vision tasks due to their superior ability to capture long-distance
dependencies using the self-attention mechanism. Contemporary ViTs like Data
Efficient Transformers (DeiT) can effectively learn both global semantic
information and local texture information from images, achieving performance
comparable to traditional CNNs. However, their impressive performance comes
with a high computational cost due to very large number of parameters,
hindering their deployment on devices with limited resources like smartphones,
cameras, drones etc. Additionally, ViTs require a large amount of data for
training to achieve performance comparable to benchmark CNN models. Therefore,
we identified two key challenges in deploying ViTs on smaller form factor
devices: the high computational requirements of large models and the need for
extensive training data. As a solution to these challenges, we propose
compressing large ViT models using Knowledge Distillation (KD), which is
implemented data-free to circumvent limitations related to data availability.
Additionally, we conducted experiments on object detection within the same
environment in addition to classification tasks. Based on our analysis, we
found that datafree knowledge distillation is an effective method to overcome
both issues, enabling the deployment of ViTs on less resourceconstrained
devices.

中文翻译:
Transformer在自然语言处理（NLP）任务中的突破性表现，凭借自注意力机制实现的高效与精准，使其成功取代了传统卷积神经网络（CNN）。这一成就激励研究者将transformer架构引入计算机视觉领域，以获取更强的长程语义感知能力。视觉Transformer（ViT）凭借自注意力机制在捕捉长距离依赖关系方面的卓越表现，已在各类视觉任务中展现出优势。当代ViT模型如数据高效Transformer（DeiT）能同时学习图像的全局语义信息与局部纹理特征，其性能已可比肩传统CNN。然而，这种卓越性能伴随着高昂的计算代价——海量参数规模阻碍了其在智能手机、摄像设备、无人机等资源受限终端上的部署。此外，ViT需要大量训练数据才能达到基准CNN模型的性能水平。因此，我们识别出ViT在小型化设备部署面临的两大核心挑战：大模型的高计算需求与庞大数据训练要求。针对这些挑战，我们提出采用无数据知识蒸馏（KD）技术压缩大型ViT模型，以规避数据可获性限制。除分类任务外，我们还在相同环境下进行了目标检测实验。分析表明，无数据知识蒸馏能有效解决上述双重难题，为ViT在资源受限设备的部署提供了可行方案。
