# Are Large Language Models Post Hoc Explainers?

链接: http://arxiv.org/abs/2310.05797v2

原文摘要:
Recent advancements in Large Language Models (LLMs) have demonstrated
exceptional capabilities in complex tasks like machine translation, commonsense
reasoning, and language understanding. One of the primary reasons for the
adaptability of LLMs in such diverse tasks is their in-context learning (ICL)
capability, which allows them to perform well on new tasks by simply using a
few task samples in the prompt. Despite their effectiveness in enhancing the
performance of LLMs on diverse language and tabular tasks, these methods have
not been thoroughly explored for their potential to generate post hoc
explanations. In this work, we carry out one of the first explorations to
analyze the effectiveness of LLMs in explaining other complex predictive models
using ICL. To this end, we propose a novel framework, In-Context Explainers,
comprising of three novel approaches that exploit the ICL capabilities of LLMs
to explain the predictions made by other predictive models. We conduct
extensive analysis with these approaches on real-world tabular and text
datasets and demonstrate that LLMs are capable of explaining other predictive
models similar to state-of-the-art post hoc explainers, opening up promising
avenues for future research into LLM-based post hoc explanations of complex
predictive models.

中文翻译:
大型语言模型（LLM）的最新进展在机器翻译、常识推理和语言理解等复杂任务中展现出卓越能力。其适应多样化任务的关键因素之一是上下文学习（ICL）能力——仅需在提示中加入少量任务样本，模型即可在新任务中表现优异。尽管这些方法能有效提升LLM在语言与表格任务中的性能，但其生成事后解释的潜力尚未得到充分探索。本研究首次系统分析了LLM利用ICL解释其他复杂预测模型的效果，提出创新框架"上下文解释器"，包含三种新方法以挖掘LLM的ICL潜力来解释其他预测模型的决策。通过在真实世界表格与文本数据集上的广泛实验，我们证明LLM能够媲美最先进的事后解释方法，为未来基于LLM的复杂预测模型事后解释研究开辟了新的可能性。
