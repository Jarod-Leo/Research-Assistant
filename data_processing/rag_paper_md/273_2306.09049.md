# Mapping Researcher Activity based on Publication Data by means of Transformers

链接: http://arxiv.org/abs/2306.09049v1

原文摘要:
Modern performance on several natural language processing (NLP) tasks has
been enhanced thanks to the Transformer-based pre-trained language model BERT.
We employ this concept to investigate a local publication database. Research
papers are encoded and clustered to form a landscape view of the scientific
topics, in which research is active. Authors working on similar topics can be
identified by calculating the similarity between their papers. Based on this,
we define a similarity metric between authors. Additionally we introduce the
concept of self-similarity to indicate the topical variety of authors.

中文翻译:
得益于基于Transformer的预训练语言模型BERT，多项自然语言处理（NLP）任务的现代性能得到了显著提升。本研究运用这一技术对本地出版物数据库进行探索：通过将科研论文进行编码与聚类，构建出活跃科研领域的主题全景图。通过计算论文间的相似度，可识别出从事相关课题研究的学者，据此我们定义了学者间的相似性度量指标。此外，我们还提出了"自我相似性"这一新概念，用以衡量学者研究主题的多样性。

（翻译说明：
1. 采用学术论文摘要的规范表达方式，使用"本研究"替代原文第一人称
2. 将长句合理切分为符合中文阅读习惯的短句结构
3. 专业术语如"self-similarity"译为"自我相似性"并添加引号强调
4. "landscape view"意译为"全景图"既保留意象又符合中文表达
5. 被动语态转换为主动语态（如"are encoded"处理为"进行编码"）
6. 保持"similarity metric"等专业表述的准确性
7. 通过冒号和分段优化原文的信息层次结构）
