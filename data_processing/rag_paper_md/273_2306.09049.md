# Mapping Researcher Activity based on Publication Data by means of Transformers

链接: http://arxiv.org/abs/2306.09049v1

原文摘要:
Modern performance on several natural language processing (NLP) tasks has
been enhanced thanks to the Transformer-based pre-trained language model BERT.
We employ this concept to investigate a local publication database. Research
papers are encoded and clustered to form a landscape view of the scientific
topics, in which research is active. Authors working on similar topics can be
identified by calculating the similarity between their papers. Based on this,
we define a similarity metric between authors. Additionally we introduce the
concept of self-similarity to indicate the topical variety of authors.

中文翻译:
得益于基于Transformer的预训练语言模型BERT，多项自然语言处理（NLP）任务的现代性能得到了显著提升。我们运用这一理念对本地出版物数据库展开研究：通过编码和聚类科研论文，构建出活跃研究领域的科学主题全景视图。通过计算论文间的相似度，可识别出从事相近课题的研究人员，据此我们定义了作者间的相似性度量指标。此外，我们引入自相似性概念来表征作者研究主题的多样性特征。
