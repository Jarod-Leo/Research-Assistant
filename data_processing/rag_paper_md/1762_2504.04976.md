# A Domain-Based Taxonomy of Jailbreak Vulnerabilities in Large Language Models

链接: http://arxiv.org/abs/2504.04976v1

原文摘要:
The study of large language models (LLMs) is a key area in open-world machine
learning. Although LLMs demonstrate remarkable natural language processing
capabilities, they also face several challenges, including consistency issues,
hallucinations, and jailbreak vulnerabilities. Jailbreaking refers to the
crafting of prompts that bypass alignment safeguards, leading to unsafe outputs
that compromise the integrity of LLMs. This work specifically focuses on the
challenge of jailbreak vulnerabilities and introduces a novel taxonomy of
jailbreak attacks grounded in the training domains of LLMs. It characterizes
alignment failures through generalization, objectives, and robustness gaps. Our
primary contribution is a perspective on jailbreak, framed through the
different linguistic domains that emerge during LLM training and alignment.
This viewpoint highlights the limitations of existing approaches and enables us
to classify jailbreak attacks on the basis of the underlying model deficiencies
they exploit. Unlike conventional classifications that categorize attacks based
on prompt construction methods (e.g., prompt templating), our approach provides
a deeper understanding of LLM behavior. We introduce a taxonomy with four
categories -- mismatched generalization, competing objectives, adversarial
robustness, and mixed attacks -- offering insights into the fundamental nature
of jailbreak vulnerabilities. Finally, we present key lessons derived from this
taxonomic study.

中文翻译:
大型语言模型（LLMs）的研究是开放世界机器学习的关键领域。尽管LLMs展现出卓越的自然语言处理能力，但也面临诸多挑战，包括一致性问题、幻觉现象和越狱漏洞。越狱指通过精心设计的提示绕过对齐防护机制，导致模型输出危害其完整性的不安全内容。本研究聚焦越狱漏洞问题，提出了一种基于LLM训练领域的新型越狱攻击分类法，从泛化性、目标函数和鲁棒性三个维度解析对齐失效机制。我们的核心贡献是从LLM训练与对齐过程中产生的不同语言域视角来解构越狱现象，这一视角不仅揭示了现有方法的局限性，更使我们能够根据攻击所利用的模型缺陷本质进行分类。与传统基于提示构造方法（如提示模板）的分类方式不同，我们的方法能更深入地理解LLM行为特征。我们构建了包含四类攻击的 taxonomy——泛化失配、目标冲突、对抗鲁棒性和混合攻击，从而揭示越狱漏洞的本质特性。最后，我们总结了该分类研究得出的关键启示。
