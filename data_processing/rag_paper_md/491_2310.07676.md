# Composite Backdoor Attacks Against Large Language Models

链接: http://arxiv.org/abs/2310.07676v1

原文摘要:
Large language models (LLMs) have demonstrated superior performance compared
to previous methods on various tasks, and often serve as the foundation models
for many researches and services. However, the untrustworthy third-party LLMs
may covertly introduce vulnerabilities for downstream tasks. In this paper, we
explore the vulnerability of LLMs through the lens of backdoor attacks.
Different from existing backdoor attacks against LLMs, ours scatters multiple
trigger keys in different prompt components. Such a Composite Backdoor Attack
(CBA) is shown to be stealthier than implanting the same multiple trigger keys
in only a single component. CBA ensures that the backdoor is activated only
when all trigger keys appear. Our experiments demonstrate that CBA is effective
in both natural language processing (NLP) and multimodal tasks. For instance,
with $3\%$ poisoning samples against the LLaMA-7B model on the Emotion dataset,
our attack achieves a $100\%$ Attack Success Rate (ASR) with a False Triggered
Rate (FTR) below $2.06\%$ and negligible model accuracy degradation. Our work
highlights the necessity of increased security research on the trustworthiness
of foundation LLMs.

中文翻译:
大型语言模型（LLM）在多项任务中展现出超越传统方法的卓越性能，常被作为众多研究与服务的基础模型。然而，不可信的第三方LLM可能潜藏漏洞，危及下游任务安全。本文从后门攻击视角探究LLM的脆弱性，提出一种创新攻击策略：现有研究多将触发器集中于单一提示组件，而我们的复合后门攻击（CBA）将多个触发密钥分散嵌入不同提示组件。实验证明，相较于单组件植入相同数量触发器的方案，CBA具有更强的隐蔽性——仅当所有触发密钥同时出现时才会激活后门。在自然语言处理和多模态任务测试中，CBA均表现显著：例如针对LLaMA-7B模型在Emotion数据集仅注入3%的毒化样本，攻击成功率即达100%，误触发率低于2.06%，且模型精度损失可忽略。本研究揭示了加强基础LLM可信安全研究的紧迫性。
