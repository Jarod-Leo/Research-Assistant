# NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark

链接: http://arxiv.org/abs/2310.18018v1

原文摘要:
In this position paper, we argue that the classical evaluation on Natural
Language Processing (NLP) tasks using annotated benchmarks is in trouble. The
worst kind of data contamination happens when a Large Language Model (LLM) is
trained on the test split of a benchmark, and then evaluated in the same
benchmark. The extent of the problem is unknown, as it is not straightforward
to measure. Contamination causes an overestimation of the performance of a
contaminated model in a target benchmark and associated task with respect to
their non-contaminated counterparts. The consequences can be very harmful, with
wrong scientific conclusions being published while other correct ones are
discarded. This position paper defines different levels of data contamination
and argues for a community effort, including the development of automatic and
semi-automatic measures to detect when data from a benchmark was exposed to a
model, and suggestions for flagging papers with conclusions that are
compromised by data contamination.

中文翻译:
在本立场文件中，我们指出基于标注基准测试的自然语言处理（NLP）任务传统评估方式正面临严峻挑战。当大语言模型（LLM）在某个基准测试的测试集上进行训练后，又使用同一基准进行评估时，就会发生最严重的数据污染问题。由于该问题难以直接量化，其影响范围尚不明确。数据污染会导致受污染模型在目标基准测试及相关任务中的表现被高估，从而掩盖未受污染模型的真实性能差距。这种状况可能引发严重后果——被污染数据支撑的错误科学结论得以发表，而正确的结论反而被忽视。本文界定了不同级别的数据污染，并呼吁学界共同采取行动：包括开发自动/半自动检测工具来识别模型是否接触过基准测试数据，同时对因数据污染导致结论可信度存疑的论文建立标记机制。
