# Transformers in Reinforcement Learning: A Survey

链接: http://arxiv.org/abs/2307.05979v1

原文摘要:
Transformers have significantly impacted domains like natural language
processing, computer vision, and robotics, where they improve performance
compared to other neural networks. This survey explores how transformers are
used in reinforcement learning (RL), where they are seen as a promising
solution for addressing challenges such as unstable training, credit
assignment, lack of interpretability, and partial observability. We begin by
providing a brief domain overview of RL, followed by a discussion on the
challenges of classical RL algorithms. Next, we delve into the properties of
the transformer and its variants and discuss the characteristics that make them
well-suited to address the challenges inherent in RL. We examine the
application of transformers to various aspects of RL, including representation
learning, transition and reward function modeling, and policy optimization. We
also discuss recent research that aims to enhance the interpretability and
efficiency of transformers in RL, using visualization techniques and efficient
training strategies. Often, the transformer architecture must be tailored to
the specific needs of a given application. We present a broad overview of how
transformers have been adapted for several applications, including robotics,
medicine, language modeling, cloud computing, and combinatorial optimization.
We conclude by discussing the limitations of using transformers in RL and
assess their potential for catalyzing future breakthroughs in this field.

中文翻译:
Transformer模型已显著影响了自然语言处理、计算机视觉和机器人技术等领域，相较于其他神经网络展现出更优性能。本文综述探讨了Transformer在强化学习（RL）中的应用——其被视为解决训练不稳定、信用分配、可解释性缺失及部分可观测性等挑战的潜在方案。我们首先概述强化学习领域，继而分析传统RL算法的核心难题。随后深入解析Transformer及其变体的特性，阐明其适配RL内在挑战的优势所在。重点考察Transformer在RL多环节的应用，包括表征学习、状态转移与奖励函数建模、策略优化等。同时评述了通过可视化技术与高效训练策略来增强Transformer可解释性与效率的最新研究。针对不同应用场景的需求，Transformer架构常需定制化调整，本文系统梳理了其在机器人、医疗、语言建模、云计算及组合优化等领域的适配方案。最后，我们讨论了Transformer在RL中的局限性，并评估其推动该领域未来突破的潜力。
