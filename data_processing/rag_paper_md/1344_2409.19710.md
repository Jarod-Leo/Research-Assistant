# A multimodal LLM for the non-invasive decoding of spoken text from brain recordings

链接: http://arxiv.org/abs/2409.19710v1

原文摘要:
Brain-related research topics in artificial intelligence have recently gained
popularity, particularly due to the expansion of what multimodal architectures
can do from computer vision to natural language processing. Our main goal in
this work is to explore the possibilities and limitations of these
architectures in spoken text decoding from non-invasive fMRI recordings.
Contrary to vision and textual data, fMRI data represent a complex modality due
to the variety of brain scanners, which implies (i) the variety of the recorded
signal formats, (ii) the low resolution and noise of the raw signals, and (iii)
the scarcity of pretrained models that can be leveraged as foundation models
for generative learning. These points make the problem of the non-invasive
decoding of text from fMRI recordings very challenging. In this paper, we
propose and end-to-end multimodal LLM for decoding spoken text from fMRI
signals. The proposed architecture is founded on (i) an encoder derived from a
specific transformer incorporating an augmented embedding layer for the encoder
and a better-adjusted attention mechanism than that present in the state of the
art, and (ii) a frozen large language model adapted to align the embedding of
the input text and the encoded embedding of brain activity to decode the output
text. A benchmark in performed on a corpus consisting of a set of interactions
human-human and human-robot interactions where fMRI and conversational signals
are recorded synchronously. The obtained results are very promising, as our
proposal outperforms the evaluated models, and is able to generate text
capturing more accurate semantics present in the ground truth. The
implementation code is provided in https://github.com/Hmamouche/brain_decode.

中文翻译:
人工智能领域中与大脑相关的研究课题近期备受关注，这主要得益于多模态架构从计算机视觉到自然语言处理的能力拓展。本研究旨在探索此类架构在非侵入式功能磁共振成像（fMRI）记录中解码口语文本的可能性与局限性。与视觉和文本数据不同，fMRI数据因脑扫描仪多样性而呈现复杂模态特征，具体表现为：（1）记录信号格式的多样性；（2）原始信号分辨率低且含噪声；（3）缺乏可作为生成学习基础模型的预训练模型。这些因素使得基于fMRI记录的非侵入式文本解码极具挑战性。本文提出了一种端到端多模态大语言模型，用于从fMRI信号解码口语文本。该架构的创新点在于：（1）采用改进型编码器——基于特定Transformer结构，配备增强型嵌入层和优于现有技术的注意力机制；（2）集成冻结参数的大语言模型，通过对齐输入文本嵌入与脑活动编码嵌入来实现文本解码。我们在同步记录fMRI与会话信号的人-人及人-机器人交互语料库上进行基准测试，结果表明：所提模型性能优于现有评估模型，能生成更准确反映真实语义的文本，展现出显著优势。实现代码已发布于https://github.com/Hmamouche/brain_decode。
