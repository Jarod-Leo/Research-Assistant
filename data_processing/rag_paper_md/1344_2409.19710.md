# A multimodal LLM for the non-invasive decoding of spoken text from brain recordings

链接: http://arxiv.org/abs/2409.19710v1

原文摘要:
Brain-related research topics in artificial intelligence have recently gained
popularity, particularly due to the expansion of what multimodal architectures
can do from computer vision to natural language processing. Our main goal in
this work is to explore the possibilities and limitations of these
architectures in spoken text decoding from non-invasive fMRI recordings.
Contrary to vision and textual data, fMRI data represent a complex modality due
to the variety of brain scanners, which implies (i) the variety of the recorded
signal formats, (ii) the low resolution and noise of the raw signals, and (iii)
the scarcity of pretrained models that can be leveraged as foundation models
for generative learning. These points make the problem of the non-invasive
decoding of text from fMRI recordings very challenging. In this paper, we
propose and end-to-end multimodal LLM for decoding spoken text from fMRI
signals. The proposed architecture is founded on (i) an encoder derived from a
specific transformer incorporating an augmented embedding layer for the encoder
and a better-adjusted attention mechanism than that present in the state of the
art, and (ii) a frozen large language model adapted to align the embedding of
the input text and the encoded embedding of brain activity to decode the output
text. A benchmark in performed on a corpus consisting of a set of interactions
human-human and human-robot interactions where fMRI and conversational signals
are recorded synchronously. The obtained results are very promising, as our
proposal outperforms the evaluated models, and is able to generate text
capturing more accurate semantics present in the ground truth. The
implementation code is provided in https://github.com/Hmamouche/brain_decode.

中文翻译:
人工智能领域中与大脑相关的研究课题近期备受关注，这主要得益于多模态架构的能力边界从计算机视觉扩展至自然语言处理领域。本研究旨在探索此类架构在非侵入式功能磁共振成像（fMRI）语音文本解码中的应用潜力与局限。与视觉和文本数据不同，fMRI数据因其（i）脑扫描设备多样性导致的信号格式差异，（ii）原始信号的低分辨率与高噪声特性，以及（iii）缺乏可作为生成学习基础模型的预训练模型，构成了高度复杂的模态特征。这些因素使得基于fMRI记录的非侵入式文本解码极具挑战性。本文提出了一种端到端多模态大语言模型，用于从fMRI信号解码语音文本。该架构的创新性体现在：（1）采用改进型Transformer编码器，通过增强的嵌入层和优于现有技术的注意力机制；（2）适配冻结的大型语言模型，使输入文本嵌入与脑活动编码嵌入对齐以实现文本解码。我们在同步记录fMRI与会话信号的人-人及人-机器人交互语料库上进行基准测试，结果表明：所提模型性能优于现有评估模型，生成文本能更精准捕捉真实语义。项目代码已开源：https://github.com/Hmamouche/brain_decode。
