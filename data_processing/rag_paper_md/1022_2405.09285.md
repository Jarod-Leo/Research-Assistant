# Positional Knowledge is All You Need: Position-induced Transformer (PiT) for Operator Learning

链接: http://arxiv.org/abs/2405.09285v1

原文摘要:
Operator learning for Partial Differential Equations (PDEs) is rapidly
emerging as a promising approach for surrogate modeling of intricate systems.
Transformers with the self-attention mechanism$\unicode{x2013}$a powerful tool
originally designed for natural language processing$\unicode{x2013}$have
recently been adapted for operator learning. However, they confront challenges,
including high computational demands and limited interpretability. This raises
a critical question: Is there a more efficient attention mechanism for
Transformer-based operator learning? This paper proposes the Position-induced
Transformer (PiT), built on an innovative position-attention mechanism, which
demonstrates significant advantages over the classical self-attention in
operator learning. Position-attention draws inspiration from numerical methods
for PDEs. Different from self-attention, position-attention is induced by only
the spatial interrelations of sampling positions for input functions of the
operators, and does not rely on the input function values themselves, thereby
greatly boosting efficiency. PiT exhibits superior performance over current
state-of-the-art neural operators in a variety of complex operator learning
tasks across diverse PDE benchmarks. Additionally, PiT possesses an enhanced
discretization convergence feature, compared to the widely-used Fourier neural
operator.

中文翻译:
偏微分方程（PDE）的算子学习正迅速成为复杂系统代理建模的一种前沿方法。基于自注意力机制的Transformer——这一最初为自然语言处理设计的强大工具——近期被引入算子学习领域。然而，该方法面临计算需求高、可解释性有限等挑战，这引发了一个关键问题：是否存在更高效的注意力机制适用于基于Transformer的算子学习？本文提出位置诱导Transformer（PiT），其创新性地构建于位置注意力机制之上，在算子学习中展现出相较于经典自注意力的显著优势。位置注意力的设计灵感源自PDE数值解法，与自注意力不同，它仅由算子输入函数采样点的空间相互关系所诱导，而不依赖于输入函数值本身，从而大幅提升效率。在多种PDE基准测试的复杂算子学习任务中，PiT表现出超越当前最先进神经算子的性能优势。此外，与广泛应用的傅里叶神经算子相比，PiT还具有更强的离散化收敛特性。
