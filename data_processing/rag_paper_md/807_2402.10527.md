# Zero-shot sampling of adversarial entities in biomedical question answering

链接: http://arxiv.org/abs/2402.10527v1

原文摘要:
The increasing depth of parametric domain knowledge in large language models
(LLMs) is fueling their rapid deployment in real-world applications.
Understanding model vulnerabilities in high-stakes and knowledge-intensive
tasks is essential for quantifying the trustworthiness of model predictions and
regulating their use. The recent discovery of named entities as adversarial
examples (i.e. adversarial entities) in natural language processing tasks
raises questions about their potential impact on the knowledge robustness of
pre-trained and finetuned LLMs in high-stakes and specialized domains. We
examined the use of type-consistent entity substitution as a template for
collecting adversarial entities for billion-parameter LLMs with biomedical
knowledge. To this end, we developed an embedding-space attack based on
powerscaled distance-weighted sampling to assess the robustness of their
biomedical knowledge with a low query budget and controllable coverage. Our
method has favorable query efficiency and scaling over alternative approaches
based on random sampling and blackbox gradient-guided search, which we
demonstrated for adversarial distractor generation in biomedical question
answering. Subsequent failure mode analysis uncovered two regimes of
adversarial entities on the attack surface with distinct characteristics and we
showed that entity substitution attacks can manipulate token-wise Shapley value
explanations, which become deceptive in this setting. Our approach complements
standard evaluations for high-capacity models and the results highlight the
brittleness of domain knowledge in LLMs.

中文翻译:
大型语言模型（LLMs）中参数化领域知识的不断深化，正推动其在现实应用中的快速部署。理解模型在高风险与知识密集型任务中的脆弱性，对于量化预测可信度及规范其使用至关重要。近期研究发现，自然语言处理任务中命名实体可作为对抗样本（即对抗性实体），这引发了关于其对预训练及微调LLMs在专业高风险领域知识鲁棒性潜在影响的疑问。本研究采用类型一致实体替换作为模板，为具备生物医学知识的百亿参数级LLMs构建对抗性实体集。为此，我们开发了基于幂律缩放距离加权采样的嵌入空间攻击方法，以低查询预算和可控覆盖度评估其生物医学知识鲁棒性。相较于随机采样和黑盒梯度引导搜索等替代方案，本方法在生物医学问答对抗干扰项生成中展现出更优的查询效率与扩展性。后续失效模式分析揭示了攻击面上两类特性迥异的对抗性实体机制，并证明实体替换攻击可操纵基于词元的Shapley值解释，使其在此情境下产生误导性。该方法为高容量模型提供了标准评估之外的补充视角，研究结果凸显了LLMs领域知识的脆弱性。
