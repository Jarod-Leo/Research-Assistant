# Comparative Analysis of Efficient Adapter-Based Fine-Tuning of State-of-the-Art Transformer Models

链接: http://arxiv.org/abs/2501.08271v1

原文摘要:
In this work, we investigate the efficacy of various adapter architectures on
supervised binary classification tasks from the SuperGLUE benchmark as well as
a supervised multi-class news category classification task from Kaggle.
Specifically, we compare classification performance and time complexity of
three transformer models, namely DistilBERT, ELECTRA, and BART, using
conventional fine-tuning as well as nine state-of-the-art (SoTA) adapter
architectures. Our analysis reveals performance differences across adapter
architectures, highlighting their ability to achieve comparable or better
performance relative to fine-tuning at a fraction of the training time. Similar
results are observed on the new classification task, further supporting our
findings and demonstrating adapters as efficient and flexible alternatives to
fine-tuning. This study provides valuable insights and guidelines for selecting
and implementing adapters in diverse natural language processing (NLP)
applications.

中文翻译:
本研究探讨了多种适配器架构在SuperGLUE基准测试中的监督式二元分类任务及Kaggle多类新闻分类任务中的效能表现。我们系统对比了DistilBERT、ELECTRA和BART三种Transformer模型在传统微调与九种前沿适配器架构下的分类性能与时间复杂度。分析结果表明：不同适配器架构存在显著性能差异，这些架构仅需部分训练时间即可达到与微调相当或更优的效果。在新分类任务中观察到的相似结论进一步验证了我们的发现，表明适配器可作为高效灵活的微调替代方案。本研究为自然语言处理领域中适配器的选择与实施提供了具有实践价值的指导原则。

（翻译说明：
1. 专业术语处理："state-of-the-art"译为"前沿"而非字面直译，符合学术语境
2. 句式重构：将原文复合长句拆分为符合中文表达习惯的短句，如将"achieving comparable..."处理为因果句式
3. 语态转换：被动语态"are observed"主动化为"观察到的结论"
4. 概念显化："fraction of the training time"意译为"部分训练时间"以增强可读性
5. 学术风格保持：使用"效能表现""验证了发现"等符合论文摘要的规范表述
6. 逻辑连接：通过"结果表明""进一步验证"等短语强化论证链条的连贯性）
