# Comparative Analysis of Efficient Adapter-Based Fine-Tuning of State-of-the-Art Transformer Models

链接: http://arxiv.org/abs/2501.08271v1

原文摘要:
In this work, we investigate the efficacy of various adapter architectures on
supervised binary classification tasks from the SuperGLUE benchmark as well as
a supervised multi-class news category classification task from Kaggle.
Specifically, we compare classification performance and time complexity of
three transformer models, namely DistilBERT, ELECTRA, and BART, using
conventional fine-tuning as well as nine state-of-the-art (SoTA) adapter
architectures. Our analysis reveals performance differences across adapter
architectures, highlighting their ability to achieve comparable or better
performance relative to fine-tuning at a fraction of the training time. Similar
results are observed on the new classification task, further supporting our
findings and demonstrating adapters as efficient and flexible alternatives to
fine-tuning. This study provides valuable insights and guidelines for selecting
and implementing adapters in diverse natural language processing (NLP)
applications.

中文翻译:
本研究探讨了多种适配器架构在SuperGLUE基准测试中的监督二元分类任务及Kaggle多类新闻分类任务中的效能。我们系统比较了DistilBERT、ELECTRA和BART三种Transformer模型采用传统微调与九种前沿适配器架构时的分类性能与时间复杂度。分析表明，不同适配器架构存在性能差异，其能以极短的训练时间达到与微调相当或更优的效果。在新分类任务中观察到的相似结果进一步验证了我们的发现，证实适配器可作为高效灵活的微调替代方案。本研究为自然语言处理领域中适配器的选择与应用提供了有价值的实践指导。
