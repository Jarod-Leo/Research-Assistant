# DFPE: A Diverse Fingerprint Ensemble for Enhancing LLM Performance

链接: http://arxiv.org/abs/2501.17479v1

原文摘要:
Large Language Models (LLMs) have shown remarkable capabilities across
various natural language processing tasks but often struggle to excel uniformly
in diverse or complex domains. We propose a novel ensemble method - Diverse
Fingerprint Ensemble (DFPE), which leverages the complementary strengths of
multiple LLMs to achieve more robust performance. Our approach involves: (1)
clustering models based on response "fingerprints" patterns, (2) applying a
quantile-based filtering mechanism to remove underperforming models at a
per-subject level, and (3) assigning adaptive weights to remaining models based
on their subject-wise validation accuracy. In experiments on the Massive
Multitask Language Understanding (MMLU) benchmark, DFPE outperforms the best
single model by 3% overall accuracy and 5% in discipline-level accuracy. This
method increases the robustness and generalization of LLMs and underscores how
model selection, diversity preservation, and performance-driven weighting can
effectively address challenging, multi-faceted language understanding tasks.

中文翻译:
大语言模型（LLMs）在各类自然语言处理任务中展现出卓越能力，但在处理多样化或复杂领域时往往难以保持均衡表现。我们提出一种新颖的集成方法——多样性指纹集成（DFPE），通过协同利用多个LLMs的互补优势来实现更稳健的性能。该方法包含：（1）基于响应"指纹"模式对模型进行聚类；（2）采用分位数过滤机制逐科目剔除低效模型；（3）根据各模型在验证集上的科目级准确率动态分配权重。在MMLU（大规模多任务语言理解）基准测试中，DFPE相较最佳单模型整体准确率提升3%，学科级准确率提升5%。该方法不仅增强了LLMs的鲁棒性和泛化能力，更揭示了模型选择、多样性保持与性能导向加权如何有效应对具有挑战性的多维度语言理解任务。
