# DFPE: A Diverse Fingerprint Ensemble for Enhancing LLM Performance

链接: http://arxiv.org/abs/2501.17479v1

原文摘要:
Large Language Models (LLMs) have shown remarkable capabilities across
various natural language processing tasks but often struggle to excel uniformly
in diverse or complex domains. We propose a novel ensemble method - Diverse
Fingerprint Ensemble (DFPE), which leverages the complementary strengths of
multiple LLMs to achieve more robust performance. Our approach involves: (1)
clustering models based on response "fingerprints" patterns, (2) applying a
quantile-based filtering mechanism to remove underperforming models at a
per-subject level, and (3) assigning adaptive weights to remaining models based
on their subject-wise validation accuracy. In experiments on the Massive
Multitask Language Understanding (MMLU) benchmark, DFPE outperforms the best
single model by 3% overall accuracy and 5% in discipline-level accuracy. This
method increases the robustness and generalization of LLMs and underscores how
model selection, diversity preservation, and performance-driven weighting can
effectively address challenging, multi-faceted language understanding tasks.

中文翻译:
以下是符合您要求的中文翻译：

大型语言模型（LLMs）在各类自然语言处理任务中展现出卓越能力，但在多样化或复杂领域往往难以保持均衡优异表现。我们提出一种新型集成方法——多样性指纹集成（DFPE），通过协同利用多个LLMs的互补优势来实现更稳健的性能。该方法包含三个关键步骤：（1）基于响应"指纹"模式对模型进行聚类；（2）采用分位数过滤机制逐科目剔除低效模型；（3）根据各模型在科目级验证准确率分配自适应权重。在MMLU（大规模多任务语言理解）基准测试中，DFPE整体准确率超越最佳单模型3%，学科级准确率提升达5%。该方法不仅增强了LLMs的鲁棒性和泛化能力，更通过模型选择、多样性保持和性能导向加权三重机制，为应对多维度语言理解挑战提供了有效解决方案。

翻译说明：
1. 专业术语处理：LLMs/Massive Multitask Language Understanding等专业术语保留英文缩写并添加中文全称
2. 技术概念转译："fingerprints"译为"指纹"并添加引号保持原意
3. 长句拆分：将原文复合长句按中文习惯分解为多个短句
4. 逻辑显化：通过"三重机制"等概括性表述强化方法论层次
5. 数据呈现：准确保留3%/5%等关键性能指标
6. 学术风格：使用"协同利用""自适应权重"等符合学术论文表达的术语
7. 被动语态转换："are assigned"等英文被动结构转为中文主动表述
