# Prompting Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages

链接: http://arxiv.org/abs/2303.13592v1

原文摘要:
While code-mixing is a common linguistic practice in many parts of the world,
collecting high-quality and low-cost code-mixed data remains a challenge for
natural language processing (NLP) research. The recent proliferation of Large
Language Models (LLMs) compels one to ask: how capable are these systems in
generating code-mixed data? In this paper, we explore prompting multilingual
LLMs in a zero-shot manner to generate code-mixed data for seven languages in
South East Asia (SEA), namely Indonesian, Malay, Chinese, Tagalog, Vietnamese,
Tamil, and Singlish. We find that publicly available multilingual
instruction-tuned models such as BLOOMZ and Flan-T5-XXL are incapable of
producing texts with phrases or clauses from different languages. ChatGPT
exhibits inconsistent capabilities in generating code-mixed texts, wherein its
performance varies depending on the prompt template and language pairing. For
instance, ChatGPT generates fluent and natural Singlish texts (an English-based
creole spoken in Singapore), but for English-Tamil language pair, the system
mostly produces grammatically incorrect or semantically meaningless utterances.
Furthermore, it may erroneously introduce languages not specified in the
prompt. Based on our investigation, existing multilingual LLMs exhibit a wide
range of proficiency in code-mixed data generation for SEA languages. As such,
we advise against using LLMs in this context without extensive human checks.

中文翻译:
尽管语码混合在全球许多地区是一种普遍的语言现象，但为自然语言处理（NLP）研究采集高质量、低成本的混合语料仍面临挑战。随着大语言模型（LLMs）的迅猛发展，一个核心问题随之浮现：这些系统生成混合语料的能力究竟如何？本文通过零样本提示的方式，探索多语言大模型对东南亚七种语言（印尼语、马来语、中文、他加禄语、越南语、泰米尔语及新加坡式英语）的混合文本生成能力。研究发现，当前公开的多语言指令微调模型（如BLOOMZ和Flan-T5-XXL）无法生成包含跨语言短语或从句的文本。ChatGPT在混合文本生成上表现出不稳定的能力，其表现受提示模板和语言组合的显著影响：例如该模型能生成流畅自然的新加坡式英语（一种基于英语的克里奥尔语），但在英语-泰米尔语组合中，其输出多为语法错误或语义混乱的语句，甚至可能错误引入提示中未指定的语言。研究表明，现有多语言大模型对东南亚语言的混合文本生成存在显著的能力差异，因此我们强烈建议在此类应用场景中需经过严格人工校验方可使用大模型。
