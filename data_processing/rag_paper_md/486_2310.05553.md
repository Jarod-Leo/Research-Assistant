# Regulation and NLP (RegNLP): Taming Large Language Models

链接: http://arxiv.org/abs/2310.05553v1

原文摘要:
The scientific innovation in Natural Language Processing (NLP) and more
broadly in artificial intelligence (AI) is at its fastest pace to date. As
large language models (LLMs) unleash a new era of automation, important debates
emerge regarding the benefits and risks of their development, deployment and
use. Currently, these debates have been dominated by often polarized narratives
mainly led by the AI Safety and AI Ethics movements. This polarization, often
amplified by social media, is swaying political agendas on AI regulation and
governance and posing issues of regulatory capture. Capture occurs when the
regulator advances the interests of the industry it is supposed to regulate, or
of special interest groups rather than pursuing the general public interest.
Meanwhile in NLP research, attention has been increasingly paid to the
discussion of regulating risks and harms. This often happens without systematic
methodologies or sufficient rooting in the disciplines that inspire an extended
scope of NLP research, jeopardizing the scientific integrity of these
endeavors. Regulation studies are a rich source of knowledge on how to
systematically deal with risk and uncertainty, as well as with scientific
evidence, to evaluate and compare regulatory options. This resource has largely
remained untapped so far. In this paper, we argue how NLP research on these
topics can benefit from proximity to regulatory studies and adjacent fields. We
do so by discussing basic tenets of regulation, and risk and uncertainty, and
by highlighting the shortcomings of current NLP discussions dealing with risk
assessment. Finally, we advocate for the development of a new multidisciplinary
research space on regulation and NLP (RegNLP), focused on connecting scientific
knowledge to regulatory processes based on systematic methodologies.

中文翻译:
自然语言处理（NLP）及更广泛的人工智能（AI）领域的科学创新正以前所未有的速度推进。随着大语言模型（LLMs）开启自动化新纪元，关于其开发、部署与使用的利弊之争日益凸显。当前，这些讨论主要由AI安全与AI伦理运动主导，形成了两极分化的叙事框架。这种常被社交媒体放大的对立态势，正左右着AI监管与治理的政治议程，并引发"监管俘获"问题——即监管机构为被监管行业或特殊利益集团谋利，而非维护公共利益。与此同时，NLP研究领域对风险与危害监管的讨论日渐增多，但往往缺乏系统方法论支撑，也未能充分扎根于那些启发NLP研究扩展的学科领域，危及了相关研究的科学严谨性。  

规制研究作为系统性应对风险与不确定性、运用科学证据评估比较监管方案的知识宝库，其价值迄今尚未得到充分发掘。本文通过阐释规制基础理论、风险与不确定性原理，并指出现有NLP风险评估讨论的缺陷，论证了NLP研究如何通过与规制研究及相关领域的交叉融合获得发展契机。最后，我们主张建立规制与NLP（RegNLP）这一新兴跨学科研究领域，致力于基于系统方法论将科学知识与规制实践相衔接。
