# CLR-Fact: Evaluating the Complex Logical Reasoning Capability of Large Language Models over Factual Knowledge

链接: http://arxiv.org/abs/2407.20564v1

原文摘要:
While large language models (LLMs) have demonstrated impressive capabilities
across various natural language processing tasks by acquiring rich factual
knowledge from their broad training data, their ability to synthesize and
logically reason with this knowledge in complex ways remains underexplored. In
this work, we present a systematic evaluation of state-of-the-art LLMs' complex
logical reasoning abilities through a novel benchmark of automatically
generated complex reasoning questions over general domain and biomedical
knowledge graphs. Our extensive experiments, employing diverse in-context
learning techniques, reveal that LLMs excel at reasoning over general world
knowledge but face significant challenges with specialized domain-specific
knowledge. We find that prompting with explicit Chain-of-Thought demonstrations
can substantially improve LLM performance on complex logical reasoning tasks
with diverse logical operations. Interestingly, our controlled evaluations
uncover an asymmetry where LLMs display proficiency at set union operations,
but struggle considerably with set intersections - a key building block of
logical reasoning. To foster further work, we will publicly release our
evaluation benchmark and code.

中文翻译:
虽然大型语言模型（LLMs）通过从广泛的训练数据中获取丰富的事实知识，在各种自然语言处理任务中展现出令人印象深刻的能力，但其以复杂方式综合运用这些知识进行逻辑推理的能力仍未得到充分探索。本研究通过构建一个基于通用领域和生物医学知识图谱自动生成的复杂推理问题新基准，对前沿LLMs的复杂逻辑推理能力进行了系统评估。我们采用多样化的上下文学习技术开展大量实验，结果表明：LLMs在通用世界知识推理方面表现优异，但在专业领域知识处理上存在显著困难。研究发现，采用显式思维链（Chain-of-Thought）提示能显著提升LLMs在涉及多样化逻辑运算的复杂推理任务中的表现。有趣的是，对照实验揭示了一种不对称现象：LLMs擅长集合的并集运算，却在作为逻辑推理关键要素的集合交集运算上表现欠佳。为促进后续研究，我们将公开评估基准与相关代码。
