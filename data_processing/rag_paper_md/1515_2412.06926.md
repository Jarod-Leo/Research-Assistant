# When Every Token Counts: Optimal Segmentation for Low-Resource Language Models

链接: http://arxiv.org/abs/2412.06926v1

原文摘要:
Traditional greedy tokenization methods have been a critical step in Natural
Language Processing (NLP), influencing how text is converted into tokens and
directly impacting model performance. While subword tokenizers like Byte-Pair
Encoding (BPE) are widely used, questions remain about their optimality across
model scales and languages. In this work, we demonstrate through extensive
experiments that an optimal BPE configuration significantly reduces token count
compared to greedy segmentation, yielding improvements in token-saving
percentages and performance benefits, particularly for smaller models. We
evaluate tokenization performance across various intrinsic and extrinsic tasks,
including generation and classification. Our findings suggest that
compression-optimized tokenization strategies could provide substantial
advantages for multilingual and low-resource language applications,
highlighting a promising direction for further research and inclusive NLP.

中文翻译:
传统的贪心分词方法一直是自然语言处理（NLP）中的关键环节，其将文本转化为词元的方式直接影响模型性能。尽管诸如字节对编码（BPE）等子词分词器被广泛采用，但其在不同规模模型和语言中的最优性仍存疑问。本研究通过大量实验证明，相较于贪心分割策略，优化配置的BPE能显著减少词元数量，在词元节省率和模型性能（尤其是小规模模型）上均取得提升。我们从生成和分类等内外任务维度评估了分词性能，结果表明：以压缩优化为目标的分词策略可为多语言及低资源语言应用带来显著优势，这为未来研究及包容性NLP发展指明了富有前景的方向。
