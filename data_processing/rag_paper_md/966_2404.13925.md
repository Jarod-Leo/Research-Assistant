# MARIO Eval: Evaluate Your Math LLM with your Math LLM--A mathematical dataset evaluation toolkit

链接: http://arxiv.org/abs/2404.13925v1

原文摘要:
Large language models (LLMs) have been explored in a variety of reasoning
tasks including solving of mathematical problems. Each math dataset typically
includes its own specially designed evaluation script, which, while suitable
for its intended use, lacks generalizability across different datasets.
Consequently, updates and adaptations to these evaluation tools tend to occur
without being systematically reported, leading to inconsistencies and obstacles
to fair comparison across studies. To bridge this gap, we introduce a
comprehensive mathematical evaluation toolkit that not only utilizes a python
computer algebra system (CAS) for its numerical accuracy, but also integrates
an optional LLM, known for its considerable natural language processing
capabilities. To validate the effectiveness of our toolkit, we manually
annotated two distinct datasets. Our experiments demonstrate that the toolkit
yields more robust evaluation results compared to prior works, even without an
LLM. Furthermore, when an LLM is incorporated, there is a notable enhancement.
The code for our method will be made available at
\url{https://github.com/MARIO-Math-Reasoning/math_evaluation}.

中文翻译:
以下为英文论文摘要的中文翻译：

大型语言模型（LLMs）已在包括数学问题求解在内的多种推理任务中得到探索。当前每个数学数据集通常配备专门设计的评估脚本，虽然适用于其特定用途，但缺乏跨数据集的通用性。这导致评估工具的更新和适配往往缺乏系统性记录，进而引发研究间的不一致性与公平比较障碍。为弥补这一缺口，我们推出一个综合性数学评估工具包：其不仅利用Python计算机代数系统（CAS）确保数值计算精度，还整合了以强大自然语言处理能力著称的（可选）大型语言模型。为验证工具包的有效性，我们人工标注了两个不同数据集。实验表明，即使不启用LLM，该工具包相比现有方法仍能产生更稳健的评估结果；而引入LLM后，性能更获得显著提升。本方法代码已发布于\url{https://github.com/MARIO-Math-Reasoning/math_evaluation}。

（翻译说明：  
1. 专业术语保留英文缩写（LLMs/CAS）并首次出现时标注全称  
2. 被动语态转换为中文主动表述（如"have been explored"译为"得到探索"）  
3. 长难句拆分重组（如"which, while..."处理为转折分句）  
4. 技术概念准确传达（如"numerical accuracy"译为"数值计算精度"）  
5. 保持学术文本的简洁性与客观性）
