# MARIO Eval: Evaluate Your Math LLM with your Math LLM--A mathematical dataset evaluation toolkit

链接: http://arxiv.org/abs/2404.13925v1

原文摘要:
Large language models (LLMs) have been explored in a variety of reasoning
tasks including solving of mathematical problems. Each math dataset typically
includes its own specially designed evaluation script, which, while suitable
for its intended use, lacks generalizability across different datasets.
Consequently, updates and adaptations to these evaluation tools tend to occur
without being systematically reported, leading to inconsistencies and obstacles
to fair comparison across studies. To bridge this gap, we introduce a
comprehensive mathematical evaluation toolkit that not only utilizes a python
computer algebra system (CAS) for its numerical accuracy, but also integrates
an optional LLM, known for its considerable natural language processing
capabilities. To validate the effectiveness of our toolkit, we manually
annotated two distinct datasets. Our experiments demonstrate that the toolkit
yields more robust evaluation results compared to prior works, even without an
LLM. Furthermore, when an LLM is incorporated, there is a notable enhancement.
The code for our method will be made available at
\url{https://github.com/MARIO-Math-Reasoning/math_evaluation}.

中文翻译:
大型语言模型（LLMs）已在包括数学问题求解在内的多种推理任务中得到广泛应用。然而，当前各数学数据集通常配备独立设计的评估脚本，虽能满足特定需求，却缺乏跨数据集的通用性。这导致评估工具的更新与适配往往缺乏系统性记录，进而引发结果不一致性，阻碍了研究间的公平比较。为填补这一空白，我们推出了一款综合性数学评估工具包：其核心采用Python计算机代数系统（CAS）确保数值计算精度，同时创新性地集成可选的大型语言模型组件以发挥其卓越的自然语言处理能力。为验证工具包效能，我们人工标注了两个差异化数据集。实验表明，即使不启用语言模型，该工具包相比现有方案仍能产生更稳健的评估结果；而引入语言模型后，性能表现更获得显著提升。本方法代码已公开于\url{https://github.com/MARIO-Math-Reasoning/math_evaluation}。
