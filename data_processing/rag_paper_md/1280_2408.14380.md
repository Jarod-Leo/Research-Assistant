# Probing Causality Manipulation of Large Language Models

链接: http://arxiv.org/abs/2408.14380v1

原文摘要:
Large language models (LLMs) have shown various ability on natural language
processing, including problems about causality. It is not intuitive for LLMs to
command causality, since pretrained models usually work on statistical
associations, and do not focus on causes and effects in sentences. So that
probing internal manipulation of causality is necessary for LLMs. This paper
proposes a novel approach to probe causality manipulation hierarchically, by
providing different shortcuts to models and observe behaviors. We exploit
retrieval augmented generation (RAG) and in-context learning (ICL) for models
on a designed causality classification task. We conduct experiments on
mainstream LLMs, including GPT-4 and some smaller and domain-specific models.
Our results suggest that LLMs can detect entities related to causality and
recognize direct causal relationships. However, LLMs lack specialized cognition
for causality, merely treating them as part of the global semantic of the
sentence.

中文翻译:
大语言模型（LLMs）在自然语言处理中展现出多样能力，包括因果相关任务。然而LLMs掌握因果关系并非易事，因为预训练模型通常基于统计关联运作，并未聚焦于句子中的因果逻辑。因此有必要探究LLMs内部的因果机制。本文提出一种分层探测因果操作的新方法，通过为模型提供不同捷径并观察其行为。我们在设计的因果分类任务中，采用检索增强生成（RAG）和上下文学习（ICL）技术对模型进行测试。实验对象包括GPT-4等主流LLMs及部分小型领域专用模型。结果表明：LLMs能够识别与因果相关的实体并判断直接因果关系，但缺乏对因果关系的专门化认知，仅将其视为句子全局语义的组成部分。
