# Procedural Text Mining with Large Language Models

链接: http://arxiv.org/abs/2310.03376v1

原文摘要:
Recent advancements in the field of Natural Language Processing, particularly
the development of large-scale language models that are pretrained on vast
amounts of knowledge, are creating novel opportunities within the realm of
Knowledge Engineering. In this paper, we investigate the usage of large
language models (LLMs) in both zero-shot and in-context learning settings to
tackle the problem of extracting procedures from unstructured PDF text in an
incremental question-answering fashion. In particular, we leverage the current
state-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model,
accompanied by two variations of in-context learning that involve an ontology
with definitions of procedures and steps and a limited number of samples of
few-shot learning. The findings highlight both the promise of this approach and
the value of the in-context learning customisations. These modifications have
the potential to significantly address the challenge of obtaining sufficient
training data, a hurdle often encountered in deep learning-based Natural
Language Processing techniques for procedure extraction.

中文翻译:
自然语言处理领域的最新进展，尤其是基于海量知识预训练的大规模语言模型的发展，正在为知识工程领域创造新的机遇。本文研究了大型语言模型（LLMs）在零样本学习和上下文学习两种设置中的应用，以增量式问答的方式解决从非结构化PDF文本中提取流程的问题。我们特别采用了当前最先进的GPT-4（生成式预训练变换器4）模型，并结合两种上下文学习变体：一种是包含流程与步骤定义的领域本体，另一种是少量样本的小样本学习。研究结果既验证了该方法的可行性，也凸显了上下文学习定制化方案的价值。这些改进有望显著缓解基于深度学习的流程提取技术中经常面临的训练数据不足这一关键挑战。
