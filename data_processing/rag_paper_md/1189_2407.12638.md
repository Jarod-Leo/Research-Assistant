# ARTEMIS: A Mixed Analog-Stochastic In-DRAM Accelerator for Transformer Neural Networks

链接: http://arxiv.org/abs/2407.12638v1

原文摘要:
Transformers have emerged as a powerful tool for natural language processing
(NLP) and computer vision. Through the attention mechanism, these models have
exhibited remarkable performance gains when compared to conventional approaches
like recurrent neural networks (RNNs) and convolutional neural networks (CNNs).
Nevertheless, transformers typically demand substantial execution time due to
their extensive computations and large memory footprint. Processing in-memory
(PIM) and near-memory computing (NMC) are promising solutions to accelerating
transformers as they offer high compute parallelism and memory bandwidth.
However, designing PIM/NMC architectures to support the complex operations and
massive amounts of data that need to be moved between layers in transformer
neural networks remains a challenge. We propose ARTEMIS, a mixed
analog-stochastic in-DRAM accelerator for transformer models. Through employing
minimal changes to the conventional DRAM arrays, ARTEMIS efficiently alleviates
the costs associated with transformer model execution by supporting stochastic
computing for multiplications and temporal analog accumulations using a novel
in-DRAM metal-on-metal capacitor. Our analysis indicates that ARTEMIS exhibits
at least 3.0x speedup, 1.8x lower energy, and 1.9x better energy efficiency
compared to GPU, TPU, CPU, and state-of-the-art PIM transformer hardware
accelerators.

中文翻译:
Transformer已成为自然语言处理(NLP)和计算机视觉领域的强大工具。通过注意力机制，这些模型相较于循环神经网络(RNN)和卷积神经网络(CNN)等传统方法展现出显著性能提升。然而，由于庞大的计算量和内存占用，Transformer通常需要较长的执行时间。存内计算(PIM)和近存计算(NMC)因其高计算并行性和内存带宽优势，被视为加速Transformer的潜在解决方案。但如何设计能支持Transformer神经网络复杂运算及层间海量数据传输的PIM/NMC架构仍具挑战性。我们提出ARTEMIS——一种面向Transformer模型的混合模拟-随机DRAM内存加速器。该方案通过对传统DRAM阵列进行最小改动，利用新型DRAM金属-金属电容器实现随机乘法计算和时序模拟累加，有效降低了Transformer模型的执行开销。分析表明，相较于GPU、TPU、CPU及最先进的PIM Transformer硬件加速器，ARTEMIS至少实现3倍加速、能耗降低1.8倍，能效提升1.9倍。
