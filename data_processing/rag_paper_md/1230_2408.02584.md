# Leveraging the Power of LLMs: A Fine-Tuning Approach for High-Quality Aspect-Based Summarization

链接: http://arxiv.org/abs/2408.02584v1

原文摘要:
The ever-increasing volume of digital information necessitates efficient
methods for users to extract key insights from lengthy documents. Aspect-based
summarization offers a targeted approach, generating summaries focused on
specific aspects within a document. Despite advancements in aspect-based
summarization research, there is a continuous quest for improved model
performance. Given that large language models (LLMs) have demonstrated the
potential to revolutionize diverse tasks within natural language processing,
particularly in the problem of summarization, this paper explores the potential
of fine-tuning LLMs for the aspect-based summarization task. We evaluate the
impact of fine-tuning open-source foundation LLMs, including Llama2, Mistral,
Gemma and Aya, on a publicly available domain-specific aspect based summary
dataset. We hypothesize that this approach will enable these models to
effectively identify and extract aspect-related information, leading to
superior quality aspect-based summaries compared to the state-of-the-art. We
establish a comprehensive evaluation framework to compare the performance of
fine-tuned LLMs against competing aspect-based summarization methods and
vanilla counterparts of the fine-tuned LLMs. Our work contributes to the field
of aspect-based summarization by demonstrating the efficacy of fine-tuning LLMs
for generating high-quality aspect-based summaries. Furthermore, it opens doors
for further exploration of using LLMs for targeted information extraction tasks
across various NLP domains.

中文翻译:
随着数字信息量的持续激增，用户亟需高效方法从冗长文档中提取关键信息。基于方面的摘要技术提供了一种针对性解决方案，能够生成聚焦文档特定方面的摘要内容。尽管该领域研究已取得进展，但提升模型性能始终是核心追求。鉴于大语言模型（LLMs）在自然语言处理各类任务中展现出变革性潜力——尤其在摘要生成领域，本文探索了微调LLMs用于基于方面的摘要任务的可行性。我们评估了Llama2、Mistral、Gemma和Aya等开源基础LLMs在公开领域特定方面摘要数据集上的微调效果，并提出假设：该方法可使模型有效识别和提取方面相关信息，从而生成优于现有技术的方面摘要。通过建立综合评估框架，我们将微调后的LLMs与当前主流基于方面的摘要方法及原始LLMs进行性能对比。本研究通过验证微调LLMs生成高质量方面摘要的有效性，推动了该领域的发展，同时为探索LLMs在其他NLP领域针对性信息抽取任务中的应用开辟了新路径。
