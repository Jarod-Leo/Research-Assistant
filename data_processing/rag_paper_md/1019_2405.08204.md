# A Semantic and Motion-Aware Spatiotemporal Transformer Network for Action Detection

链接: http://arxiv.org/abs/2405.08204v1

原文摘要:
This paper presents a novel spatiotemporal transformer network that
introduces several original components to detect actions in untrimmed videos.
First, the multi-feature selective semantic attention model calculates the
correlations between spatial and motion features to model spatiotemporal
interactions between different action semantics properly. Second, the
motion-aware network encodes the locations of action semantics in video frames
utilizing the motion-aware 2D positional encoding algorithm. Such a
motion-aware mechanism memorizes the dynamic spatiotemporal variations in
action frames that current methods cannot exploit. Third, the sequence-based
temporal attention model captures the heterogeneous temporal dependencies in
action frames. In contrast to standard temporal attention used in natural
language processing, primarily aimed at finding similarities between linguistic
words, the proposed sequence-based temporal attention is designed to determine
both the differences and similarities between video frames that jointly define
the meaning of actions. The proposed approach outperforms the state-of-the-art
solutions on four spatiotemporal action datasets: AVA 2.2, AVA 2.1, UCF101-24,
and EPIC-Kitchens.

中文翻译:
本文提出了一种新颖的时空变换网络，通过引入多项原创组件来检测未剪辑视频中的动作。首先，多特征选择性语义注意力模型通过计算空间特征与运动特征之间的相关性，准确建模不同动作语义间的时空交互关系。其次，运动感知网络采用运动感知二维位置编码算法，对视频帧中动作语义的位置信息进行编码。这种运动感知机制能捕捉当前方法无法利用的动作帧动态时空变化特征。第三，基于序列的时序注意力模型能够捕获动作帧间的异质时序依赖关系。与自然语言处理中主要用于发现词语相似性的标准时序注意力不同，所提出的序列时序注意力专门设计用于联合判定视频帧间差异性与相似性，从而共同定义动作含义。该方法在AVA 2.2、AVA 2.1、UCF101-24和EPIC-Kitchens四个时空动作数据集上的性能表现均超越了当前最先进解决方案。
