# Language Model Analysis for Ontology Subsumption Inference

链接: http://arxiv.org/abs/2302.06761v1

原文摘要:
Investigating whether pre-trained language models (LMs) can function as
knowledge bases (KBs) has raised wide research interests recently. However,
existing works focus on simple, triple-based, relational KBs, but omit more
sophisticated, logic-based, conceptualised KBs such as OWL ontologies. To
investigate an LM's knowledge of ontologies, we propose OntoLAMA, a set of
inference-based probing tasks and datasets from ontology subsumption axioms
involving both atomic and complex concepts. We conduct extensive experiments on
ontologies of different domains and scales, and our results demonstrate that
LMs encode relatively less background knowledge of Subsumption Inference (SI)
than traditional Natural Language Inference (NLI) but can improve on SI
significantly when a small number of samples are given. We will open-source our
code and datasets.

中文翻译:
近年来，预训练语言模型（LMs）能否作为知识库（KBs）使用引发了广泛研究兴趣。然而现有工作主要关注基于三元组的简单关系型知识库，却忽略了更复杂、基于逻辑的概念化知识库（如OWL本体）。为探究语言模型对本体的知识掌握程度，我们提出OntoLAMA——一套基于推理的探测任务与数据集，其构建来源为包含原子概念与复合概念的本体蕴含公理。通过对不同领域和规模的本体进行大量实验，结果表明：与传统自然语言推理（NLI）相比，语言模型对蕴含推理（SI）的背景知识编码相对不足，但在少量样本提供后可显著提升SI表现。我们将开源相关代码与数据集。
