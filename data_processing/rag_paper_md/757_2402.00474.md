# SA-MDKIF: A Scalable and Adaptable Medical Domain Knowledge Injection Framework for Large Language Models

链接: http://arxiv.org/abs/2402.00474v1

原文摘要:
Recent advances in large language models (LLMs) have demonstrated exceptional
performance in various natural language processing (NLP) tasks. However, their
effective application in the medical domain is hampered by a lack of medical
domain knowledge. In this study, we present SA-MDKIF, a scalable and adaptable
framework that aims to inject medical knowledge into general-purpose LLMs
through instruction tuning, thereby enabling adaptability for various
downstream tasks. SA-MDKIF consists of two stages: skill training and skill
adaptation. In the first stage, we define 12 basic medical skills and use
AdaLoRA to train these skills based on uniformly formatted instructional
datasets that we have constructed. In the next stage, we train the skill router
using task-specific downstream data and use this router to integrate the
acquired skills with LLMs during inference. Experimental results on 9 different
medical tasks show that SA-MDKIF improves performance by 10-20% compared to the
original LLMs. Notably, this improvement is particularly pronounced for unseen
medical tasks, showing an improvement of up to 30%.

中文翻译:
近期，大型语言模型（LLMs）在自然语言处理（NLP）任务中展现出卓越性能，但其在医疗领域的有效应用因缺乏专业医学知识而受限。本研究提出SA-MDKIF框架——一个通过指令微调将医学知识注入通用LLMs的可扩展自适应框架，从而增强模型对多样化下游任务的适应能力。该框架包含两个阶段：技能训练与技能适配。第一阶段定义了12项基础医学技能，基于我们构建的统一格式化指令数据集，采用AdaLoRA方法进行技能训练；第二阶段利用任务特异性下游数据训练技能路由器，在推理阶段通过该路由器将习得技能与LLMs动态整合。在9项不同医疗任务上的实验表明，SA-MDKIF相较原始LLMs性能提升10%-20%，其中对未见医疗任务的提升尤为显著，最高可达30%。
