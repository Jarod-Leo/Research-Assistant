# Can a Frozen Pretrained Language Model be used for Zero-shot Neural Retrieval on Entity-centric Questions?

链接: http://arxiv.org/abs/2303.05153v1

原文摘要:
Neural document retrievers, including dense passage retrieval (DPR), have
outperformed classical lexical-matching retrievers, such as BM25, when
fine-tuned and tested on specific question-answering datasets. However, it has
been shown that the existing dense retrievers do not generalize well not only
out of domain but even in domain such as Wikipedia, especially when a named
entity in a question is a dominant clue for retrieval. In this paper, we
propose an approach toward in-domain generalization using the embeddings
generated by the frozen language model trained with the entities in the domain.
By not fine-tuning, we explore the possibility that the rich knowledge
contained in a pretrained language model can be used for retrieval tasks. The
proposed method outperforms conventional DPRs on entity-centric questions in
Wikipedia domain and achieves almost comparable performance to BM25 and
state-of-the-art SPAR model. We also show that the contextualized keys lead to
strong improvements compared to BM25 when the entity names consist of common
words. Our results demonstrate the feasibility of the zero-shot retrieval
method for entity-centric questions of Wikipedia domain, where DPR has
struggled to perform.

中文翻译:
神经文档检索器，包括密集段落检索（DPR），在经过特定问答数据集的微调和测试后，其表现已超越传统基于词汇匹配的检索器（如BM25）。然而，研究表明，现有密集检索器不仅在跨领域时泛化能力不足，甚至在维基百科等同一领域内也表现欠佳，尤其是当问题中的命名实体成为检索主导线索时。本文提出一种利用领域内实体训练的冻结语言模型生成嵌入向量以实现领域内泛化的方法。通过避免微调，我们探索了预训练语言模型中蕴含的丰富知识直接应用于检索任务的可能性。该方法在维基百科领域以实体为核心的问题上优于传统DPR模型，其性能与BM25及当前最先进的SPAR模型基本相当。我们还证明，当实体名称由常见词汇构成时，上下文感知键相较于BM25能带来显著提升。这些结果表明，针对DPR此前难以处理的维基百科领域实体类问题，零样本检索方法具有可行性。
