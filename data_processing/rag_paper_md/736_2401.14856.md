# Memory-Inspired Temporal Prompt Interaction for Text-Image Classification

链接: http://arxiv.org/abs/2401.14856v1

原文摘要:
In recent years, large-scale pre-trained multimodal models (LMM) generally
emerge to integrate the vision and language modalities, achieving considerable
success in various natural language processing and computer vision tasks. The
growing size of LMMs, however, results in a significant computational cost for
fine-tuning these models for downstream tasks. Hence, prompt-based interaction
strategy is studied to align modalities more efficiently. In this contex, we
propose a novel prompt-based multimodal interaction strategy inspired by human
memory strategy, namely Memory-Inspired Temporal Prompt Interaction (MITP). Our
proposed method involves in two stages as in human memory strategy: the
acquiring stage, and the consolidation and activation stage. We utilize
temporal prompts on intermediate layers to imitate the acquiring stage,
leverage similarity-based prompt interaction to imitate memory consolidation,
and employ prompt generation strategy to imitate memory activation. The main
strength of our paper is that we interact the prompt vectors on intermediate
layers to leverage sufficient information exchange between modalities, with
compressed trainable parameters and memory usage. We achieve competitive
results on several datasets with relatively small memory usage and 2.0M of
trainable parameters (about 1% of the pre-trained foundation model).

中文翻译:
近年来，大规模预训练多模态模型（LMM）通过整合视觉与语言模态，在各类自然语言处理和计算机视觉任务中取得显著成效。然而，随着模型规模不断扩大，针对下游任务进行微调所需的计算成本急剧攀升。为此，基于提示的交互策略被提出以实现更高效的模态对齐。本文受人类记忆机制启发，创新性地提出了一种记忆驱动的时序提示交互方法（MITP）。该方法模拟人类记忆的两阶段过程：在获取阶段，通过在中间层嵌入时序提示向量；在巩固与激活阶段，采用基于相似度的提示交互实现记忆强化，并利用提示生成策略模拟记忆激活。其核心优势在于：通过中间层提示向量的交互实现模态间充分信息交换，同时大幅压缩可训练参数量与内存占用。实验表明，该方法在多个数据集上以仅2.0M可训练参数（约为预训练基础模型的1%）和较低内存消耗，取得了具有竞争力的性能表现。
