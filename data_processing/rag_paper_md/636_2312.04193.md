# Language Model Knowledge Distillation for Efficient Question Answering in Spanish

链接: http://arxiv.org/abs/2312.04193v1

原文摘要:
Recent advances in the development of pre-trained Spanish language models has
led to significant progress in many Natural Language Processing (NLP) tasks,
such as question answering. However, the lack of efficient models imposes a
barrier for the adoption of such models in resource-constrained environments.
Therefore, smaller distilled models for the Spanish language could be proven to
be highly scalable and facilitate their further adoption on a variety of tasks
and scenarios. In this work, we take one step in this direction by developing
SpanishTinyRoBERTa, a compressed language model based on RoBERTa for efficient
question answering in Spanish. To achieve this, we employ knowledge
distillation from a large model onto a lighter model that allows for a wider
implementation, even in areas with limited computational resources, whilst
attaining negligible performance sacrifice. Our experiments show that the dense
distilled model can still preserve the performance of its larger counterpart,
while significantly increasing inference speedup. This work serves as a
starting point for further research and investigation of model compression
efforts for Spanish language models across various NLP tasks.

中文翻译:
近年来，预训练西班牙语模型的研发进展显著推动了多项自然语言处理任务（如问答系统）的性能提升。然而，由于高效模型的匮乏，这类模型在资源受限环境中的实际应用面临障碍。为此，针对西班牙语开发的小型蒸馏模型有望展现出高度可扩展性，促进其在多样化任务和场景中的广泛应用。本研究通过构建SpanishTinyRoBERTa——一个基于RoBERTa架构的压缩语言模型，专为西班牙语高效问答系统设计，向该方向迈出了重要一步。我们采用知识蒸馏技术，将大模型能力迁移至轻量化模型，使其即便在计算资源有限的领域也能广泛部署，同时保持性能损失可忽略不计。实验表明，这种稠密蒸馏模型在显著提升推理速度的同时，仍能保留原大模型的性能优势。本研究成果为西班牙语模型在各种自然语言处理任务中的压缩研究提供了基础，将推动该领域的进一步探索。
