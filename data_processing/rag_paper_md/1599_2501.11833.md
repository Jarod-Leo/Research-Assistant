# Is your LLM trapped in a Mental Set? Investigative study on how mental sets affect the reasoning capabilities of LLMs

链接: http://arxiv.org/abs/2501.11833v1

原文摘要:
In this paper, we present an investigative study on how Mental Sets influence
the reasoning capabilities of LLMs. LLMs have excelled in diverse natural
language processing (NLP) tasks, driven by advancements in parameter-efficient
fine-tuning (PEFT) and emergent capabilities like in-context learning (ICL).
For complex reasoning tasks, selecting the right model for PEFT or ICL is
critical, often relying on scores on benchmarks such as MMLU, MATH, and GSM8K.
However, current evaluation methods, based on metrics like F1 Score or
reasoning chain assessments by larger models, overlook a key dimension:
adaptability to unfamiliar situations and overcoming entrenched thinking
patterns. In cognitive psychology, Mental Set refers to the tendency to persist
with previously successful strategies, even when they become inefficient - a
challenge for problem solving and reasoning. We compare the performance of LLM
models like Llama-3.1-8B-Instruct, Llama-3.1-70B-Instruct and GPT-4o in the
presence of mental sets. To the best of our knowledge, this is the first study
to integrate cognitive psychology concepts into the evaluation of LLMs for
complex reasoning tasks, providing deeper insights into their adaptability and
problem-solving efficacy.

中文翻译:
本文针对心理定势如何影响大语言模型（LLM）的推理能力展开研究。随着参数高效微调（PEFT）和上下文学习（ICL）等新兴能力的发展，LLM在各类自然语言处理任务中表现卓越。面对复杂推理任务时，选择适合PEFT或ICL的模型至关重要，这一过程通常依赖MMLU、MATH和GSM8K等基准测试得分。然而当前基于F1分数或大模型推理链评估的测评方法忽视了一个关键维度：模型对陌生情境的适应能力及突破固有思维模式的表现。认知心理学中的"心理定势"指人们倾向于坚持使用曾成功但已低效的策略——这正是问题解决与推理过程中的典型障碍。我们对比了Llama-3.1-8B-Instruct、Llama-3.1-70B-Instruct和GPT-4o等模型在心理定势影响下的表现。据我们所知，这是首次将认知心理学概念引入LLM复杂推理能力评估的研究，为理解模型适应性与问题解决效能提供了新视角。
