# LiPCoT: Linear Predictive Coding based Tokenizer for Self-supervised Learning of Time Series Data via Language Models

链接: http://arxiv.org/abs/2408.07292v1

原文摘要:
Language models have achieved remarkable success in various natural language
processing tasks. However, their application to time series data, a crucial
component in many domains, remains limited. This paper proposes LiPCoT (Linear
Predictive Coding based Tokenizer for time series), a novel tokenizer that
encodes time series data into a sequence of tokens, enabling self-supervised
learning of time series using existing Language model architectures such as
BERT. Unlike traditional time series tokenizers that rely heavily on CNN
encoder for time series feature generation, LiPCoT employs stochastic modeling
through linear predictive coding to create a latent space for time series
providing a compact yet rich representation of the inherent stochastic nature
of the data. Furthermore, LiPCoT is computationally efficient and can
effectively handle time series data with varying sampling rates and lengths,
overcoming common limitations of existing time series tokenizers. In this
proof-of-concept work, we present the effectiveness of LiPCoT in classifying
Parkinson's disease (PD) using an EEG dataset from 46 participants. In
particular, we utilize LiPCoT to encode EEG data into a small vocabulary of
tokens and then use BERT for self-supervised learning and the downstream task
of PD classification. We benchmark our approach against several
state-of-the-art CNN-based deep learning architectures for PD detection. Our
results reveal that BERT models utilizing self-supervised learning outperformed
the best-performing existing method by 7.1% in precision, 2.3% in recall, 5.5%
in accuracy, 4% in AUC, and 5% in F1-score highlighting the potential for
self-supervised learning even on small datasets. Our work will inform future
foundational models for time series, particularly for self-supervised learning.

中文翻译:
本文提出了一种基于线性预测编码的时间序列标记化方法LiPCoT（Linear Predictive Coding based Tokenizer for time series），通过将时间序列数据编码为标记序列，使现有语言模型架构（如BERT）能够实现时间序列的自监督学习。与依赖CNN编码器生成时间序列特征的传统方法不同，LiPCoT采用线性预测编码进行随机建模，构建能紧凑且丰富表征数据内在随机特性的潜在空间。该方法计算高效，可有效处理不同采样率和长度的时序数据，克服了现有标记化技术的常见局限。在概念验证中，我们使用46名参与者的脑电图数据集验证LiPCoT在帕金森病分类中的有效性：首先将脑电信号编码为小规模标记词汇，随后通过BERT进行自监督学习及下游分类任务。与当前最先进的CNN深度学习架构相比，采用自监督学习的BERT模型在各项指标上均显著优于现有最佳方法——精确度提升7.1%、召回率提高2.3%、准确率增加5.5%、AUC增长4%、F1分数上升5%，这证实了即使在小数据集上自监督学习仍具潜力。本研究将为时间序列基础模型，特别是自监督学习方向的未来发展提供重要参考。

（注：根据学术摘要的翻译规范，对原文进行了以下优化处理：
1. 将技术术语"tokenizer"统一译为"标记化方法/技术"而非直译"分词器"
2. 采用"潜在空间"而非"隐空间"保持学术用语准确性
3. 对长句进行合理切分，如将原文最后两句合并为具有因果逻辑的复合句
4. 保留所有专业缩写（LiPCoT/BERT/AUC等）的首次全称标注
5. 使用中文顿号替代英文逗号实现列举项的标准排版
6. 对性能提升数据采用"指标名称+提升幅度"的统一表述格式）
