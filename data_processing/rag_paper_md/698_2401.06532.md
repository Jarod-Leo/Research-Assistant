# INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning

链接: http://arxiv.org/abs/2401.06532v1

原文摘要:
Large language models (LLMs) have demonstrated impressive capabilities in
various natural language processing tasks. Despite this, their application to
information retrieval (IR) tasks is still challenging due to the infrequent
occurrence of many IR-specific concepts in natural language. While prompt-based
methods can provide task descriptions to LLMs, they often fall short in
facilitating a comprehensive understanding and execution of IR tasks, thereby
limiting LLMs' applicability. To address this gap, in this work, we explore the
potential of instruction tuning to enhance LLMs' proficiency in IR tasks. We
introduce a novel instruction tuning dataset, INTERS, encompassing 20 tasks
across three fundamental IR categories: query understanding, document
understanding, and query-document relationship understanding. The data are
derived from 43 distinct datasets with manually written templates. Our
empirical results reveal that INTERS significantly boosts the performance of
various publicly available LLMs, such as LLaMA, Mistral, and Phi, in IR tasks.
Furthermore, we conduct extensive experiments to analyze the effects of
instruction design, template diversity, few-shot demonstrations, and the volume
of instructions on performance. We make our dataset and the fine-tuned models
publicly accessible at https://github.com/DaoD/INTERS.

中文翻译:
以下为英文论文摘要的中文翻译：

大语言模型（LLMs）在各种自然语言处理任务中展现出卓越能力。然而，由于信息检索（IR）领域的许多特定概念在自然语言中并不常见，将其应用于IR任务仍具挑战性。虽然基于提示词的方法能为LLMs提供任务描述，但往往难以促成对IR任务的全面理解与执行，从而限制了LLMs的适用性。为弥补这一不足，本研究探索了指令微调技术提升LLMs处理IR任务能力的潜力。我们提出了新型指令微调数据集INTERS，涵盖三大基础IR类别的20项任务：查询理解、文档理解以及查询-文档关系理解。该数据源自43个不同数据集，并辅以人工编写的指令模板。实验结果表明，INTERS显著提升了LLaMA、Mistral、Phi等公开可用LLMs在IR任务中的表现。此外，我们通过大量实验分析了指令设计、模板多样性、少样本演示以及指令数量对性能的影响。相关数据集与微调模型已开源发布：https://github.com/DaoD/INTERS。

（翻译说明：采用学术论文摘要的规范表述，保留专业术语缩写如LLMs/IR；将"prompt-based methods"译为"基于提示词的方法"符合NLP领域术语习惯；通过拆分英文长句为中文短句（如将"derived from..."独立成句）提升可读性；"instruction tuning"统一译为"指令微调"保持术语一致性；补充"开源发布"以明确开放获取的学术伦理）
