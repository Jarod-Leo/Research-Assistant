# Bioformer: an efficient transformer language model for biomedical text mining

链接: http://arxiv.org/abs/2302.01588v1

原文摘要:
Pretrained language models such as Bidirectional Encoder Representations from
Transformers (BERT) have achieved state-of-the-art performance in natural
language processing (NLP) tasks. Recently, BERT has been adapted to the
biomedical domain. Despite the effectiveness, these models have hundreds of
millions of parameters and are computationally expensive when applied to
large-scale NLP applications. We hypothesized that the number of parameters of
the original BERT can be dramatically reduced with minor impact on performance.
In this study, we present Bioformer, a compact BERT model for biomedical text
mining. We pretrained two Bioformer models (named Bioformer8L and Bioformer16L)
which reduced the model size by 60% compared to BERTBase. Bioformer uses a
biomedical vocabulary and was pre-trained from scratch on PubMed abstracts and
PubMed Central full-text articles. We thoroughly evaluated the performance of
Bioformer as well as existing biomedical BERT models including BioBERT and
PubMedBERT on 15 benchmark datasets of four different biomedical NLP tasks:
named entity recognition, relation extraction, question answering and document
classification. The results show that with 60% fewer parameters, Bioformer16L
is only 0.1% less accurate than PubMedBERT while Bioformer8L is 0.9% less
accurate than PubMedBERT. Both Bioformer16L and Bioformer8L outperformed
BioBERTBase-v1.1. In addition, Bioformer16L and Bioformer8L are 2-3 fold as
fast as PubMedBERT/BioBERTBase-v1.1. Bioformer has been successfully deployed
to PubTator Central providing gene annotations over 35 million PubMed abstracts
and 5 million PubMed Central full-text articles. We make Bioformer publicly
available via https://github.com/WGLab/bioformer, including pre-trained models,
datasets, and instructions for downstream use.

中文翻译:
诸如双向编码器表示（BERT）等预训练语言模型在自然语言处理（NLP）任务中已实现最先进的性能表现。近期，BERT被成功迁移至生物医学领域应用。尽管效果显著，这些模型通常包含数亿参数，在大规模NLP应用中存在高昂计算成本。我们提出假设：在保持性能影响最小的前提下，原始BERT模型的参数量可大幅削减。本研究推出Bioformer——一种面向生物医学文本挖掘的轻量化BERT模型。通过预训练两个版本（Bioformer8L与Bioformer16L），模型体积较BERTBase缩减60%。Bioformer采用生物医学专用词表，并基于PubMed摘要及PubMed Central全文数据从头训练。我们在15个生物医学NLP基准数据集上系统评估了Bioformer及现有生物医学BERT模型（包括BioBERT和PubMedBERT）在四大类任务中的表现：命名实体识别、关系抽取、问答系统和文档分类。实验结果表明：参数量减少60%的Bioformer16L准确率仅比PubMedBERT低0.1%，Bioformer8L差距为0.9%，两者均优于BioBERTBase-v1.1。此外，Bioformer16L和Bioformer8L的运行速度达到PubMedBERT/BioBERTBase-v1.1的2-3倍。目前Bioformer已成功部署于PubTator Central平台，为超过3500万篇PubMed摘要和500万篇PMC全文提供基因注释服务。我们通过https://github.com/WGLab/bioformer公开Bioformer全部资源，包括预训练模型、数据集及下游应用指南。
