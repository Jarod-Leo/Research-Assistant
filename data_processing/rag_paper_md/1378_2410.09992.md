# Evaluating Gender Bias of LLMs in Making Morality Judgements

链接: http://arxiv.org/abs/2410.09992v1

原文摘要:
Large Language Models (LLMs) have shown remarkable capabilities in a
multitude of Natural Language Processing (NLP) tasks. However, these models are
still not immune to limitations such as social biases, especially gender bias.
This work investigates whether current closed and open-source LLMs possess
gender bias, especially when asked to give moral opinions. To evaluate these
models, we curate and introduce a new dataset GenMO (Gender-bias in Morality
Opinions) comprising parallel short stories featuring male and female
characters respectively. Specifically, we test models from the GPT family
(GPT-3.5-turbo, GPT-3.5-turbo-instruct, GPT-4-turbo), Llama 3 and 3.1 families
(8B/70B), Mistral-7B and Claude 3 families (Sonnet and Opus). Surprisingly,
despite employing safety checks, all production-standard models we tested
display significant gender bias with GPT-3.5-turbo giving biased opinions in
24% of the samples. Additionally, all models consistently favour female
characters, with GPT showing bias in 68-85% of cases and Llama 3 in around
81-85% instances. Additionally, our study investigates the impact of model
parameters on gender bias and explores real-world situations where LLMs reveal
biases in moral decision-making.

中文翻译:
大型语言模型（LLMs）在众多自然语言处理（NLP）任务中展现出卓越能力，但仍无法完全规避社会偏见（尤其是性别偏见）等局限性。本研究探讨了当前闭源与开源LLMs是否持有性别偏见，特别是在要求其表达道德观点时。为评估这些模型，我们构建并发布了新数据集GenMO（道德观点中的性别偏见），该数据集包含分别以男性和女性为主角的平行短篇故事。具体测试对象包括：GPT系列（GPT-3.5-turbo、GPT-3.5-turbo-instruct、GPT-4-turbo）、Llama 3及3.1系列（8B/70B）、Mistral-7B以及Claude 3系列（Sonnet和Opus）。令人惊讶的是，尽管采用了安全审查机制，所有测试的生产级模型均表现出显著性别偏见，其中GPT-3.5-turbo在24%的样本中给出带有偏见的观点。此外，所有模型均持续偏向女性角色：GPT系列在68-85%的案例中呈现偏见，Llama 3系列约81-85%的实例存在偏向。研究还揭示了模型参数对性别偏见的影响，并探索了LLMs在现实道德决策场景中暴露偏见的案例。
