# Protecting Your LLMs with Information Bottleneck

链接: http://arxiv.org/abs/2404.13968v1

原文摘要:
The advent of large language models (LLMs) has revolutionized the field of
natural language processing, yet they might be attacked to produce harmful
content. Despite efforts to ethically align LLMs, these are often fragile and
can be circumvented by jailbreaking attacks through optimized or manual
adversarial prompts. To address this, we introduce the Information Bottleneck
Protector (IBProtector), a defense mechanism grounded in the information
bottleneck principle, and we modify the objective to avoid trivial solutions.
The IBProtector selectively compresses and perturbs prompts, facilitated by a
lightweight and trainable extractor, preserving only essential information for
the target LLMs to respond with the expected answer. Moreover, we further
consider a situation where the gradient is not visible to be compatible with
any LLM. Our empirical evaluations show that IBProtector outperforms current
defense methods in mitigating jailbreak attempts, without overly affecting
response quality or inference speed. Its effectiveness and adaptability across
various attack methods and target LLMs underscore the potential of IBProtector
as a novel, transferable defense that bolsters the security of LLMs without
requiring modifications to the underlying models.

中文翻译:
大型语言模型（LLM）的出现彻底改变了自然语言处理领域，但它们可能遭受攻击而生成有害内容。尽管已通过伦理对齐技术对LLM进行防护，这些防御往往较为脆弱，可能被经过优化的或人工设计的对抗性提示（即越狱攻击）所绕过。为此，我们提出基于信息瓶颈原理的防御机制——信息瓶颈保护器（IBProtector），并通过修改目标函数避免平凡解。该机制通过轻量级可训练的提取器，对输入提示进行选择性压缩和扰动，仅保留目标LLM生成预期响应所需的关键信息。此外，我们还进一步考虑了梯度不可见的场景以兼容任意LLM。实证评估表明，IBProtector在抵御越狱攻击方面优于现有防御方法，且不会显著影响响应质量或推理速度。其在多种攻击方法和目标LLM上展现的有效性与适应性，印证了该机制作为一种新型可迁移防御方案的潜力——无需修改底层模型即可增强LLM的安全性。
