# The What, Why, and How of Context Length Extension Techniques in Large Language Models -- A Detailed Survey

链接: http://arxiv.org/abs/2401.07872v1

原文摘要:
The advent of Large Language Models (LLMs) represents a notable breakthrough
in Natural Language Processing (NLP), contributing to substantial progress in
both text comprehension and generation. However, amidst these advancements, it
is noteworthy that LLMs often face a limitation in terms of context length
extrapolation. Understanding and extending the context length for LLMs is
crucial in enhancing their performance across various NLP applications. In this
survey paper, we delve into the multifaceted aspects of exploring why it is
essential, and the potential transformations that superior techniques could
bring to NLP applications. We study the inherent challenges associated with
extending context length and present an organized overview of the existing
strategies employed by researchers. Additionally, we discuss the intricacies of
evaluating context extension techniques and highlight the open challenges that
researchers face in this domain. Furthermore, we explore whether there is a
consensus within the research community regarding evaluation standards and
identify areas where further agreement is needed. This comprehensive survey
aims to serve as a valuable resource for researchers, guiding them through the
nuances of context length extension techniques and fostering discussions on
future advancements in this evolving field.

中文翻译:
大型语言模型（LLMs）的出现标志着自然语言处理（NLP）领域的重大突破，显著提升了文本理解与生成能力。然而值得注意的是，LLMs在上下文长度外推方面普遍存在局限。理解并扩展LLMs的上下文长度对于提升其在各类NLP应用中的表现至关重要。本综述论文从多维度探讨了这一问题的核心价值、优化技术可能为NLP应用带来的变革，系统分析了现有扩展方法的挑战与策略框架。我们深入剖析了上下文扩展技术的评估复杂性，指明该领域亟待解决的关键问题，同时探讨研究界在评估标准共识度方面的现状与不足。本综述旨在为研究者提供系统性参考，引导上下文长度扩展技术的精细化发展，推动这一动态领域的未来探索。
