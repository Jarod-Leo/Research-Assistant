# Anatomy of Neural Language Models

链接: http://arxiv.org/abs/2401.03797v1

原文摘要:
The fields of generative AI and transfer learning have experienced remarkable
advancements in recent years especially in the domain of Natural Language
Processing (NLP). Transformers have been at the heart of these advancements
where the cutting-edge transformer-based Language Models (LMs) have led to new
state-of-the-art results in a wide spectrum of applications. While the number
of research works involving neural LMs is exponentially increasing, their vast
majority are high-level and far from self-contained. Consequently, a deep
understanding of the literature in this area is a tough task especially in the
absence of a unified mathematical framework explaining the main types of neural
LMs. We address the aforementioned problem in this tutorial where the objective
is to explain neural LMs in a detailed, simplified and unambiguous mathematical
framework accompanied by clear graphical illustrations. Concrete examples on
widely used models like BERT and GPT2 are explored. Finally, since transformers
pretrained on language-modeling-like tasks have been widely adopted in computer
vision and time series applications, we briefly explore some examples of such
solutions in order to enable readers to understand how transformers work in the
aforementioned domains and compare this use with the original one in NLP.

中文翻译:
近年来，生成式人工智能与迁移学习领域取得了显著进展，尤其在自然语言处理（NLP）方向表现突出。这一进步的核心驱动力是Transformer架构——基于该技术的尖端语言模型（LMs）已在众多应用中创造了全新的性能标杆。尽管涉及神经语言模型的研究数量呈指数级增长，但绝大多数成果停留在高层抽象层面，缺乏自成体系的完整阐述。因此，在缺乏统一数学框架来解释主要神经语言模型类型的情况下，要深入理解该领域文献变得尤为困难。本教程针对上述问题，旨在通过详细、简化的数学表达框架配合清晰的图示，对神经语言模型进行明确解析。文中以BERT、GPT2等广泛使用的模型为具体案例展开分析。最后，鉴于最初面向语言建模任务设计的预训练Transformer模型已广泛应用于计算机视觉和时间序列领域，我们简要探讨了若干跨领域应用实例，帮助读者理解Transformer在这些场景中的运作机制，并与原始NLP应用进行对比分析。
