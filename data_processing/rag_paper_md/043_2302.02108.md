# Knowledge Distillation in Vision Transformers: A Critical Review

链接: http://arxiv.org/abs/2302.02108v1

原文摘要:
In Natural Language Processing (NLP), Transformers have already
revolutionized the field by utilizing an attention-based encoder-decoder model.
Recently, some pioneering works have employed Transformer-like architectures in
Computer Vision (CV) and they have reported outstanding performance of these
architectures in tasks such as image classification, object detection, and
semantic segmentation. Vision Transformers (ViTs) have demonstrated impressive
performance improvements over Convolutional Neural Networks (CNNs) due to their
competitive modelling capabilities. However, these architectures demand massive
computational resources which makes these models difficult to be deployed in
the resource-constrained applications. Many solutions have been developed to
combat this issue, such as compressive transformers and compression functions
such as dilated convolution, min-max pooling, 1D convolution, etc. Model
compression has recently attracted considerable research attention as a
potential remedy. A number of model compression methods have been proposed in
the literature such as weight quantization, weight multiplexing, pruning and
Knowledge Distillation (KD). However, techniques like weight quantization,
pruning and weight multiplexing typically involve complex pipelines for
performing the compression. KD has been found to be a simple and much effective
model compression technique that allows a relatively simple model to perform
tasks almost as accurately as a complex model. This paper discusses various
approaches based upon KD for effective compression of ViT models. The paper
elucidates the role played by KD in reducing the computational and memory
requirements of these models. The paper also presents the various challenges
faced by ViTs that are yet to be resolved.

中文翻译:
在自然语言处理（NLP）领域，基于注意力机制的编码器-解码器模型Transformer已彻底革新了该领域。近期，一些开创性研究将类Transformer架构引入计算机视觉（CV），这些架构在图像分类、目标检测和语义分割等任务中展现出卓越性能。视觉Transformer（ViT）凭借其强大的建模能力，相较卷积神经网络（CNN）实现了显著的性能提升。然而，这类架构需要消耗大量计算资源，导致模型难以部署在资源受限的应用场景中。

为应对这一挑战，研究者已提出多种解决方案，如压缩型Transformer、扩张卷积等压缩函数，以及最小-最大池化、一维卷积等技术。模型压缩作为潜在解决途径，近年来受到广泛关注。现有文献提出了权重量化、权重复用、剪枝和知识蒸馏（KD）等多种压缩方法。但权重量化、剪枝和权重复用等技术通常涉及复杂的压缩流程。相比之下，知识蒸馏因其简洁高效的特点脱颖而出，能使轻量模型以接近复杂模型的精度完成任务。

本文系统论述了基于知识蒸馏的ViT模型高效压缩方法，阐释了知识蒸馏在降低模型计算与内存需求方面的核心作用，并探讨了视觉Transformer尚未解决的技术难题。
