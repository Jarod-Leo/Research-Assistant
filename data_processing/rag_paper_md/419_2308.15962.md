# WALL-E: Embodied Robotic WAiter Load Lifting with Large Language Model

链接: http://arxiv.org/abs/2308.15962v1

原文摘要:
Enabling robots to understand language instructions and react accordingly to
visual perception has been a long-standing goal in the robotics research
community. Achieving this goal requires cutting-edge advances in natural
language processing, computer vision, and robotics engineering. Thus, this
paper mainly investigates the potential of integrating the most recent Large
Language Models (LLMs) and existing visual grounding and robotic grasping
system to enhance the effectiveness of the human-robot interaction. We
introduce the WALL-E (Embodied Robotic WAiter load lifting with Large Language
model) as an example of this integration. The system utilizes the LLM of
ChatGPT to summarize the preference object of the users as a target instruction
via the multi-round interactive dialogue. The target instruction is then
forwarded to a visual grounding system for object pose and size estimation,
following which the robot grasps the object accordingly. We deploy this
LLM-empowered system on the physical robot to provide a more user-friendly
interface for the instruction-guided grasping task. The further experimental
results on various real-world scenarios demonstrated the feasibility and
efficacy of our proposed framework. See the project website at:
https://star-uu-wang.github.io/WALL-E/

中文翻译:
让机器人理解语言指令并根据视觉感知做出相应反应，一直是机器人研究领域的长期目标。实现这一目标需要自然语言处理、计算机视觉和机器人工程学领域的前沿技术突破。为此，本研究重点探索了将最新的大型语言模型（LLMs）与现有视觉定位及机器人抓取系统相融合的潜力，以提升人机交互的效能。我们以WALL-E（基于大型语言模型的具身服务机器人负载搬运系统）为例展示这种融合：系统利用ChatGPT的LLM模型通过多轮交互对话总结用户偏好的目标物体指令，随后将该指令传输至视觉定位系统进行物体位姿与尺寸估计，最终驱动机器人完成抓取操作。我们将这一LLM赋能系统部署于实体机器人，为指令引导的抓取任务提供了更友好的用户界面。多场景真实环境实验结果表明，所提框架具有可行性与有效性。项目详情参见：https://star-uu-wang.github.io/WALL-E/
