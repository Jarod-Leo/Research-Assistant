# Structured Code Representations Enable Data-Efficient Adaptation of Code Language Models

链接: http://arxiv.org/abs/2401.10716v1

原文摘要:
Current language models tailored for code tasks often adopt the
pre-training-then-fine-tuning paradigm from natural language processing,
modeling source code as plain text. This approach, however, overlooks the
unambiguous structures inherent in programming languages. In this work, we
explore data-efficient adaptation of pre-trained code models by further
pre-training and fine-tuning them with program structures. Specifically, we
represent programs as parse trees -- also known as concrete syntax trees (CSTs)
-- and adapt pre-trained models on serialized CSTs. Although the models that we
adapt have been pre-trained only on the surface form of programs, we find that
a small amount of continual pre-training and fine-tuning on CSTs without
changing the model architecture yields improvements over the baseline approach
across various code tasks. The improvements are found to be particularly
significant when there are limited training examples, demonstrating the
effectiveness of integrating program structures with plain-text representation
even when working with backbone models that have not been pre-trained with
structures.

中文翻译:
当前针对代码任务优化的语言模型通常沿袭自然语言处理领域"预训练-微调"的范式，将源代码视为纯文本进行建模。然而这种方法忽视了编程语言与生俱来的明确结构特征。本研究探索了通过程序结构进行持续预训练和微调，实现预训练代码模型的高效数据适应。具体而言，我们将程序表示为解析树（又称具体语法树/CSTs），并在序列化的CSTs上对预训练模型进行适配。尽管所适配的模型仅基于程序表层形式进行过预训练，但我们发现：在不改变模型架构的前提下，对CSTs进行少量持续预训练和微调，就能在多类代码任务上取得优于基线方法的效果。当训练样本有限时，这种改进尤为显著，这表明即便主干模型未经结构化预训练，将程序结构与纯文本表征相结合仍能有效提升模型性能。
