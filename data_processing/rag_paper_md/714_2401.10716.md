# Structured Code Representations Enable Data-Efficient Adaptation of Code Language Models

链接: http://arxiv.org/abs/2401.10716v1

原文摘要:
Current language models tailored for code tasks often adopt the
pre-training-then-fine-tuning paradigm from natural language processing,
modeling source code as plain text. This approach, however, overlooks the
unambiguous structures inherent in programming languages. In this work, we
explore data-efficient adaptation of pre-trained code models by further
pre-training and fine-tuning them with program structures. Specifically, we
represent programs as parse trees -- also known as concrete syntax trees (CSTs)
-- and adapt pre-trained models on serialized CSTs. Although the models that we
adapt have been pre-trained only on the surface form of programs, we find that
a small amount of continual pre-training and fine-tuning on CSTs without
changing the model architecture yields improvements over the baseline approach
across various code tasks. The improvements are found to be particularly
significant when there are limited training examples, demonstrating the
effectiveness of integrating program structures with plain-text representation
even when working with backbone models that have not been pre-trained with
structures.

中文翻译:
当前针对代码任务优化的语言模型，通常沿袭自然语言处理中"预训练-微调"的范式，将源代码视为纯文本建模。然而这种方法忽略了编程语言固有的明确结构特征。本研究探索通过程序结构进行继续预训练和微调，实现预训练代码模型的数据高效适配。具体而言，我们将程序表示为解析树（又称具体语法树CST），并在序列化的CST上适配预训练模型。尽管被适配的模型仅在程序表层形式上进行过预训练，但我们发现：在不改变模型架构的前提下，对CST进行少量持续预训练和微调，就能在各种代码任务上获得优于基线方法的表现。当训练样本有限时，这种改进尤为显著，这表明即使面对未经过结构预训练的基础模型，将程序结构与纯文本表征相结合仍具有显著效果。
