# Exploring Safety Generalization Challenges of Large Language Models via Code

链接: http://arxiv.org/abs/2403.07865v1

原文摘要:
The rapid advancement of Large Language Models (LLMs) has brought about
remarkable generative capabilities but also raised concerns about their
potential misuse. While strategies like supervised fine-tuning and
reinforcement learning from human feedback have enhanced their safety, these
methods primarily focus on natural languages, which may not generalize to other
domains. This paper introduces CodeAttack, a framework that transforms natural
language inputs into code inputs, presenting a novel environment for testing
the safety generalization of LLMs. Our comprehensive studies on
state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a
new and universal safety vulnerability of these models against code input:
CodeAttack bypasses the safety guardrails of all models more than 80\% of the
time. We find that a larger distribution gap between CodeAttack and natural
language leads to weaker safety generalization, such as encoding natural
language input with data structures. Furthermore, we give our hypotheses about
the success of CodeAttack: the misaligned bias acquired by LLMs during code
training, prioritizing code completion over avoiding the potential safety risk.
Finally, we analyze potential mitigation measures. These findings highlight new
safety risks in the code domain and the need for more robust safety alignment
algorithms to match the code capabilities of LLMs.

中文翻译:
大型语言模型（LLM）的快速发展带来了卓越的生成能力，同时也引发了对其潜在滥用的担忧。尽管监督微调和人类反馈强化学习等策略增强了模型的安全性，但这些方法主要针对自然语言场景，可能无法泛化至其他领域。本文提出CodeAttack框架，通过将自然语言输入转化为代码输入，为测试LLM的安全泛化能力构建了全新实验环境。我们对包括GPT-4、Claude-2和Llama-2系列在内的前沿模型进行全面研究，揭示了这些模型面对代码输入时存在的新型普适性安全漏洞：CodeAttack能以超过80%的成功率绕过所有模型的安全防护。研究发现，当CodeAttack与自然语言的分布差异越大时（如使用数据结构编码自然语言输入），模型的安全泛化能力越弱。进一步地，我们提出CodeAttack成功的原因假设：LLM在代码训练过程中形成的偏差错位，使其更倾向于完成代码而非规避潜在安全风险。最后，我们分析了可能的缓解措施。这些发现揭示了代码领域的新安全风险，表明需要开发更鲁棒的安全对齐算法以匹配LLM的代码能力。
