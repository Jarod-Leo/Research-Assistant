# ADAT: Time-Series-Aware Adaptive Transformer Architecture for Sign Language Translation

链接: http://arxiv.org/abs/2504.11942v1

原文摘要:
Current sign language machine translation systems rely on recognizing hand
movements, facial expressions and body postures, and natural language
processing, to convert signs into text. Recent approaches use Transformer
architectures to model long-range dependencies via positional encoding.
However, they lack accuracy in recognizing fine-grained, short-range temporal
dependencies between gestures captured at high frame rates. Moreover, their
high computational complexity leads to inefficient training. To mitigate these
issues, we propose an Adaptive Transformer (ADAT), which incorporates
components for enhanced feature extraction and adaptive feature weighting
through a gating mechanism to emphasize contextually relevant features while
reducing training overhead and maintaining translation accuracy. To evaluate
ADAT, we introduce MedASL, the first public medical American Sign Language
dataset. In sign-to-gloss-to-text experiments, ADAT outperforms the
encoder-decoder transformer, improving BLEU-4 accuracy by 0.1% while reducing
training time by 14.33% on PHOENIX14T and 3.24% on MedASL. In sign-to-text
experiments, it improves accuracy by 8.7% and reduces training time by 2.8% on
PHOENIX14T and achieves 4.7% higher accuracy and 7.17% faster training on
MedASL. Compared to encoder-only and decoder-only baselines in sign-to-text,
ADAT is at least 6.8% more accurate despite being up to 12.1% slower due to its
dual-stream structure.

中文翻译:
当前手语机器翻译系统通过识别手部动作、面部表情与身体姿态，结合自然语言处理技术将手语转化为文本。最新研究采用Transformer架构，通过位置编码建模长程依赖关系，但这类方法对高帧率采集手势间细粒度短时依赖的识别精度不足，且高计算复杂度导致训练效率低下。为解决这些问题，我们提出自适应Transformer（ADAT），通过门控机制集成增强特征提取与自适应特征加权组件，在降低训练开销的同时保持翻译精度，突出上下文相关特征。为评估ADAT，我们构建首个公开医疗领域美国手语数据集MedASL。在手语-词义-文本实验中，ADAT在PHOENIX14T数据集上BLEU-4准确率提升0.1%，训练时间减少14.33%；在MedASL上训练时间缩短3.24%。直接手语-文本实验中，PHOENIX14T准确率提升8.7%，训练加速2.8%；MedASL准确率提高4.7%，训练提速7.17%。与单编码器/解码器基线相比，ADAT在手语-文本任务中准确率至少提升6.8%，虽因双流结构导致速度最多降低12.1%。
