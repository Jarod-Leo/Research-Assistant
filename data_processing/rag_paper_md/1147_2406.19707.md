# InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management

链接: http://arxiv.org/abs/2406.19707v1

原文摘要:
Transformer-based large language models (LLMs) demonstrate impressive
performance across various natural language processing tasks. Serving LLM
inference for generating long contents, however, poses a challenge due to the
enormous memory footprint of the transient state, known as the key-value (KV)
cache, which scales with the sequence length and batch size. In this paper, we
present InfiniGen, a novel KV cache management framework tailored for long-text
generation, which synergistically works with modern offloading-based inference
systems. InfiniGen leverages the key insight that a few important tokens that
are essential for computing the subsequent attention layer in the Transformer
can be speculated by performing a minimal rehearsal with the inputs of the
current layer and part of the query weight and key cache of the subsequent
layer. This allows us to prefetch only the essential KV cache entries (without
fetching them all), thereby mitigating the fetch overhead from the host memory
in offloading-based LLM serving systems. Our evaluation on several
representative LLMs shows that InfiniGen improves the overall performance of a
modern offloading-based system by up to 3.00x compared to prior KV cache
management methods while offering substantially better model accuracy.

中文翻译:
基于Transformer架构的大规模语言模型（LLMs）在各类自然语言处理任务中展现出卓越性能。然而，在生成长文本内容时，LLM推理服务面临巨大挑战——临时状态（即键值缓存KV cache）因序列长度和批处理规模呈指数级增长，导致内存占用激增。本文提出InfiniGen，一种专为长文本生成设计的新型KV缓存管理框架，可与现代基于卸载的推理系统协同工作。其核心创新在于：通过利用当前层输入及下一层部分查询权重与键缓存的极简预演，可精准预测Transformer下一注意力层计算所需的关键少量令牌。这一机制使得系统仅需预取必要的KV缓存条目（而非全部加载），从而显著降低基于卸载的LLM服务系统中主机内存的读取开销。实验表明，在多种代表性LLM上，InfiniGen将现代卸载系统的整体性能较现有KV缓存管理方法提升最高达3倍，同时大幅提高模型准确率。
