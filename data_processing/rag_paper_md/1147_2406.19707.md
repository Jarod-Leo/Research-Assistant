# InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management

链接: http://arxiv.org/abs/2406.19707v1

原文摘要:
Transformer-based large language models (LLMs) demonstrate impressive
performance across various natural language processing tasks. Serving LLM
inference for generating long contents, however, poses a challenge due to the
enormous memory footprint of the transient state, known as the key-value (KV)
cache, which scales with the sequence length and batch size. In this paper, we
present InfiniGen, a novel KV cache management framework tailored for long-text
generation, which synergistically works with modern offloading-based inference
systems. InfiniGen leverages the key insight that a few important tokens that
are essential for computing the subsequent attention layer in the Transformer
can be speculated by performing a minimal rehearsal with the inputs of the
current layer and part of the query weight and key cache of the subsequent
layer. This allows us to prefetch only the essential KV cache entries (without
fetching them all), thereby mitigating the fetch overhead from the host memory
in offloading-based LLM serving systems. Our evaluation on several
representative LLMs shows that InfiniGen improves the overall performance of a
modern offloading-based system by up to 3.00x compared to prior KV cache
management methods while offering substantially better model accuracy.

中文翻译:
基于Transformer架构的大语言模型（LLMs）在各类自然语言处理任务中展现出卓越性能。然而，在生成长文本内容时，LLM推理服务面临巨大挑战——临时状态（即键值缓存KV cache）产生的内存占用会随序列长度和批处理规模线性增长。本文提出InfiniGen，这是一种专为长文本生成设计的新型KV缓存管理框架，可与现代基于卸载技术的推理系统协同工作。该框架的核心洞见在于：通过当前层输入与下一层部分查询权重及键缓存的极简预演，可推测出对后续Transformer注意力层计算至关重要的少量关键token。这一机制使我们能仅预取必要的KV缓存条目（而非全部获取），从而显著降低基于卸载技术的LLM服务系统中主机内存的读取开销。在多个代表性LLM上的实验表明，相较于现有KV缓存管理方法，InfiniGen将现代卸载系统的整体性能提升最高达3.00倍，同时显著提高了模型准确率。

（翻译说明：
1. 专业术语处理：保留"Transformer"、"KV cache"等技术术语，采用"键值缓存"的括号补充说明
2. 长句拆分：将原文复合长句分解为符合中文表达习惯的短句结构
3. 被动语态转换："can be speculated"译为主动式"可推测出"
4. 概念显化："rehearsal"在技术语境下意译为"预演"而非字面排练
5. 数据呈现：精确保留"3.00x"等技术指标
6. 逻辑连接：通过"这一机制"等衔接词保持论证连贯性
7. 技术准确性：确保"注意力层"、"查询权重"等专业表述与NLP领域术语一致）
