# LGViT: Dynamic Early Exiting for Accelerating Vision Transformer

链接: http://arxiv.org/abs/2308.00255v1

原文摘要:
Recently, the efficient deployment and acceleration of powerful vision
transformers (ViTs) on resource-limited edge devices for providing multimedia
services have become attractive tasks. Although early exiting is a feasible
solution for accelerating inference, most works focus on convolutional neural
networks (CNNs) and transformer models in natural language processing
(NLP).Moreover, the direct application of early exiting methods to ViTs may
result in substantial performance degradation. To tackle this challenge, we
systematically investigate the efficacy of early exiting in ViTs and point out
that the insufficient feature representations in shallow internal classifiers
and the limited ability to capture target semantic information in deep internal
classifiers restrict the performance of these methods. We then propose an early
exiting framework for general ViTs termed LGViT, which incorporates
heterogeneous exiting heads, namely, local perception head and global
aggregation head, to achieve an efficiency-accuracy trade-off. In particular,
we develop a novel two-stage training scheme, including end-to-end training and
self-distillation with the backbone frozen to generate early exiting ViTs,
which facilitates the fusion of global and local information extracted by the
two types of heads. We conduct extensive experiments using three popular ViT
backbones on three vision datasets. Results demonstrate that our LGViT can
achieve competitive performance with approximately 1.8 $\times$ speed-up.

中文翻译:
近年来，在资源受限的边缘设备上高效部署和加速强大的视觉Transformer（ViTs）以提供多媒体服务，已成为备受关注的任务。尽管早期退出是加速推理的可行方案，但现有研究多集中于卷积神经网络（CNNs）和自然语言处理（NLP）中的Transformer模型。此外，直接将早期退出方法应用于ViTs可能导致显著的性能下降。为应对这一挑战，我们系统研究了早期退出在ViTs中的有效性，指出浅层内部分类器特征表示不足与深层内部分类器目标语义信息捕获能力有限是制约性能的关键因素。为此，我们提出了一种通用ViTs早期退出框架LGViT，通过引入异构退出头（局部感知头和全局聚合头）实现效率与精度的平衡。特别地，我们开发了包含主干网络冻结的两阶段训练方案——端到端训练与自蒸馏学习，以促进两类头部提取的全局与局部信息融合。基于三种主流ViT架构在三个视觉数据集上的实验表明，LGViT能以约1.8倍加速比保持极具竞争力的性能。
