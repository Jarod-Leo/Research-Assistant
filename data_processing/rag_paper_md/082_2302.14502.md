# A Survey on Long Text Modeling with Transformers

链接: http://arxiv.org/abs/2302.14502v1

原文摘要:
Modeling long texts has been an essential technique in the field of natural
language processing (NLP). With the ever-growing number of long documents, it
is important to develop effective modeling methods that can process and analyze
such texts. However, long texts pose important research challenges for existing
text models, with more complex semantics and special characteristics. In this
paper, we provide an overview of the recent advances on long texts modeling
based on Transformer models. Firstly, we introduce the formal definition of
long text modeling. Then, as the core content, we discuss how to process long
input to satisfy the length limitation and design improved Transformer
architectures to effectively extend the maximum context length. Following this,
we discuss how to adapt Transformer models to capture the special
characteristics of long texts. Finally, we describe four typical applications
involving long text modeling and conclude this paper with a discussion of
future directions. Our survey intends to provide researchers with a synthesis
and pointer to related work on long text modeling.

中文翻译:
长文本建模一直是自然语言处理（NLP）领域的关键技术。随着长文档数量的持续增长，开发能有效处理和分析此类文本的建模方法显得尤为重要。然而，长文本因其更复杂的语义和特殊属性，对现有文本模型提出了重要研究挑战。本文系统综述了基于Transformer模型的长文本建模最新进展：首先明确长文本建模的形式化定义；随后以核心内容详述了如何通过处理长输入以满足长度限制，并设计改进的Transformer架构以有效扩展最大上下文长度；进而探讨如何调整Transformer模型以捕捉长文本的特殊特征；最后阐述了长文本建模的四个典型应用场景，并通过未来研究方向讨论作结。本综述旨在为研究人员提供长文本建模相关工作的系统性梳理与研究指引。
