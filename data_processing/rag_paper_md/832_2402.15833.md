# Prompt Perturbation Consistency Learning for Robust Language Models

链接: http://arxiv.org/abs/2402.15833v1

原文摘要:
Large language models (LLMs) have demonstrated impressive performance on a
number of natural language processing tasks, such as question answering and
text summarization. However, their performance on sequence labeling tasks such
as intent classification and slot filling (IC-SF), which is a central component
in personal assistant systems, lags significantly behind discriminative models.
Furthermore, there is a lack of substantive research on the robustness of LLMs
to various perturbations in the input prompts. The contributions of this paper
are three-fold. First, we show that fine-tuning sufficiently large LLMs can
produce IC-SF performance comparable to discriminative models. Next, we
systematically analyze the performance deterioration of those fine-tuned models
due to three distinct yet relevant types of input perturbations - oronyms,
synonyms, and paraphrasing. Finally, we propose an efficient mitigation
approach, Prompt Perturbation Consistency Learning (PPCL), which works by
regularizing the divergence between losses from clean and perturbed samples.
Our experiments demonstrate that PPCL can recover on average 59% and 69% of the
performance drop for IC and SF tasks, respectively. Furthermore, PPCL beats the
data augmentation approach while using ten times fewer augmented data samples.

中文翻译:
大型语言模型（LLMs）在问答、文本摘要等多项自然语言处理任务中展现出卓越性能，然而其在意图分类与槽位填充（IC-SF）这类序列标注任务上的表现——作为个人助理系统的核心组件——仍显著落后于判别式模型。此外，针对输入提示中各类扰动的模型鲁棒性研究尚存明显空白。本文贡献包含三方面：首先，我们证明对足够大规模的LLMs进行微调可获得与判别式模型相当的IC-SF性能；其次，系统分析了微调模型因同音异义词、近义词及释义改写这三类相关输入扰动导致的性能衰退；最后提出高效缓解方法——提示扰动一致性学习（PPCL），通过约束干净样本与扰动样本间的损失差异实现正则化。实验表明PPCL平均可恢复IC任务59%、SF任务69%的性能下降，且仅需十分之一的增强数据样本即可超越数据增强方法的效果。
