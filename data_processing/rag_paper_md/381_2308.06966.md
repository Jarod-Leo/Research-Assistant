# EcomGPT: Instruction-tuning Large Language Model with Chain-of-Task Tasks for E-commerce

链接: http://arxiv.org/abs/2308.06966v1

原文摘要:
Recently, instruction-following Large Language Models (LLMs) , represented by
ChatGPT, have exhibited exceptional performance in general Natural Language
Processing (NLP) tasks. However, the unique characteristics of E-commerce data
pose significant challenges to general LLMs. An LLM tailored specifically for
E-commerce scenarios, possessing robust cross-dataset/task generalization
capabilities, is a pressing necessity. To solve this issue, in this work, we
proposed the first e-commerce instruction dataset EcomInstruct, with a total of
2.5 million instruction data. EcomInstruct scales up the data size and task
diversity by constructing atomic tasks with E-commerce basic data types, such
as product information, user reviews. Atomic tasks are defined as intermediate
tasks implicitly involved in solving a final task, which we also call
Chain-of-Task tasks. We developed EcomGPT with different parameter scales by
training the backbone model BLOOMZ with the EcomInstruct. Benefiting from the
fundamental semantic understanding capabilities acquired from the Chain-of-Task
tasks, EcomGPT exhibits excellent zero-shot generalization capabilities.
Extensive experiments and human evaluations demonstrate that EcomGPT
outperforms ChatGPT in term of cross-dataset/task generalization on E-commerce
tasks.

中文翻译:
近年来，以ChatGPT为代表的指令跟随型大语言模型（LLM）在通用自然语言处理（NLP）任务中展现出卓越性能。然而，电子商务数据特有的属性对通用大语言模型提出了重大挑战。当前亟需一款专为电商场景定制、具备强大跨数据集/任务泛化能力的大语言模型。为此，本研究首创了包含250万条指令数据的电商领域指令数据集EcomInstruct。该数据集通过构建基于商品信息、用户评论等电商基础数据类型的原子任务，实现了数据规模与任务多样性的双重扩展。原子任务被定义为解决最终任务过程中隐式涉及的中间任务，我们亦称之为"任务链"任务。基于BLOOMZ基座模型结合EcomInstruct进行训练，我们开发了不同参数规模的EcomGPT模型。得益于从"任务链"中习得的基础语义理解能力，EcomGPT展现出优异的零样本泛化性能。大量实验与人工评估表明，在电商任务的跨数据集/任务泛化能力方面，EcomGPT显著优于ChatGPT。

（翻译说明：
1. 专业术语处理：LLM统一译为"大语言模型"，"zero-shot"译为"零样本"，保持学术规范性
2. 长句拆分：将原文复合句按中文表达习惯分解为多个短句，如原子任务定义部分
3. 概念显化："Chain-of-Task tasks"首次出现时译为"任务链"并添加解释性翻译
4. 被动语态转换："are defined as"等被动结构转为中文主动句式
5. 数据呈现：2.5 million采用中文计数习惯译为"250万条"
6. 术语统一性：全文保持"泛化能力"、"电商场景"等术语的一致性）
