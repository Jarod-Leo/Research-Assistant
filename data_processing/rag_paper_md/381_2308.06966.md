# EcomGPT: Instruction-tuning Large Language Model with Chain-of-Task Tasks for E-commerce

链接: http://arxiv.org/abs/2308.06966v1

原文摘要:
Recently, instruction-following Large Language Models (LLMs) , represented by
ChatGPT, have exhibited exceptional performance in general Natural Language
Processing (NLP) tasks. However, the unique characteristics of E-commerce data
pose significant challenges to general LLMs. An LLM tailored specifically for
E-commerce scenarios, possessing robust cross-dataset/task generalization
capabilities, is a pressing necessity. To solve this issue, in this work, we
proposed the first e-commerce instruction dataset EcomInstruct, with a total of
2.5 million instruction data. EcomInstruct scales up the data size and task
diversity by constructing atomic tasks with E-commerce basic data types, such
as product information, user reviews. Atomic tasks are defined as intermediate
tasks implicitly involved in solving a final task, which we also call
Chain-of-Task tasks. We developed EcomGPT with different parameter scales by
training the backbone model BLOOMZ with the EcomInstruct. Benefiting from the
fundamental semantic understanding capabilities acquired from the Chain-of-Task
tasks, EcomGPT exhibits excellent zero-shot generalization capabilities.
Extensive experiments and human evaluations demonstrate that EcomGPT
outperforms ChatGPT in term of cross-dataset/task generalization on E-commerce
tasks.

中文翻译:
近年来，以ChatGPT为代表的指令跟随型大语言模型（LLMs）在通用自然语言处理（NLP）任务中展现出卓越性能。然而，电子商务数据特有的属性对通用大语言模型构成了显著挑战。当前亟需一款专为电商场景定制、具备强大跨数据集/任务泛化能力的大语言模型。为此，本研究首创了首个电商指令数据集EcomInstruct，共包含250万条指令数据。该数据集通过构建基于商品信息、用户评论等电商基础数据类型的原子任务，实现了数据规模与任务多样性的双重扩展。原子任务被定义为解决最终任务时隐式涉及的中间任务，我们亦称之为任务链式任务。基于BLOOMZ基座模型结合EcomInstruct进行训练，我们开发了不同参数规模的EcomGPT模型。得益于从任务链式任务中习得的基础语义理解能力，EcomGPT展现出优异的零样本泛化性能。大量实验与人工评估表明，在电商任务的跨数据集/任务泛化能力方面，EcomGPT显著优于ChatGPT。
