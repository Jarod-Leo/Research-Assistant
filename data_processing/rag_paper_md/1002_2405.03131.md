# WDMoE: Wireless Distributed Large Language Models with Mixture of Experts

链接: http://arxiv.org/abs/2405.03131v1

原文摘要:
Large Language Models (LLMs) have achieved significant success in various
natural language processing tasks, but how wireless communications can support
LLMs has not been extensively studied. In this paper, we propose a wireless
distributed LLMs paradigm based on Mixture of Experts (MoE), named WDMoE,
deploying LLMs collaboratively across edge servers of base station (BS) and
mobile devices in the wireless communications system. Specifically, we
decompose the MoE layer in LLMs by deploying the gating network and the
preceding neural network layer at BS, while distributing the expert networks
across the devices. This arrangement leverages the parallel capabilities of
expert networks on distributed devices. Moreover, to overcome the instability
of wireless communications, we design an expert selection policy by taking into
account both the performance of the model and the end-to-end latency, which
includes both transmission delay and inference delay. Evaluations conducted
across various LLMs and multiple datasets demonstrate that WDMoE not only
outperforms existing models, such as Llama 2 with 70 billion parameters, but
also significantly reduces end-to-end latency.

中文翻译:
大型语言模型（LLMs）在各类自然语言处理任务中取得了显著成功，但无线通信如何有效支撑LLMs尚未得到充分研究。本文提出了一种基于专家混合系统（MoE）的无线分布式LLMs范式WDMoE，将模型协同部署于基站边缘服务器与移动设备构成的无线通信系统中。具体而言，我们通过将MoE层的门控网络及前置神经网络层部署在基站，同时将专家网络分布式分配至终端设备，充分利用分布式设备的并行计算能力。针对无线信道不稳定性，我们设计了一种综合考虑模型性能与端到端延迟（含传输时延和推理时延）的专家选择策略。在不同规模LLMs及多数据集上的实验表明，WDMoE不仅性能超越700亿参数的Llama 2等现有模型，更能显著降低端到端延迟。
