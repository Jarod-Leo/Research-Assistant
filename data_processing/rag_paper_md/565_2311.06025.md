# ChiMed-GPT: A Chinese Medical Large Language Model with Full Training Regime and Better Alignment to Human Preferences

链接: http://arxiv.org/abs/2311.06025v1

原文摘要:
Recently, the increasing demand for superior medical services has highlighted
the discrepancies in the medical infrastructure. With big data, especially
texts, forming the foundation of medical services, there is an exigent need for
effective natural language processing (NLP) solutions tailored to the
healthcare domain. Conventional approaches leveraging pre-trained models
present promising results in this domain and current large language models
(LLMs) offer advanced foundation for medical text processing. However, most
medical LLMs are trained only with supervised fine-tuning (SFT), even though it
efficiently empowers LLMs to understand and respond to medical instructions but
is ineffective in learning domain knowledge and aligning with human preference.
In this work, we propose ChiMed-GPT, a new benchmark LLM designed explicitly
for Chinese medical domain, and undergoes a comprehensive training regime with
pre-training, SFT, and RLHF. Evaluations on tasks including information
extraction, question answering, and dialogue generation demonstrate
ChiMed-GPT's superior performance over general domain LLMs. Furthermore, we
analyze possible biases through prompting ChiMed-GPT to perform attitude scales
regarding discrimination of patients, so as to contribute to further
responsible development of LLMs in the medical domain. The code and model are
released at https://github.com/synlp/ChiMed-GPT.

中文翻译:
近年来，对优质医疗服务日益增长的需求凸显出医疗基础设施的不足。大数据尤其是文本数据作为医疗服务的基础，亟需针对医疗领域定制高效的自然语言处理（NLP）解决方案。基于预训练模型的传统方法在该领域已展现出良好效果，而当前大语言模型（LLM）为医疗文本处理提供了更先进的基础架构。然而，现有医疗LLM大多仅采用监督微调（SFT）进行训练，虽然能有效提升模型理解和响应医疗指令的能力，但在领域知识学习与人机偏好对齐方面存在局限。

本研究提出ChiMed-GPT——专为中文医疗领域设计的新型基准LLM，通过预训练、监督微调及基于人类反馈的强化学习（RLHF）三阶段完整训练框架。在信息抽取、问答系统和对话生成等任务上的评估表明，ChiMed-GPT性能显著优于通用领域LLM。此外，我们通过提示模型完成针对患者歧视态度的量表测试，系统分析了可能存在的偏见，以促进医疗领域LLM的负责任发展。相关代码与模型已开源发布于https://github.com/synlp/ChiMed-GPT。
