# The Model Arena for Cross-lingual Sentiment Analysis: A Comparative Study in the Era of Large Language Models

链接: http://arxiv.org/abs/2406.19358v1

原文摘要:
Sentiment analysis serves as a pivotal component in Natural Language
Processing (NLP). Advancements in multilingual pre-trained models such as XLM-R
and mT5 have contributed to the increasing interest in cross-lingual sentiment
analysis. The recent emergence in Large Language Models (LLM) has significantly
advanced general NLP tasks, however, the capability of such LLMs in
cross-lingual sentiment analysis has not been fully studied. This work
undertakes an empirical analysis to compare the cross-lingual transfer
capability of public Small Multilingual Language Models (SMLM) like XLM-R,
against English-centric LLMs such as Llama-3, in the context of sentiment
analysis across English, Spanish, French and Chinese. Our findings reveal that
among public models, SMLMs exhibit superior zero-shot cross-lingual performance
relative to LLMs. However, in few-shot cross-lingual settings, public LLMs
demonstrate an enhanced adaptive potential. In addition, we observe that
proprietary GPT-3.5 and GPT-4 lead in zero-shot cross-lingual capability, but
are outpaced by public models in few-shot scenarios.

中文翻译:
情感分析是自然语言处理(NLP)中的关键组成部分。随着XLM-R和mT5等多语言预训练模型的发展，跨语言情感分析研究日益受到关注。尽管近期兴起的大语言模型(LLM)显著推动了通用NLP任务的进步，但这类模型在跨语言情感分析中的能力尚未得到充分研究。本文通过实证分析比较了公开的小型多语言模型(SMLM，如XLM-R)与以英语为中心的大语言模型(如Llama-3)在英语、西班牙语、法语和中文情感分析任务中的跨语言迁移能力。研究发现：在公开模型中，SMLM展现出优于LLM的零样本跨语言性能；但在小样本跨语言场景下，公开LLM显示出更强的适应潜力。此外，研究还发现商业模型GPT-3.5和GPT-4在零样本跨语言能力上领先，但在小样本情境下会被公开模型超越。
