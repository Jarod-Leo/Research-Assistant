# Retrieval-based Knowledge Transfer: An Effective Approach for Extreme Large Language Model Compression

链接: http://arxiv.org/abs/2310.15594v1

原文摘要:
Large-scale pre-trained language models (LLMs) have demonstrated exceptional
performance in various natural language processing (NLP) tasks. However, the
massive size of these models poses huge challenges for their deployment in
real-world applications. While numerous model compression techniques have been
proposed, most of them are not well-suited for achieving extreme model
compression when there is a significant gap in model scale. In this paper, we
introduce a novel compression paradigm called Retrieval-based Knowledge
Transfer (RetriKT), which effectively transfers the knowledge of LLMs to
extremely small-scale models (e.g., 1%). In particular, our approach extracts
knowledge from LLMs to construct a knowledge store, from which the small-scale
model can retrieve relevant information and leverage it for effective
inference. To improve the quality of the model, soft prompt tuning and Proximal
Policy Optimization (PPO) reinforcement learning techniques are employed.
Extensive experiments are conducted on low-resource tasks from SuperGLUE and
GLUE benchmarks. The results demonstrate that the proposed approach
significantly enhances the performance of small-scale models by leveraging the
knowledge from LLMs.

中文翻译:
以下是符合要求的学术论文摘要中文翻译：

大规模预训练语言模型（LLMs）在各类自然语言处理（NLP）任务中展现出卓越性能。然而，这些模型的庞大规模给实际应用部署带来了巨大挑战。尽管已有大量模型压缩技术被提出，当模型规模存在显著差距时，大多数方法难以实现极致的模型压缩。本文提出了一种新型压缩范式——基于检索的知识迁移（RetriKT），能够将LLMs的知识有效迁移至极小规模模型（如原模型1%参数量）。具体而言，我们的方法从LLMs中提取知识构建知识库，使小规模模型能够检索相关信息并利用其进行有效推理。为提升模型质量，本研究采用软提示调优和近端策略优化（PPO）强化学习技术。基于SuperGLUE和GLUE基准中的低资源任务开展大量实验，结果表明所提方法通过利用LLMs知识，显著提升了小规模模型的性能表现。

（翻译严格遵循以下原则：
1. 专业术语准确统一（如Proximal Policy Optimization译为"近端策略优化"）
2. 被动语态转换为中文主动表达（如"are employed"译为"采用"）
3. 长难句合理切分（如将原文复合句拆分为多个短句）
4. 学术用语规范（如"paradigm"译为"范式"）
5. 保留关键数据（如"1%"精确译出）
6. 逻辑关系显化（通过"具体而言"等连接词明确层次））
