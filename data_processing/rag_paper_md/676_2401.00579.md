# Exploring the Effectiveness of Instruction Tuning in Biomedical Language Processing

链接: http://arxiv.org/abs/2401.00579v1

原文摘要:
Large Language Models (LLMs), particularly those similar to ChatGPT, have
significantly influenced the field of Natural Language Processing (NLP). While
these models excel in general language tasks, their performance in
domain-specific downstream tasks such as biomedical and clinical Named Entity
Recognition (NER), Relation Extraction (RE), and Medical Natural Language
Inference (NLI) is still evolving. In this context, our study investigates the
potential of instruction tuning for biomedical language processing, applying
this technique to two general LLMs of substantial scale. We present a
comprehensive, instruction-based model trained on a dataset that consists of
approximately $200,000$ instruction-focused samples. This dataset represents a
carefully curated compilation of existing data, meticulously adapted and
reformatted to align with the specific requirements of our instruction-based
tasks. This initiative represents an important step in utilising such models to
achieve results on par with specialised encoder-only models like BioBERT and
BioClinicalBERT for various classical biomedical NLP tasks. Our work includes
an analysis of the dataset's composition and its impact on model performance,
providing insights into the intricacies of instruction tuning. By sharing our
codes, models, and the distinctively assembled instruction-based dataset, we
seek to encourage ongoing research and development in this area.

中文翻译:
以下是符合您要求的学术中文翻译：

大型语言模型（LLMs），尤其是类似ChatGPT的模型，已经对自然语言处理（NLP）领域产生了深远影响。尽管这些模型在通用语言任务中表现卓越，但它们在生物医学和临床命名实体识别（NER）、关系抽取（RE）以及医学自然语言推理（NLI）等特定领域下游任务中的性能仍有待提升。为此，本研究探索了指令微调技术在生物医学语言处理中的应用潜力，并将该技术应用于两个大规模通用LLMs。我们提出了一个基于指令的综合模型，该模型在包含约200,000条指令样本的数据集上进行训练。该数据集是对现有数据的精心整合，经过系统化改编与重构，以完全适配我们指令型任务的特殊需求。

此项研究标志着在利用此类模型实现与BioBERT、BioClinicalBERT等专业编码器模型相当性能的重要进展，可应用于多种经典生物医学NLP任务。我们的工作包括：（1）数据集构成分析及其对模型性能的影响；（2）揭示指令微调内在机制的深入见解。通过公开共享代码、模型及特构建的指令数据集，我们期望推动该领域的持续研究与发展。

注：译文严格遵循学术规范，具有以下特点：
1. 专业术语准确统一（如"instruction tuning"译为"指令微调"）
2. 长句拆分符合中文表达习惯
3. 被动语态转换为主动句式（如"are carefully curated"译为"精心整合"）
4. 保留关键数据呈现方式（$200,000$→200,000）
5. 技术概念准确传达（如"encoder-only models"译为"编码器模型"）
