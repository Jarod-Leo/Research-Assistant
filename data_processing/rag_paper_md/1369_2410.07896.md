# Executing Arithmetic: Fine-Tuning Large Language Models as Turing Machines

链接: http://arxiv.org/abs/2410.07896v1

原文摘要:
Large Language Models (LLMs) have demonstrated remarkable capabilities across
a wide range of natural language processing and reasoning tasks. However, their
performance in the foundational domain of arithmetic remains unsatisfactory.
When dealing with arithmetic tasks, LLMs often memorize specific examples
rather than learning the underlying computational logic, limiting their ability
to generalize to new problems. In this paper, we propose a Composable
Arithmetic Execution Framework (CAEF) that enables LLMs to learn to execute
step-by-step computations by emulating Turing Machines, thereby gaining a
genuine understanding of computational logic. Moreover, the proposed framework
is highly scalable, allowing composing learned operators to significantly
reduce the difficulty of learning complex operators. In our evaluation, CAEF
achieves nearly 100% accuracy across seven common mathematical operations on
the LLaMA 3.1-8B model, effectively supporting computations involving operands
with up to 100 digits, a level where GPT-4o falls short noticeably in some
settings.

中文翻译:
大型语言模型（LLMs）在各类自然语言处理与推理任务中展现出卓越能力，但其在基础算术领域的表现仍不尽如人意。面对算术任务时，LLMs往往通过记忆特定示例而非学习底层计算逻辑，这限制了其解决新问题的泛化能力。本文提出可组合算术执行框架（CAEF），通过模拟图灵机使LLMs学会分步计算执行，从而真正理解计算逻辑。该框架具备高度可扩展性，可将已学习的运算符组合使用，显著降低复杂运算符的学习难度。在LLaMA 3.1-8B模型上的评估显示，CAEF在七种常见数学运算中实现近100%准确率，有效支持高达100位操作数的计算任务，而同等条件下GPT-4o在某些场景中表现明显不足。
