# Training Ultra Long Context Language Model with Fully Pipelined Distributed Transformer

链接: http://arxiv.org/abs/2408.16978v1

原文摘要:
Large Language Models (LLMs) with long context capabilities are integral to
complex tasks in natural language processing and computational biology, such as
text generation and protein sequence analysis. However, training LLMs directly
on extremely long contexts demands considerable GPU resources and increased
memory, leading to higher costs and greater complexity. Alternative approaches
that introduce long context capabilities via downstream finetuning or
adaptations impose significant design limitations. In this paper, we propose
Fully Pipelined Distributed Transformer (FPDT) for efficiently training
long-context LLMs with extreme hardware efficiency. For GPT and Llama models,
we achieve a 16x increase in sequence length that can be trained on the same
hardware compared to current state-of-the-art solutions. With our dedicated
sequence chunk pipeline design, we can now train 8B LLM with 2 million sequence
length on only 4 GPUs, while also maintaining over 55% of MFU. Our proposed
FPDT is agnostic to existing training techniques and is proven to work
efficiently across different LLM models.

中文翻译:
具备长上下文处理能力的大型语言模型（LLM）是自然语言处理与计算生物学中复杂任务（如文本生成与蛋白质序列分析）的核心组件。然而，直接在超长上下文上训练LLM需要消耗大量GPU资源并增加内存占用，导致成本攀升与系统复杂度上升。现有通过下游微调或适配引入长上下文能力的替代方案存在显著设计局限性。本文提出全流水线分布式Transformer（FPDT），能以极高的硬件效率实现长上下文LLM的高效训练。针对GPT和Llama模型，我们在相同硬件条件下实现了相比当前最优方案16倍的序列长度提升。通过专有的序列分块流水线设计，仅需4块GPU即可训练序列长度达200万的80亿参数LLM，同时保持超过55%的模型浮点运算利用率（MFU）。FPDT方案与现有训练技术正交，经实证可高效适配不同LLM架构。
