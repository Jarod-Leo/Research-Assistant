# Training Ultra Long Context Language Model with Fully Pipelined Distributed Transformer

链接: http://arxiv.org/abs/2408.16978v1

原文摘要:
Large Language Models (LLMs) with long context capabilities are integral to
complex tasks in natural language processing and computational biology, such as
text generation and protein sequence analysis. However, training LLMs directly
on extremely long contexts demands considerable GPU resources and increased
memory, leading to higher costs and greater complexity. Alternative approaches
that introduce long context capabilities via downstream finetuning or
adaptations impose significant design limitations. In this paper, we propose
Fully Pipelined Distributed Transformer (FPDT) for efficiently training
long-context LLMs with extreme hardware efficiency. For GPT and Llama models,
we achieve a 16x increase in sequence length that can be trained on the same
hardware compared to current state-of-the-art solutions. With our dedicated
sequence chunk pipeline design, we can now train 8B LLM with 2 million sequence
length on only 4 GPUs, while also maintaining over 55% of MFU. Our proposed
FPDT is agnostic to existing training techniques and is proven to work
efficiently across different LLM models.

中文翻译:
以下是符合要求的学术论文摘要中文翻译：

具备长文本处理能力的大语言模型（LLMs）对自然语言处理和计算生物学中的复杂任务（如文本生成和蛋白质序列分析）至关重要。然而，直接在超长文本上训练LLMs需要消耗大量GPU资源和内存，导致成本攀升与复杂度增加。现有通过下游微调或适配来扩展上下文窗口的替代方案存在显著的设计局限性。本文提出全流水线分布式Transformer（FPDT），能以极高的硬件效率实现长上下文LLMs的高效训练。对于GPT和Llama模型，我们在相同硬件条件下实现了比当前最优方案提升16倍的训练序列长度。通过专用的序列分块流水线设计，现仅需4块GPU即可训练序列长度达200万的80亿参数LLM，同时保持55%以上的模型浮点运算利用率（MFU）。FPDT与现有训练技术具有兼容性，实证表明其在不同LLM架构上均能高效运行。

（翻译严格遵循以下原则：
1. 专业术语准确统一（如MFU译为"模型浮点运算利用率"并保留英文缩写）
2. 被动语态转换为中文主动句式（如"are integral to"译为"对...至关重要"）
3. 长难句合理切分（如将原文复合句拆解为多个中文短句）
4. 学术用语规范（如"agnostic to"译为"与...具有兼容性"）
5. 数字单位准确转换（8B译为"80亿"符合中文计量习惯）
6. 保留技术概念精确性（如"sequence chunk pipeline design"译为"序列分块流水线设计"））
