# A Survey on Large Language Models from Concept to Implementation

链接: http://arxiv.org/abs/2403.18969v1

原文摘要:
Recent advancements in Large Language Models (LLMs), particularly those built
on Transformer architectures, have significantly broadened the scope of natural
language processing (NLP) applications, transcending their initial use in
chatbot technology. This paper investigates the multifaceted applications of
these models, with an emphasis on the GPT series. This exploration focuses on
the transformative impact of artificial intelligence (AI) driven tools in
revolutionizing traditional tasks like coding and problem-solving, while also
paving new paths in research and development across diverse industries. From
code interpretation and image captioning to facilitating the construction of
interactive systems and advancing computational domains, Transformer models
exemplify a synergy of deep learning, data analysis, and neural network design.
This survey provides an in-depth look at the latest research in Transformer
models, highlighting their versatility and the potential they hold for
transforming diverse application sectors, thereby offering readers a
comprehensive understanding of the current and future landscape of
Transformer-based LLMs in practical applications.

中文翻译:
近年来，大型语言模型（LLMs）尤其是基于Transformer架构的模型取得了显著进展，其应用范围已超越聊天机器人技术的初始领域，极大拓展了自然语言处理（NLP）的应用边界。本文以GPT系列模型为重点，系统探究了这类模型的多维应用场景：既聚焦人工智能（AI）驱动工具在代码编写与问题求解等传统任务中的变革性影响，也揭示其在跨行业研发领域开辟的新路径。从代码解析、图像描述生成，到交互式系统构建与计算领域推进，Transformer模型展现了深度学习、数据分析和神经网络设计的协同效应。本综述通过深入剖析Transformer模型的最新研究进展，凸显其多功能特性及在改造各应用领域的潜力，为读者全面理解基于Transformer的LLMs在当前及未来实际应用中的发展图景提供了系统性认知。
