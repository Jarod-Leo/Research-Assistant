# Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment

链接: http://arxiv.org/abs/2405.03594v1

原文摘要:
Large language models (LLMs) have revolutionized Natural Language Processing
(NLP), but their size creates computational bottlenecks. We introduce a novel
approach to create accurate, sparse foundational versions of performant LLMs
that achieve full accuracy recovery for fine-tuning tasks at up to 70%
sparsity. We achieve this for the LLaMA-2 7B model by combining the SparseGPT
one-shot pruning method and sparse pretraining of those models on a subset of
the SlimPajama dataset mixed with a Python subset of The Stack dataset. We
exhibit training acceleration due to sparsity on Cerebras CS-3 chips that
closely matches theoretical scaling. In addition, we establish inference
acceleration of up to 3x on CPUs by utilizing Neural Magic's DeepSparse engine
and 1.7x on GPUs through Neural Magic's nm-vllm engine. The above gains are
realized via sparsity alone, thus enabling further gains through additional use
of quantization. Specifically, we show a total speedup on CPUs for
sparse-quantized LLaMA models of up to 8.6x. We demonstrate these results
across diverse, challenging tasks, including chat, instruction following, code
generation, arithmetic reasoning, and summarization to prove their generality.
This work paves the way for rapidly creating smaller and faster LLMs without
sacrificing accuracy.

中文翻译:
大型语言模型（LLM）彻底改变了自然语言处理（NLP）领域，但其庞大的规模也带来了计算瓶颈。我们提出了一种创新方法，通过构建高性能LLM的精确稀疏基础版本，在微调任务中实现高达70%稀疏度的完全准确率恢复。针对LLaMA-2 7B模型，我们结合了SparseGPT一次性剪枝技术，并在混合SlimPajama数据集子集与The Stack数据集Python子集上进行了稀疏预训练。实验显示，在Cerebras CS-3芯片上借助稀疏性实现的训练加速效果与理论预测高度吻合。此外，通过Neural Magic的DeepSparse引擎在CPU上实现了最高3倍的推理加速，利用nm-vllm引擎在GPU上获得了1.7倍的加速效果。上述性能提升仅通过稀疏化实现，因此结合量化技术可进一步增效——我们验证了稀疏量化LLaMA模型在CPU上最高达8.6倍的综合加速。这些成果在对话、指令跟随、代码生成、算术推理和文本摘要等多样化挑战性任务中均得到验证，充分证明了方法的普适性。该研究为快速构建更小、更快且不损失准确率的LLM开辟了新途径。
