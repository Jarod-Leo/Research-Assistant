# MEGAVERSE: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks

链接: http://arxiv.org/abs/2311.07463v1

原文摘要:
There has been a surge in LLM evaluation research to understand LLM
capabilities and limitations. However, much of this research has been confined
to English, leaving LLM building and evaluation for non-English languages
relatively unexplored. Several new LLMs have been introduced recently,
necessitating their evaluation on non-English languages. This study aims to
perform a thorough evaluation of the non-English capabilities of SoTA LLMs
(GPT-3.5-Turbo, GPT-4, PaLM2, Gemini-Pro, Mistral, Llama2, and Gemma) by
comparing them on the same set of multilingual datasets. Our benchmark
comprises 22 datasets covering 83 languages, including low-resource African
languages. We also include two multimodal datasets in the benchmark and compare
the performance of LLaVA models, GPT-4-Vision and Gemini-Pro-Vision. Our
experiments show that larger models such as GPT-4, Gemini-Pro and PaLM2
outperform smaller models on various tasks, notably on low-resource languages,
with GPT-4 outperforming PaLM2 and Gemini-Pro on more datasets. We also perform
a study on data contamination and find that several models are likely to be
contaminated with multilingual evaluation benchmarks, necessitating approaches
to detect and handle contamination while assessing the multilingual performance
of LLMs.

中文翻译:
近年来，关于大语言模型（LLM）能力与局限性的评估研究呈现爆发式增长。然而，这类研究大多局限于英语领域，针对非英语语言的LLM构建与评估仍处于相对空白状态。随着GPT-3.5-Turbo、GPT-4、PaLM2、Gemini-Pro、Mistral、Llama2和Gemma等新一代LLM相继问世，对其非英语能力的系统评估显得尤为重要。本研究通过统一的多语言数据集对比测试，对上述前沿LLM的非英语能力进行全面评估。我们构建的基准测试涵盖22个数据集，涉及83种语言（包括非洲低资源语言），并纳入两个多模态数据集以对比LLaVA模型、GPT-4-Vision和Gemini-Pro-Vision的表现。实验表明：GPT-4、Gemini-Pro和PaLM2等大规模模型在各类任务（尤其是低资源语言）上显著优于小规模模型，其中GPT-4在多数数据集的表现超越PaLM2和Gemini-Pro。此外，我们开展了数据污染专项研究，发现多个模型可能受到多语言评估基准数据的污染，这提示在评估LLM多语言性能时，亟需建立污染检测与处理机制。

（翻译说明：采用学术论文摘要的标准结构，通过以下处理实现专业性与可读性平衡：
1. 术语统一："SoTA LLMs"译为"前沿LLM"，"low-resource"译为"低资源"
2. 长句拆分：将原文复合句分解为符合中文表达习惯的短句
3. 被动语态转化："has been confined"译为主动态"局限于"
4. 逻辑显化：通过"随着...""这提示..."等连接词强化行文逻辑
5. 专业表达："data contamination"译为"数据污染"，"benchmark"译为"基准测试"
6. 文化适配：保留"GPT-4"等技术名词原称以保持专业性）
