# Unveiling Safety Vulnerabilities of Large Language Models

链接: http://arxiv.org/abs/2311.04124v1

原文摘要:
As large language models become more prevalent, their possible harmful or
inappropriate responses are a cause for concern. This paper introduces a unique
dataset containing adversarial examples in the form of questions, which we call
AttaQ, designed to provoke such harmful or inappropriate responses. We assess
the efficacy of our dataset by analyzing the vulnerabilities of various models
when subjected to it. Additionally, we introduce a novel automatic approach for
identifying and naming vulnerable semantic regions - input semantic areas for
which the model is likely to produce harmful outputs. This is achieved through
the application of specialized clustering techniques that consider both the
semantic similarity of the input attacks and the harmfulness of the model's
responses. Automatically identifying vulnerable semantic regions enhances the
evaluation of model weaknesses, facilitating targeted improvements to its
safety mechanisms and overall reliability.

中文翻译:
随着大型语言模型的广泛应用，其可能产生的有害或不恰当回应引发关注。本文提出一个独特的对抗性示例问题数据集AttaQ，旨在诱发此类不良响应。我们通过分析不同模型在该数据集上的脆弱性表现验证其有效性。此外，我们创新性地提出一种自动化方法，用于识别和命名脆弱语义区域——即模型易产生有害输出的输入语义区间。该方法采用特殊聚类技术，综合考量输入攻击的语义相似度与模型响应的危害程度。自动识别脆弱语义区域可强化模型缺陷评估，为针对性增强安全机制与整体可靠性提供支持。
