# MuGI: Enhancing Information Retrieval through Multi-Text Generation Intergration with Large Language Models

链接: http://arxiv.org/abs/2401.06311v1

原文摘要:
Large Language Models (LLMs) are foundational in language technologies,
particularly in information retrieval (IR). Previous studies have utilized LLMs
for query expansion, achieving notable improvements in IR. In this paper, we
thoroughly explore the best practice of leveraging LLMs for query expansion. To
this end, we introduce a training-free, straightforward yet effective framework
called Multi-Text Generation Integration (\textsc{MuGI}). It leverages LLMs to
generate multiple pseudo-references, integrating them with queries to enhance
both sparse and dense retrievers. Our empirical findings reveal that: (1)
Increasing the number of samples from LLMs benefits IR systems; (2) A balance
between the query and pseudo-documents, and an effective integration strategy,
is critical for high performance; (3) Contextual information from LLMs is
essential, even boost a 23M model to outperform a 7B baseline model; (4) Pseudo
relevance feedback can further calibrate queries for improved performance; and
(5) Query expansion is widely applicable and versatile, consistently enhancing
models ranging from 23M to 7B parameters. Our code and all generated references
are made available at \url{https://github.com/lezhang7/Retrieval_MuGI}

中文翻译:
大语言模型（LLMs）是语言技术的核心基础，尤其在信息检索（IR）领域表现突出。已有研究利用LLMs进行查询扩展，显著提升了检索效果。本文系统探索了LLMs用于查询扩展的最佳实践，提出了一种无需训练、简洁高效的框架——多文本生成集成（MuGI）。该框架通过LLMs生成多组伪参考文本，将其与原始查询融合以增强稀疏检索器和稠密检索器的性能。实证研究表明：（1）增加LLMs生成样本量能持续提升IR系统性能；（2）查询与伪文档间的平衡策略及有效集成方法是实现高性能的关键；（3）LLMs提供的上下文信息至关重要，甚至能使2300万参数模型超越70亿参数的基线模型；（4）伪相关反馈可进一步校准查询以优化效果；（5）查询扩展具有广泛适用性，可稳定提升从2300万到70亿参数量级的不同模型。代码及生成的所有参考文本已开源于\url{https://github.com/lezhang7/Retrieval_MuGI}。
