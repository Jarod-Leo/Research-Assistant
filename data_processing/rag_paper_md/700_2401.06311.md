# MuGI: Enhancing Information Retrieval through Multi-Text Generation Intergration with Large Language Models

链接: http://arxiv.org/abs/2401.06311v1

原文摘要:
Large Language Models (LLMs) are foundational in language technologies,
particularly in information retrieval (IR). Previous studies have utilized LLMs
for query expansion, achieving notable improvements in IR. In this paper, we
thoroughly explore the best practice of leveraging LLMs for query expansion. To
this end, we introduce a training-free, straightforward yet effective framework
called Multi-Text Generation Integration (\textsc{MuGI}). It leverages LLMs to
generate multiple pseudo-references, integrating them with queries to enhance
both sparse and dense retrievers. Our empirical findings reveal that: (1)
Increasing the number of samples from LLMs benefits IR systems; (2) A balance
between the query and pseudo-documents, and an effective integration strategy,
is critical for high performance; (3) Contextual information from LLMs is
essential, even boost a 23M model to outperform a 7B baseline model; (4) Pseudo
relevance feedback can further calibrate queries for improved performance; and
(5) Query expansion is widely applicable and versatile, consistently enhancing
models ranging from 23M to 7B parameters. Our code and all generated references
are made available at \url{https://github.com/lezhang7/Retrieval_MuGI}

中文翻译:
大语言模型（LLMs）是语言技术的基石，尤其在信息检索（IR）领域具有重要地位。先前研究已利用LLMs进行查询扩展，显著提升了检索性能。本文深入探索了基于LLMs的查询扩展最佳实践，提出了一种无需训练、简洁高效的框架——多文本生成集成（MuGI）。该框架通过LLMs生成多组伪参考文本，将其与原始查询融合以增强稀疏检索器和稠密检索器的性能。实证研究表明：（1）增加LLMs生成样本数量对IR系统具有增益效应；（2）查询与伪文档间的平衡关系及有效集成策略是获得高性能的关键；（3）LLMs提供的上下文信息至关重要，甚至能使2300万参数模型超越70亿参数的基线模型；（4）伪相关反馈可进一步校准查询以提升性能；（5）查询扩展具有广泛适用性，可稳定提升从2300万到70亿参数量级的不同模型。相关代码及生成文本已开源：https://github.com/lezhang7/Retrieval_MuGI

（翻译说明：采用学术论文摘要的规范表达，处理要点包括：
1. 专业术语统一："pseudo-references"译为"伪参考文本"，"retrievers"译为"检索器"
2. 技术概念准确传达："sparse and dense retrievers"译为"稀疏检索器和稠密检索器"
3. 数字规范处理："23M/7B"译为"2300万/70亿"，符合中文计量习惯
4. 被动语态转化："are made available"译为主动式"已开源"
5. 长句拆分重组：将原文第五点拆分为两个分句，符合中文表达习惯
6. 保持学术严谨性：关键术语如"MuGI"保留英文缩写并首次出现标注中文全称）
