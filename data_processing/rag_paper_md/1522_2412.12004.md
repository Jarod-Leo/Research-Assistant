# The Open Source Advantage in Large Language Models (LLMs)

链接: http://arxiv.org/abs/2412.12004v1

原文摘要:
Large language models (LLMs) have rapidly advanced natural language
processing, driving significant breakthroughs in tasks such as text generation,
machine translation, and domain-specific reasoning. The field now faces a
critical dilemma in its approach: closed-source models like GPT-4 deliver
state-of-the-art performance but restrict reproducibility, accessibility, and
external oversight, while open-source frameworks like LLaMA and Mixtral
democratize access, foster collaboration, and support diverse applications,
achieving competitive results through techniques like instruction tuning and
LoRA. Hybrid approaches address challenges like bias mitigation and resource
accessibility by combining the scalability of closed-source systems with the
transparency and inclusivity of open-source framework. However, in this
position paper, we argue that open-source remains the most robust path for
advancing LLM research and ethical deployment.

中文翻译:
大型语言模型（LLM）的快速发展推动了自然语言处理领域的重大突破，在文本生成、机器翻译和领域特定推理等任务中表现卓越。当前该领域面临方法论上的关键抉择：以GPT-4为代表的闭源模型虽能提供最先进的性能，却限制了可复现性、可访问性和外部监督；而LLaMA、Mixtral等开源框架通过指令微调与LoRA等技术，不仅实现了具有竞争力的效果，更促进了技术民主化、协作生态构建及多样化应用开发。混合方法尝试结合闭源系统的扩展性与开源框架的透明度及包容性，以应对偏见缓解和资源可及性等挑战。但本立场文件认为，开源路线仍是推进LLM研究与伦理部署的最稳健路径。
