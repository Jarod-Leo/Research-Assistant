# Full Parameter Fine-tuning for Large Language Models with Limited Resources

链接: http://arxiv.org/abs/2306.09782v1

原文摘要:
Large Language Models (LLMs) have revolutionized Natural Language Processing
(NLP) but demand massive GPU resources for training. Lowering the threshold for
LLMs training would encourage greater participation from researchers,
benefiting both academia and society. While existing approaches have focused on
parameter-efficient fine-tuning, which tunes or adds a small number of
parameters, few have addressed the challenge of tuning the full parameters of
LLMs with limited resources. In this work, we propose a new optimizer,
LOw-Memory Optimization (LOMO), which fuses the gradient computation and the
parameter update in one step to reduce memory usage. By integrating LOMO with
existing memory saving techniques, we reduce memory usage to 10.8% compared to
the standard approach (DeepSpeed solution). Consequently, our approach enables
the full parameter fine-tuning of a 65B model on a single machine with 8 RTX
3090, each with 24GB memory.Code and data are available at
https://github.com/OpenLMLab/LOMO.

中文翻译:
以下是您提供的英文论文摘要的中文翻译：

大型语言模型（LLMs）为自然语言处理（NLP）带来了革命性变革，但其训练过程需要消耗大量GPU资源。降低LLMs的训练门槛将吸引更多研究者参与，这对学术界和社会都具有积极意义。现有方法主要关注参数高效微调（即仅调整或添加少量参数），鲜有研究能在有限资源下实现LLMs全参数调优的挑战。本研究中，我们提出了一种新型优化器——低内存优化（LOw-Memory Optimization，LOMO），通过将梯度计算与参数更新融合为单步操作来降低内存占用。结合现有内存节省技术，我们的方法相较于标准方案（DeepSpeed解决方案）将内存使用量降低至10.8%。最终，该技术使得在配备8块24GB显存的RTX 3090显卡的单台机器上完成650亿参数模型的全参数微调成为可能。相关代码与数据已开源：https://github.com/OpenLMLab/LOMO。

（翻译说明：1. 专业术语保留英文缩写并首次出现时标注全称；2. 长句按中文习惯切分为短句；3. "benefiting both..."译为"具有积极意义"以符合学术表达；4. 显存单位"24GB"保留国际通用写法；5. 技术名称"LOMO"保留英文缩写并通过破折号连接中文译名，符合计算机领域术语翻译规范）
