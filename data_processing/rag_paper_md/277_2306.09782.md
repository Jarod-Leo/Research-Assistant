# Full Parameter Fine-tuning for Large Language Models with Limited Resources

链接: http://arxiv.org/abs/2306.09782v1

原文摘要:
Large Language Models (LLMs) have revolutionized Natural Language Processing
(NLP) but demand massive GPU resources for training. Lowering the threshold for
LLMs training would encourage greater participation from researchers,
benefiting both academia and society. While existing approaches have focused on
parameter-efficient fine-tuning, which tunes or adds a small number of
parameters, few have addressed the challenge of tuning the full parameters of
LLMs with limited resources. In this work, we propose a new optimizer,
LOw-Memory Optimization (LOMO), which fuses the gradient computation and the
parameter update in one step to reduce memory usage. By integrating LOMO with
existing memory saving techniques, we reduce memory usage to 10.8% compared to
the standard approach (DeepSpeed solution). Consequently, our approach enables
the full parameter fine-tuning of a 65B model on a single machine with 8 RTX
3090, each with 24GB memory.Code and data are available at
https://github.com/OpenLMLab/LOMO.

中文翻译:
大型语言模型（LLMs）彻底改变了自然语言处理（NLP）领域，但其训练过程需要消耗大量GPU资源。降低LLMs训练门槛将吸引更多研究者参与，对学术界和社会均有裨益。现有方法主要聚焦于参数高效微调——即仅调整或添加少量参数，鲜有研究探讨如何在有限资源下对LLMs全部参数进行调优。本研究提出新型优化器LOMO（低内存优化），通过将梯度计算与参数更新融合为单步操作来降低内存占用。结合现有内存节省技术，相较于标准方案（DeepSpeed解决方案），我们的方法将内存使用量降至10.8%。由此，在配备8块24GB显存的RTX 3090显卡的单台机器上，我们实现了650亿参数模型的全参数微调。代码与数据已开源：https://github.com/OpenLMLab/LOMO。
