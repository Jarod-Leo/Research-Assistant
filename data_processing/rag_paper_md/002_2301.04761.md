# NarrowBERT: Accelerating Masked Language Model Pretraining and Inference

链接: http://arxiv.org/abs/2301.04761v1

原文摘要:
Large-scale language model pretraining is a very successful form of
self-supervised learning in natural language processing, but it is increasingly
expensive to perform as the models and pretraining corpora have become larger
over time. We propose NarrowBERT, a modified transformer encoder that increases
the throughput for masked language model pretraining by more than $2\times$.
NarrowBERT sparsifies the transformer model such that the self-attention
queries and feedforward layers only operate on the masked tokens of each
sentence during pretraining, rather than all of the tokens as with the usual
transformer encoder. We also show that NarrowBERT increases the throughput at
inference time by as much as $3.5\times$ with minimal (or no) performance
degradation on sentence encoding tasks like MNLI. Finally, we examine the
performance of NarrowBERT on the IMDB and Amazon reviews classification and
CoNLL NER tasks and show that it is also comparable to standard BERT
performance.

中文翻译:
大规模语言模型预训练是自然语言处理中一种极为成功的自监督学习形式，但随着模型规模与预训练语料的持续扩大，其计算成本日益攀升。本文提出NarrowBERT——一种改进的Transformer编码器，通过创新性稀疏化设计将掩码语言模型预训练吞吐量提升超过2倍。该模型的核心在于预训练阶段仅对句子中被掩码的标记执行自注意力查询和前馈层运算，而非传统Transformer编码器对所有标记进行处理。实验表明，NarrowBERT在推理阶段可实现高达3.5倍的吞吐量提升，且在MNLI等句子编码任务上性能损失极小（甚至无损）。进一步在IMDB/Amazon评论分类及CoNLL命名实体识别任务上的测试结果证实，其性能与标准BERT模型具有可比性。
