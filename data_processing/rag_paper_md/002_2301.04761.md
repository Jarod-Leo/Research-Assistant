# NarrowBERT: Accelerating Masked Language Model Pretraining and Inference

链接: http://arxiv.org/abs/2301.04761v1

原文摘要:
Large-scale language model pretraining is a very successful form of
self-supervised learning in natural language processing, but it is increasingly
expensive to perform as the models and pretraining corpora have become larger
over time. We propose NarrowBERT, a modified transformer encoder that increases
the throughput for masked language model pretraining by more than $2\times$.
NarrowBERT sparsifies the transformer model such that the self-attention
queries and feedforward layers only operate on the masked tokens of each
sentence during pretraining, rather than all of the tokens as with the usual
transformer encoder. We also show that NarrowBERT increases the throughput at
inference time by as much as $3.5\times$ with minimal (or no) performance
degradation on sentence encoding tasks like MNLI. Finally, we examine the
performance of NarrowBERT on the IMDB and Amazon reviews classification and
CoNLL NER tasks and show that it is also comparable to standard BERT
performance.

中文翻译:
大规模语言模型预训练是自然语言处理中一种极为成功的自监督学习范式，但随着模型规模与预训练语料的持续扩大，其计算成本日益高昂。本文提出NarrowBERT——一种改进的Transformer编码器，可将掩码语言模型预训练吞吐量提升2倍以上。该模型通过稀疏化改造，使得自注意力查询和前馈网络层在预训练期间仅作用于每个句子中被掩码的标记（而非传统Transformer编码器中的全部标记）。实验表明，NarrowBERT在推理阶段的吞吐量最高可提升3.5倍，且在MNLI等句子编码任务上仅产生微小（或可忽略）的性能损失。此外，我们在IMDB/Amazon评论分类及CoNLL命名实体识别任务上的测试表明，其性能与标准BERT模型基本持平。

（翻译说明：
1. 专业术语处理："self-supervised learning"译为"自监督学习"，"transformer encoder"保留技术名词"Transformer编码器"，"masked language model"译为"掩码语言模型"
2. 长句拆分：将原文复合长句分解为符合中文表达习惯的短句，如将"sparsifies...during pretraining"部分独立成句
3. 被动语态转换："it is increasingly expensive"处理为主动式"计算成本日益高昂"
4. 数学表示统一：保留"$2\times$"等原始数学符号格式
5. 技术概念显化：如将"feedforward layers"明确译为"前馈网络层"而非字面直译
6. 术语一致性：保持"BERT"、"MNLI"等专有名词原貌不翻译）
