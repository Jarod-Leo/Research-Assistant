# Amharic LLaMA and LLaVA: Multimodal LLMs for Low Resource Languages

链接: http://arxiv.org/abs/2403.06354v1

原文摘要:
Large Language Models (LLMs) like GPT-4 and LLaMA have shown incredible
proficiency at natural language processing tasks and have even begun to excel
at tasks across other modalities such as vision and audio. Despite their
success, LLMs often struggle to perform well on low-resource languages because
there is so little training data available. This shortcoming is especially
prevalent with open source models. In this work, we explore training LLaMA-2 to
speak Amharic, a language which is spoken by over 50 million people world wide,
but has orders of magnitude less data available than languages like English. We
employ methods previously used for training LLMs on other languages with data
scarcity, and use open source translation models to perform data augmentation
and grow our dataset from millions of tokens to billions. We further enhance
the capabilities of our model by connecting an image encoder and training on a
translated visual instruction tuning dataset in the same manner as LLaVA,
resulting in a multimodal Amharic LLM that can understand images along with
text. We introduce an Amharic version of a popular benchmarking dataset to
evaluate our work. Our models and dataset are open sourced and available on
GitHub.

中文翻译:
诸如GPT-4和LLaMA之类的大型语言模型（LLMs）在自然语言处理任务中展现出惊人的熟练度，甚至开始在多模态任务（如视觉与音频领域）中表现卓越。然而，尽管取得了这些成就，LLMs在低资源语言上的表现往往不尽如人意，主要原因在于可用训练数据极为匮乏。这一缺陷在开源模型中尤为突出。本研究探索了如何训练LLaMA-2模型掌握阿姆哈拉语——这种语言全球使用人口超过5000万，但其可用数据量相比英语等语言却相差数个数量级。我们采用了先前针对数据稀缺语言训练LLMs的方法，并利用开源翻译模型进行数据增强，将数据集规模从数百万标记扩展至数十亿。通过连接图像编码器，并按照LLaVA的方式对翻译后的视觉指令调优数据集进行训练，我们进一步提升了模型的多模态能力，使其能够同时理解图像与文本，最终开发出支持阿姆哈拉语的多模态大型语言模型。我们还引入了阿姆哈拉语版本的流行基准测试数据集以评估模型性能。所有模型及数据集均已开源并发布于GitHub平台。
