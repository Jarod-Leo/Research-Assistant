# Simple, Efficient and Scalable Structure-aware Adapter Boosts Protein Language Models

链接: http://arxiv.org/abs/2404.14850v1

原文摘要:
Fine-tuning Pre-trained protein language models (PLMs) has emerged as a
prominent strategy for enhancing downstream prediction tasks, often
outperforming traditional supervised learning approaches. As a widely applied
powerful technique in natural language processing, employing
Parameter-Efficient Fine-Tuning techniques could potentially enhance the
performance of PLMs. However, the direct transfer to life science tasks is
non-trivial due to the different training strategies and data forms. To address
this gap, we introduce SES-Adapter, a simple, efficient, and scalable adapter
method for enhancing the representation learning of PLMs. SES-Adapter
incorporates PLM embeddings with structural sequence embeddings to create
structure-aware representations. We show that the proposed method is compatible
with different PLM architectures and across diverse tasks. Extensive
evaluations are conducted on 2 types of folding structures with notable quality
differences, 9 state-of-the-art baselines, and 9 benchmark datasets across
distinct downstream tasks. Results show that compared to vanilla PLMs,
SES-Adapter improves downstream task performance by a maximum of 11% and an
average of 3%, with significantly accelerated training speed by a maximum of
1034% and an average of 362%, the convergence rate is also improved by
approximately 2 times. Moreover, positive optimization is observed even with
low-quality predicted structures. The source code for SES-Adapter is available
at https://github.com/tyang816/SES-Adapter.

中文翻译:
以下是符合要求的学术中文翻译：

微调预训练蛋白质语言模型（PLMs）已成为提升下游预测任务性能的重要策略，其表现通常优于传统监督学习方法。作为自然语言处理领域广泛应用的高效技术，参数高效微调方法有望进一步提升PLMs的性能。然而，由于训练策略与数据形式的差异，该技术向生命科学任务的直接迁移存在显著挑战。为此，我们提出SES-Adapter——一种简单、高效且可扩展的适配器方法，用于增强PLMs的表征学习能力。该方法通过整合PLM嵌入与结构序列嵌入，构建具有结构感知能力的表征体系。研究表明，所提方法可兼容不同PLM架构并适用于多种任务。我们在2种质量差异显著的折叠结构类型、9个前沿基线模型和9个涵盖不同下游任务的基准数据集上进行了全面评估。实验结果表明：相比原始PLMs，SES-Adapter最大可提升下游任务性能11%（平均提升3%），训练速度最高加速1034%（平均加速362%），收敛速率提升约2倍。值得注意的是，即使在低质量预测结构下仍能实现正向优化。SES-Adapter源代码已发布于https://github.com/tyang816/SES-Adapter。

（翻译严格遵循以下原则：
1. 专业术语准确统一："fine-tuning"译为"微调"，"adapter"译为"适配器"
2. 被动语态转化：将英文被动结构转换为中文主动表达（如"is conducted"译为"进行了"）
3. 长句拆分重组：将原文复合长句按中文习惯分解为短句群
4. 数据呈现规范：百分比和数值格式严格保留原文精确性
5. 学术风格保持：使用"表征学习""基准数据集"等规范学术用语
6. 逻辑关系显化：通过"为此""值得注意的是"等连接词强化论证逻辑）
