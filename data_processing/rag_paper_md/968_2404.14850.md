# Simple, Efficient and Scalable Structure-aware Adapter Boosts Protein Language Models

链接: http://arxiv.org/abs/2404.14850v1

原文摘要:
Fine-tuning Pre-trained protein language models (PLMs) has emerged as a
prominent strategy for enhancing downstream prediction tasks, often
outperforming traditional supervised learning approaches. As a widely applied
powerful technique in natural language processing, employing
Parameter-Efficient Fine-Tuning techniques could potentially enhance the
performance of PLMs. However, the direct transfer to life science tasks is
non-trivial due to the different training strategies and data forms. To address
this gap, we introduce SES-Adapter, a simple, efficient, and scalable adapter
method for enhancing the representation learning of PLMs. SES-Adapter
incorporates PLM embeddings with structural sequence embeddings to create
structure-aware representations. We show that the proposed method is compatible
with different PLM architectures and across diverse tasks. Extensive
evaluations are conducted on 2 types of folding structures with notable quality
differences, 9 state-of-the-art baselines, and 9 benchmark datasets across
distinct downstream tasks. Results show that compared to vanilla PLMs,
SES-Adapter improves downstream task performance by a maximum of 11% and an
average of 3%, with significantly accelerated training speed by a maximum of
1034% and an average of 362%, the convergence rate is also improved by
approximately 2 times. Moreover, positive optimization is observed even with
low-quality predicted structures. The source code for SES-Adapter is available
at https://github.com/tyang816/SES-Adapter.

中文翻译:
微调预训练蛋白质语言模型（PLMs）已成为提升下游预测任务性能的主流策略，其表现通常优于传统监督学习方法。作为自然语言处理领域广泛应用的高效技术，参数高效微调方法有望进一步提升PLMs的性能。然而，由于训练策略和数据形式的差异，直接迁移至生命科学任务面临显著挑战。为此，我们提出SES-Adapter——一种简单、高效且可扩展的适配器方法，用于增强PLMs的表征学习能力。该方法通过整合PLM嵌入与结构序列嵌入，构建具有结构感知能力的表征体系。

研究表明，SES-Adapter能兼容不同PLM架构并适用于多种任务场景。我们在质量差异显著的2类折叠结构、9个前沿基线模型及涵盖不同下游任务的9个基准数据集上进行了全面评估。结果显示：相比原始PLMs，SES-Adapter使下游任务性能最高提升11%（平均提升3%），训练速度最高加速1034%（平均加速362%），收敛速率也提升约2倍。值得注意的是，即使采用低质量预测结构仍能实现正向优化。项目源代码已发布于https://github.com/tyang816/SES-Adapter。
