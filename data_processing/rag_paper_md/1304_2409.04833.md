# Achieving Peak Performance for Large Language Models: A Systematic Review

链接: http://arxiv.org/abs/2409.04833v1

原文摘要:
In recent years, large language models (LLMs) have achieved remarkable
success in natural language processing (NLP). LLMs require an extreme amount of
parameters to attain high performance. As models grow into the
trillion-parameter range, computational and memory costs increase
significantly. This makes it difficult for many researchers to access the
resources needed to train or apply these models. Optimizing LLM performance
involves two main approaches: fine-tuning pre-trained models for specific tasks
to achieve state-of-the-art performance, and reducing costs or improving
training time while maintaining similar performance. This paper presents a
systematic literature review (SLR) following the Preferred Reporting Items for
Systematic Reviews and Meta-Analyses (PRISMA) statement. We reviewed 65
publications out of 983 from 2017 to December 2023, retrieved from 5 databases.
The study presents methods to optimize and accelerate LLMs while achieving
cutting-edge results without sacrificing accuracy. We begin with an overview of
the development of language modeling, followed by a detailed explanation of
commonly used frameworks and libraries, and a taxonomy for improving and
speeding up LLMs based on three classes: LLM training, LLM inference, and
system serving. We then delve into recent optimization and acceleration
strategies such as training optimization, hardware optimization, scalability
and reliability, accompanied by the taxonomy and categorization of these
strategies. Finally, we provide an in-depth comparison of each class and
strategy, with two case studies on optimizing model training and enhancing
inference efficiency. These case studies showcase practical approaches to
address LLM resource limitations while maintaining performance.

中文翻译:
近年来，大型语言模型（LLM）在自然语言处理（NLP）领域取得了显著成就。这类模型需要海量参数以实现高性能，当参数量突破万亿级别时，计算与内存成本会急剧攀升，这使得许多研究者难以获取训练或应用这些模型所需的资源。优化LLM性能主要涉及两种路径：通过对预训练模型进行微调使其在特定任务中达到最优表现；或在保持相近性能的前提下降低计算成本或缩短训练时间。本文采用系统文献综述法（SLR），依据PRISMA声明规范，从5个数据库中筛选出2017年至2023年12月期间的983篇文献，最终纳入65篇进行评述。研究系统梳理了在不牺牲准确性的前提下优化与加速LLM、同时实现尖端性能的方法体系：首先概述语言建模的发展历程，继而详解常用框架与工具库，提出基于LLM训练、推理和系统服务三大维度的优化加速分类法；随后深入解析训练优化、硬件优化、可扩展性与可靠性等最新策略，并建立相应的分类体系；最后通过模型训练优化与推理效率提升两个案例研究，对各优化策略进行深度对比分析，为突破LLM资源限制同时保持性能提供了实践范例。
