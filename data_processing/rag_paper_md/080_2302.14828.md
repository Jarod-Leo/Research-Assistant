# Automatic Scoring of Dream Reports' Emotional Content with Large Language Models

链接: http://arxiv.org/abs/2302.14828v1

原文摘要:
In the field of dream research, the study of dream content typically relies
on the analysis of verbal reports provided by dreamers upon awakening from
their sleep. This task is classically performed through manual scoring provided
by trained annotators, at a great time expense. While a consistent body of work
suggests that natural language processing (NLP) tools can support the automatic
analysis of dream reports, proposed methods lacked the ability to reason over a
report's full context and required extensive data pre-processing. Furthermore,
in most cases, these methods were not validated against standard manual scoring
approaches. In this work, we address these limitations by adopting large
language models (LLMs) to study and replicate the manual annotation of dream
reports, using a mixture of off-the-shelf and bespoke approaches, with a focus
on references to reports' emotions. Our results show that the off-the-shelf
method achieves a low performance probably in light of inherent linguistic
differences between reports collected in different (groups of) individuals. On
the other hand, the proposed bespoke text classification method achieves a high
performance, which is robust against potential biases. Overall, these
observations indicate that our approach could find application in the analysis
of large dream datasets and may favour reproducibility and comparability of
results across studies.

中文翻译:
在梦境研究领域，对梦境内容的分析通常依赖于记录梦者醒来后提供的口头报告。这项传统工作需由训练有素的标注员进行人工评分，耗时巨大。尽管已有大量研究表明自然语言处理（NLP）工具可支持梦境报告的自动化分析，但现有方法存在两大局限：无法对报告整体语境进行推理，且需要繁重的数据预处理。更重要的是，这些方法大多未通过标准人工评分流程的验证。

本研究通过采用大语言模型（LLMs）来突破这些限制，结合现成方案与定制化方法，重点针对梦境报告中情绪指涉的标注任务进行复现与验证。结果显示：现成方案因不同（群体）个体梦境报告存在的固有语言差异而表现欠佳；而提出的定制化文本分类方法则展现出优异性能，且对潜在偏差具有强鲁棒性。这些发现表明，该方法不仅适用于大规模梦境数据集分析，更有助于提升跨研究结果的可重复性与可比性。
