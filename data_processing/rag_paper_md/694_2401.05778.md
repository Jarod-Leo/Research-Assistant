# Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems

链接: http://arxiv.org/abs/2401.05778v1

原文摘要:
Large language models (LLMs) have strong capabilities in solving diverse
natural language processing tasks. However, the safety and security issues of
LLM systems have become the major obstacle to their widespread application.
Many studies have extensively investigated risks in LLM systems and developed
the corresponding mitigation strategies. Leading-edge enterprises such as
OpenAI, Google, Meta, and Anthropic have also made lots of efforts on
responsible LLMs. Therefore, there is a growing need to organize the existing
studies and establish comprehensive taxonomies for the community. In this
paper, we delve into four essential modules of an LLM system, including an
input module for receiving prompts, a language model trained on extensive
corpora, a toolchain module for development and deployment, and an output
module for exporting LLM-generated content. Based on this, we propose a
comprehensive taxonomy, which systematically analyzes potential risks
associated with each module of an LLM system and discusses the corresponding
mitigation strategies. Furthermore, we review prevalent benchmarks, aiming to
facilitate the risk assessment of LLM systems. We hope that this paper can help
LLM participants embrace a systematic perspective to build their responsible
LLM systems.

中文翻译:
大型语言模型（LLM）在解决多样化自然语言处理任务方面展现出强大能力，然而其系统存在的安全隐患已成为阻碍广泛应用的主要障碍。众多研究已深入探讨了LLM系统的潜在风险，并制定了相应的缓解策略。OpenAI、谷歌、Meta和Anthropic等前沿企业也在负责任LLM领域做出了大量努力。因此，学术界亟需对现有研究进行系统梳理并建立全面分类体系。本文深入剖析了LLM系统的四个核心模块：接收提示词的输入模块、基于海量语料训练的语言模型、用于开发部署的工具链模块以及输出生成内容的导出模块。基于此，我们提出了一套完整的分类框架，系统分析了LLM系统各模块的潜在风险，并探讨了相应的缓解策略。此外，本文还综述了当前主流评估基准，旨在为LLM系统的风险评估提供支持。我们期望这项工作能帮助LLM从业者以系统性视角构建负责任的LLM系统。
