# Leveraging Large Language Models to Generate Answer Set Programs

链接: http://arxiv.org/abs/2307.07699v1

原文摘要:
Large language models (LLMs), such as GPT-3 and GPT-4, have demonstrated
exceptional performance in various natural language processing tasks and have
shown the ability to solve certain reasoning problems. However, their reasoning
capabilities are limited and relatively shallow, despite the application of
various prompting techniques. In contrast, formal logic is adept at handling
complex reasoning, but translating natural language descriptions into formal
logic is a challenging task that non-experts struggle with. This paper proposes
a neuro-symbolic method that combines the strengths of large language models
and answer set programming. Specifically, we employ an LLM to transform natural
language descriptions of logic puzzles into answer set programs. We carefully
design prompts for an LLM to convert natural language descriptions into answer
set programs in a step by step manner. Surprisingly, with just a few in-context
learning examples, LLMs can generate reasonably complex answer set programs.
The majority of errors made are relatively simple and can be easily corrected
by humans, thus enabling LLMs to effectively assist in the creation of answer
set programs.

中文翻译:
诸如GPT-3和GPT-4之类的大语言模型（LLMs）已在各类自然语言处理任务中展现出卓越性能，并显示出解决某些推理问题的能力。然而，尽管应用了多种提示技术，其推理能力仍存在局限且相对浅层。相比之下，形式逻辑擅长处理复杂推理，但将自然语言描述转化为形式逻辑是一项极具挑战性的任务，非专业人士往往难以完成。本文提出一种神经符号方法，结合了大语言模型与回答集编程的优势。具体而言，我们利用LLM将逻辑谜题的自然语言描述分步骤转化为回答集程序。通过精心设计的提示模板，引导LLM逐步完成自然语言到回答集程序的转换。令人惊讶的是，仅需少量上下文学习示例，LLMs就能生成合理复杂的回答集程序。其所犯错误大多较为简单，可由人工轻松修正，从而使LLMs能有效辅助回答集程序的创建。
