# $γ-$MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models

链接: http://arxiv.org/abs/2410.13859v1

原文摘要:
Despite the significant progress in multimodal large language models (MLLMs),
their high computational cost remains a barrier to real-world deployment.
Inspired by the mixture of depths (MoDs) in natural language processing, we aim
to address this limitation from the perspective of ``activated tokens''. Our
key insight is that if most tokens are redundant for the layer computation,
then can be skipped directly via the MoD layer. However, directly converting
the dense layers of MLLMs to MoD layers leads to substantial performance
degradation. To address this issue, we propose an innovative MoD adaptation
strategy for existing MLLMs called $\gamma$-MoD. In $\gamma$-MoD, a novel
metric is proposed to guide the deployment of MoDs in the MLLM, namely rank of
attention maps (ARank). Through ARank, we can effectively identify which layer
is redundant and should be replaced with the MoD layer. Based on ARank, we
further propose two novel designs to maximize the computational sparsity of
MLLM while maintaining its performance, namely shared vision-language router
and masked routing learning. With these designs, more than 90% dense layers of
the MLLM can be effectively converted to the MoD ones. To validate our method,
we apply it to three popular MLLMs, and conduct extensive experiments on 9
benchmark datasets. Experimental results not only validate the significant
efficiency benefit of $\gamma$-MoD to existing MLLMs but also confirm its
generalization ability on various MLLMs. For example, with a minor performance
drop, i.e., -1.5%, $\gamma$-MoD can reduce the training and inference time of
LLaVA-HR by 31.0% and 53.2%, respectively.

中文翻译:
尽管多模态大语言模型（MLLMs）已取得显著进展，但其高昂的计算成本仍是实际部署的主要障碍。受自然语言处理中混合深度（MoD）方法的启发，我们尝试从"激活令牌"的视角突破这一局限。核心发现是：若多数令牌对层级计算冗余，则可通过MoD层直接跳过。然而，直接将MLLMs的稠密层转换为MoD层会导致性能显著下降。为此，我们提出创新性适配策略γ-MoD，通过注意力图秩次（ARank）这一新指标指导MoD在MLLMs中的部署，有效识别应替换为MoD层的冗余层级。基于ARank，我们进一步设计共享视觉-语言路由器和掩码路由学习两种机制，在保持性能前提下最大化计算稀疏性，实现90%以上稠密层的有效转换。通过在三个主流MLLMs和9个基准数据集上的实验验证，γ-MoD不仅能显著提升效率（如LLaVA-HR训练和推理时间分别减少31.0%和53.2%，性能仅下降1.5%），还展现出优异的跨模型泛化能力。
