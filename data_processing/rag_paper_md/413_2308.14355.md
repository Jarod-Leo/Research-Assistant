# Can Transformer and GNN Help Each Other?

链接: http://arxiv.org/abs/2308.14355v1

原文摘要:
Graph Neural Networks (GNNs) have emerged as promising solutions for
collaborative filtering (CF) through the modeling of user-item interaction
graphs. The nucleus of existing GNN-based recommender systems involves
recursive message passing along user-item interaction edges to refine encoded
embeddings. Despite their demonstrated effectiveness, current GNN-based methods
encounter challenges of limited receptive fields and the presence of noisy
"interest-irrelevant" connections. In contrast, Transformer-based methods excel
in aggregating information adaptively and globally. Nevertheless, their
application to large-scale interaction graphs is hindered by inherent
complexities and challenges in capturing intricate, entangled structural
information. In this paper, we propose TransGNN, a novel model that integrates
Transformer and GNN layers in an alternating fashion to mutually enhance their
capabilities. Specifically, TransGNN leverages Transformer layers to broaden
the receptive field and disentangle information aggregation from edges, which
aggregates information from more relevant nodes, thereby enhancing the message
passing of GNNs. Additionally, to capture graph structure information
effectively, positional encoding is meticulously designed and integrated into
GNN layers to encode such structural knowledge into node attributes, thus
enhancing the Transformer's performance on graphs. Efficiency considerations
are also alleviated by proposing the sampling of the most relevant nodes for
the Transformer, along with two efficient sample update strategies to reduce
complexity. Furthermore, theoretical analysis demonstrates that TransGNN offers
increased expressiveness compared to GNNs, with only a marginal increase in
linear complexity. Extensive experiments on five public datasets validate the
effectiveness and efficiency of TransGNN.

中文翻译:
图神经网络（GNN）通过建模用户-物品交互图，已成为协同过滤（CF）领域颇具前景的解决方案。现有基于GNN的推荐系统核心在于沿用户-物品交互边进行递归消息传递以优化编码嵌入。尽管已证实其有效性，当前基于GNN的方法仍面临感受野受限和存在噪声性"兴趣无关"连接的挑战。相比之下，基于Transformer的方法擅长自适应地全局聚合信息，但其在大规模交互图上的应用受限于固有复杂性及捕捉复杂纠缠结构信息的困难。

本文提出TransGNN——一种通过交替堆叠Transformer与GNN层实现能力互补的新型模型。具体而言，TransGNN利用Transformer层拓宽感受野并将信息聚合与边解耦，从而从更相关的节点聚合信息以增强GNN的消息传递。为有效捕捉图结构信息，我们精心设计位置编码并将其融入GNN层，将此类结构知识编码至节点属性，进而提升Transformer在图数据上的表现。通过为Transformer采样最相关节点并引入两种高效采样更新策略，模型复杂度得到显著降低。理论分析表明，TransGNN在仅线性复杂度轻微增加的情况下，相比GNN具备更强的表达能力。在五个公开数据集上的大量实验验证了TransGNN的有效性与高效性。
