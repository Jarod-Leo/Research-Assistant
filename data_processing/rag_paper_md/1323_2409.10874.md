# American Sign Language to Text Translation using Transformer and Seq2Seq with LSTM

链接: http://arxiv.org/abs/2409.10874v1

原文摘要:
Sign language translation is one of the important issues in communication
between deaf and hearing people, as it expresses words through hand, body, and
mouth movements. American Sign Language is one of the sign languages used, one
of which is the alphabetic sign. The development of neural machine translation
technology is moving towards sign language translation. Transformer became the
state-of-the-art in natural language processing. This study compares the
Transformer with the Sequence-to-Sequence (Seq2Seq) model in translating sign
language to text. In addition, an experiment was conducted by adding Residual
Long Short-Term Memory (ResidualLSTM) in the Transformer. The addition of
ResidualLSTM to the Transformer reduces the performance of the Transformer
model by 23.37% based on the BLEU Score value. In comparison, the Transformer
itself increases the BLEU Score value by 28.14 compared to the Seq2Seq model.

中文翻译:
手语翻译作为通过手部、身体及口部动作表达语言的方式，是聋人与听人沟通的重要桥梁之一。美国手语作为其中一种常用手语，其字母手势是基础组成部分。随着神经机器翻译技术的进步，手语翻译领域也迎来发展机遇，Transformer模型已成为自然语言处理领域的尖端技术。本研究对比了Transformer与序列到序列（Seq2Seq）模型在手语转文本任务中的表现，并创新性地在Transformer架构中融入残差长短时记忆网络（ResidualLSTM）进行实验。实验结果表明：基于BLEU评分指标，添加ResidualLSTM反而使Transformer模型性能下降23.37%；而原生Transformer模型相较Seq2Seq模型显著提升了28.14个BLEU分值。
