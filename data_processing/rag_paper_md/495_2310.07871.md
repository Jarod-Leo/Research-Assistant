# Hierarchical Pretraining on Multimodal Electronic Health Records

链接: http://arxiv.org/abs/2310.07871v1

原文摘要:
Pretraining has proven to be a powerful technique in natural language
processing (NLP), exhibiting remarkable success in various NLP downstream
tasks. However, in the medical domain, existing pretrained models on electronic
health records (EHR) fail to capture the hierarchical nature of EHR data,
limiting their generalization capability across diverse downstream tasks using
a single pretrained model. To tackle this challenge, this paper introduces a
novel, general, and unified pretraining framework called MEDHMP, specifically
designed for hierarchically multimodal EHR data. The effectiveness of the
proposed MEDHMP is demonstrated through experimental results on eight
downstream tasks spanning three levels. Comparisons against eighteen baselines
further highlight the efficacy of our approach.

中文翻译:
预训练已被证明是自然语言处理（NLP）领域的一项强大技术，在各种NLP下游任务中展现出显著成效。然而在医疗领域，现有基于电子健康记录（EHR）的预训练模型未能捕捉EHR数据的层级特性，限制了单一预训练模型在多样化下游任务中的泛化能力。为应对这一挑战，本文提出了一种新颖、通用且统一的预训练框架MEDHMP，专为层级化多模态EHR数据设计。通过在三个层级八项下游任务上的实验结果验证了MEDHMP的有效性，与十八个基线模型的对比进一步凸显了该方法的优越性。
