# PointCAT: Cross-Attention Transformer for point cloud

链接: http://arxiv.org/abs/2304.03012v1

原文摘要:
Transformer-based models have significantly advanced natural language
processing and computer vision in recent years. However, due to the irregular
and disordered structure of point cloud data, transformer-based models for 3D
deep learning are still in their infancy compared to other methods. In this
paper we present Point Cross-Attention Transformer (PointCAT), a novel
end-to-end network architecture using cross-attentions mechanism for point
cloud representing. Our approach combines multi-scale features via two seprate
cross-attention transformer branches. To reduce the computational increase
brought by multi-branch structure, we further introduce an efficient model for
shape classification, which only process single class token of one branch as a
query to calculate attention map with the other. Extensive experiments
demonstrate that our method outperforms or achieves comparable performance to
several approaches in shape classification, part segmentation and semantic
segmentation tasks.

中文翻译:
近年来，基于Transformer的模型显著推动了自然语言处理和计算机视觉领域的发展。然而，由于点云数据具有不规则和无序的结构特性，与其他方法相比，基于Transformer的三维深度学习模型仍处于发展初期。本文提出点云交叉注意力Transformer（PointCAT），这是一种利用交叉注意力机制进行点云表征的端到端新型网络架构。该方法通过两个独立的交叉注意力Transformer分支实现多尺度特征融合。为降低多分支结构带来的计算量增长，我们进一步设计了一个高效的形状分类模型，该模型仅处理单分支的类别标记作为查询向量，与另一分支计算注意力图。大量实验表明，在形状分类、部件分割和语义分割任务中，我们的方法性能优于或持平于多种现有方法。
