# PointCAT: Cross-Attention Transformer for point cloud

链接: http://arxiv.org/abs/2304.03012v1

原文摘要:
Transformer-based models have significantly advanced natural language
processing and computer vision in recent years. However, due to the irregular
and disordered structure of point cloud data, transformer-based models for 3D
deep learning are still in their infancy compared to other methods. In this
paper we present Point Cross-Attention Transformer (PointCAT), a novel
end-to-end network architecture using cross-attentions mechanism for point
cloud representing. Our approach combines multi-scale features via two seprate
cross-attention transformer branches. To reduce the computational increase
brought by multi-branch structure, we further introduce an efficient model for
shape classification, which only process single class token of one branch as a
query to calculate attention map with the other. Extensive experiments
demonstrate that our method outperforms or achieves comparable performance to
several approaches in shape classification, part segmentation and semantic
segmentation tasks.

中文翻译:
基于Transformer的模型近年来显著推动了自然语言处理和计算机视觉领域的发展。然而由于点云数据具有不规则和无序的结构特性，与其他方法相比，基于Transformer的三维深度学习模型仍处于发展初期。本文提出点云交叉注意力Transformer（PointCAT）——一种利用交叉注意力机制进行点云表征的端到端新型网络架构。该方法通过两个独立的交叉注意力Transformer分支实现多尺度特征融合。为降低多分支结构带来的计算量增长，我们进一步提出高效的形状分类模型，该模型仅处理单分支的类别标记作为查询向量，与另一分支进行注意力图计算。大量实验表明，在形状分类、部件分割和语义分割任务中，我们的方法性能优于或持平于多种现有方案。

（翻译说明：
1. 专业术语处理："cross-attention"译为"交叉注意力"，"end-to-end"保留技术界惯用译法"端到端"
2. 长句拆分：将原文复合句按中文表达习惯拆分为多个短句，如第二句拆分为因果关系的两个分句
3. 被动语态转换："are still in their infancy"转化为主动态"仍处于发展初期"
4. 概念显化处理："multi-scale features"译为"多尺度特征融合"，补充"融合"使技术含义更明确
5. 句式重构：将"which only process..."定语从句转换为独立分句，符合中文多用动词短句的特点
6. 学术用语规范："achieves comparable performance to"采用学术论文标准表述"性能优于或持平于"）
