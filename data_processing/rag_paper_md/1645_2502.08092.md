# GCoT: Chain-of-Thought Prompt Learning for Graphs

链接: http://arxiv.org/abs/2502.08092v1

原文摘要:
Chain-of-thought (CoT) prompting has achieved remarkable success in natural
language processing (NLP). However, its vast potential remains largely
unexplored for graphs. This raises an interesting question: How can we design
CoT prompting for graphs to guide graph models to learn step by step? On one
hand, unlike natural languages, graphs are non-linear and characterized by
complex topological structures. On the other hand, many graphs lack textual
data, making it difficult to formulate language-based CoT prompting. In this
work, we propose the first CoT prompt learning framework for text-free graphs,
GCoT. Specifically, we decompose the adaptation process for each downstream
task into a series of inference steps, with each step consisting of
prompt-based inference, ``thought'' generation, and thought-conditioned prompt
learning. While the steps mimic CoT prompting in NLP, the exact mechanism
differs significantly. Specifically, at each step, an input graph, along with a
prompt, is first fed into a pre-trained graph encoder for prompt-based
inference. We then aggregate the hidden layers of the encoder to construct a
``thought'', which captures the working state of each node in the current step.
Conditioned on this thought, we learn a prompt specific to each node based on
the current state. These prompts are fed into the next inference step,
repeating the cycle. To evaluate and analyze the effectiveness of GCoT, we
conduct comprehensive experiments on eight public datasets, which demonstrate
the advantage of our approach.

中文翻译:
以下是您提供的英文论文摘要的中文翻译：

思维链（CoT）提示在自然语言处理（NLP）领域取得了显著成功，但其在图数据中的巨大潜力仍未被充分探索。这引发了一个有趣的问题：如何为图结构设计CoT提示，以引导图模型逐步学习？一方面，与自然语言不同，图具有非线性特征和复杂的拓扑结构；另一方面，许多图缺乏文本数据，难以构建基于语言的CoT提示。本研究提出了首个面向无文本图数据的CoT提示学习框架GCoT。具体而言，我们将每个下游任务的适配过程分解为一系列推理步骤，每个步骤包含基于提示的推理、"思维"生成以及思维条件化提示学习。虽然这些步骤模仿了NLP中的CoT提示机制，但其具体实现存在显著差异：在每一步中，输入图与提示首先被输入预训练图编码器进行基于提示的推理；随后我们聚合编码器的隐藏层构建"思维"，该思维捕捉了当前步骤中每个节点的工作状态；基于此思维，我们根据当前状态学习每个节点的特定提示。这些提示将被输入下一推理步骤，形成循环迭代。为评估和分析GCoT的有效性，我们在八个公共数据集上进行了全面实验，结果验证了本方法的优势。

（翻译说明：1. 专业术语如"Chain-of-thought"采用学界通用译法"思维链"；2. 被动语态转换为中文主动表达；3. 长难句按中文习惯拆分为短句；4. 技术概念如"thought-conditioned"采用"思维条件化"的意译；5. 保持"prompt"统一译为"提示"；6. 补充"形成循环迭代"等衔接词增强逻辑性）
