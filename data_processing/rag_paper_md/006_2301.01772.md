# Infomaxformer: Maximum Entropy Transformer for Long Time-Series Forecasting Problem

链接: http://arxiv.org/abs/2301.01772v1

原文摘要:
The Transformer architecture yields state-of-the-art results in many tasks
such as natural language processing (NLP) and computer vision (CV), since the
ability to efficiently capture the precise long-range dependency coupling
between input sequences. With this advanced capability, however, the quadratic
time complexity and high memory usage prevents the Transformer from dealing
with long time-series forecasting problem (LTFP). To address these
difficulties: (i) we revisit the learned attention patterns of the vanilla
self-attention, redesigned the calculation method of self-attention based the
Maximum Entropy Principle. (ii) we propose a new method to sparse the
self-attention, which can prevent the loss of more important self-attention
scores due to random sampling.(iii) We propose Keys/Values Distilling method
motivated that a large amount of feature in the original self-attention map is
redundant, which can further reduce the time and spatial complexity and make it
possible to input longer time-series. Finally, we propose a method that
combines the encoder-decoder architecture with seasonal-trend decomposition,
i.e., using the encoder-decoder architecture to capture more specific seasonal
parts. A large number of experiments on several large-scale datasets show that
our Infomaxformer is obviously superior to the existing methods. We expect this
to open up a new solution for Transformer to solve LTFP, and exploring the
ability of the Transformer architecture to capture much longer temporal
dependencies.

中文翻译:
Transformer架构凭借其高效捕捉输入序列间精确长程依赖关系的能力，在自然语言处理（NLP）和计算机视觉（CV）等诸多任务中取得了最先进的性能表现。然而这种卓越能力伴随着二次方时间复杂度和高内存消耗的代价，阻碍了Transformer在长时间序列预测（LTFP）问题中的应用。针对这些挑战：（1）我们重新审视了原始自注意力机制的学习模式，基于最大熵原理重新设计了自注意力计算方法；（2）提出一种新型自注意力稀疏化策略，可避免随机采样导致重要注意力得分的丢失；（3）基于原始注意力图中存在大量冗余特征的发现，创新性地提出键值蒸馏方法，进一步降低时空复杂度以实现更长序列的输入。最终，我们提出将编码器-解码器架构与季节性趋势分解相结合的方法，即利用编码器-解码器结构更精准地捕捉季节性成分。在多个大规模数据集上的大量实验表明，所提出的Infomaxformer模型显著优于现有方法。这项工作不仅为Transformer解决LTFP问题开辟了新路径，更为探索该架构捕获超长时序依赖关系的能力提供了重要启示。
