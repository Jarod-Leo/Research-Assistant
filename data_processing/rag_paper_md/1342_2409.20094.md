# Aggressive Post-Training Compression on Extremely Large Language Models

链接: http://arxiv.org/abs/2409.20094v1

原文摘要:
The increasing size and complexity of Large Language Models (LLMs) pose
challenges for their deployment on personal computers and mobile devices.
Aggressive post-training model compression is necessary to reduce the models'
size, but it often results in significant accuracy loss. To address this
challenge, we propose a novel network pruning technology that utilizes over 0.7
sparsity and less than 8 bits of quantization. Our approach enables the
compression of prevailing LLMs within a couple of hours while maintaining a
relatively small accuracy loss. In experimental evaluations, our method
demonstrates effectiveness and potential for practical deployment. By making
LLMs available on domestic devices, our work can facilitate a new era of
natural language processing applications with wide-ranging impacts.

中文翻译:
大型语言模型(LLMs)日益增长的规模和复杂性为其在个人计算机及移动设备上的部署带来了挑战。为实现模型体积缩减，必须采用激进的训练后压缩技术，但这往往导致显著的精度损失。为解决这一难题，我们提出了一种创新性网络剪枝技术，该技术利用超过0.7的稀疏度和低于8位的量化精度。我们的方法能在数小时内完成主流LLMs的压缩，同时保持相对较小的精度损失。实验评估表明，该方法展现出实际部署的有效性和潜力。通过使LLMs能够在民用设备上运行，本项工作有望推动自然语言处理应用进入具有广泛影响力的新时代。
