# A Survey on Transformer Context Extension: Approaches and Evaluation

链接: http://arxiv.org/abs/2503.13299v1

原文摘要:
Large language models (LLMs) based on Transformer have been widely applied in
the filed of natural language processing (NLP), demonstrating strong
performance, particularly in handling short text tasks. However, when it comes
to long context scenarios, the performance of LLMs degrades due to some
challenges. To alleviate this phenomenon, there is a number of work proposed
recently. In this survey, we first list the challenges of applying pre-trained
LLMs to process long contexts. Then systematically review the approaches
related to long context and propose our taxonomy categorizing them into four
main types: positional encoding, context compression, retrieval augmented, and
attention pattern. In addition to the approaches, we focus on the evaluation of
long context, organizing relevant data, tasks, and metrics based on existing
long context benchmarks. Finally, we summarize unresolved issues in the long
context domain and put forward our views on future developments.

中文翻译:
基于Transformer架构的大语言模型（LLMs）已在自然语言处理（NLP）领域得到广泛应用，尤其在短文本任务中展现出卓越性能。然而面对长上下文场景时，这些模型因存在若干挑战而出现性能下降。为缓解这一现象，近期学界提出了诸多解决方案。本综述首先系统梳理了预训练大语言模型处理长上下文时面临的核心挑战，继而从方法论角度对现有研究进行体系化归类，提出四类主流技术路径：位置编码优化、上下文压缩、检索增强及注意力机制改进。除方法学探讨外，本文重点梳理了长上下文评估体系，基于现有基准测试对相关数据集、任务类型及评价指标进行系统归纳。最后，我们总结了该领域尚未解决的关键问题，并对未来发展方向提出了前瞻性见解。
