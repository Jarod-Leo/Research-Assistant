# A Survey on Transformer Context Extension: Approaches and Evaluation

链接: http://arxiv.org/abs/2503.13299v1

原文摘要:
Large language models (LLMs) based on Transformer have been widely applied in
the filed of natural language processing (NLP), demonstrating strong
performance, particularly in handling short text tasks. However, when it comes
to long context scenarios, the performance of LLMs degrades due to some
challenges. To alleviate this phenomenon, there is a number of work proposed
recently. In this survey, we first list the challenges of applying pre-trained
LLMs to process long contexts. Then systematically review the approaches
related to long context and propose our taxonomy categorizing them into four
main types: positional encoding, context compression, retrieval augmented, and
attention pattern. In addition to the approaches, we focus on the evaluation of
long context, organizing relevant data, tasks, and metrics based on existing
long context benchmarks. Finally, we summarize unresolved issues in the long
context domain and put forward our views on future developments.

中文翻译:
基于Transformer架构的大规模语言模型（LLMs）已在自然语言处理（NLP）领域得到广泛应用，尤其在短文本任务中展现出卓越性能。然而面对长上下文场景时，这些模型因存在若干挑战而性能下降。为缓解此现象，近期涌现出大量相关研究。本文首先系统梳理了预训练LLMs处理长上下文时面临的核心挑战，继而提出分类框架，将现有方法归纳为四大类型：位置编码优化、上下文压缩、检索增强和注意力模式改进。除方法学综述外，我们聚焦长上下文评估体系，依据现有基准测试对相关数据集、任务指标进行系统梳理。最后总结了该领域尚未解决的关键问题，并对未来发展方向提出前瞻性见解。
