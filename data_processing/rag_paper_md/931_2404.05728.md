# A Large-Scale Exploration of $μ$-Transfer

链接: http://arxiv.org/abs/2404.05728v1

原文摘要:
Deep learning models have become a cornerstone of modern AI research, yet
their initializations and learning rates may at times be set in an opaque or
ad-hoc fashion due to the high cost of hyperparameter sweeps. The
$\mu$-Parameterization ($\mu$P) offers a possible solution to this challenge,
yielding scaling rules for model initialization and learning rates while
reportedly enabling zero-shot hyperparameter transfer from small to large
models. Despite its evident promise, the $\mu$P method is not yet widely
adopted, perhaps due to higher implementation complexity, many variations, or
complex theoretical background. This work considers $\mu$P empirically,
focusing on the popular transformer architecture, and aims to answer a simple
question: does $\mu$-Transfer yield near-optimal learning rates in practice?
Studying over a dozen ablations with up to 1.2B parameters and 33B tokens and a
large-scale experiment with up to 10B parameters and 190B tokens, we observe a
positive answer for most settings, and discuss improvements otherwise.

中文翻译:
深度学习模型已成为现代人工智能研究的基石，然而由于超参数扫描的高成本，其初始化和学习率的设置有时仍依赖于不透明或临时性的方式。μ参数化（μP）为这一挑战提供了可能的解决方案，它通过制定模型初始化与学习率的缩放规则，据称能够实现从小模型到大模型的零样本超参数迁移。尽管该方法展现出显著潜力，但或许由于较高的实现复杂度、众多变体或复杂的理论背景，μP尚未被广泛采用。本研究从实证角度探讨μP在主流Transformer架构中的应用，核心目标是回答一个简单问题：μ迁移在实践中能否产生接近最优的学习率？通过对超过12项参数规模达12亿、训练数据量达330亿标记的消融实验，以及参数规模达100亿、训练数据量达1900亿标记的大规模实验进行观察，我们发现该方法在多数场景下确实有效，并针对例外情况探讨了改进方案。
