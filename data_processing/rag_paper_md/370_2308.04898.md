# An Empirical Study on Using Large Language Models to Analyze Software Supply Chain Security Failures

链接: http://arxiv.org/abs/2308.04898v1

原文摘要:
As we increasingly depend on software systems, the consequences of breaches
in the software supply chain become more severe. High-profile cyber attacks
like those on SolarWinds and ShadowHammer have resulted in significant
financial and data losses, underlining the need for stronger cybersecurity. One
way to prevent future breaches is by studying past failures. However,
traditional methods of analyzing these failures require manually reading and
summarizing reports about them. Automated support could reduce costs and allow
analysis of more failures. Natural Language Processing (NLP) techniques such as
Large Language Models (LLMs) could be leveraged to assist the analysis of
failures. In this study, we assessed the ability of Large Language Models
(LLMs) to analyze historical software supply chain breaches. We used LLMs to
replicate the manual analysis of 69 software supply chain security failures
performed by members of the Cloud Native Computing Foundation (CNCF). We
developed prompts for LLMs to categorize these by four dimensions: type of
compromise, intent, nature, and impact. GPT 3.5s categorizations had an average
accuracy of 68% and Bard had an accuracy of 58% over these dimensions. We
report that LLMs effectively characterize software supply chain failures when
the source articles are detailed enough for consensus among manual analysts,
but cannot yet replace human analysts. Future work can improve LLM performance
in this context, and study a broader range of articles and failures.

中文翻译:
随着我们对软件系统的依赖日益加深，软件供应链漏洞所引发的后果也愈发严重。SolarWinds和ShadowHammer等高调网络攻击事件已造成重大财务和数据损失，凸显了加强网络安全的必要性。研究历史漏洞是预防未来事件的有效途径之一。然而，传统漏洞分析方法需要人工阅读并总结相关报告，而自动化技术支持能降低成本并扩大分析范围。自然语言处理（NLP）技术如大语言模型（LLMs）可辅助漏洞分析工作。本研究评估了大语言模型分析历史软件供应链漏洞的能力，通过LLMs复现了云原生计算基金会（CNCF）成员对69个软件供应链安全漏洞的手动分析过程。我们开发了提示词模板，指导LLMs从漏洞类型、攻击意图、性质及影响四个维度进行分类。结果显示GPT-3.5的平均分类准确率为68%，Bard为58%。研究表明，当源文件信息足够详细且能达成人工分析共识时，LLMs能有效描述软件供应链漏洞，但目前尚无法完全替代人工分析。未来研究可提升LLMs在此场景下的表现，并拓展分析样本的广度和深度。
