# Hierarchical Context Merging: Better Long Context Understanding for Pre-trained LLMs

链接: http://arxiv.org/abs/2404.10308v1

原文摘要:
Large language models (LLMs) have shown remarkable performance in various
natural language processing tasks. However, a primary constraint they face is
the context limit, i.e., the maximum number of tokens they can process.
Previous works have explored architectural changes and modifications in
positional encoding to relax the constraint, but they often require expensive
training or do not address the computational demands of self-attention. In this
paper, we present Hierarchical cOntext MERging (HOMER), a new training-free
scheme designed to overcome the limitations. HOMER uses a divide-and-conquer
algorithm, dividing long inputs into manageable chunks. Each chunk is then
processed collectively, employing a hierarchical strategy that merges adjacent
chunks at progressive transformer layers. A token reduction technique precedes
each merging, ensuring memory usage efficiency. We also propose an optimized
computational order reducing the memory requirement to logarithmically scale
with respect to input length, making it especially favorable for environments
with tight memory restrictions. Our experiments demonstrate the proposed
method's superior performance and memory efficiency, enabling the broader use
of LLMs in contexts requiring extended context. Code is available at
https://github.com/alinlab/HOMER.

中文翻译:
大型语言模型（LLMs）在各类自然语言处理任务中展现出卓越性能，但其面临的主要制约是上下文长度限制——即模型能处理的最大令牌数量。先前研究通过调整架构或改进位置编码来缓解这一限制，但这些方法往往需要高昂的训练成本，或未能解决自注意力机制的计算需求。本文提出了一种无需重新训练的解决方案——分层上下文融合（HOMER），该方案采用分治算法将长输入分割为可处理的片段，随后通过分层策略在Transformer各层逐步合并相邻片段。每次合并前采用令牌缩减技术确保内存使用效率，并提出优化计算顺序使内存需求随输入长度呈对数级增长，特别适用于内存受限环境。实验证明该方法在性能和内存效率上的优势，为需要长上下文的场景拓展了LLMs的应用边界。代码已开源于https://github.com/alinlab/HOMER。
