# AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning

链接: http://arxiv.org/abs/2406.18060v1

原文摘要:
Fine-tuning large language models (LLMs) has achieved remarkable performance
across various natural language processing tasks, yet it demands more and more
memory as model sizes keep growing. To address this issue, the recently
proposed Memory-efficient Zeroth-order (MeZO) methods attempt to fine-tune LLMs
using only forward passes, thereby avoiding the need for a backpropagation
graph. However, significant performance drops and a high risk of divergence
have limited their widespread adoption. In this paper, we propose the Adaptive
Zeroth-order Tensor-Train Adaption (AdaZeta) framework, specifically designed
to improve the performance and convergence of the ZO methods. To enhance
dimension-dependent ZO estimation accuracy, we introduce a fast-forward,
low-parameter tensorized adapter. To tackle the frequently observed divergence
issue in large-scale ZO fine-tuning tasks, we propose an adaptive query number
schedule that guarantees convergence. Detailed theoretical analysis and
extensive experimental results on Roberta-Large and Llama-2-7B models
substantiate the efficacy of our AdaZeta framework in terms of accuracy, memory
efficiency, and convergence speed.

中文翻译:
微调大型语言模型（LLMs）在各类自然语言处理任务中展现出卓越性能，但随着模型规模持续扩大，其对内存的需求也日益增长。为解决这一问题，近期提出的内存高效零阶优化方法（MeZO）尝试仅通过前向传播微调LLMs，从而避免反向传播计算图的存储开销。然而，这类方法存在显著性能下降和高发散风险，限制了其广泛应用。本文提出自适应零阶张量训练适配框架（AdaZeta），专门用于提升零阶方法的性能与收敛性。为增强维度依赖的零阶估计精度，我们设计了一种快速前向传播、低参数量化的张量化适配器；针对大规模零阶微调中常见的发散问题，提出可保证收敛的自适应查询次数调度机制。基于Roberta-Large和Llama-2-7B模型的理论分析及大量实验表明，AdaZeta框架在精度、内存效率和收敛速度方面均具有显著优势。
