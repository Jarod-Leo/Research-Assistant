# Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers

链接: http://arxiv.org/abs/2308.13191v1

原文摘要:
Although dominant in natural language processing, transformer-based models
remain challenged by the task of long-sequence processing, because the
computational cost of self-attention operations in transformers swells
quadratically with the input sequence length. To alleviate the complexity of
long-sequence processing, we propose a simple framework to enable the
offthe-shelf pre-trained transformers to process much longer sequences, while
the computation and memory costs remain growing linearly with the input
sequence lengths. More specifically, our method divides each long-sequence
input into a batch of chunks, then aligns the interchunk information during the
encoding steps, and finally selects the most representative hidden states from
the encoder for the decoding process. To extract inter-chunk semantic
information, we align the start and end token embeddings among chunks in each
encoding transformer block. To learn an effective hidden selection policy, we
design a dual updating scheme inspired by reinforcement learning, which regards
the decoders of transformers as environments, and the downstream performance
metrics as the rewards to evaluate the hidden selection actions. Our empirical
results on real-world long-text summarization and reading comprehension tasks
demonstrate effective improvements compared to prior longsequence processing
baselines.

中文翻译:
尽管基于Transformer的模型在自然语言处理领域占据主导地位，但其在处理长序列任务时仍面临挑战，因为自注意力机制的计算成本会随输入序列长度呈二次方增长。为降低长序列处理的复杂度，我们提出了一种简洁框架，使得现成的预训练Transformer模型能够处理更长的序列，同时保持计算和内存成本仅随序列长度线性增长。具体而言，该方法将长序列输入划分为多个片段批次，在编码阶段对齐片段间的信息交互，并最终从编码器中筛选最具代表性的隐藏状态用于解码过程。为捕获片段间的语义关联，我们在每个Transformer编码块中对齐各片段起始和结束标记的嵌入表示。此外，受强化学习启发设计了一种双重更新机制：将Transformer解码器视为环境，下游性能指标作为评估隐藏状态选择行为的奖励信号，从而学习最优的选择策略。在真实场景的长文本摘要和阅读理解任务上的实验表明，该方法较现有长序列处理基线模型取得了显著提升。
