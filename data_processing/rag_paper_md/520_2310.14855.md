# Contextual Refinement of Translations: Large Language Models for Sentence and Document-Level Post-Editing

链接: http://arxiv.org/abs/2310.14855v1

原文摘要:
Large Language Models (LLM's) have demonstrated considerable success in
various Natural Language Processing tasks, but they have yet to attain
state-of-the-art performance in Neural Machine Translation (NMT). Nevertheless,
their significant performance in tasks demanding a broad understanding and
contextual processing shows their potential for translation. To exploit these
abilities, we investigate using LLM's for MT and explore recent
parameter-efficient fine-tuning techniques. Surprisingly, our initial
experiments find that fine-tuning for translation purposes even led to
performance degradation. To overcome this, we propose an alternative approach:
adapting LLM's as Automatic Post-Editors (APE) rather than direct translators.
Building on the LLM's exceptional ability to process and generate lengthy
sequences, we also propose extending our approach to document-level
translation. We show that leveraging Low-Rank-Adapter fine-tuning for APE can
yield significant improvements across both sentence and document-level metrics
while generalizing to out-of-domain data. Most notably, we achieve a
state-of-the-art accuracy rate of 89\% on the ContraPro test set, which
specifically assesses the model's ability to resolve pronoun ambiguities when
translating from English to German. Lastly, we investigate a practical scenario
involving manual post-editing for document-level translation, where reference
context is made available. Here, we demonstrate that leveraging human
corrections can significantly reduce the number of edits required for
subsequent translations (Interactive Demo for integrating manual feedback can
be found here:
https://huggingface.co/spaces/skoneru/contextual_refinement_ende).

中文翻译:
大型语言模型（LLM）在各类自然语言处理任务中已展现出显著成效，然而其在神经机器翻译（NMT）领域尚未达到顶尖水平。不过，这些模型在需要广泛理解与上下文处理的任务中表现卓越，暗示了其在翻译领域的潜力。为挖掘这种潜力，我们研究了将LLM应用于机器翻译，并探索了近期高效的参数微调技术。令人意外的是，初步实验发现针对翻译任务的微调甚至会导致性能下降。为此，我们提出替代方案：将LLM调整为自动后编辑（APE）工具而非直接翻译器。基于LLM处理生成长序列的卓越能力，我们进一步将方法扩展至文档级翻译。研究表明，采用低秩适配器微调技术进行自动后编辑，能在句子和文档级指标上实现显著提升，同时保持跨领域数据的泛化能力。最突出的是，我们在ContraPro测试集（专为评估英语到德语翻译中代词歧义消解能力设计）上取得了89%的顶尖准确率。最后，我们探索了文档级翻译中结合人工后编辑的实际场景（参考上下文可获取时），证明利用人工修正能大幅减少后续翻译所需的编辑次数（整合人工反馈的交互演示详见：https://huggingface.co/spaces/skoneru/contextual_refinement_ende）。
