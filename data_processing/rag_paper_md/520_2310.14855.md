# Contextual Refinement of Translations: Large Language Models for Sentence and Document-Level Post-Editing

链接: http://arxiv.org/abs/2310.14855v1

原文摘要:
Large Language Models (LLM's) have demonstrated considerable success in
various Natural Language Processing tasks, but they have yet to attain
state-of-the-art performance in Neural Machine Translation (NMT). Nevertheless,
their significant performance in tasks demanding a broad understanding and
contextual processing shows their potential for translation. To exploit these
abilities, we investigate using LLM's for MT and explore recent
parameter-efficient fine-tuning techniques. Surprisingly, our initial
experiments find that fine-tuning for translation purposes even led to
performance degradation. To overcome this, we propose an alternative approach:
adapting LLM's as Automatic Post-Editors (APE) rather than direct translators.
Building on the LLM's exceptional ability to process and generate lengthy
sequences, we also propose extending our approach to document-level
translation. We show that leveraging Low-Rank-Adapter fine-tuning for APE can
yield significant improvements across both sentence and document-level metrics
while generalizing to out-of-domain data. Most notably, we achieve a
state-of-the-art accuracy rate of 89\% on the ContraPro test set, which
specifically assesses the model's ability to resolve pronoun ambiguities when
translating from English to German. Lastly, we investigate a practical scenario
involving manual post-editing for document-level translation, where reference
context is made available. Here, we demonstrate that leveraging human
corrections can significantly reduce the number of edits required for
subsequent translations (Interactive Demo for integrating manual feedback can
be found here:
https://huggingface.co/spaces/skoneru/contextual_refinement_ende).

中文翻译:
大型语言模型（LLM）在各类自然语言处理任务中已展现出显著成效，但其在神经机器翻译（NMT）领域尚未达到最先进水平。值得注意的是，该模型在需要广泛理解与上下文处理的任务中表现卓越，这揭示了其在翻译领域的潜力。为挖掘这种潜能，我们研究了LLM在机器翻译中的应用，并探索了近期提出的参数高效微调技术。出乎意料的是，初期实验表明：针对翻译任务进行微调反而会导致性能下降。为此，我们提出创新方案——将LLM改造为自动后编辑系统（APE）而非直接翻译器。基于LLM处理长序列的卓越能力，我们进一步将方案扩展至文档级翻译。研究表明，采用低秩适配器（Low-Rank-Adapter）微调技术进行自动后编辑，能在句子级和文档级指标上实现显著提升，同时保持跨领域数据的泛化能力。最突出的成果是：在专门评估英语译德语中代词消歧能力的ContraPro测试集上，我们以89%的准确率刷新了当前最佳记录。最后，我们模拟了文档级翻译中人工后编辑的实际场景（提供参考上下文），证实利用人工修正能大幅减少后续翻译所需的编辑次数（整合人工反馈的交互演示详见：https://huggingface.co/spaces/skoneru/contextual_refinement_ende）。

（翻译说明：
1. 专业术语处理：LLM/NMT/APE等技术术语保留英文缩写并首次出现时标注全称，符合学术规范
2. 长句拆分：将原文复合长句按中文表达习惯分解为多个短句，如将"Building on..."独立成句
3. 被动语态转换："it is shown"等结构转化为主动式"研究表明"
4. 数据呈现优化：89%精度保留数字形式，符合中文科技论文表述惯例
5. 超链接处理：完整保留演示链接及平台信息，确保功能性
6. 概念显化："generalizing to out-of-domain data"译为"跨领域数据的泛化能力"以明确技术含义
7. 逻辑连接词补充：增加"为此""最突出的成果是"等衔接词强化论证逻辑）
