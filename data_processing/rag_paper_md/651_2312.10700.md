# Cross-Domain Robustness of Transformer-based Keyphrase Generation

链接: http://arxiv.org/abs/2312.10700v1

原文摘要:
Modern models for text generation show state-of-the-art results in many
natural language processing tasks. In this work, we explore the effectiveness
of abstractive text summarization models for keyphrase selection. A list of
keyphrases is an important element of a text in databases and repositories of
electronic documents. In our experiments, abstractive text summarization models
fine-tuned for keyphrase generation show quite high results for a target text
corpus. However, in most cases, the zero-shot performance on other corpora and
domains is significantly lower. We investigate cross-domain limitations of
abstractive text summarization models for keyphrase generation. We present an
evaluation of the fine-tuned BART models for the keyphrase selection task
across six benchmark corpora for keyphrase extraction including scientific
texts from two domains and news texts. We explore the role of transfer learning
between different domains to improve the BART model performance on small text
corpora. Our experiments show that preliminary fine-tuning on out-of-domain
corpora can be effective under conditions of a limited number of samples.

中文翻译:
现代文本生成模型在众多自然语言处理任务中展现出最先进的性能。本研究探讨了抽象文本摘要模型在关键词选择任务中的有效性。关键词列表是电子文档数据库与知识库中文本的重要构成要素。实验表明，经过关键词生成任务微调的抽象文本摘要模型在目标文本语料上取得了较高性能，但在多数情况下，该模型在其他领域语料库上的零样本迁移表现显著下降。我们系统研究了抽象文本摘要模型在跨领域关键词生成任务中的局限性，并对微调后的BART模型在六个关键词抽取基准语料库（涵盖两个领域的科技文本和新闻文本）上的表现进行了评估。通过探究跨领域迁移学习的作用，我们验证了该方法对提升BART模型在小规模文本语料上性能的有效性。实验结果表明，在样本数量有限的条件下，基于外领域语料库的预微调策略能显著提升模型表现。

（翻译说明：
1. 专业术语处理："state-of-the-art"译为"最先进的"，"zero-shot"保留专业概念译为"零样本迁移"
2. 长句拆分：将原文复合长句按中文表达习惯分解为多个短句，如将实验发现部分拆分为两个对比句
3. 被动语态转换："is an important element"转为主动式"是...重要构成要素"
4. 概念显化："transfer learning"扩展译为"跨领域迁移学习"以明确技术内涵
5. 逻辑连接：添加"通过"、"验证了"等连接词强化论证逻辑
6. 学术风格：使用"探究"、"系统研究"、"结果表明"等符合学术摘要的规范表达）
