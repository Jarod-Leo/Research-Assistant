# Cross-Domain Robustness of Transformer-based Keyphrase Generation

链接: http://arxiv.org/abs/2312.10700v1

原文摘要:
Modern models for text generation show state-of-the-art results in many
natural language processing tasks. In this work, we explore the effectiveness
of abstractive text summarization models for keyphrase selection. A list of
keyphrases is an important element of a text in databases and repositories of
electronic documents. In our experiments, abstractive text summarization models
fine-tuned for keyphrase generation show quite high results for a target text
corpus. However, in most cases, the zero-shot performance on other corpora and
domains is significantly lower. We investigate cross-domain limitations of
abstractive text summarization models for keyphrase generation. We present an
evaluation of the fine-tuned BART models for the keyphrase selection task
across six benchmark corpora for keyphrase extraction including scientific
texts from two domains and news texts. We explore the role of transfer learning
between different domains to improve the BART model performance on small text
corpora. Our experiments show that preliminary fine-tuning on out-of-domain
corpora can be effective under conditions of a limited number of samples.

中文翻译:
现代文本生成模型在众多自然语言处理任务中展现出顶尖性能。本研究聚焦于探究抽象式文本摘要模型在关键词抽取任务中的适用性。作为电子文档数据库与知识库中文本的重要组成部分，关键词列表的生成质量至关重要。实验表明，经过关键词生成任务微调的抽象式摘要模型在目标文本语料上表现优异，但其零样本迁移至其他领域或语料库时性能普遍显著下降。我们系统考察了抽象式摘要模型在跨领域关键词生成中的局限性，通过对微调BART模型在六个基准关键词抽取语料库（涵盖两个学科领域的科技文献及新闻文本）上的评估，揭示了不同领域间迁移学习对提升小规模语料场景下模型性能的作用。实验证实：在样本数量受限条件下，基于跨领域语料的预训练微调策略能有效提升模型表现。
