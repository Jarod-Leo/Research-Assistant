# DR.GAP: Mitigating Bias in Large Language Models using Gender-Aware Prompting with Demonstration and Reasoning

链接: http://arxiv.org/abs/2502.11603v1

原文摘要:
Large Language Models (LLMs) exhibit strong natural language processing
capabilities but also inherit and amplify societal biases, including gender
bias, raising fairness concerns. Existing debiasing methods face significant
limitations: parameter tuning requires access to model weights, prompt-based
approaches often degrade model utility, and optimization-based techniques lack
generalizability. To address these challenges, we propose DR.GAP (Demonstration
and Reasoning for Gender-Aware Prompting), an automated and model-agnostic
approach that mitigates gender bias while preserving model performance. DR.GAP
selects bias-revealing examples and generates structured reasoning to guide
models toward more impartial responses. Extensive experiments on coreference
resolution and QA tasks across multiple LLMs (GPT-3.5, Llama3, and
Llama2-Alpaca) demonstrate its effectiveness, generalization ability, and
robustness. DR.GAP can generalize to vision-language models (VLMs), achieving
significant bias reduction.

中文翻译:
大语言模型（LLMs）展现出强大的自然语言处理能力，但也继承并放大了包括性别偏见在内的社会偏见，引发了公平性担忧。现有去偏方法存在显著局限：参数调整需访问模型权重，基于提示的方法常损害模型效用，而基于优化的技术缺乏泛化性。为此，我们提出DR.GAP（性别感知提示的示范与推理），这是一种自动化且与模型无关的方法，能在保持模型性能的同时减轻性别偏见。DR.GAP通过筛选偏见揭示示例并生成结构化推理，引导模型作出更公正的响应。在指代消解和问答任务上对多种LLM（GPT-3.5、Llama3和Llama2-Alpaca）的广泛实验验证了其有效性、泛化能力和鲁棒性。该方法可推广至视觉语言模型（VLMs），实现显著的偏见削减。
