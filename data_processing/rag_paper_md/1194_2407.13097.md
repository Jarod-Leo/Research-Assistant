# AlcLaM: Arabic Dialectal Language Model

链接: http://arxiv.org/abs/2407.13097v1

原文摘要:
Pre-trained Language Models (PLMs) are integral to many modern natural
language processing (NLP) systems. Although multilingual models cover a wide
range of languages, they often grapple with challenges like high inference
costs and a lack of diverse non-English training data. Arabic-specific PLMs are
trained predominantly on modern standard Arabic, which compromises their
performance on regional dialects. To tackle this, we construct an Arabic
dialectal corpus comprising 3.4M sentences gathered from social media
platforms. We utilize this corpus to expand the vocabulary and retrain a
BERT-based model from scratch. Named AlcLaM, our model was trained using only
13 GB of text, which represents a fraction of the data used by existing models
such as CAMeL, MARBERT, and ArBERT, compared to 7.8%, 10.2%, and 21.3%,
respectively. Remarkably, AlcLaM demonstrates superior performance on a variety
of Arabic NLP tasks despite the limited training data. AlcLaM is available at
GitHub https://github.com/amurtadha/Alclam and HuggingFace
https://huggingface.co/rahbi.

中文翻译:
预训练语言模型（PLMs）已成为现代自然语言处理（NLP）系统的核心组件。尽管多语言模型覆盖了广泛语种，但其常面临推理成本高昂、非英语训练数据多样性不足等挑战。针对阿拉伯语的专用PLMs主要基于现代标准阿拉伯语训练，导致对方言变体的处理性能欠佳。为此，我们构建了一个包含340万句社交媒体文本的阿拉伯方言语料库，并利用该语料库进行词汇扩展，从头开始重新训练基于BERT架构的模型。命名为AlcLaM的该模型仅使用13GB文本训练，数据量分别仅为CAMeL、MARBERT和ArBERT等现有模型的7.8%、10.2%和21.3%。值得注意的是，在训练数据有限的情况下，AlcLaM在多项阿拉伯语NLP任务中展现出卓越性能。该模型已开源发布于GitHub（https://github.com/amurtadha/Alclam）和HuggingFace平台（https://huggingface.co/rahbi）。
