# Distributed LLMs and Multimodal Large Language Models: A Survey on Advances, Challenges, and Future Directions

链接: http://arxiv.org/abs/2503.16585v1

原文摘要:
Language models (LMs) are machine learning models designed to predict
linguistic patterns by estimating the probability of word sequences based on
large-scale datasets, such as text. LMs have a wide range of applications in
natural language processing (NLP) tasks, including autocomplete and machine
translation. Although larger datasets typically enhance LM performance,
scalability remains a challenge due to constraints in computational power and
resources. Distributed computing strategies offer essential solutions for
improving scalability and managing the growing computational demand. Further,
the use of sensitive datasets in training and deployment raises significant
privacy concerns. Recent research has focused on developing decentralized
techniques to enable distributed training and inference while utilizing diverse
computational resources and enabling edge AI. This paper presents a survey on
distributed solutions for various LMs, including large language models (LLMs),
vision language models (VLMs), multimodal LLMs (MLLMs), and small language
models (SLMs). While LLMs focus on processing and generating text, MLLMs are
designed to handle multiple modalities of data (e.g., text, images, and audio)
and to integrate them for broader applications. To this end, this paper reviews
key advancements across the MLLM pipeline, including distributed training,
inference, fine-tuning, and deployment, while also identifying the
contributions, limitations, and future areas of improvement. Further, it
categorizes the literature based on six primary focus areas of
decentralization. Our analysis describes gaps in current methodologies for
enabling distributed solutions for LMs and outline future research directions,
emphasizing the need for novel solutions to enhance the robustness and
applicability of distributed LMs.

中文翻译:
语言模型（LMs）是通过基于大规模数据集（如文本）估计词序列概率来预测语言模式的机器学习模型。该技术在自然语言处理（NLP）任务中应用广泛，包括自动补全和机器翻译。尽管更大规模的数据集通常能提升模型性能，但受限于计算能力和资源，可扩展性仍是挑战。分布式计算策略为提升可扩展性及应对日益增长的计算需求提供了关键解决方案。此外，训练与部署过程中使用敏感数据引发了重大隐私隐忧。近期研究聚焦于开发去中心化技术，以利用多样化计算资源实现分布式训练与推理，并推动边缘AI发展。本文系统综述了针对各类语言模型的分布式解决方案，涵盖大语言模型（LLMs）、视觉语言模型（VLMs）、多模态大语言模型（MLLMs）及小语言模型（SLMs）。其中LLMs专注于文本处理与生成，而MLLMs则设计用于处理多模态数据（如文本、图像和音频）并将其整合以实现更广泛的应用。为此，本文梳理了MLLM流程中的关键进展，包括分布式训练、推理、微调与部署，同时指出技术贡献、现存局限及未来改进方向。研究文献根据去中心化的六大核心领域进行分类。分析揭示了当前LM分布式解决方案的方法论缺口，并规划了未来研究方向，强调需通过创新方案增强分布式语言模型的鲁棒性与适用性。
