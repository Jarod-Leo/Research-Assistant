# Distributed LLMs and Multimodal Large Language Models: A Survey on Advances, Challenges, and Future Directions

链接: http://arxiv.org/abs/2503.16585v1

原文摘要:
Language models (LMs) are machine learning models designed to predict
linguistic patterns by estimating the probability of word sequences based on
large-scale datasets, such as text. LMs have a wide range of applications in
natural language processing (NLP) tasks, including autocomplete and machine
translation. Although larger datasets typically enhance LM performance,
scalability remains a challenge due to constraints in computational power and
resources. Distributed computing strategies offer essential solutions for
improving scalability and managing the growing computational demand. Further,
the use of sensitive datasets in training and deployment raises significant
privacy concerns. Recent research has focused on developing decentralized
techniques to enable distributed training and inference while utilizing diverse
computational resources and enabling edge AI. This paper presents a survey on
distributed solutions for various LMs, including large language models (LLMs),
vision language models (VLMs), multimodal LLMs (MLLMs), and small language
models (SLMs). While LLMs focus on processing and generating text, MLLMs are
designed to handle multiple modalities of data (e.g., text, images, and audio)
and to integrate them for broader applications. To this end, this paper reviews
key advancements across the MLLM pipeline, including distributed training,
inference, fine-tuning, and deployment, while also identifying the
contributions, limitations, and future areas of improvement. Further, it
categorizes the literature based on six primary focus areas of
decentralization. Our analysis describes gaps in current methodologies for
enabling distributed solutions for LMs and outline future research directions,
emphasizing the need for novel solutions to enhance the robustness and
applicability of distributed LMs.

中文翻译:
语言模型（Language Models, LMs）是一类通过基于大规模数据集（如文本）估计词序列概率来预测语言规律的机器学习模型。该技术在自然语言处理（NLP）任务中应用广泛，涵盖自动补全、机器翻译等多个领域。尽管更大规模的数据集通常能提升语言模型性能，但受计算能力和资源限制，可扩展性仍是关键挑战。分布式计算策略为提升可扩展性、应对日益增长的计算需求提供了重要解决方案。此外，训练与部署过程中敏感数据集的使用引发了显著的隐私担忧。近期研究致力于开发去中心化技术，以利用多样化计算资源实现分布式训练与推理，并推动边缘AI发展。

本文系统综述了针对各类语言模型的分布式解决方案，包括大语言模型（LLMs）、视觉语言模型（VLMs）、多模态大语言模型（MLLMs）及小语言模型（SLMs）。其中，LLMs专注于文本处理与生成，而MLLMs则设计用于处理文本、图像、音频等多模态数据，并通过跨模态整合拓展应用场景。为此，本文回顾了MLLM流程中的关键进展，包括分布式训练、推理、微调与部署，同时剖析了现有贡献、局限性及未来改进方向。研究进一步根据去中心化的六大核心关注领域对文献进行分类。通过分析，我们揭示了当前语言模型分布式解决方案的方法论缺口，并规划了未来研究方向，强调需要创新方案以增强分布式语言模型的鲁棒性与适用性。
