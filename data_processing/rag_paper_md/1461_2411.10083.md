# Xmodel-1.5: An 1B-scale Multilingual LLM

链接: http://arxiv.org/abs/2411.10083v1

原文摘要:
We introduce Xmodel-1.5, a 1-billion-parameter multilingual large language
model pretrained on 2 trillion tokens, designed for balanced performance and
scalability. Unlike most large models that use the BPE tokenizer, Xmodel-1.5
employs a custom unigram tokenizer with 65,280 tokens, optimizing both
efficiency and accuracy. The model delivers competitive results across multiple
languages, including Thai, Arabic, French, Chinese, and English, outperforming
Alibaba's PolyLM-1.7B on respective evaluation datasets. Xmodel-1.5 excels in
benchmarks like mMMLU and PIQA, and achieves state-of-the-art results in Thai.
To support low-resource language research, we release Xdata_Thai, a
Thai-specific evaluation dataset featuring unique linguistic challenges such as
gendered particles and idioms. While the model demonstrates strong performance,
there is still room for improvement in handling culturally specific nuances. We
hope this work contributes to advancements in multilingual AI research. Models
and code are publicly available on GitHub at
https://github.com/XiaoduoAILab/XmodelLM-1.5

中文翻译:
我们推出Xmodel-1.5——一个基于2万亿token预训练、参数规模达10亿的多语言大模型，专为平衡性能与可扩展性而设计。与多数采用BPE分词器的大模型不同，该模型创新性地使用包含65,280个token的自定义一元分词器，在效率与准确性上实现双重优化。该模型在泰语、阿拉伯语、法语、中文及英语等语言评估中均展现出竞争优势，尤其在泰语评测中取得最先进成果，并在mMMLU和PIQA等基准测试中表现优异，全面超越阿里巴巴PolyLM-1.7B模型。为支持低资源语言研究，我们同步开放泰语专项评估数据集Xdata_Thai，其特色在于涵盖性别助词与习语等独特语言现象。尽管模型整体表现强劲，但在处理文化特定语境方面仍有提升空间。本研究旨在推动多语言AI领域发展，完整模型及代码已开源发布于GitHub（https://github.com/XiaoduoAILab/XmodelLM-1.5）。
