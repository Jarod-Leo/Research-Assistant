# Xmodel-1.5: An 1B-scale Multilingual LLM

链接: http://arxiv.org/abs/2411.10083v1

原文摘要:
We introduce Xmodel-1.5, a 1-billion-parameter multilingual large language
model pretrained on 2 trillion tokens, designed for balanced performance and
scalability. Unlike most large models that use the BPE tokenizer, Xmodel-1.5
employs a custom unigram tokenizer with 65,280 tokens, optimizing both
efficiency and accuracy. The model delivers competitive results across multiple
languages, including Thai, Arabic, French, Chinese, and English, outperforming
Alibaba's PolyLM-1.7B on respective evaluation datasets. Xmodel-1.5 excels in
benchmarks like mMMLU and PIQA, and achieves state-of-the-art results in Thai.
To support low-resource language research, we release Xdata_Thai, a
Thai-specific evaluation dataset featuring unique linguistic challenges such as
gendered particles and idioms. While the model demonstrates strong performance,
there is still room for improvement in handling culturally specific nuances. We
hope this work contributes to advancements in multilingual AI research. Models
and code are publicly available on GitHub at
https://github.com/XiaoduoAILab/XmodelLM-1.5

中文翻译:
我们推出Xmodel-1.5——一个基于2万亿token预训练、具有10亿参数的多语言大语言模型，专为平衡性能与可扩展性而设计。与多数采用BPE分词器的大模型不同，Xmodel-1.5创新性地使用包含65,280个token的自定义一元分词器，在效率与准确性上实现双重优化。该模型在泰语、阿拉伯语、法语、中文及英语等多元语言环境中均展现出竞争优势，在相应评估数据集上超越阿里巴巴的PolyLM-1.7B模型。Xmodel-1.5在mMMLU和PIQA等基准测试中表现优异，并在泰语任务上取得最先进成果。为支持低资源语言研究，我们同步发布泰语专项评估数据集Xdata_Thai，其包含性别化助词与习语等独特语言挑战。尽管模型展现出强劲性能，但在处理文化特异性语言细微差异方面仍有提升空间。我们期待这项工作能推动多语言AI研究的进步。模型与代码已在GitHub开源：https://github.com/XiaoduoAILab/XmodelLM-1.5

（翻译说明：采用技术文档专业译法，将"1-billion-parameter"译为"10亿参数"符合中文计量习惯；"unigram tokenizer"译为"一元分词器"是NLP领域标准术语；"state-of-the-art"译为"最先进成果"准确传达原意；长句拆分如"optimizing both..."处理为"在...上实现双重优化"增强可读性；文化负载词如"gendered particles"译为"性别化助词"确保专业准确性；GitHub链接保留原格式符合技术规范）
