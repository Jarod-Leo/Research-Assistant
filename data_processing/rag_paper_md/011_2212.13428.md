# A Survey on Knowledge-Enhanced Pre-trained Language Models

链接: http://arxiv.org/abs/2212.13428v1

原文摘要:
Natural Language Processing (NLP) has been revolutionized by the use of
Pre-trained Language Models (PLMs) such as BERT. Despite setting new records in
nearly every NLP task, PLMs still face a number of challenges including poor
interpretability, weak reasoning capability, and the need for a lot of
expensive annotated data when applied to downstream tasks. By integrating
external knowledge into PLMs,
\textit{\underline{K}nowledge-\underline{E}nhanced \underline{P}re-trained
\underline{L}anguage \underline{M}odels} (KEPLMs) have the potential to
overcome the above-mentioned limitations. In this paper, we examine KEPLMs
systematically through a series of studies. Specifically, we outline the common
types and different formats of knowledge to be integrated into KEPLMs, detail
the existing methods for building and evaluating KEPLMS, present the
applications of KEPLMs in downstream tasks, and discuss the future research
directions. Researchers will benefit from this survey by gaining a quick and
comprehensive overview of the latest developments in this field.

中文翻译:
预训练语言模型（如BERT）的应用为自然语言处理（NLP）领域带来了革命性变革。尽管在几乎所有NLP任务中刷新了记录，这类模型仍面临可解释性差、推理能力弱、下游任务需依赖大量昂贵标注数据等挑战。通过将外部知识融入预训练模型，**知识增强型预训练语言模型**（KEPLMs）展现出突破上述局限的潜力。本文通过系统研究，全面剖析了KEPLMs技术体系：首先梳理了模型融合知识的常见类型与不同组织形式，详细阐述了现有KEPLMs的构建方法与评估体系，总结了模型在下游任务中的应用效果，并探讨了未来研究方向。本综述有助于研究者快速把握该领域最新进展，为其研究提供全局视角。
