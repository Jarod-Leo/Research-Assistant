# Unveiling Cultural Blind Spots: Analyzing the Limitations of mLLMs in Procedural Text Comprehension

链接: http://arxiv.org/abs/2502.14315v1

原文摘要:
Despite the impressive performance of multilingual large language models
(mLLMs) in various natural language processing tasks, their ability to
understand procedural texts, particularly those with culture-specific content,
remains largely unexplored. Texts describing cultural procedures, including
rituals, traditional craftsmanship, and social etiquette, require an inherent
understanding of cultural context, presenting a significant challenge for
mLLMs. In this work, we introduce CAPTex, a benchmark designed to evaluate
mLLMs' ability to process and reason about culturally diverse procedural texts
across multiple languages using various methodologies to assess their
performance. Our findings indicate that (1) mLLMs face difficulties with
culturally contextualized procedural texts, showing notable performance
declines in low-resource languages, (2) model performance fluctuates across
cultural domains, with some areas presenting greater difficulties, and (3)
language models exhibit better performance on multiple-choice tasks within
conversational frameworks compared to direct questioning. These results
underscore the current limitations of mLLMs in handling culturally nuanced
procedural texts and highlight the need for culturally aware benchmarks like
CAPTex to enhance their adaptability and comprehension across diverse
linguistic and cultural landscapes.

中文翻译:
尽管多语言大语言模型（mLLMs）在各种自然语言处理任务中表现卓越，但其对程序性文本——尤其是蕴含文化特异性内容文本的理解能力仍鲜有研究。描述文化习俗（如仪式、传统工艺、社交礼仪等）的文本需要内在的文化语境理解能力，这对mLLMs构成了重大挑战。本研究推出CAPTex评估基准，通过多方法学系统检验mLLMs处理跨文化程序性文本的推理能力。关键发现包括：（1）mLLMs在文化语境化的程序性文本上表现欠佳，低资源语言场景性能下降尤为显著；（2）模型表现存在文化领域波动性，某些文化领域更具挑战性；（3）对话框架下的多选题任务表现优于直接提问模式。这些结果揭示了当前mLLMs在处理文化敏感程序性文本时的局限性，凸显了CAPTex等文化感知基准对提升模型跨语言文化适应力的必要性。
