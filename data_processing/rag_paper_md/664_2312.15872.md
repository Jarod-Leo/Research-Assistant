# Heterogeneous Encoders Scaling In The Transformer For Neural Machine Translation

链接: http://arxiv.org/abs/2312.15872v1

原文摘要:
Although the Transformer is currently the best-performing architecture in the
homogeneous configuration (self-attention only) in Neural Machine Translation,
many State-of-the-Art models in Natural Language Processing are made of a
combination of different Deep Learning approaches. However, these models often
focus on combining a couple of techniques only and it is unclear why some
methods are chosen over others. In this work, we investigate the effectiveness
of integrating an increasing number of heterogeneous methods. Based on a simple
combination strategy and performance-driven synergy criteria, we designed the
Multi-Encoder Transformer, which consists of up to five diverse encoders.
Results showcased that our approach can improve the quality of the translation
across a variety of languages and dataset sizes and it is particularly
effective in low-resource languages where we observed a maximum increase of
7.16 BLEU compared to the single-encoder model.

中文翻译:
尽管Transformer在神经机器翻译的同构配置（仅自注意力机制）中表现最优，但自然语言处理领域众多顶尖模型实则为多种深度学习方法的融合体。然而这些模型通常仅聚焦于少数技术的组合，且缺乏对不同方法选择依据的明确解释。本研究通过渐进式整合异构方法，探究其协同效应。基于简单的组合策略与性能导向的协同标准，我们设计了包含多达五种异构编码器的多编码器Transformer架构。实验结果表明，该方案能显著提升多语种及不同数据规模下的翻译质量，在低资源语言中效果尤为突出——相较于单编码器模型，我们观测到最高达7.16 BLEU值的性能提升。
