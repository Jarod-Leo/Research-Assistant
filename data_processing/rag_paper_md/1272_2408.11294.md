# RedWhale: An Adapted Korean LLM Through Efficient Continual Pretraining

链接: http://arxiv.org/abs/2408.11294v1

原文摘要:
The field of Natural Language Processing (NLP) has seen significant
advancements with the development of Large Language Models (LLMs). However,
much of this research remains focused on English, often overlooking
low-resource languages like Korean. This oversight presents challenges due to
the unique non-alphabetic token structure of Korean and the substantial memory
and computational demands required for LLM training, which frequently lead to
memory constraints and out-of-memory errors. To address these issues, we
present RedWhale, a model specifically tailored for Korean language processing.
RedWhale is developed using an efficient continual pretraining approach that
includes a comprehensive Korean corpus preprocessing pipeline, a specialized
tokenizer, an optimized model initialization technique, and a multistage
pretraining strategy. These innovations collectively reduce training time and
computational costs while maintaining high levels of accuracy and
comprehension. By leveraging cross-lingual transfer learning, which exploits
shared linguistic similarities across languages, RedWhale builds on English
models to enhance Korean language processing. Experimental results demonstrate
that RedWhale outperforms other leading models on Korean NLP benchmarks,
including the Korean Balanced Evaluation of Significant Tasks (KoBEST), showing
superior understanding and generation of Korean text. Furthermore, RedWhale
showed no signs of convergence even after pretraining on 9.7 billion tokens,
indicating the potential for further improvements with additional training.
This work represents a significant advancement in bridging the linguistic
divide, particularly in enhancing NLP capabilities for the Korean language.

中文翻译:
随着大语言模型（LLMs）的发展，自然语言处理（NLP）领域取得了显著进展。然而，这些研究大多集中于英语，往往忽视了韩语等低资源语言。由于韩语独特的非字母化词元结构以及LLM训练所需的高内存与计算需求——这常导致内存限制和溢出错误——这种忽视带来了诸多挑战。为应对这些问题，我们推出了专为韩语处理定制的RedWhale模型。该模型通过高效的持续预训练方法开发，包含完整的韩语语料预处理流程、专用分词器、优化的模型初始化技术和多阶段预训练策略。这些创新在保持高准确性与理解力的同时，显著降低了训练时间和计算成本。通过利用跨语言迁移学习（挖掘语言间共享的相似性），RedWhale基于英语模型增强了韩语处理能力。实验结果表明，在包括韩语重要任务平衡评估（KoBEST）在内的韩语NLP基准测试中，RedWhale的表现优于其他主流模型，展现出更优异的韩语文本理解与生成能力。值得注意的是，RedWhale在完成97亿词元的预训练后仍未出现收敛迹象，表明通过进一步训练仍有提升潜力。这项研究在弥合语言鸿沟方面——特别是提升韩语NLP能力——实现了重要突破。
