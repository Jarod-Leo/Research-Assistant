# LaVin-DiT: Large Vision Diffusion Transformer

链接: http://arxiv.org/abs/2411.11505v1

原文摘要:
This paper presents the Large Vision Diffusion Transformer (LaVin-DiT), a
scalable and unified foundation model designed to tackle over 20 computer
vision tasks in a generative framework. Unlike existing large vision models
directly adapted from natural language processing architectures, which rely on
less efficient autoregressive techniques and disrupt spatial relationships
essential for vision data, LaVin-DiT introduces key innovations to optimize
generative performance for vision tasks. First, to address the high
dimensionality of visual data, we incorporate a spatial-temporal variational
autoencoder that encodes data into a continuous latent space. Second, for
generative modeling, we develop a joint diffusion transformer that
progressively produces vision outputs. Third, for unified multi-task training,
in-context learning is implemented. Input-target pairs serve as task context,
which guides the diffusion transformer to align outputs with specific tasks
within the latent space. During inference, a task-specific context set and test
data as queries allow LaVin-DiT to generalize across tasks without fine-tuning.
Trained on extensive vision datasets, the model is scaled from 0.1B to 3.4B
parameters, demonstrating substantial scalability and state-of-the-art
performance across diverse vision tasks. This work introduces a novel pathway
for large vision foundation models, underscoring the promising potential of
diffusion transformers. The code and models are available.

中文翻译:
本文提出了一种可扩展且统一的视觉基础模型——大规模视觉扩散变换器（LaVin-DiT），该模型在生成式框架下能够处理超过20种计算机视觉任务。与现有直接从自然语言处理架构改造而来的大型视觉模型不同（这些模型依赖效率较低的自回归技术，且破坏了视觉数据关键的空间关系），LaVin-DiT通过三项关键创新优化了视觉任务的生成性能：首先，针对视觉数据的高维特性，我们引入时空变分自编码器将数据编码至连续潜在空间；其次，在生成建模方面，开发了联合扩散变换器逐步生成视觉输出；第三，为实现统一的多任务训练，采用上下文学习方法——以输入-目标对作为任务上下文，引导扩散变换器在潜在空间中将输出与特定任务对齐。推理阶段，模型通过任务特定上下文集和测试查询数据，无需微调即可实现跨任务泛化。该模型在大量视觉数据集上训练，参数规模从0.1B扩展至3.4B，展现出卓越的可扩展性，并在多样化视觉任务中达到最先进性能。这项工作为大型视觉基础模型开辟了新路径，彰显了扩散变换器的巨大潜力。相关代码与模型已开源。
