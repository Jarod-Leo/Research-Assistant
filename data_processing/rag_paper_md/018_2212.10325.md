# SeqDiffuSeq: Text Diffusion with Encoder-Decoder Transformers

链接: http://arxiv.org/abs/2212.10325v1

原文摘要:
Diffusion model, a new generative modelling paradigm, has achieved great
success in image, audio, and video generation. However, considering the
discrete categorical nature of text, it is not trivial to extend continuous
diffusion models to natural language, and text diffusion models are less
studied. Sequence-to-sequence text generation is one of the essential natural
language processing topics. In this work, we apply diffusion models to approach
sequence-to-sequence text generation, and explore whether the superiority
generation performance of diffusion model can transfer to natural language
domain. We propose SeqDiffuSeq, a text diffusion model for sequence-to-sequence
generation. SeqDiffuSeq uses an encoder-decoder Transformers architecture to
model denoising function. In order to improve generation quality, SeqDiffuSeq
combines the self-conditioning technique and a newly proposed adaptive noise
schedule technique. The adaptive noise schedule has the difficulty of denoising
evenly distributed across time steps, and considers exclusive noise schedules
for tokens at different positional order. Experiment results illustrate the
good performance on sequence-to-sequence generation in terms of text quality
and inference time.

中文翻译:
扩散模型作为一种新兴的生成建模范式，已在图像、音频和视频生成领域取得显著成功。然而考虑到文本的离散类别特性，将连续扩散模型扩展至自然语言处理并非易事，目前针对文本扩散模型的研究相对匮乏。序列到序列的文本生成是自然语言处理的核心课题之一。本研究将扩散模型应用于序列到序列文本生成任务，探究该模型在生成性能上的优势能否迁移至自然语言领域。我们提出SeqDiffuSeq——一种面向序列到序列生成的文本扩散模型，该模型采用编码器-解码器结构的Transformer来建模去噪函数。为提升生成质量，SeqDiffuSeq融合了自条件技术及新提出的自适应噪声调度技术：后者通过均衡分配各时间步的去噪难度，并为不同位置序号的令牌设计专属噪声调度方案。实验结果表明，该模型在文本质量和推理时间方面均展现出优异的序列到序列生成性能。
