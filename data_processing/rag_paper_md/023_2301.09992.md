# Multitask Instruction-based Prompting for Fallacy Recognition

链接: http://arxiv.org/abs/2301.09992v1

原文摘要:
Fallacies are used as seemingly valid arguments to support a position and
persuade the audience about its validity. Recognizing fallacies is an
intrinsically difficult task both for humans and machines. Moreover, a big
challenge for computational models lies in the fact that fallacies are
formulated differently across the datasets with differences in the input format
(e.g., question-answer pair, sentence with fallacy fragment), genre (e.g.,
social media, dialogue, news), as well as types and number of fallacies (from 5
to 18 types per dataset). To move towards solving the fallacy recognition task,
we approach these differences across datasets as multiple tasks and show how
instruction-based prompting in a multitask setup based on the T5 model improves
the results against approaches built for a specific dataset such as T5, BERT or
GPT-3. We show the ability of this multitask prompting approach to recognize 28
unique fallacies across domains and genres and study the effect of model size
and prompt choice by analyzing the per-class (i.e., fallacy type) results.
Finally, we analyze the effect of annotation quality on model performance, and
the feasibility of complementing this approach with external knowledge.

中文翻译:
谬误常被用作看似合理的论据来支持某种立场，并说服受众其有效性。无论对人类还是机器而言，识别谬误本质上都是一项极具挑战性的任务。计算模型面临的主要困难在于：不同数据集对谬误的表述方式存在显著差异，包括输入格式（如问答对、含谬误片段的句子）、文本类型（如社交媒体、对话、新闻）以及谬误类型与数量（各数据集涵盖5至18种谬误类型）。为推进谬误识别任务的研究，我们将这些数据集差异视为多任务处理问题，并证明基于T5模型的多任务指令提示方法相较于T5、BERT或GPT-3等针对单一数据集构建的模型能显著提升效果。该研究展示了多任务提示方法跨领域、跨文本类型识别28种独特谬误的能力，通过逐类（即谬误类型）结果分析，探讨了模型规模与提示选择的影响。最后，我们分析了标注质量对模型性能的作用，以及结合外部知识增强该方法可行性的途径。
