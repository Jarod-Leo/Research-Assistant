# Upcycling Instruction Tuning from Dense to Mixture-of-Experts via Parameter Merging

链接: http://arxiv.org/abs/2410.01610v1

原文摘要:
Mixture-of-Experts (MoE) shines brightly in large language models (LLMs) and
demonstrates outstanding performance in plentiful natural language processing
tasks. However, existing methods transforming LLMs from dense to MoE face
significant data requirements and typically rely on large-scale post-training.
In this paper, we propose Upcycling Instruction Tuning (UpIT), a data-efficient
approach for tuning a dense pre-trained model into a MoE instruction model.
Specifically, we first point out that intermediate checkpoints during
instruction tuning of the dense model are naturally suitable for specialized
experts, and then propose an expert expansion stage to flexibly achieve models
with flexible numbers of experts, where genetic algorithm and parameter merging
are introduced to ensure sufficient diversity of new extended experts. To
ensure that each specialized expert in the MoE model works as expected, we
select a small amount of seed data that each expert excels to pre-optimize the
router. Extensive experiments with various data scales and upcycling settings
demonstrate the outstanding performance and data efficiency of UpIT, as well as
stable improvement in expert or data scaling. Further analysis reveals the
importance of ensuring expert diversity in upcycling.

中文翻译:
专家混合模型（MoE）在大语言模型（LLMs）领域表现卓越，在众多自然语言处理任务中展现出优异性能。然而，现有将稠密模型转化为MoE架构的方法面临巨大数据需求，通常依赖大规模的后训练过程。本文提出"指令微调升级法"（UpIT），这是一种数据高效的方法，可将预训练的稠密模型转化为MoE指令模型。

具体而言，我们首先指出：稠密模型在指令微调过程中产生的中间检查点天然适合作为专业专家模块；随后提出专家扩展阶段，通过引入遗传算法和参数融合技术确保新扩展专家的充分多样性，从而灵活构建不同专家数量的模型。为保证MoE模型中每个专业专家按预期工作，我们精选少量各专家擅长的种子数据对路由机制进行预优化。

通过不同数据规模和升级设置的广泛实验表明，UpIT方法兼具卓越性能和超高数据效率，且在专家扩展或数据扩容时均能保持稳定提升。进一步分析揭示了确保专家多样性在模型升级过程中的关键作用。
