# Towards a Foundation Purchasing Model: Pretrained Generative Autoregression on Transaction Sequences

链接: http://arxiv.org/abs/2401.01641v2

原文摘要:
Machine learning models underpin many modern financial systems for use cases
such as fraud detection and churn prediction. Most are based on supervised
learning with hand-engineered features, which relies heavily on the
availability of labelled data. Large self-supervised generative models have
shown tremendous success in natural language processing and computer vision,
yet so far they haven't been adapted to multivariate time series of financial
transactions. In this paper, we present a generative pretraining method that
can be used to obtain contextualised embeddings of financial transactions.
Benchmarks on public datasets demonstrate that it outperforms state-of-the-art
self-supervised methods on a range of downstream tasks. We additionally perform
large-scale pretraining of an embedding model using a corpus of data from 180
issuing banks containing 5.1 billion transactions and apply it to the card
fraud detection problem on hold-out datasets. The embedding model significantly
improves value detection rate at high precision thresholds and transfers well
to out-of-domain distributions.

中文翻译:
机器学习模型已成为现代金融系统的核心支柱，广泛应用于欺诈检测、客户流失预测等场景。当前主流方法依赖于基于人工特征工程的监督学习，其性能高度依赖于标注数据的可获得性。尽管大型自监督生成模型在自然语言处理和计算机视觉领域取得了巨大成功，但迄今为止尚未成功适配金融交易的多变量时间序列数据。本文提出了一种生成式预训练方法，可用于获取金融交易的上下文感知嵌入表示。在公开数据集上的测试表明，该方法在一系列下游任务中超越了当前最先进的自监督学习方法。我们进一步利用来自180家发卡银行的51亿笔交易数据，对嵌入模型进行了大规模预训练，并在保留数据集上应用于信用卡欺诈检测问题。实验证明，该嵌入模型在高精度阈值下显著提升了价值检测率，并展现出优异的跨领域分布迁移能力。
