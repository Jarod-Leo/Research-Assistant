# SPARQL Generation with Entity Pre-trained GPT for KG Question Answering

链接: http://arxiv.org/abs/2402.00969v1

原文摘要:
Knowledge Graphs popularity has been rapidly growing in last years. All that
knowledge is available for people to query it through the many online databases
on the internet. Though, it would be a great achievement if non-programmer
users could access whatever information they want to know. There has been a lot
of effort oriented to solve this task using natural language processing tools
and creativity encouragement by way of many challenges. Our approach focuses on
assuming a correct entity linking on the natural language questions and
training a GPT model to create SPARQL queries from them. We managed to isolate
which property of the task can be the most difficult to solve at few or
zero-shot and we proposed pre-training on all entities (under CWA) to improve
the performance. We obtained a 62.703% accuracy of exact SPARQL matches on
testing at 3-shots, a F1 of 0.809 on the entity linking challenge and a F1 of
0.009 on the question answering challenge.

中文翻译:
近年来，知识图谱的普及度迅速攀升。互联网上众多在线数据库使得这些知识可供人们查询。然而，若能实现非程序员用户也能自由获取所需信息，将是一项重大突破。为此，研究者们投入大量精力，通过自然语言处理工具和创新激励（如各类挑战赛）来攻克这一难题。  

本研究采用的方法聚焦于：在自然语言问题中预设正确的实体链接，并训练GPT模型从中生成SPARQL查询。我们成功识别了该任务在少样本或零样本场景下最具挑战性的属性，进而提出基于封闭世界假设（CWA）的全实体预训练策略以提升性能。实验结果显示：在3样本测试中，SPARQL查询的精确匹配准确率达62.703%；实体链接挑战赛的F1值为0.809；问答挑战赛的F1值为0.009。
