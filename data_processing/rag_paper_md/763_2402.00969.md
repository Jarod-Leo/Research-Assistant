# SPARQL Generation with Entity Pre-trained GPT for KG Question Answering

链接: http://arxiv.org/abs/2402.00969v1

原文摘要:
Knowledge Graphs popularity has been rapidly growing in last years. All that
knowledge is available for people to query it through the many online databases
on the internet. Though, it would be a great achievement if non-programmer
users could access whatever information they want to know. There has been a lot
of effort oriented to solve this task using natural language processing tools
and creativity encouragement by way of many challenges. Our approach focuses on
assuming a correct entity linking on the natural language questions and
training a GPT model to create SPARQL queries from them. We managed to isolate
which property of the task can be the most difficult to solve at few or
zero-shot and we proposed pre-training on all entities (under CWA) to improve
the performance. We obtained a 62.703% accuracy of exact SPARQL matches on
testing at 3-shots, a F1 of 0.809 on the entity linking challenge and a F1 of
0.009 on the question answering challenge.

中文翻译:
以下是符合学术规范的中文翻译：

知识图谱在过去几年中迅速普及。互联网上已有大量在线数据库可供人们查询这些知识。然而，若能实现非程序员用户自由获取所需信息，将是重大突破。当前已有大量研究致力于通过自然语言处理工具解决这一任务，并通过多种挑战赛促进创新。我们的研究方法聚焦于：假设自然语言问题中的实体链接正确，并训练GPT模型将其转换为SPARQL查询。我们成功识别出该任务在少样本或零样本情况下最难解决的特性，提出采用封闭世界假设（CWA）对所有实体进行预训练以提升性能。实验结果显示：在3样本测试中获得了62.703%的SPARQL精确匹配准确率，实体链接挑战赛F1值0.809，问答挑战赛F1值0.009。

（翻译说明：
1. 专业术语处理："Knowledge Graphs"译为通用译名"知识图谱"，"SPARQL"保留原名，"CWA"采用"封闭世界假设"标准译法并首次出现标注英文缩写
2. 长句拆分：将原文复合句按中文表达习惯分解为多个短句
3. 被动语态转换："it would be a great achievement"转为主动句式"将是重大突破"
4. 数据呈现：精确保持原数值格式，技术指标F1值统一使用小数点后三位
5. 学术规范：避免口语化表达，使用"聚焦于""识别出"等学术用语）
