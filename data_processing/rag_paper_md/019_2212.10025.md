# When Federated Learning Meets Pre-trained Language Models' Parameter-Efficient Tuning Methods

链接: http://arxiv.org/abs/2212.10025v1

原文摘要:
With increasing privacy concerns on data, recent studies have made
significant progress using federated learning (FL) on privacy-sensitive natural
language processing (NLP) tasks. Much literature suggests fully fine-tuning
pre-trained language models (PLMs) in the FL paradigm can mitigate the data
heterogeneity problem and close the performance gap with centralized training.
However, large PLMs bring the curse of prohibitive communication overhead and
local model adaptation costs for the FL system. To this end, we introduce
various parameter-efficient tuning (PETuning) methods into federated learning.
Specifically, we provide a holistic empirical study of representative PLMs
tuning methods in FL. The experimental results cover the analysis of data
heterogeneity levels, data scales, and different FL scenarios. Overall
communication overhead can be significantly reduced by locally tuning and
globally aggregating lightweight model parameters while maintaining acceptable
performance in various FL settings. To facilitate the research of PETuning in
FL, we also develop a federated tuning framework FedPETuning, which allows
practitioners to exploit different PETuning methods under the FL training
paradigm conveniently. The source code is available at
\url{https://github.com/iezhuozhuo/FedETuning/tree/deltaTuning}.

中文翻译:
随着数据隐私问题日益受到关注，近期研究在隐私敏感的自然语言处理任务中应用联邦学习（FL）取得了显著进展。大量文献表明，在FL范式下对预训练语言模型（PLMs）进行全参数微调能够缓解数据异构性问题，并缩小与集中式训练的性能差距。然而，大规模PLMs为FL系统带来了通信开销过高和本地模型适配成本激增的难题。为此，我们将多种参数高效调优（PETuning）方法引入联邦学习框架。具体而言，本研究对FL环境下具有代表性的PLMs调优方法进行了系统性实证分析，实验结果涵盖了不同数据异构程度、数据规模及多样化FL场景的对比研究。通过本地调优与全局聚合轻量级模型参数的策略，能在保持各类FL场景可接受性能的同时，显著降低总体通信开销。为促进FL中PETuning技术的研究，我们还开发了联邦调优框架FedPETuning，该框架支持研究者在FL训练范式下便捷地探索不同PETuning方法。源代码已发布于\url{https://github.com/iezhuozhuo/FedETuning/tree/deltaTuning}。
