# AAVENUE: Detecting LLM Biases on NLU Tasks in AAVE via a Novel Benchmark

链接: http://arxiv.org/abs/2408.14845v1

原文摘要:
Detecting biases in natural language understanding (NLU) for African American
Vernacular English (AAVE) is crucial to developing inclusive natural language
processing (NLP) systems. To address dialect-induced performance discrepancies,
we introduce AAVENUE ({AAVE} {N}atural Language {U}nderstanding {E}valuation),
a benchmark for evaluating large language model (LLM) performance on NLU tasks
in AAVE and Standard American English (SAE). AAVENUE builds upon and extends
existing benchmarks like VALUE, replacing deterministic syntactic and
morphological transformations with a more flexible methodology leveraging
LLM-based translation with few-shot prompting, improving performance across our
evaluation metrics when translating key tasks from the GLUE and SuperGLUE
benchmarks. We compare AAVENUE and VALUE translations using five popular LLMs
and a comprehensive set of metrics including fluency, BARTScore, quality,
coherence, and understandability. Additionally, we recruit fluent AAVE speakers
to validate our translations for authenticity. Our evaluations reveal that LLMs
consistently perform better on SAE tasks than AAVE-translated versions,
underscoring inherent biases and highlighting the need for more inclusive NLP
models. We have open-sourced our source code on GitHub and created a website to
showcase our work at https://aavenue.live.

中文翻译:
检测自然语言理解（NLU）系统对非裔美国人白话英语（AAVE）的偏见，对于构建包容性自然语言处理（NLP）模型至关重要。为解决方言导致的性能差异，我们推出AAVENUE基准测试（AAVE自然语言理解评估），用于评估大语言模型（LLM）在AAVE与标准美式英语（SAE）NLU任务中的表现。该基准在VALUE等现有评估体系基础上进行拓展，摒弃了确定性句法形态转换方法，采用基于LLM的少样本提示翻译策略，在转换GLUE和SuperGLUE核心任务时显著提升了各项评估指标表现。我们使用五种主流LLM，通过流畅度、BARTScore、质量、连贯性和可理解性等多维度指标，对AAVENUE与VALUE的翻译结果进行系统对比。研究还招募了AAVE母语者对翻译文本的地道性进行验证。评估结果表明，所有LLM在SAE任务上的表现均优于AAVE转换版本，这一系统性差异揭示了模型固有偏见，凸显了开发更具包容性NLP模型的迫切需求。我们已在GitHub开源项目代码，并建立专题网站https://aavenue.live展示研究成果。
