# A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial Sentiment Analysis

链接: http://arxiv.org/abs/2312.08725v1

原文摘要:
Financial sentiment analysis plays a crucial role in uncovering latent
patterns and detecting emerging trends, enabling individuals to make
well-informed decisions that may yield substantial advantages within the
constantly changing realm of finance. Recently, Large Language Models (LLMs)
have demonstrated their effectiveness in diverse domains, showcasing remarkable
capabilities even in zero-shot and few-shot in-context learning for various
Natural Language Processing (NLP) tasks. Nevertheless, their potential and
applicability in the context of financial sentiment analysis have not been
thoroughly explored yet. To bridge this gap, we employ two approaches:
in-context learning (with a focus on gpt-3.5-turbo model) and fine-tuning LLMs
on a finance-domain dataset. Given the computational costs associated with
fine-tuning LLMs with large parameter sizes, our focus lies on smaller LLMs,
spanning from 250M to 3B parameters for fine-tuning. We then compare the
performances with state-of-the-art results to evaluate their effectiveness in
the finance-domain. Our results demonstrate that fine-tuned smaller LLMs can
achieve comparable performance to state-of-the-art fine-tuned LLMs, even with
models having fewer parameters and a smaller training dataset. Additionally,
the zero-shot and one-shot performance of LLMs produces comparable results with
fine-tuned smaller LLMs and state-of-the-art outcomes. Furthermore, our
analysis demonstrates that there is no observed enhancement in performance for
finance-domain sentiment analysis when the number of shots for in-context
learning is increased.

中文翻译:
金融情感分析在揭示潜在模式和检测新兴趋势方面发挥着关键作用，使个人能够在不断变化的金融领域做出明智决策，从而获得显著优势。近期，大型语言模型（LLMs）在多个领域展现出卓越效能，即便在零样本和少样本的上下文学习中，也能为各类自然语言处理（NLP）任务提供出色表现。然而，其在金融情感分析中的潜力与应用价值尚未得到充分探索。为填补这一空白，我们采用两种方法：上下文学习（以gpt-3.5-turbo模型为重点）和在金融领域数据集上对LLMs进行微调。鉴于大参数量LLMs微调的计算成本较高，我们专注于参数量从2.5亿到30亿的小型LLMs进行微调，并将其性能与最先进成果对比以评估其在金融领域的有效性。研究结果表明，经过微调的小型LLMs即使参数更少、训练数据集较小，也能达到与最先进微调LLMs相媲美的性能。此外，LLMs的零样本和单样本表现与微调小型LLMs及最优结果相当。进一步分析显示，在金融领域情感分析中，增加上下文学习的样本数量并未观察到性能提升。
