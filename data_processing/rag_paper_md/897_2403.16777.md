# Can Machine Translation Bridge Multilingual Pretraining and Cross-lingual Transfer Learning?

链接: http://arxiv.org/abs/2403.16777v1

原文摘要:
Multilingual pretraining and fine-tuning have remarkably succeeded in various
natural language processing tasks. Transferring representations from one
language to another is especially crucial for cross-lingual learning. One can
expect machine translation objectives to be well suited to fostering such
capabilities, as they involve the explicit alignment of semantically equivalent
sentences from different languages. This paper investigates the potential
benefits of employing machine translation as a continued training objective to
enhance language representation learning, bridging multilingual pretraining and
cross-lingual applications. We study this question through two lenses: a
quantitative evaluation of the performance of existing models and an analysis
of their latent representations. Our results show that, contrary to
expectations, machine translation as the continued training fails to enhance
cross-lingual representation learning in multiple cross-lingual natural
language understanding tasks. We conclude that explicit sentence-level
alignment in the cross-lingual scenario is detrimental to cross-lingual
transfer pretraining, which has important implications for future cross-lingual
transfer studies. We furthermore provide evidence through similarity measures
and investigation of parameters that this lack of positive influence is due to
output separability -- which we argue is of use for machine translation but
detrimental elsewhere.

中文翻译:
多语言预训练与微调技术在各类自然语言处理任务中取得了显著成功。实现语言间的表征迁移对于跨语言学习尤为关键。理论上，机器翻译目标函数应能有效促进这种能力，因为它显式地对齐了不同语言中语义等价的句子。本文探讨了将机器翻译作为持续训练目标来增强语言表征学习、衔接多语言预训练与跨语言应用的潜在优势。我们通过双重维度展开研究：对现有模型性能的量化评估及其潜在表征的分析。实验结果表明，与预期相反，在多项跨语言自然语言理解任务中，采用机器翻译进行持续训练并未提升跨语言表征学习效果。我们得出结论：跨语言场景下显式的句子级对齐会损害跨语言迁移预训练效果，这对未来跨语言迁移研究具有重要启示。通过相似性度量和参数分析，我们进一步证明这种积极影响的缺失源于输出可分离性——我们认为该特性虽有利于机器翻译任务，却对其他任务产生负面影响。
