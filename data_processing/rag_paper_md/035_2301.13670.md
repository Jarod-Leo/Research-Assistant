# What Makes Good Examples for Visual In-Context Learning?

链接: http://arxiv.org/abs/2301.13670v2

原文摘要:
Large-scale models trained on broad data have recently become the mainstream
architecture in computer vision due to their strong generalization performance.
In this paper, the main focus is on an emergent ability in large vision models,
known as in-context learning, which allows inference on unseen tasks by
conditioning on in-context examples (a.k.a.~prompt) without updating the model
parameters. This concept has been well-known in natural language processing but
has only been studied very recently for large vision models. We for the first
time provide a comprehensive investigation on the impact of in-context examples
in computer vision, and find that the performance is highly sensitive to the
choice of in-context examples. To overcome the problem, we propose a prompt
retrieval framework to automate the selection of in-context examples.
Specifically, we present (1) an unsupervised prompt retrieval method based on
nearest example search using an off-the-shelf model, and (2) a supervised
prompt retrieval method, which trains a neural network to choose examples that
directly maximize in-context learning performance. The results demonstrate that
our methods can bring non-trivial improvements to visual in-context learning in
comparison to the commonly-used random selection.

中文翻译:
基于广泛数据训练的大规模模型因其卓越的泛化性能，已成为计算机视觉领域的主流架构。本文聚焦于大型视觉模型中的一项新兴能力——上下文学习，该能力通过输入上下文示例（即提示）进行未见任务的推理，而无需更新模型参数。这一概念在自然语言处理中已广为人知，但对大型视觉模型的研究则刚刚起步。我们首次系统探究了上下文示例对计算机视觉任务的影响，发现模型性能对示例选择极为敏感。为解决该问题，我们提出了一种提示检索框架来自动化上下文示例的选择过程。具体包括：（1）基于现成模型的最近邻搜索无监督提示检索方法；（2）监督式提示检索方法，通过训练神经网络直接优化上下文学习性能来选择示例。实验结果表明，相较于常用的随机选择策略，我们的方法能为视觉上下文学习带来显著提升。
