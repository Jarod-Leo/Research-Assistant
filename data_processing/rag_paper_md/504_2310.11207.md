# Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations

链接: http://arxiv.org/abs/2310.11207v1

原文摘要:
Large language models (LLMs) such as ChatGPT have demonstrated superior
performance on a variety of natural language processing (NLP) tasks including
sentiment analysis, mathematical reasoning and summarization. Furthermore,
since these models are instruction-tuned on human conversations to produce
"helpful" responses, they can and often will produce explanations along with
the response, which we call self-explanations. For example, when analyzing the
sentiment of a movie review, the model may output not only the positivity of
the sentiment, but also an explanation (e.g., by listing the sentiment-laden
words such as "fantastic" and "memorable" in the review). How good are these
automatically generated self-explanations? In this paper, we investigate this
question on the task of sentiment analysis and for feature attribution
explanation, one of the most commonly studied settings in the interpretability
literature (for pre-ChatGPT models). Specifically, we study different ways to
elicit the self-explanations, evaluate their faithfulness on a set of
evaluation metrics, and compare them to traditional explanation methods such as
occlusion or LIME saliency maps. Through an extensive set of experiments, we
find that ChatGPT's self-explanations perform on par with traditional ones, but
are quite different from them according to various agreement metrics, meanwhile
being much cheaper to produce (as they are generated along with the
prediction). In addition, we identified several interesting characteristics of
them, which prompt us to rethink many current model interpretability practices
in the era of ChatGPT(-like) LLMs.

中文翻译:
诸如ChatGPT之类的大型语言模型（LLM）在情感分析、数学推理和文本摘要等多种自然语言处理（NLP）任务中展现出卓越性能。这些模型经过对人类对话的指令微调，能够生成"有帮助"的响应，因此它们常常会在输出答案的同时附带解释，我们称之为自我解释。例如，在分析电影评论的情感倾向时，模型不仅会判断情感的积极程度，还可能提供解释（如列举评论中带有情感色彩的词汇，如"精彩绝伦"和"难忘"）。这些自动生成的自我解释质量如何？本文针对情感分析任务和特征归因解释（这是可解释性文献中针对前ChatGPT模型最常研究的设定之一）展开探讨。具体而言，我们研究了多种激发自我解释的方法，通过一系列评估指标检验其忠实度，并将其与传统解释方法（如遮挡法或LIME显著性图）进行比较。通过大量实验发现，ChatGPT的自我解释与传统方法表现相当，但根据多种一致性指标显示其解释存在显著差异，同时生成成本更低（因其与预测结果同步产生）。此外，我们还发现了这些自我解释的若干有趣特性，这些发现促使我们重新思考当前ChatGPT类大模型时代的模型可解释性实践。
