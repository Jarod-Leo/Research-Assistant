# Instruction-following Evaluation through Verbalizer Manipulation

链接: http://arxiv.org/abs/2307.10558v1

原文摘要:
While instruction-tuned models have shown remarkable success in various
natural language processing tasks, accurately evaluating their ability to
follow instructions remains challenging. Existing benchmarks primarily focus on
common instructions that align well with what the model learned during
training. However, proficiency in responding to these instructions does not
necessarily imply strong ability in instruction following. In this paper, we
propose a novel instruction-following evaluation protocol called verbalizer
manipulation. It instructs the model to verbalize the task label with words
aligning with model priors to different extents, adopting verbalizers from
highly aligned (e.g., outputting ``postive'' for positive sentiment), to
minimally aligned (e.g., outputting ``negative'' for positive sentiment).
Verbalizer manipulation can be seamlessly integrated with any classification
benchmark to examine the model's reliance on priors and its ability to override
them to accurately follow the instructions. We conduct a comprehensive
evaluation of four major model families across nine datasets, employing twelve
sets of verbalizers for each of them. We observe that the instruction-following
abilities of models, across different families and scales, are significantly
distinguished by their performance on less natural verbalizers. Even the
strongest GPT-4 model struggles to perform better than random guessing on the
most challenging verbalizer, emphasizing the need for continued advancements to
improve their instruction-following abilities.

中文翻译:
尽管指令调优模型在各类自然语言处理任务中展现出卓越性能，但其遵循指令能力的精准评估仍存在挑战。现有基准测试主要关注与模型训练所学高度契合的常规指令，然而对这些指令的应答熟练度并不等同于强大的指令遵循能力。本文提出一种创新的指令遵循评估方法——表达器调控技术，通过设计不同程度贴合模型先验的标签词表达方案（从高度匹配如用"积极"表示正面情感，到完全背离如用"消极"表示正面情感），系统检验模型克服先验依赖、精准执行指令的能力。该技术可无缝集成至任意分类基准测试，我们在九大数据集上对四大模型家族展开全面评估，每个数据集配置十二组表达器。研究发现：不同架构和规模的模型在非常规表达器上的表现差异显著，即使最强的GPT-4模型在最严苛表达器上的表现也仅优于随机猜测，这凸显了提升指令遵循能力仍需持续突破。
