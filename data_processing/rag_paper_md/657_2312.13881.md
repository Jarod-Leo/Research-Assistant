# Diversifying Knowledge Enhancement of Biomedical Language Models using Adapter Modules and Knowledge Graphs

链接: http://arxiv.org/abs/2312.13881v1

原文摘要:
Recent advances in natural language processing (NLP) owe their success to
pre-training language models on large amounts of unstructured data. Still,
there is an increasing effort to combine the unstructured nature of LMs with
structured knowledge and reasoning. Particularly in the rapidly evolving field
of biomedical NLP, knowledge-enhanced language models (KELMs) have emerged as
promising tools to bridge the gap between large language models and
domain-specific knowledge, considering the available biomedical knowledge
graphs (KGs) curated by experts over the decades. In this paper, we develop an
approach that uses lightweight adapter modules to inject structured biomedical
knowledge into pre-trained language models (PLMs). We use two large KGs, the
biomedical knowledge system UMLS and the novel biochemical ontology OntoChem,
with two prominent biomedical PLMs, PubMedBERT and BioLinkBERT. The approach
includes partitioning knowledge graphs into smaller subgraphs, fine-tuning
adapter modules for each subgraph, and combining the knowledge in a fusion
layer. We test the performance on three downstream tasks: document
classification,question answering, and natural language inference. We show that
our methodology leads to performance improvements in several instances while
keeping requirements in computing power low. Finally, we provide a detailed
interpretation of the results and report valuable insights for future work.

中文翻译:
自然语言处理（NLP）领域近年来的突破性进展，很大程度上得益于对海量非结构化数据的语言模型预训练。然而，越来越多的研究正致力于将语言模型的无序特性与结构化知识及推理能力相结合。在快速发展的生物医学NLP领域，考虑到专家数十年积累的生物医学知识图谱（KGs），知识增强型语言模型（KELMs）已成为弥合大语言模型与领域专业知识之间差距的重要工具。本文提出一种轻量级适配器模块方法，将结构化生物医学知识注入预训练语言模型（PLMs）。我们采用两大知识图谱——生物医学知识系统UMLS和新型生化本体OntoChem，结合两种主流生物医学PLMs（PubMedBERT和BioLinkBERT）进行实验。该方法包括：将知识图谱划分为子图、为每个子图微调适配器模块、通过融合层整合知识。我们在文档分类、问答系统和自然语言推理三个下游任务中测试性能，结果表明该方法在多个场景下显著提升模型表现，同时保持较低算力需求。最后，我们对实验结果进行了深度解析，并为未来研究提供了有价值的见解。
