# Leveraging Large Language Models for Enhanced NLP Task Performance through Knowledge Distillation and Optimized Training Strategies

链接: http://arxiv.org/abs/2402.09282v1

原文摘要:
Emerging Large Language Models (LLMs) like GPT-4 have revolutionized Natural
Language Processing (NLP), showing potential in traditional tasks such as Named
Entity Recognition (NER). Our study explores a three-phase training strategy
that harnesses GPT-4's capabilities to enhance the BERT model's performance on
NER. Initially, GPT-4 annotates a subset of the CONLL2003 and additional BBC
dataset without fine-tuning. We then train BERT using a mix of original and
LLM-annotated data, analyzing the efficacy of LLM annotations against
traditional methods. The second phase involves comparative experiments with
different training regimens, assessing the synergy between distilled and
original data. We observe that sequential strategies, particularly a simple mix
of training first with distilled data followed by original data, significantly
boost performance. In the third phase, we investigate various data blending
techniques, including sigmoid and power decay functions, to optimize the
training process further. Our results indicate that a strategic mix of
distilled and original data markedly elevates the NER capabilities of BERT. Our
approach presents a scalable methodology that reduces manual annotation costs
and increases efficiency, making it especially pertinent in resource-limited
and closed-network environments. The study concludes that while the 'Simple
Mix' strategy yields the best results, understanding its underlying mechanisms
requires further research. Future work will also focus on refining prompt
designs and enhancing annotation selection processes, aiming to extend our
methodology to diverse NLP tasks.

中文翻译:
本研究探讨了一种基于GPT-4的三阶段训练策略，旨在提升BERT模型在命名实体识别（NER）任务中的性能。以GPT-4为代表的大语言模型（LLMs）正在重塑自然语言处理（NLP）领域，其在传统NLP任务中展现出巨大潜力。研究首先在未经微调的情况下，利用GPT-4对CONLL2003数据集子集及BBC补充数据集进行自动标注；随后采用原始数据与LLM标注数据的混合训练BERT模型，系统评估LLM标注与传统标注方法的效能差异。第二阶段通过设计不同训练方案的对比实验，探究知识蒸馏数据与原始数据的协同效应，发现"先蒸馏数据后原始数据"的简单混合策略能显著提升模型表现。第三阶段则研究了包括S型函数和幂衰减函数在内的多种数据混合技术，以进一步优化训练过程。实验结果表明，蒸馏数据与原始数据的策略性混合能有效增强BERT的NER能力。该方法构建了可扩展的技术路径，在降低人工标注成本的同时提升效率，对资源受限和封闭网络环境具有特殊应用价值。研究发现"简单混合"策略虽能获得最佳效果，但其作用机制仍需深入探究。未来工作将聚焦于优化提示词设计和完善标注选择流程，以推动该方法在多样化NLP任务中的应用拓展。
