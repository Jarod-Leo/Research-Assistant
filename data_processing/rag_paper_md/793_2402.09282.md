# Leveraging Large Language Models for Enhanced NLP Task Performance through Knowledge Distillation and Optimized Training Strategies

链接: http://arxiv.org/abs/2402.09282v1

原文摘要:
Emerging Large Language Models (LLMs) like GPT-4 have revolutionized Natural
Language Processing (NLP), showing potential in traditional tasks such as Named
Entity Recognition (NER). Our study explores a three-phase training strategy
that harnesses GPT-4's capabilities to enhance the BERT model's performance on
NER. Initially, GPT-4 annotates a subset of the CONLL2003 and additional BBC
dataset without fine-tuning. We then train BERT using a mix of original and
LLM-annotated data, analyzing the efficacy of LLM annotations against
traditional methods. The second phase involves comparative experiments with
different training regimens, assessing the synergy between distilled and
original data. We observe that sequential strategies, particularly a simple mix
of training first with distilled data followed by original data, significantly
boost performance. In the third phase, we investigate various data blending
techniques, including sigmoid and power decay functions, to optimize the
training process further. Our results indicate that a strategic mix of
distilled and original data markedly elevates the NER capabilities of BERT. Our
approach presents a scalable methodology that reduces manual annotation costs
and increases efficiency, making it especially pertinent in resource-limited
and closed-network environments. The study concludes that while the 'Simple
Mix' strategy yields the best results, understanding its underlying mechanisms
requires further research. Future work will also focus on refining prompt
designs and enhancing annotation selection processes, aiming to extend our
methodology to diverse NLP tasks.

中文翻译:
以GPT-4为代表的大型语言模型（LLMs）正在重塑自然语言处理（NLP）领域，在命名实体识别（NER）等传统任务中展现出巨大潜力。本研究提出一种三阶段训练策略，通过利用GPT-4的能力来提升BERT模型在NER任务中的表现。第一阶段中，未经微调的GPT-4直接对CONLL2003数据集子集及补充的BBC数据集进行标注。随后我们采用原始数据与LLM标注数据的混合样本来训练BERT，系统评估了LLM标注与传统标注方法的效能差异。

第二阶段通过设计不同训练方案的对比实验，探究蒸馏数据与原始数据的协同效应。研究发现，采用"先蒸馏数据后原始数据"的简单混合顺序策略能显著提升模型性能。第三阶段则重点研究了包括S型函数和幂衰减函数在内的多种数据混合技术，以进一步优化训练过程。实验结果表明，蒸馏数据与原始数据的策略性混合能显著增强BERT的NER能力。

本研究提出的方法构建了可扩展的技术路径，在降低人工标注成本的同时提升效率，特别适用于资源受限和封闭网络环境。虽然"简单混合"策略取得了最佳效果，但研究指出其作用机制仍需深入探索。未来工作将聚焦于优化提示词设计和完善标注选择流程，以推动该方法在多样化NLP任务中的应用。
