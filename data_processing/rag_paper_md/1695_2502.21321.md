# LLM Post-Training: A Deep Dive into Reasoning Large Language Models

链接: http://arxiv.org/abs/2502.21321v1

原文摘要:
Large Language Models (LLMs) have transformed the natural language processing
landscape and brought to life diverse applications. Pretraining on vast
web-scale data has laid the foundation for these models, yet the research
community is now increasingly shifting focus toward post-training techniques to
achieve further breakthroughs. While pretraining provides a broad linguistic
foundation, post-training methods enable LLMs to refine their knowledge,
improve reasoning, enhance factual accuracy, and align more effectively with
user intents and ethical considerations. Fine-tuning, reinforcement learning,
and test-time scaling have emerged as critical strategies for optimizing LLMs
performance, ensuring robustness, and improving adaptability across various
real-world tasks. This survey provides a systematic exploration of
post-training methodologies, analyzing their role in refining LLMs beyond
pretraining, addressing key challenges such as catastrophic forgetting, reward
hacking, and inference-time trade-offs. We highlight emerging directions in
model alignment, scalable adaptation, and inference-time reasoning, and outline
future research directions. We also provide a public repository to continually
track developments in this fast-evolving field:
https://github.com/mbzuai-oryx/Awesome-LLM-Post-training.

中文翻译:
以下是符合要求的学术化中文翻译：

大型语言模型（LLMs）彻底改变了自然语言处理领域的发展格局，并催生了多样化的应用场景。虽然基于海量网络数据的预训练为这些模型奠定了基础，但研究界正日益将关注焦点转向训练后优化技术以寻求进一步突破。预训练提供了广泛的语言学基础，而训练后方法能使LLMs精炼知识体系、提升推理能力、增强事实准确性，并更有效地适应用户意图与伦理要求。微调技术、强化学习以及测试时缩放等策略已成为优化LLM性能、确保模型鲁棒性、提高现实任务适应性的关键手段。本综述系统性地探讨了训练后优化方法体系，分析了其在预训练基础上提升LLMs的作用机制，重点解决了灾难性遗忘、奖励破解和推理时权衡等核心挑战。我们着重阐述了模型对齐、可扩展适应和推理时思维链等新兴研究方向，并展望了未来研究路径。同时建立了开源知识库持续追踪这一快速演进领域的最新进展：https://github.com/mbzuai-oryx/Awesome-LLM-Post-training

翻译说明：
1. 专业术语处理：LLMs统一译为"大型语言模型"并保留英文缩写，专业术语如"fine-tuning"译为"微调技术"符合中文文献惯例
2. 句式重构：将英文长句拆解为符合中文表达习惯的短句结构（如将"Pretraining...foundation"处理为转折句式）
3. 学术表达："post-training"统一译为"训练后"而非字面直译，保持术语一致性
4. 概念显化："reward hacking"译为"奖励破解"而非直译，准确传达对抗训练含义
5. 动态词处理："brought to life"转化为"催生"，"shifting focus"译为"将关注焦点转向"体现学术文本特征
6. 链接保留：完整保留原始GitHub链接格式，符合技术文献规范
