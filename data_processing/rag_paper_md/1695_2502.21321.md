# LLM Post-Training: A Deep Dive into Reasoning Large Language Models

链接: http://arxiv.org/abs/2502.21321v1

原文摘要:
Large Language Models (LLMs) have transformed the natural language processing
landscape and brought to life diverse applications. Pretraining on vast
web-scale data has laid the foundation for these models, yet the research
community is now increasingly shifting focus toward post-training techniques to
achieve further breakthroughs. While pretraining provides a broad linguistic
foundation, post-training methods enable LLMs to refine their knowledge,
improve reasoning, enhance factual accuracy, and align more effectively with
user intents and ethical considerations. Fine-tuning, reinforcement learning,
and test-time scaling have emerged as critical strategies for optimizing LLMs
performance, ensuring robustness, and improving adaptability across various
real-world tasks. This survey provides a systematic exploration of
post-training methodologies, analyzing their role in refining LLMs beyond
pretraining, addressing key challenges such as catastrophic forgetting, reward
hacking, and inference-time trade-offs. We highlight emerging directions in
model alignment, scalable adaptation, and inference-time reasoning, and outline
future research directions. We also provide a public repository to continually
track developments in this fast-evolving field:
https://github.com/mbzuai-oryx/Awesome-LLM-Post-training.

中文翻译:
大型语言模型（LLMs）彻底改变了自然语言处理领域，催生出多样化的应用。基于海量网络数据的预训练为这些模型奠定了基础，而当前研究界正日益将焦点转向训练后优化技术以寻求进一步突破。预训练虽提供了广泛的语言基础，但训练后方法能使LLMs精炼知识、提升推理能力、增强事实准确性，并更有效地契合用户意图与伦理考量。微调、强化学习和测试时扩展已成为优化模型性能、确保鲁棒性以及提升跨领域任务适应性的关键策略。本综述系统梳理了训练后优化方法体系，剖析其在预训练基础上提升LLMs的作用机理，重点探讨灾难性遗忘、奖励破解和推理时权衡等核心挑战。我们聚焦模型对齐、可扩展适应和推理时推理等新兴方向，并展望未来研究路径。同时建立了开源知识库持续追踪这一快速发展领域的最新进展：https://github.com/mbzuai-oryx/Awesome-LLM-Post-training。
