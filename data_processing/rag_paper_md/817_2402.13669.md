# Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning

链接: http://arxiv.org/abs/2402.13669v1

原文摘要:
The surge in Large Language Models (LLMs) has revolutionized natural language
processing, but fine-tuning them for specific tasks often encounters challenges
in balancing performance and preserving general instruction-following
abilities. In this paper, we posit that the distribution gap between task
datasets and the LLMs serves as the primary underlying cause. To address the
problem, we introduce Self-Distillation Fine-Tuning (SDFT), a novel approach
that bridges the distribution gap by guiding fine-tuning with a distilled
dataset generated by the model itself to match its original distribution.
Experimental results on the Llama-2-chat model across various benchmarks
demonstrate that SDFT effectively mitigates catastrophic forgetting while
achieving comparable or superior performance on downstream tasks compared to
the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain
the helpfulness and safety alignment of LLMs. Our code is available at
https://github.com/sail-sg/sdft.

中文翻译:
大型语言模型(LLMs)的崛起为自然语言处理带来了革命性变革，但在针对特定任务进行微调时，往往面临性能与通用指令遵循能力难以兼顾的挑战。本文提出任务数据集与LLMs之间的分布差异是导致这一问题的根本原因。为解决该问题，我们创新性地提出了自蒸馏微调(SDFT)方法，通过引导模型使用其自身生成的蒸馏数据集进行微调，使数据分布与原始模型相匹配，从而弥合分布鸿沟。基于Llama-2-chat模型在多个基准测试上的实验表明，相比传统微调方法，SDFT不仅能有效缓解灾难性遗忘现象，在下游任务中取得相当或更优的性能表现，还展现出保持LLMs帮助性和安全对齐特性的潜力。相关代码已开源在https://github.com/sail-sg/sdft。
