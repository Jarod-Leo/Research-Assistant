# Comparative Analysis of Pooling Mechanisms in LLMs: A Sentiment Analysis Perspective

链接: http://arxiv.org/abs/2411.14654v1

原文摘要:
Large Language Models (LLMs) have revolutionized natural language processing
(NLP) by delivering state-of-the-art performance across a variety of tasks.
Among these, Transformer-based models like BERT and GPT rely on pooling layers
to aggregate token-level embeddings into sentence-level representations. Common
pooling mechanisms such as Mean, Max, and Weighted Sum play a pivotal role in
this aggregation process. Despite their widespread use, the comparative
performance of these strategies on different LLM architectures remains
underexplored. To address this gap, this paper investigates the effects of
these pooling mechanisms on two prominent LLM families -- BERT and GPT, in the
context of sentence-level sentiment analysis. Comprehensive experiments reveal
that each pooling mechanism exhibits unique strengths and weaknesses depending
on the task's specific requirements. Our findings underline the importance of
selecting pooling methods tailored to the demands of particular applications,
prompting a re-evaluation of common assumptions regarding pooling operations.
By offering actionable insights, this study contributes to the optimization of
LLM-based models for downstream tasks.

中文翻译:
大型语言模型（LLMs）通过在各种自然语言处理（NLP）任务中实现最先进的性能，彻底改变了该领域。其中，基于Transformer架构的BERT和GPT等模型依赖池化层将词元级嵌入聚合为句子级表征。均值池化、最大值池化和加权求和等常见机制在这一聚合过程中起着关键作用。尽管应用广泛，这些策略在不同LLM架构上的性能对比仍缺乏深入研究。为填补这一空白，本文以句子级情感分析为背景，探究了这些池化机制对BERT和GPT两大主流模型家族的影响。综合实验表明，每种池化机制根据任务特定需求展现出独特的优势与局限。研究结果揭示了针对具体应用需求选择适配池化方法的重要性，促使学界重新评估关于池化操作的常规假设。通过提供可操作的见解，本研究为优化基于LLM的下游任务模型提供了重要参考。
