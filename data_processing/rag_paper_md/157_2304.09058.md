# Revisiting k-NN for Pre-trained Language Models

链接: http://arxiv.org/abs/2304.09058v1

原文摘要:
Pre-trained Language Models (PLMs), as parametric-based eager learners, have
become the de-facto choice for current paradigms of Natural Language Processing
(NLP). In contrast, k-Nearest-Neighbor (kNN) classifiers, as the lazy learning
paradigm, tend to mitigate over-fitting and isolated noise. In this paper, we
revisit kNN classifiers for augmenting the PLMs-based classifiers. From the
methodological level, we propose to adopt kNN with textual representations of
PLMs in two steps: (1) Utilize kNN as prior knowledge to calibrate the training
process. (2) Linearly interpolate the probability distribution predicted by kNN
with that of the PLMs' classifier. At the heart of our approach is the
implementation of kNN-calibrated training, which treats predicted results as
indicators for easy versus hard examples during the training process. From the
perspective of the diversity of application scenarios, we conduct extensive
experiments on fine-tuning, prompt-tuning paradigms and zero-shot, few-shot and
fully-supervised settings, respectively, across eight diverse end-tasks. We
hope our exploration will encourage the community to revisit the power of
classical methods for efficient NLP. Code and datasets are available in
https://github.com/zjunlp/Revisit-KNN.

中文翻译:
预训练语言模型（PLMs）作为基于参数的主动学习范式，已成为当前自然语言处理（NLP）领域的事实标准。相比之下，k近邻（kNN）分类器作为惰性学习范式，能有效缓解过拟合与孤立噪声问题。本文创新性地将kNN分类器与基于PLMs的分类器进行融合：在方法层面，我们提出分两步利用PLMs文本表示构建kNN分类器——首先将kNN预测结果作为先验知识校准模型训练过程，随后对kNN与PLMs分类器的概率分布进行线性插值。该方案的核心在于实施kNN校准训练机制，通过预测结果动态区分训练样本的难易程度。在应用场景多样性方面，我们在8个不同终端任务上系统验证了方案在微调范式、提示调优范式以及零样本、少样本和全监督设置下的普适性。本研究旨在推动学界重新审视经典方法在高效NLP中的价值。代码与数据集已开源于https://github.com/zjunlp/Revisit-KNN。
