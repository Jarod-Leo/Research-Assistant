# G-Adapter: Towards Structure-Aware Parameter-Efficient Transfer Learning for Graph Transformer Networks

链接: http://arxiv.org/abs/2305.10329v1

原文摘要:
It has become a popular paradigm to transfer the knowledge of large-scale
pre-trained models to various downstream tasks via fine-tuning the entire model
parameters. However, with the growth of model scale and the rising number of
downstream tasks, this paradigm inevitably meets the challenges in terms of
computation consumption and memory footprint issues. Recently,
Parameter-Efficient Fine-Tuning (PEFT) (e.g., Adapter, LoRA, BitFit) shows a
promising paradigm to alleviate these concerns by updating only a portion of
parameters. Despite these PEFTs having demonstrated satisfactory performance in
natural language processing, it remains under-explored for the question of
whether these techniques could be transferred to graph-based tasks with Graph
Transformer Networks (GTNs). Therefore, in this paper, we fill this gap by
providing extensive benchmarks with traditional PEFTs on a range of graph-based
downstream tasks. Our empirical study shows that it is sub-optimal to directly
transfer existing PEFTs to graph-based tasks due to the issue of feature
distribution shift. To address this issue, we propose a novel structure-aware
PEFT approach, named G-Adapter, which leverages graph convolution operation to
introduce graph structure (e.g., graph adjacent matrix) as an inductive bias to
guide the updating process. Besides, we propose Bregman proximal point
optimization to further alleviate feature distribution shift by preventing the
model from aggressive update. Extensive experiments demonstrate that G-Adapter
obtains the state-of-the-art performance compared to the counterparts on nine
graph benchmark datasets based on two pre-trained GTNs, and delivers tremendous
memory footprint efficiency compared to the conventional paradigm.

中文翻译:
当前，通过微调整个预训练大模型参数来迁移知识至下游任务已成为主流范式。然而随着模型规模扩大和下游任务数量激增，该范式在计算消耗和内存占用方面面临严峻挑战。近期，参数高效微调方法（如Adapter、LoRA、BitFit）通过仅更新部分参数展现出解决这些问题的潜力。尽管这些方法在自然语言处理中表现优异，但其在图 Transformer 网络（GTNs）的图任务中的适用性仍待探索。为此，本文系统评估了传统参数高效微调方法在多种图下游任务上的表现，实证研究表明：由于特征分布偏移问题，直接迁移现有方法会导致次优结果。针对该问题，我们提出新型结构感知参数高效微调框架G-Adapter，通过图卷积操作引入图结构（如邻接矩阵）作为归纳偏置来指导参数更新。此外，采用Bregman近端点优化算法抑制激进更新，进一步缓解特征分布偏移。基于两个预训练GTNs在九个图基准数据集上的实验表明，G-Adapter不仅性能优于现有方法，相较传统全参数微调范式更能显著提升内存使用效率。
