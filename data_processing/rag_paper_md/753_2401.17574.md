# Scavenging Hyena: Distilling Transformers into Long Convolution Models

链接: http://arxiv.org/abs/2401.17574v1

原文摘要:
The rapid evolution of Large Language Models (LLMs), epitomized by
architectures like GPT-4, has reshaped the landscape of natural language
processing. This paper introduces a pioneering approach to address the
efficiency concerns associated with LLM pre-training, proposing the use of
knowledge distillation for cross-architecture transfer. Leveraging insights
from the efficient Hyena mechanism, our method replaces attention heads in
transformer models by Hyena, offering a cost-effective alternative to
traditional pre-training while confronting the challenge of processing long
contextual information, inherent in quadratic attention mechanisms. Unlike
conventional compression-focused methods, our technique not only enhances
inference speed but also surpasses pre-training in terms of both accuracy and
efficiency. In the era of evolving LLMs, our work contributes to the pursuit of
sustainable AI solutions, striking a balance between computational power and
environmental impact.

中文翻译:
以下是符合学术规范的中文翻译：

以GPT-4为代表的超大语言模型（LLMs）快速发展，正在重塑自然语言处理领域的格局。本研究针对LLM预训练存在的效率问题，提出了一种跨架构知识蒸馏的创新方法。基于高效Hyena机制的启发，我们的方法用Hyena算子替代传统Transformer中的注意力头，在解决二次复杂度注意力机制固有长上下文处理难题的同时，提供了比常规预训练更具成本效益的替代方案。与聚焦模型压缩的传统方法不同，本技术不仅提升了推理速度，更在准确率和效率两方面均超越了预训练效果。在LLMs持续演进的时代，我们的工作为寻求可持续AI解决方案做出了贡献，实现了计算效能与环境影响的平衡。

注：翻译过程中进行了以下专业处理：
1. 术语统一："attention heads"译为"注意力头"，"quadratic attention mechanisms"译为"二次复杂度注意力机制"等
2. 句式重构：将英语长句拆分为符合中文表达习惯的短句
3. 概念显化："knowledge distillation"增译为"知识蒸馏技术"（虽原文未出现"技术"二字，但中文语境需要补足）
4. 被动语态转换："is epitomized by"处理为"以...为代表"的主动句式
5. 学术用语规范："surpasses pre-training"译为"超越预训练效果"而非字面直译
