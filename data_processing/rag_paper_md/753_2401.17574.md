# Scavenging Hyena: Distilling Transformers into Long Convolution Models

链接: http://arxiv.org/abs/2401.17574v1

原文摘要:
The rapid evolution of Large Language Models (LLMs), epitomized by
architectures like GPT-4, has reshaped the landscape of natural language
processing. This paper introduces a pioneering approach to address the
efficiency concerns associated with LLM pre-training, proposing the use of
knowledge distillation for cross-architecture transfer. Leveraging insights
from the efficient Hyena mechanism, our method replaces attention heads in
transformer models by Hyena, offering a cost-effective alternative to
traditional pre-training while confronting the challenge of processing long
contextual information, inherent in quadratic attention mechanisms. Unlike
conventional compression-focused methods, our technique not only enhances
inference speed but also surpasses pre-training in terms of both accuracy and
efficiency. In the era of evolving LLMs, our work contributes to the pursuit of
sustainable AI solutions, striking a balance between computational power and
environmental impact.

中文翻译:
以GPT-4等架构为代表的大语言模型（LLM）快速发展，重塑了自然语言处理领域的格局。本文提出了一种创新方法，通过跨架构知识蒸馏技术解决LLM预训练的效率问题。基于高效Hyena机制的启发，我们的方法用Hyena算子替代传统Transformer中的注意力头，在应对二次复杂度注意力机制固有的长上下文处理挑战同时，为传统预训练提供了更具成本效益的替代方案。与聚焦模型压缩的传统方法不同，该技术不仅提升了推理速度，更在准确率和效率方面双双超越预训练效果。在LLM持续演进的时代，本研究为平衡计算效能与环境影响提供了可持续的AI解决方案。
