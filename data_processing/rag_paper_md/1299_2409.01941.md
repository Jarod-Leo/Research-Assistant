# Towards Leveraging Large Language Models for Automated Medical Q&A Evaluation

链接: http://arxiv.org/abs/2409.01941v1

原文摘要:
This paper explores the potential of using Large Language Models (LLMs) to
automate the evaluation of responses in medical Question and Answer (Q\&A)
systems, a crucial form of Natural Language Processing. Traditionally, human
evaluation has been indispensable for assessing the quality of these responses.
However, manual evaluation by medical professionals is time-consuming and
costly. Our study examines whether LLMs can reliably replicate human
evaluations by using questions derived from patient data, thereby saving
valuable time for medical experts. While the findings suggest promising
results, further research is needed to address more specific or complex
questions that were beyond the scope of this initial investigation.

中文翻译:
本文探讨了利用大型语言模型（LLM）自动化评估医疗问答系统中回答质量的潜力，这是自然语言处理中的关键环节。传统上，人工评估对于判断这些回答的质量不可或缺。然而，由医学专业人员进行的评估既耗时又成本高昂。我们的研究通过使用源自患者数据的问题，检验LLM能否可靠复现人类评估结果，从而为医学专家节省宝贵时间。尽管研究结果显示出积极前景，但针对本初步研究未涉及的更专业或复杂问题，仍需开展进一步研究。
