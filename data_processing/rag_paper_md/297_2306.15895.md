# Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias

链接: http://arxiv.org/abs/2306.15895v1

原文摘要:
Large language models (LLMs) have been recently leveraged as training data
generators for various natural language processing (NLP) tasks. While previous
research has explored different approaches to training models using generated
data, they generally rely on simple class-conditional prompts, which may limit
the diversity of the generated data and inherit systematic biases of LLM. Thus,
we investigate training data generation with diversely attributed prompts
(e.g., specifying attributes like length and style), which have the potential
to yield diverse and attributed generated data. Our investigation focuses on
datasets with high cardinality and diverse domains, wherein we demonstrate that
attributed prompts outperform simple class-conditional prompts in terms of the
resulting model's performance. Additionally, we present a comprehensive
empirical study on data generation encompassing vital aspects like bias,
diversity, and efficiency, and highlight three key observations: firstly,
synthetic datasets generated by simple prompts exhibit significant biases, such
as regional bias; secondly, attribute diversity plays a pivotal role in
enhancing model performance; lastly, attributed prompts achieve the performance
of simple class-conditional prompts while utilizing only 5\% of the querying
cost of ChatGPT associated with the latter. The data and code are available on
\url{https://github.com/yueyu1030/AttrPrompt}.

中文翻译:
以下是符合要求的学术摘要中文翻译：

大型语言模型（LLMs）近期被广泛用作各类自然语言处理（NLP）任务的训练数据生成器。尽管已有研究探索了多种基于生成数据的模型训练方法，但这些方法通常依赖简单的类别条件提示，这既可能限制生成数据的多样性，又会继承LLM的系统性偏差。为此，我们研究了基于多样化属性提示（如指定文本长度、风格等属性）的训练数据生成方法，该方法有望产生具有特定属性且多样化的生成数据。我们的研究聚焦于高基数、多领域的数据集，实验证明属性提示在模型性能表现上优于简单类别条件提示。此外，我们开展了一项涵盖偏差性、多样性与效率等关键维度的综合性数据生成实证研究，并揭示三个重要发现：首先，简单提示生成的合成数据集存在显著偏差（如地域偏见）；其次，属性多样性对提升模型性能具有关键作用；最后，属性提示仅需消耗ChatGPT在类别条件提示下5%的查询成本，即可达到同等性能水平。相关数据与代码已发布于\url{https://github.com/yueyu1030/AttrPrompt}。

（翻译说明：
1. 专业术语准确统一："cardinality"译为"基数"，"systematic biases"译为"系统性偏差"
2. 被动语态转化："have been leveraged"转为主动态"被广泛用作"
3. 长句拆分：将原文复合句分解为符合中文表达习惯的短句结构
4. 学术规范：保留技术概念"类别条件提示"的精确表述，括号补充说明属性示例
5. 数据呈现：精确转换百分比数值和成本比较表述
6. 文献引用：完整保留原始GitHub链接格式）
