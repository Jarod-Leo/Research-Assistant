# VI-OOD: A Unified Representation Learning Framework for Textual Out-of-distribution Detection

链接: http://arxiv.org/abs/2404.06217v1

原文摘要:
Out-of-distribution (OOD) detection plays a crucial role in ensuring the
safety and reliability of deep neural networks in various applications. While
there has been a growing focus on OOD detection in visual data, the field of
textual OOD detection has received less attention. Only a few attempts have
been made to directly apply general OOD detection methods to natural language
processing (NLP) tasks, without adequately considering the characteristics of
textual data. In this paper, we delve into textual OOD detection with
Transformers. We first identify a key problem prevalent in existing OOD
detection methods: the biased representation learned through the maximization
of the conditional likelihood $p(y\mid x)$ can potentially result in subpar
performance. We then propose a novel variational inference framework for OOD
detection (VI-OOD), which maximizes the likelihood of the joint distribution
$p(x, y)$ instead of $p(y\mid x)$. VI-OOD is tailored for textual OOD detection
by efficiently exploiting the representations of pre-trained Transformers.
Through comprehensive experiments on various text classification tasks, VI-OOD
demonstrates its effectiveness and wide applicability. Our code has been
released at \url{https://github.com/liam0949/LLM-OOD}.

中文翻译:
分布外（OOD）检测在确保深度神经网络于各类应用中的安全性与可靠性方面起着关键作用。尽管视觉数据的OOD检测日益受到关注，但文本领域的OOD检测研究仍显不足。目前仅有少数尝试将通用OOD检测方法直接应用于自然语言处理（NLP）任务，而未充分考虑文本数据的特性。本文深入探究基于Transformer架构的文本OOD检测，首先揭示了现有方法普遍存在的核心问题：通过最大化条件概率$p(y\mid x)$学习到的有偏表示可能导致性能欠佳。为此，我们提出了一种新颖的变分推断框架VI-OOD，该框架转而最大化联合分布$p(x, y)$的似然函数，并通过高效利用预训练Transformer的表示能力，专门针对文本OOD检测任务进行优化。在多类文本分类任务上的全面实验表明，VI-OOD具有显著效果与广泛适用性。相关代码已发布于\url{https://github.com/liam0949/LLM-OOD}。
