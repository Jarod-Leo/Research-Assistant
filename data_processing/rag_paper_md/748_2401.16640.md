# TeenyTinyLlama: open-source tiny language models trained in Brazilian Portuguese

链接: http://arxiv.org/abs/2401.16640v1

原文摘要:
Large language models (LLMs) have significantly advanced natural language
processing, but their progress has yet to be equal across languages. While most
LLMs are trained in high-resource languages like English, multilingual models
generally underperform monolingual ones. Additionally, aspects of their
multilingual foundation sometimes restrict the byproducts they produce, like
computational demands and licensing regimes. In this study, we document the
development of open-foundation models tailored for use in low-resource
settings, their limitations, and their benefits. This is the TeenyTinyLlama
pair: two compact models for Brazilian Portuguese text generation. We release
them under the permissive Apache 2.0 license on GitHub and Hugging Face for
community use and further development. See
https://github.com/Nkluge-correa/TeenyTinyLlama

中文翻译:
以下是符合要求的学术摘要中文翻译：

大语言模型（LLMs）显著推动了自然语言处理的发展，但其进步在不同语言间并不均衡。尽管大多数LLMs使用英语等高资源语言进行训练，但多语言模型的性能通常逊于单语模型。此外，其多语言基础架构的某些特性会限制衍生品的应用，例如计算资源需求和许可协议。本研究系统记录了两款专为低资源环境设计的开放基础模型的开发过程、技术局限与应用优势：即面向巴西葡萄牙语文本生成的TeenyTinyLlama微型模型对。我们以Apache 2.0开源许可协议在GitHub和Hugging Face平台发布该模型，供社区使用与持续开发。项目详情参见：https://github.com/Nkluge-correa/TeenyTinyLlama

（翻译严格遵循以下原则：
1. 专业术语准确统一："low-resource settings"译为"低资源环境"，"permissive license"译为"开源许可协议"
2. 被动语态转换："are trained"译为主动式"使用...进行训练"
3. 长句拆分重组：将原文复合句分解为符合中文表达习惯的短句结构
4. 学术规范保持：保留技术名词首字母缩写（LLMs）及版本号（Apache 2.0）
5. 文化适配："Brazilian Portuguese"明确译为"巴西葡萄牙语"以区分欧洲葡萄牙语）
