# TeenyTinyLlama: open-source tiny language models trained in Brazilian Portuguese

链接: http://arxiv.org/abs/2401.16640v1

原文摘要:
Large language models (LLMs) have significantly advanced natural language
processing, but their progress has yet to be equal across languages. While most
LLMs are trained in high-resource languages like English, multilingual models
generally underperform monolingual ones. Additionally, aspects of their
multilingual foundation sometimes restrict the byproducts they produce, like
computational demands and licensing regimes. In this study, we document the
development of open-foundation models tailored for use in low-resource
settings, their limitations, and their benefits. This is the TeenyTinyLlama
pair: two compact models for Brazilian Portuguese text generation. We release
them under the permissive Apache 2.0 license on GitHub and Hugging Face for
community use and further development. See
https://github.com/Nkluge-correa/TeenyTinyLlama

中文翻译:
大型语言模型（LLMs）在自然语言处理领域取得了显著进展，但其发展进程尚未实现跨语言的均衡。尽管多数LLMs以英语等高资源语言进行训练，多语言模型的表现通常逊色于单语模型。此外，其多语言基础架构的某些特性会限制衍生品的应用范围，例如计算资源需求和许可协议约束。本研究记录了两款专为低资源环境设计的开源基础模型的开发过程、局限性及优势——即TeenyTinyLlama双模型：针对巴西葡萄牙语文本生成的两个紧凑型模型。我们依据宽松的Apache 2.0许可证在GitHub和Hugging Face平台发布，供社区使用与持续开发。详见https://github.com/Nkluge-correa/TeenyTinyLlama。
