# A Survey of Vision Transformers in Autonomous Driving: Current Trends and Future Directions

链接: http://arxiv.org/abs/2403.07542v1

原文摘要:
This survey explores the adaptation of visual transformer models in
Autonomous Driving, a transition inspired by their success in Natural Language
Processing. Surpassing traditional Recurrent Neural Networks in tasks like
sequential image processing and outperforming Convolutional Neural Networks in
global context capture, as evidenced in complex scene recognition, Transformers
are gaining traction in computer vision. These capabilities are crucial in
Autonomous Driving for real-time, dynamic visual scene processing. Our survey
provides a comprehensive overview of Vision Transformer applications in
Autonomous Driving, focusing on foundational concepts such as self-attention,
multi-head attention, and encoder-decoder architecture. We cover applications
in object detection, segmentation, pedestrian detection, lane detection, and
more, comparing their architectural merits and limitations. The survey
concludes with future research directions, highlighting the growing role of
Vision Transformers in Autonomous Driving.

中文翻译:
本调查探讨了视觉Transformer模型在自动驾驶领域的适应性转变，这一变革源于其在自然语言处理中的成功应用。相较于传统循环神经网络在序列图像处理任务中的表现，以及超越卷积神经网络在全局上下文捕捉（如复杂场景识别）方面的能力，Transformer在计算机视觉领域正日益受到关注。这些特性对于自动驾驶实时动态视觉场景处理至关重要。我们全面综述了视觉Transformer在自动驾驶中的应用，重点解析自注意力机制、多头注意力及编码器-解码器架构等核心概念，涵盖目标检测、图像分割、行人识别、车道检测等应用场景，并对比分析不同架构的优势与局限。最后提出未来研究方向，强调视觉Transformer在自动驾驶领域日益重要的作用。
