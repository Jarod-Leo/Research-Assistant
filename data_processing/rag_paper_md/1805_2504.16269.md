# COBRA: Algorithm-Architecture Co-optimized Binary Transformer Accelerator for Edge Inference

链接: http://arxiv.org/abs/2504.16269v1

原文摘要:
Transformer-based models have demonstrated superior performance in various
fields, including natural language processing and computer vision. However,
their enormous model size and high demands in computation, memory, and
communication limit their deployment to edge platforms for local, secure
inference. Binary transformers offer a compact, low-complexity solution for
edge deployment with reduced bandwidth needs and acceptable accuracy. However,
existing binary transformers perform inefficiently on current hardware due to
the lack of binary specific optimizations. To address this, we introduce COBRA,
an algorithm-architecture co-optimized binary Transformer accelerator for edge
computing. COBRA features a real 1-bit binary multiplication unit, enabling
matrix operations with -1, 0, and +1 values, surpassing ternary methods. With
further hardware-friendly optimizations in the attention block, COBRA achieves
up to 3,894.7 GOPS throughput and 448.7 GOPS/Watt energy efficiency on edge
FPGAs, delivering a 311x energy efficiency improvement over GPUs and a 3.5x
throughput improvement over the state-of-the-art binary accelerator, with only
negligible inference accuracy degradation.

中文翻译:
基于Transformer的模型在自然语言处理和计算机视觉等多个领域展现出卓越性能，但其庞大的模型规模及对计算、内存与通信的高需求，限制了其在边缘平台本地化安全推理的部署。二值化Transformer通过压缩模型、降低复杂度，为边缘部署提供了带宽需求减少且精度可接受的解决方案。然而，现有二值化Transformer因缺乏针对二值计算的专门优化，在当前硬件上运行效率低下。为此，我们提出COBRA——一种面向边缘计算的算法-架构协同优化二值化Transformer加速器。COBRA采用真正的1比特二进制乘法单元，支持包含-1、0和+1值的矩阵运算，性能超越三值化方法。通过对注意力模块进行硬件友好优化，COBRA在边缘FPGA上实现了3,894.7 GOPS的吞吐量和448.7 GOPS/W的能效，较GPU提升311倍能效，比现有最优二值加速器提高3.5倍吞吐量，且推理精度损失可忽略不计。
