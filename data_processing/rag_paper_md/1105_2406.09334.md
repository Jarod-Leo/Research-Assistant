# ProxyLM: Predicting Language Model Performance on Multilingual Tasks via Proxy Models

链接: http://arxiv.org/abs/2406.09334v2

原文摘要:
Performance prediction is a method to estimate the performance of Language
Models (LMs) on various Natural Language Processing (NLP) tasks, mitigating
computational costs associated with model capacity and data for fine-tuning.
Our paper presents ProxyLM, a scalable task- and language-agnostic framework
designed to predict the performance of LMs using proxy models. These proxy
models act as surrogates, approximating the performance of the LM of interest.
By leveraging these proxy models, ProxyLM significantly reduces computational
overhead in task evaluations, achieving up to a 37.08x speedup over traditional
methods, even with our smallest proxy models. Our results across multiple
multilingual NLP tasks and various robustness tests demonstrate that ProxyLM
not only adapts well to previously unseen languages in pre-trained LMs, but
also generalizes effectively across different datasets, outperforming the
state-of-the-art by at least 1.78x in terms of root-mean-square error (RMSE).

中文翻译:
性能预测是一种评估语言模型（LMs）在各类自然语言处理（NLP）任务中表现的方法，旨在降低因模型容量和微调数据带来的计算成本。本文提出ProxyLM框架——一个可扩展的、与任务及语言无关的代理模型系统，通过构建代理模型来预测目标LM的性能表现。这些代理模型作为替代品，能够近似目标LM的实际表现。借助代理模型，ProxyLM在任务评估中显著减少了计算开销，即使使用最小规模的代理模型，相比传统方法仍能实现最高37.08倍的加速效果。我们在多语言NLP任务和多种鲁棒性测试中的结果表明，ProxyLM不仅能良好适配预训练LM中未见过的新语言，还能有效泛化至不同数据集，其均方根误差（RMSE）指标至少优于现有最佳方法1.78倍。
