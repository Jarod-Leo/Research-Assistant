# Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space

链接: http://arxiv.org/abs/2402.12026v1

原文摘要:
Despite the notable success of language models (LMs) in various natural
language processing (NLP) tasks, the reliability of LMs is susceptible to
backdoor attacks. Prior research attempts to mitigate backdoor learning while
training the LMs on the poisoned dataset, yet struggles against complex
backdoor attacks in real-world scenarios. In this paper, we investigate the
learning mechanisms of backdoor LMs in the frequency space by Fourier analysis.
Our findings indicate that the backdoor mapping presented on the poisoned
datasets exhibits a more discernible inclination towards lower frequency
compared to clean mapping, resulting in the faster convergence of backdoor
mapping. To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation
(MuScleLoRA), which deploys multiple radial scalings in the frequency space
with low-rank adaptation to the target model and further aligns the gradients
when updating parameters. Through downscaling in the frequency space,
MuScleLoRA encourages the model to prioritize the learning of relatively
high-frequency clean mapping, consequently mitigating backdoor learning.
Experimental results demonstrate that MuScleLoRA outperforms baselines
significantly. Notably, MuScleLoRA reduces the average success rate of diverse
backdoor attacks to below 15\% across multiple datasets and generalizes to
various backbone LMs, including BERT, RoBERTa, GPT2-XL, and Llama2. The codes
are publicly available at https://github.com/ZrW00/MuScleLoRA.

中文翻译:
尽管语言模型（LMs）在各种自然语言处理（NLP）任务中取得了显著成功，但其可靠性易受后门攻击威胁。现有研究尝试在有毒数据集上训练LMs时减轻后门学习，但在现实复杂攻击场景中仍面临挑战。本文通过傅里叶分析研究了后门LMs在频率空间中的学习机制。研究发现，相较于干净映射，有毒数据集呈现的后门映射在低频区域表现出更明显的倾向性，导致后门映射更快收敛。为缓解这一问题，我们提出多尺度低秩适配方法（MuScleLoRA），该方法在频率空间部署多个径向缩放因子，结合目标模型的低秩适配技术，并在参数更新时进一步对齐梯度。通过在频率空间进行降尺度处理，MuScleLoRA促使模型优先学习相对高频的干净映射，从而有效抑制后门学习。实验结果表明，MuScleLoRA显著优于基线方法。值得注意的是，该方法将多种后门攻击的平均成功率降至15%以下（跨多个数据集测试），并适用于BERT、RoBERTa、GPT2-XL和Llama2等不同骨干语言模型。代码已开源：https://github.com/ZrW00/MuScleLoRA。
