# Neural Comprehension: Language Models with Compiled Neural Networks

链接: http://arxiv.org/abs/2304.01665v1

原文摘要:
Language models' (LMs) proficiency in handling deterministic symbolic
reasoning and rule-based tasks remains limited due to their dependency implicit
learning on textual data. To endow LMs with genuine rule comprehension
abilities, we propose "Neural Comprehension" - a framework that synergistically
integrates compiled neural networks (CoNNs) into the standard transformer
architecture. CoNNs are neural modules designed to explicitly encode rules
through artificially generated attention weights. By incorporating CoNN
modules, the Neural Comprehension framework enables LMs to accurately and
robustly execute rule-intensive symbolic tasks. Extensive experiments
demonstrate the superiority of our approach over existing techniques in terms
of length generalization, efficiency, and interpretability for symbolic
operations. Furthermore, it can be applied to LMs across different model
scales, outperforming tool-calling methods in arithmetic reasoning tasks while
maintaining superior inference efficiency. Our work highlights the potential of
seamlessly unifying explicit rule learning via CoNNs and implicit pattern
learning in LMs, paving the way for true symbolic comprehension capabilities.

中文翻译:
由于语言模型（LMs）依赖文本数据进行隐式学习，其在处理确定性符号推理和基于规则的任务方面仍存在局限。为赋予LMs真正的规则理解能力，我们提出"神经理解"框架——通过将编译神经网络（CoNNs）协同整合至标准Transformer架构中。CoNNs是专为通过人工生成注意力权重显式编码规则而设计的神经模块。该框架通过嵌入CoNN模块，使LMs能精准且稳健地执行规则密集型符号任务。大量实验表明，在符号操作的长度泛化性、效率及可解释性方面，本方法显著优于现有技术。此外，该框架可适配不同规模的LMs，在算术推理任务中超越工具调用方法的同时保持更优的推理效率。本研究揭示了通过CoNNs实现显式规则学习与LMs隐式模式学习无缝融合的潜力，为构建真正的符号理解能力开辟了新路径。
