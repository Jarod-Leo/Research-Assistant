# ALPS: Improved Optimization for Highly Sparse One-Shot Pruning for Large Language Models

链接: http://arxiv.org/abs/2406.07831v1

原文摘要:
The impressive performance of Large Language Models (LLMs) across various
natural language processing tasks comes at the cost of vast computational
resources and storage requirements. One-shot pruning techniques offer a way to
alleviate these burdens by removing redundant weights without the need for
retraining. Yet, the massive scale of LLMs often forces current pruning
approaches to rely on heuristics instead of optimization-based techniques,
potentially resulting in suboptimal compression. In this paper, we introduce
ALPS, an optimization-based framework that tackles the pruning problem using
the operator splitting technique and a preconditioned conjugate gradient-based
post-processing step. Our approach incorporates novel techniques to accelerate
and theoretically guarantee convergence while leveraging vectorization and GPU
parallelism for efficiency. ALPS substantially outperforms state-of-the-art
methods in terms of the pruning objective and perplexity reduction,
particularly for highly sparse models. On the OPT-30B model with 70% sparsity,
ALPS achieves a 13% reduction in test perplexity on the WikiText dataset and a
19% improvement in zero-shot benchmark performance compared to existing
methods.

中文翻译:
大型语言模型（LLM）在各类自然语言处理任务中的卓越表现，是以消耗海量计算资源和存储需求为代价的。一次性剪枝技术通过移除冗余权重且无需重新训练，为缓解这一负担提供了解决方案。然而，LLM的庞大规模往往迫使现有剪枝方法依赖启发式策略而非基于优化的技术，这可能导致次优的压缩效果。本文提出ALPS框架，该优化驱动方案运用算子分裂技术和基于预条件共轭梯度的后处理步骤来解决剪枝问题。我们通过创新技术加速并理论保证收敛性，同时利用向量化和GPU并行实现高效处理。在剪枝目标和困惑度降低指标上，ALPS显著优于当前最先进方法，尤其在高稀疏度模型中表现突出。以70%稀疏度的OPT-30B模型为例，相比现有方法，ALPS在WikiText数据集上实现了测试困惑度降低13%，在零样本基准测试中性能提升19%。
