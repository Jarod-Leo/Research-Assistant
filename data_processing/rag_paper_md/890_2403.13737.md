# EthioLLM: Multilingual Large Language Models for Ethiopian Languages with Task Evaluation

链接: http://arxiv.org/abs/2403.13737v1

原文摘要:
Large language models (LLMs) have gained popularity recently due to their
outstanding performance in various downstream Natural Language Processing (NLP)
tasks. However, low-resource languages are still lagging behind current
state-of-the-art (SOTA) developments in the field of NLP due to insufficient
resources to train LLMs. Ethiopian languages exhibit remarkable linguistic
diversity, encompassing a wide array of scripts, and are imbued with profound
religious and cultural significance. This paper introduces EthioLLM --
multilingual large language models for five Ethiopian languages (Amharic,
Ge'ez, Afan Oromo, Somali, and Tigrinya) and English, and Ethiobenchmark -- a
new benchmark dataset for various downstream NLP tasks. We evaluate the
performance of these models across five downstream NLP tasks. We open-source
our multilingual language models, new benchmark datasets for various downstream
tasks, and task-specific fine-tuned language models and discuss the performance
of the models. Our dataset and models are available at the
https://huggingface.co/EthioNLP repository.

中文翻译:
大型语言模型（LLMs）凭借其在各类下游自然语言处理（NLP）任务中的卓越表现，近年来广受关注。然而，由于缺乏足够的训练资源，低资源语言在NLP领域仍落后于当前最先进（SOTA）的发展水平。埃塞俄比亚语言展现出显著的多样性，涵盖多种文字体系，并承载着深厚的宗教与文化内涵。本文推出了EthioLLM——支持五种埃塞俄比亚语言（阿姆哈拉语、吉兹语、奥罗莫语、索马里语和提格雷语）及英语的多语言大模型，以及Ethiobenchmark——一个针对多种下游NLP任务的新基准数据集。我们评估了这些模型在五项下游NLP任务中的表现，并开源了多语言模型、各任务的新基准数据集及任务专用微调模型，同时分析了模型性能。相关数据集与模型已发布于https://huggingface.co/EthioNLP仓库。
