# Systematic Weight Evaluation for Pruning Large Language Models: Enhancing Performance and Sustainability

链接: http://arxiv.org/abs/2502.17071v1

原文摘要:
The exponential growth of large language models (LLMs) like ChatGPT has
revolutionized artificial intelligence, offering unprecedented capabilities in
natural language processing. However, the extensive computational resources
required for training these models have significant environmental implications,
including high carbon emissions, energy consumption, and water usage. This
research presents a novel approach to LLM pruning, focusing on the systematic
evaluation of individual weight importance throughout the training process. By
monitoring parameter evolution over time, we propose a method that effectively
reduces model size without compromising performance. Extensive experiments with
both a scaled-down LLM and a large multimodal model reveal that moderate
pruning enhances efficiency and reduces loss, while excessive pruning
drastically deteriorates model performance. These findings highlight the
critical need for optimized AI models to ensure sustainable development,
balancing technological advancement with environmental responsibility.

中文翻译:
以ChatGPT为代表的大型语言模型(LLM)的指数级发展正在重塑人工智能领域，为自然语言处理带来了前所未有的能力。然而训练这些模型所需的庞大计算资源会引发显著环境问题，包括高碳排放、能源消耗与水资源使用。本研究提出了一种创新的LLM剪枝方法，其核心在于系统评估训练过程中各权重参数的重要性演变。通过追踪参数随时间的动态变化，我们开发出能在保持模型性能前提下有效压缩规模的技术。针对小型LLM和大型多模态模型的系列实验表明：适度剪枝可提升效率并降低损失值，而过度剪枝则会导致模型性能急剧恶化。这些发现凸显了优化人工智能模型的迫切需求，必须在技术进步与环境保护之间取得平衡，以实现可持续发展。
