# Functionality learning through specification instructions

链接: http://arxiv.org/abs/2311.08481v1

原文摘要:
Test suites assess natural language processing models' performance on
specific functionalities: cases of interest involving model robustness,
fairness, or particular linguistic capabilities. This paper introduces
specification instructions: text descriptions specifying fine-grained
task-specific behaviors. For each functionality in a suite, we generate an
instruction that describes it. We combine the specification instructions to
create specification-augmented prompts, which we feed to language models
pre-trained on natural instruction data.
  We conduct experiments to measure how optimizing for some functionalities may
negatively impact functionalities that are not covered by the specification
set. Our analyses across four tasks and models of diverse sizes and families
show that smaller models struggle to follow specification instructions.
However, larger models (>~3B params.) can benefit from specifications and --
surprisingly -- even generalize certain desirable behaviors across
functionalities.

中文翻译:
测试套件用于评估自然语言处理模型在特定功能上的表现：这些功能涉及模型鲁棒性、公平性或特定语言能力的相关案例。本文提出"规范指令"概念：即通过文本描述来细化特定任务行为。针对测试套件中的每项功能，我们生成相应的描述性指令。通过整合这些规范指令，我们构建出规范增强型提示文本，并将其输入至经过自然指令数据预训练的语言模型中。

我们通过实验测量了优化某些功能时，可能对规范指令集未覆盖功能产生的负面影响。针对四项任务及不同规模、架构的模型分析表明：较小规模模型难以遵循规范指令。然而，较大规模模型（参数＞~30亿）不仅能从规范中获益，更令人惊讶的是——它们甚至能将某些理想行为推广到其他功能上。
