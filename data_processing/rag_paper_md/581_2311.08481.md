# Functionality learning through specification instructions

链接: http://arxiv.org/abs/2311.08481v1

原文摘要:
Test suites assess natural language processing models' performance on
specific functionalities: cases of interest involving model robustness,
fairness, or particular linguistic capabilities. This paper introduces
specification instructions: text descriptions specifying fine-grained
task-specific behaviors. For each functionality in a suite, we generate an
instruction that describes it. We combine the specification instructions to
create specification-augmented prompts, which we feed to language models
pre-trained on natural instruction data.
  We conduct experiments to measure how optimizing for some functionalities may
negatively impact functionalities that are not covered by the specification
set. Our analyses across four tasks and models of diverse sizes and families
show that smaller models struggle to follow specification instructions.
However, larger models (>~3B params.) can benefit from specifications and --
surprisingly -- even generalize certain desirable behaviors across
functionalities.

中文翻译:
测试套件用于评估自然语言处理模型在特定功能上的表现，这些功能涉及模型鲁棒性、公平性或特定语言能力的关注案例。本文提出了一种规范指令方法：通过文本描述来细化任务具体行为。针对套件中的每项功能，我们生成一条描述该功能的指令。通过整合这些规范指令，我们构建了规范增强型提示，并将其输入至经过自然指令数据预训练的语言模型中。

我们通过实验测量了优化某些功能可能对规范集未覆盖功能产生的负面影响。针对四项任务及不同规模、架构的模型分析表明，较小模型难以遵循规范指令。然而，较大模型（参数>~30亿）不仅能从规范中获益，更令人惊讶的是——甚至能将某些理想行为泛化至其他功能。
