# Do Language Models Understand Morality? Towards a Robust Detection of Moral Content

链接: http://arxiv.org/abs/2406.04143v1

原文摘要:
The task of detecting moral values in text has significant implications in
various fields, including natural language processing, social sciences, and
ethical decision-making. Previously proposed supervised models often suffer
from overfitting, leading to hyper-specialized moral classifiers that struggle
to perform well on data from different domains. To address this issue, we
introduce novel systems that leverage abstract concepts and common-sense
knowledge acquired from Large Language Models and Natural Language Inference
models during previous stages of training on multiple data sources. By doing
so, we aim to develop versatile and robust methods for detecting moral values
in real-world scenarios. Our approach uses the GPT 3.5 model as a zero-shot
ready-made unsupervised multi-label classifier for moral values detection,
eliminating the need for explicit training on labeled data. We compare it with
a smaller NLI-based zero-shot model. The results show that the NLI approach
achieves competitive results compared to the Davinci model. Furthermore, we
conduct an in-depth investigation of the performance of supervised systems in
the context of cross-domain multi-label moral value detection. This involves
training supervised models on different domains to explore their effectiveness
in handling data from different sources and comparing their performance with
the unsupervised methods. Our contributions encompass a thorough analysis of
both supervised and unsupervised methodologies for cross-domain value
detection. We introduce the Davinci model as a state-of-the-art zero-shot
unsupervised moral values classifier, pushing the boundaries of moral value
detection without the need for explicit training on labeled data. Additionally,
we perform a comparative evaluation of our approach with the supervised models,
shedding light on their respective strengths and weaknesses.

中文翻译:
文本道德价值观检测任务在自然语言处理、社会科学及伦理决策等多个领域具有重要应用价值。现有监督模型常出现过拟合问题，导致生成的道德分类器过度专业化，难以适应跨领域数据的检测需求。为解决这一问题，我们提出创新性系统，通过整合大型语言模型和自然语言推理模型在前期多源数据训练中习得的抽象概念与常识知识，旨在开发适用于真实场景的通用鲁棒型道德价值观检测方法。本研究中，我们采用GPT-3.5模型作为零样本现成的无监督多标签道德分类器，无需依赖标注数据的显式训练，并与基于自然语言推理的小型零样本模型进行对比。实验结果表明，自然语言推理方法相较于Davinci模型展现出可比性能。此外，我们深入探究监督系统在跨领域多标签道德检测中的表现：通过在不同领域训练监督模型，评估其处理异源数据的有效性，并与无监督方法进行性能对比。本研究的主要贡献包括：1）对跨领域价值观检测的监督与无监督方法进行全面分析；2）将Davinci模型确立为当前最先进的零样本无监督道德分类器，突破无需标注数据训练的检测边界；3）通过监督模型的对比评估，揭示不同方法的优势与局限性。
