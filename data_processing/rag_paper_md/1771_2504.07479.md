# UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache Pruning for Efficient Long-Context LLM Inference

链接: http://arxiv.org/abs/2504.07479v1

原文摘要:
Transformer-based large language models (LLMs) have achieved impressive
performance in various natural language processing (NLP) applications. However,
the high memory and computation cost induced by the KV cache limits the
inference efficiency, especially for long input sequences. Compute-in-memory
(CIM)-based accelerators have been proposed for LLM acceleration with KV cache
pruning. However, as existing accelerators only support static pruning with a
fixed pattern or dynamic pruning with primitive implementations, they suffer
from either high accuracy degradation or low efficiency. In this paper, we
propose a ferroelectric FET (FeFET)-based unified content addressable memory
(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous
support for static and dynamic pruning with 3 computation modes: 1) in the CAM
mode, UniCAIM enables approximate similarity measurement in O(1) time for
dynamic KV cache pruning with high energy efficiency; 2) in the charge-domain
CIM mode, static pruning can be supported based on accumulative similarity
score, which is much more flexible compared to fixed patterns; 3) in the
current-domain mode, exact attention computation can be conducted with a subset
of selected KV cache. We further propose a novel CAM/CIM cell design that
leverages the multi-level characteristics of FeFETs for signed multibit storage
of the KV cache and in-place attention computation. With extensive experimental
results, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)
by 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit
level, along with high accuracy comparable with dense attention at the
application level, showing its great potential for efficient long-context LLM
inference.

中文翻译:
基于Transformer的大型语言模型（LLMs）在各种自然语言处理（NLP）任务中展现出卓越性能，但其KV缓存机制导致的高内存与计算开销严重制约了推理效率，尤其对长输入序列更为明显。现有基于存内计算（CIM）的加速器虽提出通过KV缓存剪枝来优化LLM推理，但由于仅支持固定模式的静态剪枝或实现简陋的动态剪枝，普遍存在精度损失大或效率低下的问题。本文创新性地提出一种基于铁电场效应晶体管（FeFET）的统一内容可寻址存储器（CAM）与CIM架构——UniCAIM，其具备三大计算模式：1）CAM模式下可实现O(1)时间复杂度的近似相似度测量，支持高能效动态KV缓存剪枝；2）电荷域CIM模式下基于累积相似度得分实现比固定模式更灵活的静态剪枝；3）电流域模式下可对筛选后的KV缓存子集执行精确注意力计算。通过利用FeFET的多级特性设计新型CAM/CIM单元，实现了KV缓存的有符号多位存储及原位注意力计算。实验表明，UniCAIM在电路层面将面积-能耗-时延积（AEDP）较现有CIM加速器降低8.2-831倍，同时在应用层面保持与稠密注意力相当的高精度，为长上下文LLM推理提供了高效解决方案。
