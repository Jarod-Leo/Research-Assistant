# Few shot clinical entity recognition in three languages: Masked language models outperform LLM prompting

链接: http://arxiv.org/abs/2402.12801v1

原文摘要:
Large language models (LLMs) have become the preferred solution for many
natural language processing tasks. In low-resource environments such as
specialized domains, their few-shot capabilities are expected to deliver high
performance. Named Entity Recognition (NER) is a critical task in information
extraction that is not covered in recent LLM benchmarks. There is a need for
better understanding the performance of LLMs for NER in a variety of settings
including languages other than English. This study aims to evaluate generative
LLMs, employed through prompt engineering, for few-shot clinical NER. %from the
perspective of F1 performance and environmental impact. We compare 13
auto-regressive models using prompting and 16 masked models using fine-tuning
on 14 NER datasets covering English, French and Spanish. While prompt-based
auto-regressive models achieve competitive F1 for general NER, they are
outperformed within the clinical domain by lighter biLSTM-CRF taggers based on
masked models. Additionally, masked models exhibit lower environmental impact
compared to auto-regressive models. Findings are consistent across the three
languages studied, which suggests that LLM prompting is not yet suited for NER
production in the clinical domain.

中文翻译:
大型语言模型（LLMs）已成为众多自然语言处理任务的首选解决方案。在专业领域等低资源环境中，其小样本学习能力被寄予厚望。命名实体识别（NER）作为信息抽取的核心任务，却未在近期LLM基准测试中得到充分关注。当前亟需全面评估LLMs在英语及其他语言环境下执行NER任务的表现。本研究旨在通过提示工程方法，评估生成式LLMs在临床NER任务中的小样本学习性能。我们对比了13种基于提示的自回归模型与16种基于微调的掩码语言模型，测试范围涵盖英语、法语和西班牙语的14个NER数据集。研究发现：虽然自回归模型在通用领域NER任务中能达到具有竞争力的F1值，但在临床领域，基于掩码模型的轻量级biLSTM-CRF标注器表现更优。此外，掩码模型的环境影响显著低于自回归模型。这一结论在所研究的三种语言中呈现一致性，表明基于提示的LLM方法目前尚未适用于临床领域的NER实际应用。
