# Sentence Simplification via Large Language Models

链接: http://arxiv.org/abs/2302.11957v1

原文摘要:
Sentence Simplification aims to rephrase complex sentences into simpler
sentences while retaining original meaning. Large Language models (LLMs) have
demonstrated the ability to perform a variety of natural language processing
tasks. However, it is not yet known whether LLMs can be served as a
high-quality sentence simplification system. In this work, we empirically
analyze the zero-/few-shot learning ability of LLMs by evaluating them on a
number of benchmark test sets. Experimental results show LLMs outperform
state-of-the-art sentence simplification methods, and are judged to be on a par
with human annotators.

中文翻译:
句子简化旨在将复杂句子重述为简单句，同时保留原意。大型语言模型（LLMs）已展现出执行多种自然语言处理任务的能力，但其能否作为高质量的句子简化系统尚未可知。本研究通过在多组基准测试集上的评估，实证分析了LLMs的零样本/小样本学习能力。实验结果表明，LLMs不仅优于当前最先进的句子简化方法，其表现更被判定达到人类标注者的水平。

（翻译说明：
1. 专业术语处理："zero-/few-shot learning"译为"零样本/小样本学习"，符合NLP领域规范
2. 句式重构：将原文复合句拆分为符合中文表达习惯的短句，如将"it is not yet known..."处理为独立短句
3. 被动语态转换："are judged to be"译为主动式"被判定"
4. 学术用语统一："state-of-the-art"统一译为"最先进的"
5. 逻辑显化：通过"不仅...更..."的递进结构强化实验结果对比关系
6. 文化适配："on a par with"译为"达到...水平"更符合中文评价体系表达）
