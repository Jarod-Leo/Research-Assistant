# Towards Reasoning in Large Language Models via Multi-Agent Peer Review Collaboration

链接: http://arxiv.org/abs/2311.08152v1

原文摘要:
Large Language Models (LLMs) have shown remarkable capabilities in general
natural language processing tasks but often fall short in complex reasoning
tasks. Recent studies have explored human-like problem-solving strategies, such
as self-correct, to push further the boundary of single-model reasoning
ability. In this work, we let a single model "step outside the box" by engaging
multiple models to correct each other. We introduce a multi-agent collaboration
strategy that emulates the academic peer review process. Each agent
independently constructs its own solution, provides reviews on the solutions of
others, and assigns confidence levels to its reviews. Upon receiving peer
reviews, agents revise their initial solutions. Extensive experiments on three
different types of reasoning tasks show that our collaboration approach
delivers superior accuracy across all ten datasets compared to existing
methods. Further study underscores the effectiveness of integrating confidence
in reviews, demonstrates the superiority of feedback exchange over mere
solution sharing, and highlights the role of capability and diversity in
fostering successful collaboration.

中文翻译:
大型语言模型（LLMs）在通用自然语言处理任务中展现出卓越能力，但在复杂推理任务上往往表现欠佳。近期研究探索了类人化问题解决策略（如自我修正），以进一步突破单一模型的推理能力边界。本研究通过让多个模型相互修正，实现单一模型"跳出思维定式"。我们提出一种模拟学术同行评审流程的多智能体协作策略：每个智能体独立构建解决方案，对其他方案进行评审并为评审意见赋予置信度。收到同行评审后，各智能体修正初始方案。在三种不同类型推理任务上的大量实验表明，相比现有方法，我们的协作策略在全部十个数据集上均实现了更高的准确率。进一步研究证实了评审置信度整合的有效性，论证了反馈交换机制优于单纯方案共享，同时揭示了智能体能力与多样性对成功协作的关键作用。
