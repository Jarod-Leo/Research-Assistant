# Challenges in Guardrailing Large Language Models for Science

链接: http://arxiv.org/abs/2411.08181v1

原文摘要:
The rapid development in large language models (LLMs) has transformed the
landscape of natural language processing and understanding (NLP/NLU), offering
significant benefits across various domains. However, when applied to
scientific research, these powerful models exhibit critical failure modes
related to scientific integrity and trustworthiness. Existing general-purpose
LLM guardrails are insufficient to address these unique challenges in the
scientific domain. We provide comprehensive guidelines for deploying LLM
guardrails in the scientific domain. We identify specific challenges --
including time sensitivity, knowledge contextualization, conflict resolution,
and intellectual property concerns -- and propose a guideline framework for the
guardrails that can align with scientific needs. These guardrail dimensions
include trustworthiness, ethics & bias, safety, and legal aspects. We also
outline in detail the implementation strategies that employ white-box,
black-box, and gray-box methodologies that can be enforced within scientific
contexts.

中文翻译:
大型语言模型（LLM）的快速发展重塑了自然语言处理与理解（NLP/NLU）的格局，为各领域带来了显著效益。然而当这类强大模型应用于科研场景时，却暴露出与科学诚信和可信度相关的关键缺陷模式。现有通用LLM防护机制难以应对科学领域的特殊挑战。本文提出科学领域部署LLM防护机制的完整指南：首先剖析了时间敏感性、知识情境化、冲突消解和知识产权等核心挑战，继而构建了符合科学需求的防护框架，涵盖可信度、伦理与偏见、安全性及法律合规等维度。我们详细阐述了适用于科研场景的白盒、黑盒与灰盒三类实施策略，为科学界提供了可操作的技术路线。
