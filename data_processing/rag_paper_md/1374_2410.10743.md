# NT-LLM: A Novel Node Tokenizer for Integrating Graph Structure into Large Language Models

链接: http://arxiv.org/abs/2410.10743v1

原文摘要:
Graphs are a fundamental data structure for representing relationships in
real-world scenarios. With the success of Large Language Models (LLMs) across
various natural language processing (NLP) tasks, there has been growing
interest in integrating LLMs for graph learning. However, applying LLMs to
graph-related tasks poses significant challenges, as these models are not
inherently designed to capture the complex structural information present in
graphs. Existing approaches address this challenge through two strategies: the
chain of tasks approach, which uses Graph Neural Networks (GNNs) to encode the
graph structure so that LLMs are relieved from understanding spatial positions;
and Graph-to-Text Conversion, which translates graph structures into semantic
text representations that LLMs can process. Despite their progress, these
methods often struggle to fully preserve the topological information of graphs
or require extensive computational resources, limiting their practical
applicability.
  In this work, we introduce Node Tokenizer for Large Language Models (NT-LLM),
a novel framework that efficiently encodes graph structures by selecting key
nodes as anchors and representing each node based on its relative distance to
these anchors. This position-anchored encoding effectively captures the graph
topology, enabling enhanced reasoning capabilities in LLMs over graph data.
Additionally, we implement a task-specific tuning procedure to further improve
structural understanding within LLMs. Through extensive empirical evaluations,
NT-LLM demonstrates significant performance improvements across a variety of
graph-related tasks.

中文翻译:
图是用于表示现实场景中关系的基础数据结构。随着大语言模型（LLM）在各类自然语言处理（NLP）任务中的成功应用，研究者日益关注如何将LLM整合到图学习领域。然而，将LLM应用于图相关任务面临重大挑战，因为这些模型本质上并非为捕捉图中复杂的结构信息而设计。现有方法通过两种策略应对这一挑战：任务链式方法利用图神经网络（GNN）编码图结构，使LLM无需理解空间位置；图到文本转换则将图结构转化为LLM可处理的语义文本表示。尽管取得进展，这些方法往往难以完整保留图的拓扑信息，或需要大量计算资源，限制了实际应用价值。

本文提出面向大语言模型的节点标记器（NT-LLM），该创新框架通过选取关键节点作为锚点，并根据各节点与锚点的相对距离进行表征，从而高效编码图结构。这种位置锚定编码能有效捕捉图拓扑特征，增强LLM对图数据的推理能力。此外，我们还实施了任务特定调优流程以进一步提升LLM的结构理解能力。大量实证评估表明，NT-LLM在多种图相关任务中均展现出显著的性能提升。
