# Mamba-360: Survey of State Space Models as Transformer Alternative for Long Sequence Modelling: Methods, Applications, and Challenges

链接: http://arxiv.org/abs/2404.16112v1

原文摘要:
Sequence modeling is a crucial area across various domains, including Natural
Language Processing (NLP), speech recognition, time series forecasting, music
generation, and bioinformatics. Recurrent Neural Networks (RNNs) and Long Short
Term Memory Networks (LSTMs) have historically dominated sequence modeling
tasks like Machine Translation, Named Entity Recognition (NER), etc. However,
the advancement of transformers has led to a shift in this paradigm, given
their superior performance. Yet, transformers suffer from $O(N^2)$ attention
complexity and challenges in handling inductive bias. Several variations have
been proposed to address these issues which use spectral networks or
convolutions and have performed well on a range of tasks. However, they still
have difficulty in dealing with long sequences. State Space Models(SSMs) have
emerged as promising alternatives for sequence modeling paradigms in this
context, especially with the advent of S4 and its variants, such as S4nd,
Hippo, Hyena, Diagnol State Spaces (DSS), Gated State Spaces (GSS), Linear
Recurrent Unit (LRU), Liquid-S4, Mamba, etc. In this survey, we categorize the
foundational SSMs based on three paradigms namely, Gating architectures,
Structural architectures, and Recurrent architectures. This survey also
highlights diverse applications of SSMs across domains such as vision, video,
audio, speech, language (especially long sequence modeling), medical (including
genomics), chemical (like drug design), recommendation systems, and time series
analysis, including tabular data. Moreover, we consolidate the performance of
SSMs on benchmark datasets like Long Range Arena (LRA), WikiText, Glue, Pile,
ImageNet, Kinetics-400, sstv2, as well as video datasets such as Breakfast,
COIN, LVU, and various time series datasets. The project page for Mamba-360
work is available on this webpage.\url{https://github.com/badripatro/mamba360}.

中文翻译:
序列建模是自然语言处理（NLP）、语音识别、时间序列预测、音乐生成和生物信息学等多个领域的核心研究方向。传统上，循环神经网络（RNN）和长短期记忆网络（LSTM）在机器翻译、命名实体识别（NER）等序列建模任务中占据主导地位。然而，随着Transformer模型凭借其卓越性能的崛起，这一格局发生了转变。但Transformer存在$O(N^2)$的注意力复杂度问题，且在处理归纳偏置方面面临挑战。为此，研究者提出了多种改进方案，如采用谱网络或卷积结构的变体，这些方法在一系列任务中表现良好，但仍难以应对长序列建模。

在此背景下，状态空间模型（SSMs）作为序列建模的新范式崭露头角，尤其是S4及其衍生模型（如S4nd、Hippo、Hyena、对角状态空间（DSS）、门控状态空间（GSS）、线性循环单元（LRU）、Liquid-S4、Mamba等）的出现。本综述将基础SSMs划分为三大架构范式：门控架构、结构架构和循环架构，并系统梳理了SSMs在视觉、视频、音频、语音、语言（特别是长序列建模）、医疗（包括基因组学）、化学（如药物设计）、推荐系统以及时间序列分析（含表格数据）等领域的多样化应用。

此外，我们整合了SSMs在Long Range Arena（LRA）、WikiText、Glue、Pile、ImageNet、Kinetics-400、sstv2等基准数据集，以及Breakfast、COIN、LVU等视频数据集和各种时间序列数据集上的性能表现。Mamba-360项目的详细内容可通过以下网页获取：\url{https://github.com/badripatro/mamba360}。
