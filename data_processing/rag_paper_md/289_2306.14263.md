# Revolutionizing Cyber Threat Detection with Large Language Models

链接: http://arxiv.org/abs/2306.14263v1

原文摘要:
The field of Natural Language Processing (NLP) is currently undergoing a
revolutionary transformation driven by the power of pre-trained Large Language
Models (LLMs) based on groundbreaking Transformer architectures. As the
frequency and diversity of cybersecurity attacks continue to rise, the
importance of incident detection has significantly increased. IoT devices are
expanding rapidly, resulting in a growing need for efficient techniques to
autonomously identify network-based attacks in IoT networks with both high
precision and minimal computational requirements. This paper presents
SecurityBERT, a novel architecture that leverages the Bidirectional Encoder
Representations from Transformers (BERT) model for cyber threat detection in
IoT networks. During the training of SecurityBERT, we incorporated a novel
privacy-preserving encoding technique called Privacy-Preserving Fixed-Length
Encoding (PPFLE). We effectively represented network traffic data in a
structured format by combining PPFLE with the Byte-level Byte-Pair Encoder
(BBPE) Tokenizer. Our research demonstrates that SecurityBERT outperforms
traditional Machine Learning (ML) and Deep Learning (DL) methods, such as
Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs), in
cyber threat detection. Employing the Edge-IIoTset cybersecurity dataset, our
experimental analysis shows that SecurityBERT achieved an impressive 98.2%
overall accuracy in identifying fourteen distinct attack types, surpassing
previous records set by hybrid solutions such as GAN-Transformer-based
architectures and CNN-LSTM models. With an inference time of less than 0.15
seconds on an average CPU and a compact model size of just 16.7MB, SecurityBERT
is ideally suited for real-life traffic analysis and a suitable choice for
deployment on resource-constrained IoT devices.

中文翻译:
自然语言处理（NLP）领域正经历一场由基于Transformer架构的预训练大语言模型（LLMs）驱动的革命性变革。随着网络安全攻击频率和多样性的持续攀升，事件检测的重要性显著提升。物联网设备快速普及，亟需能在保持高精度与低计算开销的前提下，自主识别物联网网络中基于网络攻击的高效技术。本文提出SecurityBERT——一种创新架构，利用Transformer双向编码器表征（BERT）模型实现物联网网络中的网络威胁检测。在模型训练阶段，我们引入名为"隐私保护定长编码"（PPFLE）的新型隐私保护编码技术，通过将PPFLE与字节级字节对编码器（BBPE）分词器相结合，有效实现了网络流量数据的结构化表征。研究表明，SecurityBERT在威胁检测任务中表现优于传统机器学习（ML）和深度学习方法（如卷积神经网络CNN或循环神经网络RNN）。基于Edge-IIoTset网络安全数据集的实验分析显示，SecurityBERT在识别14类攻击时总体准确率达98.2%，超越了基于GAN-Transformer的混合架构和CNN-LSTM模型等现有方案。该模型在普通CPU上推理时间不足0.15秒，体积仅16.7MB，非常适合实时流量分析，是资源受限物联网设备的理想部署选择。
