# OT-Transformer: A Continuous-time Transformer Architecture with Optimal Transport Regularization

链接: http://arxiv.org/abs/2501.18793v1

原文摘要:
Transformers have achieved state-of-the-art performance in numerous tasks. In
this paper, we propose a continuous-time formulation of transformers.
Specifically, we consider a dynamical system whose governing equation is
parametrized by transformer blocks. We leverage optimal transport theory to
regularize the training problem, which enhances stability in training and
improves generalization of the resulting model. Moreover, we demonstrate in
theory that this regularization is necessary as it promotes uniqueness and
regularity of solutions. Our model is flexible in that almost any existing
transformer architectures can be adopted to construct the dynamical system with
only slight modifications to the existing code. We perform extensive numerical
experiments on tasks motivated by natural language processing, image
classification, and point cloud classification. Our experimental results show
that the proposed method improves the performance of its discrete counterpart
and outperforms relevant comparing models.

中文翻译:
Transformer模型已在多项任务中展现出卓越性能。本文提出了一种连续时间框架下的Transformer建模方法。具体而言，我们构建了一个由Transformer模块参数化控制方程的动态系统，并运用最优传输理论对训练过程进行正则化处理，这不仅增强了训练稳定性，还显著提升了模型的泛化能力。理论分析表明，这种正则化处理对于保证解的唯一性和正则性具有必要性。该模型具有高度灵活性，几乎任何现有Transformer架构只需经过轻微代码修改即可用于构建动态系统。我们在自然语言处理、图像分类和点云分类等任务上开展了大量数值实验，结果表明所提方法不仅提升了原始离散模型的性能，还优于相关对比模型。
