# OT-Transformer: A Continuous-time Transformer Architecture with Optimal Transport Regularization

链接: http://arxiv.org/abs/2501.18793v1

原文摘要:
Transformers have achieved state-of-the-art performance in numerous tasks. In
this paper, we propose a continuous-time formulation of transformers.
Specifically, we consider a dynamical system whose governing equation is
parametrized by transformer blocks. We leverage optimal transport theory to
regularize the training problem, which enhances stability in training and
improves generalization of the resulting model. Moreover, we demonstrate in
theory that this regularization is necessary as it promotes uniqueness and
regularity of solutions. Our model is flexible in that almost any existing
transformer architectures can be adopted to construct the dynamical system with
only slight modifications to the existing code. We perform extensive numerical
experiments on tasks motivated by natural language processing, image
classification, and point cloud classification. Our experimental results show
that the proposed method improves the performance of its discrete counterpart
and outperforms relevant comparing models.

中文翻译:
以下是符合要求的学术中文翻译：

Transformer模型已在多项任务中取得最优性能表现。本文提出一种连续时间形式的Transformer建模框架。具体而言，我们构建了一个动力学系统，其控制方程由Transformer模块参数化。我们运用最优传输理论对训练过程进行正则化处理，这不仅增强了训练稳定性，同时提升了最终模型的泛化能力。理论分析表明，这种正则化处理通过保证解的唯一性与正则性，具有不可或缺的作用。本模型具有高度灵活性，几乎所有现有Transformer架构只需经过轻微代码修改即可用于构建该动力学系统。我们在自然语言处理、图像分类和点云分类等任务上开展了大量数值实验。结果表明：所提方法不仅提升了其离散版本模型的性能，同时优于相关对比模型。

（注：根据学术翻译规范，关键术语保持英文原名"Transformer"；长句按中文习惯切分为短句；被动语态转换为主动表述；理论证明部分采用"表明"替代直接翻译"demonstrate"；保持"optimal transport theory"专业术语译为"最优传输理论"的学界标准译法）
