# PointLLM: Empowering Large Language Models to Understand Point Clouds

链接: http://arxiv.org/abs/2308.16911v1

原文摘要:
The unprecedented advancements in Large Language Models (LLMs) have shown a
profound impact on natural language processing but are yet to fully embrace the
realm of 3D understanding. This paper introduces PointLLM, a preliminary effort
to fill this gap, enabling LLMs to understand point clouds and offering a new
avenue beyond 2D visual data. PointLLM understands colored object point clouds
with human instructions and generates contextually appropriate responses,
illustrating its grasp of point clouds and common sense. Specifically, it
leverages a point cloud encoder with a powerful LLM to effectively fuse
geometric, appearance, and linguistic information. We collect a novel dataset
comprising 660K simple and 70K complex point-text instruction pairs to enable a
two-stage training strategy: aligning latent spaces and subsequently
instruction-tuning the unified model. To rigorously evaluate the perceptual and
generalization capabilities of PointLLM, we establish two benchmarks:
Generative 3D Object Classification and 3D Object Captioning, assessed through
three different methods, including human evaluation, GPT-4/ChatGPT evaluation,
and traditional metrics. Experimental results reveal PointLLM's superior
performance over existing 2D and 3D baselines, with a notable achievement in
human-evaluated object captioning tasks where it surpasses human annotators in
over 50% of the samples. Codes, datasets, and benchmarks are available at
https://github.com/OpenRobotLab/PointLLM .

中文翻译:
大型语言模型（LLM）的前所未有进展已对自然语言处理产生深远影响，但尚未充分涉足三维理解领域。本文提出PointLLM作为填补这一空白的初步尝试，使LLM能够理解点云数据，开辟了超越二维视觉数据的新途径。PointLLM通过人类指令理解带颜色的物体点云，并生成符合上下文的响应，展现其对点云和常识的把握能力。具体而言，该方法结合点云编码器与强大LLM，有效融合几何、外观和语言信息。我们收集了包含66万条简单及7万条复杂点云-文本指令对的全新数据集，采用两阶段训练策略：先进行潜在空间对齐，再对统一模型进行指令微调。为严格评估PointLLM的感知与泛化能力，我们建立了生成式3D物体分类和3D物体描述两大基准测试，通过人工评估、GPT-4/ChatGPT评估及传统指标三种方法进行综合测评。实验结果表明，PointLLM在现有2D和3D基线模型中表现卓越，在人工评估的物体描述任务中尤为突出，超过50%的样本表现优于人类标注者。相关代码、数据集及基准测试已开源：https://github.com/OpenRobotLab/PointLLM。
