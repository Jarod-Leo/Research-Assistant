# Bringing order into the realm of Transformer-based language models for artificial intelligence and law

链接: http://arxiv.org/abs/2308.05502v1

原文摘要:
Transformer-based language models (TLMs) have widely been recognized to be a
cutting-edge technology for the successful development of deep-learning-based
solutions to problems and applications that require natural language processing
and understanding. Like for other textual domains, TLMs have indeed pushed the
state-of-the-art of AI approaches for many tasks of interest in the legal
domain. Despite the first Transformer model being proposed about six years ago,
there has been a rapid progress of this technology at an unprecedented rate,
whereby BERT and related models represent a major reference, also in the legal
domain. This article provides the first systematic overview of TLM-based
methods for AI-driven problems and tasks in the legal sphere. A major goal is
to highlight research advances in this field so as to understand, on the one
hand, how the Transformers have contributed to the success of AI in supporting
legal processes, and on the other hand, what are the current limitations and
opportunities for further research development.

中文翻译:
基于Transformer的语言模型（TLMs）已被公认为是开发深度学习解决方案的前沿技术，尤其适用于需要自然语言处理与理解的问题和应用场景。与其他文本领域类似，TLMs确实推动了法律领域诸多重点任务的人工智能技术发展。尽管首个Transformer模型问世仅约六年，但该技术以空前速度快速发展——其中BERT及其衍生模型已成为法律领域的重要技术标杆。本文首次系统梳理了法律领域中基于TLM的人工智能方法与任务研究，核心目标在于揭示该领域的研究进展：一方面阐释Transformer如何推动AI在法律流程支持中的成功应用，另一方面剖析当前技术局限性与未来研究发展机遇。
