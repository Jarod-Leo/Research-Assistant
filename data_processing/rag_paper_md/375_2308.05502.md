# Bringing order into the realm of Transformer-based language models for artificial intelligence and law

链接: http://arxiv.org/abs/2308.05502v1

原文摘要:
Transformer-based language models (TLMs) have widely been recognized to be a
cutting-edge technology for the successful development of deep-learning-based
solutions to problems and applications that require natural language processing
and understanding. Like for other textual domains, TLMs have indeed pushed the
state-of-the-art of AI approaches for many tasks of interest in the legal
domain. Despite the first Transformer model being proposed about six years ago,
there has been a rapid progress of this technology at an unprecedented rate,
whereby BERT and related models represent a major reference, also in the legal
domain. This article provides the first systematic overview of TLM-based
methods for AI-driven problems and tasks in the legal sphere. A major goal is
to highlight research advances in this field so as to understand, on the one
hand, how the Transformers have contributed to the success of AI in supporting
legal processes, and on the other hand, what are the current limitations and
opportunities for further research development.

中文翻译:
基于Transformer的语言模型（TLM）已被公认为一项前沿技术，成功推动了深度学习在自然语言处理与理解相关问题和应用中的解决方案发展。与其他文本领域类似，TLM确实将人工智能在法律领域多项重点任务中的技术水准推向了新高度。尽管首款Transformer模型问世仅约六年，该技术以空前速度迅猛发展——BERT及其衍生模型已成为法律领域的重要参照标准。本文首次系统梳理了TLM方法在法律人工智能问题与任务中的应用研究，重点在于揭示该领域的研究进展：一方面剖析Transformer如何助力人工智能在法律流程支持中取得成功，另一方面指明当前技术局限及未来研究的突破方向。
