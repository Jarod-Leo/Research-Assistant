# SpikeZIP-TF: Conversion is All You Need for Transformer-based SNN

链接: http://arxiv.org/abs/2406.03470v1

原文摘要:
Spiking neural network (SNN) has attracted great attention due to its
characteristic of high efficiency and accuracy. Currently, the ANN-to-SNN
conversion methods can obtain ANN on-par accuracy SNN with ultra-low latency (8
time-steps) in CNN structure on computer vision (CV) tasks. However, as
Transformer-based networks have achieved prevailing precision on both CV and
natural language processing (NLP), the Transformer-based SNNs are still
encounting the lower accuracy w.r.t the ANN counterparts. In this work, we
introduce a novel ANN-to-SNN conversion method called SpikeZIP-TF, where ANN
and SNN are exactly equivalent, thus incurring no accuracy degradation.
SpikeZIP-TF achieves 83.82% accuracy on CV dataset (ImageNet) and 93.79%
accuracy on NLP dataset (SST-2), which are higher than SOTA Transformer-based
SNNs. The code is available in GitHub:
https://github.com/Intelligent-Computing-Research-Group/SpikeZIP_transformer

中文翻译:
脉冲神经网络（SNN）因其高效与高精度的特性备受关注。当前基于人工神经网络（ANN）到SNN的转换方法在计算机视觉任务中，能以极低延迟（8个时间步）实现与CNN结构ANN相当的精度。然而，尽管基于Transformer的网络在计算机视觉和自然语言处理领域均取得领先精度，其对应的SNN模型仍面临精度显著低于ANN的问题。本研究提出创新性转换方法SpikeZIP-TF，通过建立ANN与SNN的严格等效性实现零精度损失，在ImageNet数据集上达到83.82%的视觉任务准确率，在SST-2情感分析数据集上获得93.79%的文本分类准确率，均超越当前最先进的基于Transformer的SNN模型。代码已开源：https://github.com/Intelligent-Computing-Research-Group/SpikeZIP_transformer
