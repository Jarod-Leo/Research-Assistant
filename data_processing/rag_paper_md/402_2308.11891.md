# Bridging the Gap: Deciphering Tabular Data Using Large Language Model

链接: http://arxiv.org/abs/2308.11891v1

原文摘要:
In the realm of natural language processing, the understanding of tabular
data has perpetually stood as a focal point of scholarly inquiry. The emergence
of expansive language models, exemplified by the likes of ChatGPT, has ushered
in a wave of endeavors wherein researchers aim to harness these models for
tasks related to table-based question answering. Central to our investigative
pursuits is the elucidation of methodologies that amplify the aptitude of such
large language models in discerning both the structural intricacies and
inherent content of tables, ultimately facilitating their capacity to provide
informed responses to pertinent queries. To this end, we have architected a
distinctive module dedicated to the serialization of tables for seamless
integration with expansive language models. Additionally, we've instituted a
corrective mechanism within the model to rectify potential inaccuracies.
Experimental results indicate that, although our proposed method trails the
SOTA by approximately 11.7% in overall metrics, it surpasses the SOTA by about
1.2% in tests on specific datasets. This research marks the first application
of large language models to table-based question answering tasks, enhancing the
model's comprehension of both table structures and content.

中文翻译:
在自然语言处理领域，表格数据的理解始终是学术研究的重点。以ChatGPT为代表的大规模语言模型的出现，引发了一系列研究者利用此类模型完成基于表格的问答任务的热潮。本研究致力于揭示增强大型语言模型解析表格结构特征与内在内容能力的方法论，从而提升其针对相关查询提供精准回答的能力。为此，我们设计了一个独特的表格序列化模块以实现与大型语言模型的无缝对接，并在模型中建立了纠错机制以修正潜在错误。实验结果表明：虽然本文提出的方法在整体指标上较SOTA落后约11.7%，但在特定数据集测试中反超SOTA约1.2%。这项研究首次将大语言模型应用于基于表格的问答任务，有效提升了模型对表格结构和内容的双重理解能力。
