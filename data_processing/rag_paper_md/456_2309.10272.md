# Mixed-Distil-BERT: Code-mixed Language Modeling for Bangla, English, and Hindi

链接: http://arxiv.org/abs/2309.10272v1

原文摘要:
One of the most popular downstream tasks in the field of Natural Language
Processing is text classification. Text classification tasks have become more
daunting when the texts are code-mixed. Though they are not exposed to such
text during pre-training, different BERT models have demonstrated success in
tackling Code-Mixed NLP challenges. Again, in order to enhance their
performance, Code-Mixed NLP models have depended on combining synthetic data
with real-world data. It is crucial to understand how the BERT models'
performance is impacted when they are pretrained using corresponding code-mixed
languages. In this paper, we introduce Tri-Distil-BERT, a multilingual model
pre-trained on Bangla, English, and Hindi, and Mixed-Distil-BERT, a model
fine-tuned on code-mixed data. Both models are evaluated across multiple NLP
tasks and demonstrate competitive performance against larger models like mBERT
and XLM-R. Our two-tiered pre-training approach offers efficient alternatives
for multilingual and code-mixed language understanding, contributing to
advancements in the field.

中文翻译:
自然语言处理领域中最常见的下游任务之一是文本分类。当文本呈现语码混合现象时，文本分类任务变得更具挑战性。尽管预训练阶段未接触此类文本，不同BERT模型已成功应对语码混合的自然语言处理难题。为提升性能，语码混合模型通常依赖合成数据与真实数据的结合使用。理解BERT模型在采用对应语码混合语言进行预训练后性能如何变化至关重要。本文提出Tri-Distil-BERT（基于孟加拉语、英语和印地语预训练的多语言模型）与Mixed-Distil-BERT（经语码混合数据微调的模型），通过多项自然语言处理任务评估表明，这两种模型相较mBERT和XLM-R等大型模型展现出竞争优势。我们提出的双层预训练策略为多语言及语码混合理解提供了高效解决方案，推动了该领域的发展。
