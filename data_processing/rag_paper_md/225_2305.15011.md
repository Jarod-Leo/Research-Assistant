# Bactrian-X : A Multilingual Replicable Instruction-Following Model with Low-Rank Adaptation

链接: http://arxiv.org/abs/2305.15011v1

原文摘要:
Instruction tuning has shown great promise in improving the performance of
large language models. However, research on multilingual instruction tuning has
been limited due to the scarcity of high-quality instruction-response datasets
across different languages. To bridge this gap, we present Bactrian-X, a
comprehensive multilingual parallel dataset of 3.4 million instruction-response
pairs across 52 languages. Leveraging this dataset, we train a set of adapters
using low-rank adaptation (LoRA), which are lightweight components that
seamlessly integrate with large language models. These adapters have a
substantially lower parameter count than the base model, making them easily
replaceable and usable as plug-ins for different languages or language groups.
Extensive experiments in various multilingual evaluation settings demonstrate
that models derived from LoRA-based training over Bactrian-X outperform both
the vanilla models and existing instruction-tuned models. The code and models
are publicly available at https://github.com/mbzuai-nlp/bactrian-x

中文翻译:
指令微调技术在提升大语言模型性能方面展现出巨大潜力，然而由于高质量多语言指令-应答数据集的稀缺，相关研究进展缓慢。为此，我们推出Bactrian-X——一个涵盖52种语言、包含340万条平行指令-应答对的大规模多语言数据集。基于该数据集，我们采用低秩自适应（LoRA）方法训练出一组轻量级适配器模块，这些模块可无缝集成至大语言模型中。相较于基础模型，适配器参数量显著减少，使其能作为可替换插件灵活应用于不同语种或语系。在多语言评估场景下的广泛实验表明，基于Bactrian-X训练的LoRA衍生模型在性能上均优于原始模型及现有指令微调模型。相关代码与模型已开源在https://github.com/mbzuai-nlp/bactrian-x。
