# Bactrian-X : A Multilingual Replicable Instruction-Following Model with Low-Rank Adaptation

链接: http://arxiv.org/abs/2305.15011v1

原文摘要:
Instruction tuning has shown great promise in improving the performance of
large language models. However, research on multilingual instruction tuning has
been limited due to the scarcity of high-quality instruction-response datasets
across different languages. To bridge this gap, we present Bactrian-X, a
comprehensive multilingual parallel dataset of 3.4 million instruction-response
pairs across 52 languages. Leveraging this dataset, we train a set of adapters
using low-rank adaptation (LoRA), which are lightweight components that
seamlessly integrate with large language models. These adapters have a
substantially lower parameter count than the base model, making them easily
replaceable and usable as plug-ins for different languages or language groups.
Extensive experiments in various multilingual evaluation settings demonstrate
that models derived from LoRA-based training over Bactrian-X outperform both
the vanilla models and existing instruction-tuned models. The code and models
are publicly available at https://github.com/mbzuai-nlp/bactrian-x

中文翻译:
以下是符合学术规范的中文翻译：

指令微调技术在提升大语言模型性能方面展现出巨大潜力。然而，由于缺乏跨语言的高质量指令-应答数据集，针对多语言指令微调的研究一直受到限制。为填补这一空白，我们推出Bactrian-X——一个涵盖52种语言、包含340万平行指令-应答对的综合性多语言数据集。基于该数据集，我们采用低秩自适应（LoRA）方法训练了一组适配器模块，这些轻量级组件可无缝集成到大语言模型中。相比基础模型，这些适配器具有显著更少的参数量，使其能够作为可替换的插件灵活应用于不同语言或语系。在多语言评估场景下的广泛实验表明，基于Bactrian-X数据集通过LoRA训练获得的模型，其性能均优于原始模型及现有指令微调模型。相关代码与模型已在https://github.com/mbzuai-nlp/bactrian-x开源。

（翻译说明：1. 专业术语如"instruction tuning"统一译为"指令微调"；2. 机构名称MBZUAI保留英文缩写；3. 数字单位"3.4 million"转换为中文计数习惯"340万"；4. 长句按中文表达习惯拆分重组；5. 技术术语"low-rank adaptation"首次出现标注英文缩写；6. 保持被动语态与原文学术风格的一致性）
