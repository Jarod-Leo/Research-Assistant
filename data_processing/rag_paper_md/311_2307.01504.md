# All in One: Multi-task Prompting for Graph Neural Networks

链接: http://arxiv.org/abs/2307.01504v1

原文摘要:
Recently, ''pre-training and fine-tuning'' has been adopted as a standard
workflow for many graph tasks since it can take general graph knowledge to
relieve the lack of graph annotations from each application. However, graph
tasks with node level, edge level, and graph level are far diversified, making
the pre-training pretext often incompatible with these multiple tasks. This gap
may even cause a ''negative transfer'' to the specific application, leading to
poor results. Inspired by the prompt learning in natural language processing
(NLP), which has presented significant effectiveness in leveraging prior
knowledge for various NLP tasks, we study the prompting topic for graphs with
the motivation of filling the gap between pre-trained models and various graph
tasks. In this paper, we propose a novel multi-task prompting method for graph
models. Specifically, we first unify the format of graph prompts and language
prompts with the prompt token, token structure, and inserting pattern. In this
way, the prompting idea from NLP can be seamlessly introduced to the graph
area. Then, to further narrow the gap between various graph tasks and
state-of-the-art pre-training strategies, we further study the task space of
various graph applications and reformulate downstream problems to the
graph-level task. Afterward, we introduce meta-learning to efficiently learn a
better initialization for the multi-task prompt of graphs so that our prompting
framework can be more reliable and general for different tasks. We conduct
extensive experiments, results from which demonstrate the superiority of our
method.

中文翻译:
近年来，“预训练-微调”模式因其能够利用通用图知识缓解各应用场景中图标注数据不足的问题，已成为众多图任务的标准流程。然而节点级、边级和图级任务的巨大差异性，导致预训练任务往往难以与多样化的下游任务兼容，这种割裂甚至可能引发针对特定应用的“负迁移”现象，导致效果劣化。受自然语言处理领域提示学习（prompt learning）在整合先验知识解决多样化任务方面显著成效的启发，我们致力于研究图领域的提示方法，以弥合预训练模型与多样化图任务之间的鸿沟。本文提出了一种创新的多任务图提示学习方法：首先通过提示标记、标记结构和插入模式的统一设计，实现图提示与语言提示的格式对齐，从而将自然语言的提示机制无缝迁移至图领域；进而通过系统分析图任务空间并将下游问题重构为图级任务，缩小各类图应用与前沿预训练策略间的差距；最后引入元学习机制高效获取多任务图提示的优化初始状态，使提示框架具备跨任务的可靠性与泛化性。大量实验结果表明，该方法具有显著优势。
