# Are Multilingual Language Models an Off-ramp for Under-resourced Languages? Will we arrive at Digital Language Equality in Europe in 2030?

链接: http://arxiv.org/abs/2502.12886v1

原文摘要:
Large language models (LLMs) demonstrate unprecedented capabilities and
define the state of the art for almost all natural language processing (NLP)
tasks and also for essentially all Language Technology (LT) applications. LLMs
can only be trained for languages for which a sufficient amount of pre-training
data is available, effectively excluding many languages that are typically
characterised as under-resourced. However, there is both circumstantial and
empirical evidence that multilingual LLMs, which have been trained using data
sets that cover multiple languages (including under-resourced ones), do exhibit
strong capabilities for some of these under-resourced languages. Eventually,
this approach may have the potential to be a technological off-ramp for those
under-resourced languages for which "native" LLMs, and LLM-based technologies,
cannot be developed due to a lack of training data. This paper, which
concentrates on European languages, examines this idea, analyses the current
situation in terms of technology support and summarises related work. The
article concludes by focusing on the key open questions that need to be
answered for the approach to be put into practice in a systematic way.

中文翻译:
大型语言模型（LLM）展现出前所未有的能力，几乎为所有自然语言处理（NLP）任务及语言技术（LT）应用设定了技术标杆。然而，这类模型仅能针对具备充足预训练数据的语言进行训练，这实质上将许多资源匮乏型语言排除在外。但间接证据与实证研究均表明，基于多语言数据集（包含部分资源匮乏语言）训练的多语言LLM，确实对某些资源不足语言表现出强大的处理能力。长远来看，这一技术路径或能为那些因训练数据不足而无法开发"原生"LLM及LLM衍生技术的弱势语言提供解决方案。本文聚焦欧洲语言，系统审视该理念，分析现有技术支持现状并梳理相关研究，最终提出需解决的核心开放性问题，以推动该方案实现系统化落地。
