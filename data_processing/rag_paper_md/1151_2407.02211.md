# PromptIntern: Saving Inference Costs by Internalizing Recurrent Prompt during Large Language Model Fine-tuning

链接: http://arxiv.org/abs/2407.02211v1

原文摘要:
Recent advances in fine-tuning large language models (LLMs) have greatly
enhanced their usage in domain-specific tasks. Despite the success, fine-tuning
continues to rely on repeated and lengthy prompts, which escalate computational
expenses, require more resources, and lead to slower inference. In this paper,
we present a novel approach, PromptIntern, which internalizes prompt knowledge
during model fine-tuning to achieve efficient inference and save costs. Instead
of compressing the prompts for a vanilla model, PromptIntern aims to embed the
recurrent prompt directly into the model parameters. We design a fine-tuning
pipeline that includes instruction template compression, few-shot example
absorption, and a progressive internalization strategy, effectively diminishing
the need for intricate prompts during inference. Comprehensive experiments on
challenging NL2Code tasks demonstrate that our method reduces input tokens by
more than 90%, accelerates inference by 4.2 times, and reduces monetary
inference costs by 88.3%.

中文翻译:
近年来，大型语言模型（LLM）微调技术的进步显著提升了其在特定领域任务中的应用效果。然而，现有微调方法仍依赖重复冗长的提示词，这不仅增加了计算开销和资源需求，还导致推理速度下降。本文提出创新方法PromptIntern，通过在模型微调过程中内化提示知识，实现高效推理与成本节约。与单纯压缩提示词不同，PromptIntern直接将高频提示信息嵌入模型参数。我们设计了包含指令模板压缩、少样本示例吸收和渐进式内化策略的微调流程，有效降低了推理时对复杂提示的依赖。在具有挑战性的自然语言转代码任务上的实验表明，该方法能减少90%以上的输入标记，推理速度提升4.2倍，并将推理成本降低88.3%。
