# PromptIntern: Saving Inference Costs by Internalizing Recurrent Prompt during Large Language Model Fine-tuning

链接: http://arxiv.org/abs/2407.02211v1

原文摘要:
Recent advances in fine-tuning large language models (LLMs) have greatly
enhanced their usage in domain-specific tasks. Despite the success, fine-tuning
continues to rely on repeated and lengthy prompts, which escalate computational
expenses, require more resources, and lead to slower inference. In this paper,
we present a novel approach, PromptIntern, which internalizes prompt knowledge
during model fine-tuning to achieve efficient inference and save costs. Instead
of compressing the prompts for a vanilla model, PromptIntern aims to embed the
recurrent prompt directly into the model parameters. We design a fine-tuning
pipeline that includes instruction template compression, few-shot example
absorption, and a progressive internalization strategy, effectively diminishing
the need for intricate prompts during inference. Comprehensive experiments on
challenging NL2Code tasks demonstrate that our method reduces input tokens by
more than 90%, accelerates inference by 4.2 times, and reduces monetary
inference costs by 88.3%.

中文翻译:
以下是符合学术规范的中文翻译：

大语言模型微调技术的最新进展显著提升了其在领域特定任务中的应用效果。然而现有微调方法仍依赖于重复冗长的提示文本，这不仅增加了计算开销和资源需求，还导致推理速度下降。本文提出创新方法PromptIntern，通过在模型微调过程中将提示知识内化，实现高效推理与成本节约。与传统的提示压缩技术不同，PromptIntern致力于将重复性提示直接嵌入模型参数。我们设计了包含指令模板压缩、少样本示例吸收和渐进式内化策略的微调流程，有效降低了推理过程中对复杂提示的依赖。在具有挑战性的自然语言转代码任务上的实验表明，本方法可减少90%以上的输入标记，实现4.2倍的推理加速，并降低88.3%的推理成本。

（说明：译文严格遵循学术翻译准则，具有以下特点：
1. 专业术语统一："fine-tuning"译为"微调"，"inference"译为"推理"
2. 被动语态转化：将英文被动结构转换为中文主动表达
3. 长句拆分：将复合长句按中文习惯分解为短句群
4. 概念准确："few-shot example"译为专业术语"少样本示例"
5. 数据呈现规范：百分数统一使用"%"符号，倍数关系保留原始数字
6. 逻辑连接词处理："Despite"转化为"然而"实现语义转折）
