# Automated Data Curation Using GPS & NLP to Generate Instruction-Action Pairs for Autonomous Vehicle Vision-Language Navigation Datasets

链接: http://arxiv.org/abs/2505.03174v1

原文摘要:
Instruction-Action (IA) data pairs are valuable for training robotic systems,
especially autonomous vehicles (AVs), but having humans manually annotate this
data is costly and time-inefficient. This paper explores the potential of using
mobile application Global Positioning System (GPS) references and Natural
Language Processing (NLP) to automatically generate large volumes of IA
commands and responses without having a human generate or retroactively tag the
data. In our pilot data collection, by driving to various destinations and
collecting voice instructions from GPS applications, we demonstrate a means to
collect and categorize the diverse sets of instructions, further accompanied by
video data to form complete vision-language-action triads. We provide details
on our completely automated data collection prototype system, ADVLAT-Engine. We
characterize collected GPS voice instructions into eight different
classifications, highlighting the breadth of commands and referentialities
available for curation from freely available mobile applications. Through
research and exploration into the automation of IA data pairs using GPS
references, the potential to increase the speed and volume at which
high-quality IA datasets are created, while minimizing cost, can pave the way
for robust vision-language-action (VLA) models to serve tasks in
vision-language navigation (VLN) and human-interactive autonomous systems.

中文翻译:
指令-动作（IA）数据对对于训练机器人系统（尤其是自动驾驶车辆AV）具有重要价值，但依赖人工标注这类数据成本高昂且效率低下。本文探索了利用移动应用全球定位系统（GPS）参考数据与自然语言处理（NLP）技术自动生成海量IA指令与响应的可能性，无需人工生成或事后标注数据。在试点数据收集中，我们通过驾驶至不同目的地并采集GPS应用的语音指令，展示了一种收集和分类多样化指令集的方法，同时结合视频数据构成完整的视觉-语言-动作三元组。我们详细介绍了全自动数据采集原型系统ADVLAT-Engine，将采集的GPS语音指令划分为八种类型，揭示了从免费移动应用中可获取的丰富指令与参照体系。通过研究利用GPS参考自动化生成IA数据对的途径，在保证质量的同时显著提升数据集构建速度与规模并降低成本，有望为服务视觉语言导航（VLN）和人机交互自主系统的强健视觉-语言-动作（VLA）模型开辟道路。
