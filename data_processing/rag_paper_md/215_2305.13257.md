# Watermarking Text Data on Large Language Models for Dataset Copyright Protection

链接: http://arxiv.org/abs/2305.13257v1

原文摘要:
Substantial research works have shown that deep models, e.g., pre-trained
models, on the large corpus can learn universal language representations, which
are beneficial for downstream NLP tasks. However, these powerful models are
also vulnerable to various privacy attacks, while much sensitive information
exists in the training dataset. The attacker can easily steal sensitive
information from public models, e.g., individuals' email addresses and phone
numbers. In an attempt to address these issues, particularly the unauthorized
use of private data, we introduce a novel watermarking technique via a
backdoor-based membership inference approach named TextMarker, which can
safeguard diverse forms of private information embedded in the training text
data. Specifically, TextMarker only requires data owners to mark a small number
of samples for data copyright protection under the black-box access assumption
to the target model. Through extensive evaluation, we demonstrate the
effectiveness of TextMarker on various real-world datasets, e.g., marking only
0.1% of the training dataset is practically sufficient for effective membership
inference with negligible effect on model utility. We also discuss potential
countermeasures and show that TextMarker is stealthy enough to bypass them.

中文翻译:
大量研究表明，在大规模语料库上训练的深度模型（如预训练模型）能够学习通用语言表征，这对下游自然语言处理任务大有裨益。然而这些强大模型也面临着各类隐私攻击的威胁，尤其是训练数据中往往包含大量敏感信息。攻击者能轻易从公开模型中窃取敏感数据，例如个人邮箱地址和电话号码。为解决这类未经授权使用私有数据的问题，我们提出了一种基于后门成员推断的新型水印技术TextMarker，可有效保护训练文本数据中嵌入的各类隐私信息。该技术仅需数据所有者在黑盒访问目标模型的假设下，对少量样本进行标记即可实现数据版权保护。通过大量实验验证，我们在多个真实数据集上证明了TextMarker的有效性——仅标记0.1%的训练数据就能实现高效的成员推断，且对模型性能影响可忽略不计。我们还探讨了可能的防御措施，并证明TextMarker具有足够的隐蔽性可规避这些防御。
