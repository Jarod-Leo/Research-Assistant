# MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens

链接: http://arxiv.org/abs/2310.02239v1

原文摘要:
The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a
profound capability in multimodal understanding. However, the simultaneous
generation of images with coherent texts is still underdeveloped. Addressing
this, we introduce a novel interleaved vision-and-language generation method,
centered around the concept of ``generative vokens". These vokens serve as
pivotal elements contributing to coherent image-text outputs. Our method is
marked by a unique two-stage training strategy for description-free multimodal
generation, which does not necessitate extensive descriptions of images. We
integrate classifier-free guidance to enhance the alignment of generated images
and texts, ensuring more seamless and contextually relevant multimodal
interactions. Our model, MiniGPT-5, exhibits substantial improvement over the
baseline models on multimodal generation datasets, including MMDialog and VIST.
The human evaluation shows MiniGPT-5 is better than the baseline model on more
than 56\% cases for multimodal generation, highlighting its efficacy across
diverse benchmarks.

中文翻译:
多模态大语言模型（MLLMs）的有效性展现了其在多模态理解方面的卓越能力。然而，同步生成图像与连贯文本的技术仍处于发展阶段。为此，我们提出了一种以"生成式视觉标记"（generative vokens）为核心的新型交错视觉-语言生成方法。这些视觉标记作为关键要素，助力生成协调一致的图文输出。我们的方法采用独特的双阶段训练策略实现免描述多模态生成，无需依赖详尽的图像描述说明。通过集成无分类器引导技术，我们增强了生成图像与文本的语义对齐，确保更流畅且符合上下文的多模态交互。实验表明，我们的MiniGPT-5模型在多模态生成数据集（包括MMDialog和VIST）上显著超越基线模型。人工评估证实，在56%以上的多模态生成案例中，MiniGPT-5优于基线模型，其在不同基准测试中的有效性得到充分彰显。

（翻译说明：  
1. 专业术语处理："vokens"音译为"视觉标记"并保留英文原词，符合学术翻译惯例  
2. 被动语态转换：将英文被动结构转换为中文主动表述（如"is marked by"译为"采用"）  
3. 长句拆分：将原文复合句分解为符合中文表达习惯的短句结构  
4. 概念显化："description-free"译为"免描述"并补充说明性翻译  
5. 数据呈现：严格保持56%等数字信息的准确性  
6. 学术风格：使用"基准测试""语义对齐"等符合计算机领域论文的规范表述）
