# MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens

链接: http://arxiv.org/abs/2310.02239v1

原文摘要:
The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a
profound capability in multimodal understanding. However, the simultaneous
generation of images with coherent texts is still underdeveloped. Addressing
this, we introduce a novel interleaved vision-and-language generation method,
centered around the concept of ``generative vokens". These vokens serve as
pivotal elements contributing to coherent image-text outputs. Our method is
marked by a unique two-stage training strategy for description-free multimodal
generation, which does not necessitate extensive descriptions of images. We
integrate classifier-free guidance to enhance the alignment of generated images
and texts, ensuring more seamless and contextually relevant multimodal
interactions. Our model, MiniGPT-5, exhibits substantial improvement over the
baseline models on multimodal generation datasets, including MMDialog and VIST.
The human evaluation shows MiniGPT-5 is better than the baseline model on more
than 56\% cases for multimodal generation, highlighting its efficacy across
diverse benchmarks.

中文翻译:
多模态大语言模型（MLLMs）的有效性展现了其在跨模态理解方面的卓越能力，然而同步生成图像与连贯文本的技术仍处于发展阶段。为此，我们提出了一种创新的交错式视觉-语言生成方法，其核心在于"生成性视觉标记"（generative vokens）这一概念。这些视觉标记作为关键要素，助力实现图像与文本输出的协调统一。我们的方法采用独特的双阶段训练策略，无需依赖大量图像描述即可实现无描述多模态生成，并通过集成无分类器引导技术来增强生成图像与文本的匹配度，确保多模态交互更流畅且符合上下文语境。实验表明，MiniGPT-5模型在多模态生成数据集（包括MMDialog和VIST）上的表现显著优于基线模型。人工评估显示，在超过56%的多模态生成案例中，MiniGPT-5优于基线模型，充分验证了其在多样化基准测试中的卓越性能。
