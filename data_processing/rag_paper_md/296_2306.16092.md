# ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases

链接: http://arxiv.org/abs/2306.16092v1

原文摘要:
AI legal assistants based on Large Language Models (LLMs) can provide
accessible legal consulting services, but the hallucination problem poses
potential legal risks. This paper presents Chatlaw, an innovative legal
assistant utilizing a Mixture-of-Experts (MoE) model and a multi-agent system
to enhance the reliability and accuracy of AI-driven legal services. By
integrating knowledge graphs with artificial screening, we construct a
high-quality legal dataset to train the MoE model. This model utilizes
different experts to address various legal issues, optimizing the accuracy of
legal responses. Additionally, Standardized Operating Procedures (SOP), modeled
after real law firm workflows, significantly reduce errors and hallucinations
in legal services. Our MoE model outperforms GPT-4 in the Lawbench and Unified
Qualification Exam for Legal Professionals by 7.73% in accuracy and 11 points,
respectively, and also surpasses other models in multiple dimensions during
real-case consultations, demonstrating our robust capability for legal
consultation.

中文翻译:
基于大语言模型（LLM）的AI法律助手能够提供便捷的法律咨询服务，但其幻觉问题可能引发法律风险。本文提出Chatlaw——一种创新性法律助手，通过混合专家模型（MoE）与多智能体系统提升AI法律服务的可靠性与准确性。我们结合知识图谱与人工筛选构建高质量法律数据集，用以训练MoE模型。该模型通过调用不同领域专家处理各类法律问题，优化法律应答的精确度。此外，参照真实律所工作流程设计的标准化操作程序（SOP），可显著降低法律服务中的错误率与幻觉现象。实验表明，我们的MoE模型在法律专业基准测试（Lawbench）和国家统一法律职业资格考试中的准确率分别较GPT-4高出7.73%和11分，在实际案例咨询的多个维度上也优于其他模型，展现出强大的法律咨询能力。  

（翻译说明：  
1. 专业术语采用国内通用译法："Mixture-of-Experts"译为"混合专家模型"，"hallucination"译为"幻觉"  
2. 机构名称保留英文缩写但补充中文全称："SOP"译为"标准化操作程序"  
3. 长句拆分重组：将原文复合句按中文表达习惯分解为多个短句  
4. 被动语态转化："are modeled after"译为主动态的"参照...设计"  
5. 数据呈现方式本地化：英文"7.73%"和"11 points"保留数字但调整表述符合中文科技论文规范  
6. 文化适配："Unified Qualification Exam for Legal Professionals"译为国内通用的"国家统一法律职业资格考试"）
