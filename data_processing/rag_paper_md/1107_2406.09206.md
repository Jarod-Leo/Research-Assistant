# Self-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models

链接: http://arxiv.org/abs/2406.09206v1

原文摘要:
Active learning is an iterative labeling process that is used to obtain a
small labeled subset, despite the absence of labeled data, thereby enabling to
train a model for supervised tasks such as text classification. While active
learning has made considerable progress in recent years due to improvements
provided by pre-trained language models, there is untapped potential in the
often neglected unlabeled portion of the data, although it is available in
considerably larger quantities than the usually small set of labeled data. In
this work, we investigate how self-training, a semi-supervised approach that
uses a model to obtain pseudo-labels for unlabeled data, can be used to improve
the efficiency of active learning for text classification. Building on a
comprehensive reproduction of four previous self-training approaches, some of
which are evaluated for the first time in the context of active learning or
natural language processing, we introduce HAST, a new and effective
self-training strategy, which is evaluated on four text classification
benchmarks. Our results show that it outperforms the reproduced self-training
approaches and reaches classification results comparable to previous
experiments for three out of four datasets, using as little as 25% of the data.
The code is publicly available at
https://github.com/chschroeder/self-training-for-sample-efficient-active-learning .

中文翻译:
主动学习是一种迭代标注过程，旨在缺乏标注数据的情况下获取少量标注样本，从而训练出适用于文本分类等监督任务的模型。尽管预训练语言模型的进步推动了主动学习近年来的显著发展，但数据中大量未标注部分（其数量通常远超有限的标注集）仍存在未被挖掘的潜力。本研究探讨了如何利用自训练（一种通过模型为未标注数据生成伪标签的半监督方法）来提升文本分类主动学习的效率。在系统复现四种现有自训练方法（其中部分方法首次在主动学习或自然语言处理领域被评估）的基础上，我们提出了HAST——一种新型高效的自训练策略，并在四个文本分类基准上进行了验证。实验结果表明，该方法优于所有复现的自训练方案，在仅使用25%数据量的情况下，于四分之三的数据集上达到了与既往实验相当的分类性能。相关代码已开源：https://github.com/chschroeder/self-training-for-sample-efficient-active-learning。
