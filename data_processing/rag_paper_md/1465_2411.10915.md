# Bias in Large Language Models: Origin, Evaluation, and Mitigation

链接: http://arxiv.org/abs/2411.10915v1

原文摘要:
Large Language Models (LLMs) have revolutionized natural language processing,
but their susceptibility to biases poses significant challenges. This
comprehensive review examines the landscape of bias in LLMs, from its origins
to current mitigation strategies. We categorize biases as intrinsic and
extrinsic, analyzing their manifestations in various NLP tasks. The review
critically assesses a range of bias evaluation methods, including data-level,
model-level, and output-level approaches, providing researchers with a robust
toolkit for bias detection. We further explore mitigation strategies,
categorizing them into pre-model, intra-model, and post-model techniques,
highlighting their effectiveness and limitations. Ethical and legal
implications of biased LLMs are discussed, emphasizing potential harms in
real-world applications such as healthcare and criminal justice. By
synthesizing current knowledge on bias in LLMs, this review contributes to the
ongoing effort to develop fair and responsible AI systems. Our work serves as a
comprehensive resource for researchers and practitioners working towards
understanding, evaluating, and mitigating bias in LLMs, fostering the
development of more equitable AI technologies.

中文翻译:
以下是符合您要求的中文翻译：

大型语言模型（LLMs）为自然语言处理带来了革命性变革，但其易受偏见影响的特性也构成了重大挑战。本综述从偏见的起源到当前缓解策略，系统考察了LLMs中的偏见研究现状。我们将偏见划分为内在与外在两类，分析其在不同NLP任务中的具体表现。通过批判性评估数据层面、模型层面和输出层面的多种偏见检测方法，为研究者提供了一套完整的偏见评估工具包。我们进一步将缓解策略归类为模型前、模型中与模型后三类技术，重点阐释其效果与局限。研究还探讨了存在偏见的LLMs所涉及的伦理与法律问题，特别关注医疗健康和刑事司法等现实应用场景中的潜在危害。通过整合当前关于LLMs偏见的研究成果，本综述为推动构建公平、负责任的AI系统提供了理论支持。这项工作为致力于理解、评估和消除LLMs偏见的研究者与实践者提供了全面参考，助力发展更具公平性的人工智能技术。

翻译说明：
1. 专业术语处理：LLMs采用"大型语言模型"标准译法，NLP保持"自然语言处理"全称
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句（如第一句的转折处理）
3. 学术规范：使用"本综述"替代第一人称，保持学术客观性
4. 概念对应："intrinsic/extrinsic"译为"内在/外在"，"pre/intra/post-model"处理为"模型前/中/后"
5. 动态对等："toolkit"译为"工具包"而非字面直译，保持技术文档特征
6. 文化适配："criminal justice"采用国内学界通用译法"刑事司法"
7. 术语统一：全篇保持"偏见"与"缓解策略"等核心概念的一致性
