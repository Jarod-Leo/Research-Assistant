# Bias in Large Language Models: Origin, Evaluation, and Mitigation

链接: http://arxiv.org/abs/2411.10915v1

原文摘要:
Large Language Models (LLMs) have revolutionized natural language processing,
but their susceptibility to biases poses significant challenges. This
comprehensive review examines the landscape of bias in LLMs, from its origins
to current mitigation strategies. We categorize biases as intrinsic and
extrinsic, analyzing their manifestations in various NLP tasks. The review
critically assesses a range of bias evaluation methods, including data-level,
model-level, and output-level approaches, providing researchers with a robust
toolkit for bias detection. We further explore mitigation strategies,
categorizing them into pre-model, intra-model, and post-model techniques,
highlighting their effectiveness and limitations. Ethical and legal
implications of biased LLMs are discussed, emphasizing potential harms in
real-world applications such as healthcare and criminal justice. By
synthesizing current knowledge on bias in LLMs, this review contributes to the
ongoing effort to develop fair and responsible AI systems. Our work serves as a
comprehensive resource for researchers and practitioners working towards
understanding, evaluating, and mitigating bias in LLMs, fostering the
development of more equitable AI technologies.

中文翻译:
大型语言模型（LLMs）彻底改变了自然语言处理领域，但其易受偏见影响的特性带来了重大挑战。本文通过系统性综述，从偏见起源到当前缓解策略，全面审视了LLMs中的偏见图景。我们将偏见划分为内在与外在两类，分析其在不同NLP任务中的具体表现。研究批判性评估了包括数据层面、模型层面和输出层面在内的多种偏见评估方法，为研究者提供了一套完整的偏见检测工具包。进一步地，我们探讨了三大类缓解策略——模型前处理、模型内处理和模型后处理技术，并着重分析了各类方法的有效性与局限性。针对LLMs偏见的伦理与法律影响，本文特别关注了医疗健康和刑事司法等现实应用场景中的潜在危害。通过整合当前关于LLMs偏见的研究成果，本综述为推动构建公平、负责任的AI系统提供了理论支撑。这项研究为致力于理解、评估和消除LLMs偏见的研究者与实践者提供了全面的资源指南，助力发展更具公平性的人工智能技术。
