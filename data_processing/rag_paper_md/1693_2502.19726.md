# Tokens for Learning, Tokens for Unlearning: Mitigating Membership Inference Attacks in Large Language Models via Dual-Purpose Training

链接: http://arxiv.org/abs/2502.19726v1

原文摘要:
Large language models (LLMs) have become the backbone of modern natural
language processing but pose privacy concerns about leaking sensitive training
data. Membership inference attacks (MIAs), which aim to infer whether a sample
is included in a model's training dataset, can serve as a foundation for
broader privacy threats. Existing defenses designed for traditional
classification models do not account for the sequential nature of text data. As
a result, they either require significant computational resources or fail to
effectively mitigate privacy risks in LLMs. In this work, we propose a
lightweight yet effective empirical privacy defense for protecting training
data of language modeling by leveraging the token-specific characteristics. By
analyzing token dynamics during training, we propose a token selection strategy
that categorizes tokens into hard tokens for learning and memorized tokens for
unlearning. Subsequently, our training-phase defense optimizes a novel
dual-purpose token-level loss to achieve a Pareto-optimal balance between
utility and privacy. Extensive experiments demonstrate that our approach not
only provides strong protection against MIAs but also improves language
modeling performance by around 10\% across various LLM architectures and
datasets compared to the baselines.

中文翻译:
大型语言模型（LLMs）已成为现代自然语言处理的核心技术，但也引发了敏感训练数据泄露的隐私隐忧。成员推理攻击（MIAs）旨在推断特定样本是否包含于模型的训练数据集中，可能构成更广泛隐私威胁的基础。现有针对传统分类模型的防御方案未能考虑文本数据的序列特性，导致其要么需要消耗大量计算资源，要么无法有效降低LLMs的隐私风险。本研究提出一种轻量级但高效的实证隐私防御方法，通过利用词汇单元特异性来保护语言建模的训练数据。通过分析训练过程中的词汇动态特征，我们提出一种词汇选择策略，将词汇划分为用于学习的困难词汇和需要遗忘的已记忆词汇。随后，我们的训练阶段防御机制通过优化新型双重用途的词汇级损失函数，在模型效用与隐私保护之间实现帕累托最优平衡。大量实验表明，相较于基线方法，我们的方案不仅能有效抵御成员推理攻击，还能使不同架构LLM和各种数据集的语言建模性能提升约10%。
