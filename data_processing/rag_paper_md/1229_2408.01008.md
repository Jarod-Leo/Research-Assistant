# Tensor Train Low-rank Approximation (TT-LoRA): Democratizing AI with Accelerated LLMs

链接: http://arxiv.org/abs/2408.01008v1

原文摘要:
In recent years, Large Language Models (LLMs) have demonstrated remarkable
capabilities across a wide range of natural language processing (NLP) tasks,
such as question-answering, sentiment analysis, text summarization, and machine
translation. However, the ever-growing complexity of LLMs demands immense
computational resources, hindering the broader research and application of
these models. To address this, various parameter-efficient fine-tuning
strategies, such as Low-Rank Approximation (LoRA) and Adapters, have been
developed. Despite their potential, these methods often face limitations in
compressibility. Specifically, LoRA struggles to scale effectively with the
increasing number of trainable parameters in modern large scale LLMs.
Additionally, Low-Rank Economic Tensor-Train Adaptation (LoRETTA), which
utilizes tensor train decomposition, has not yet achieved the level of
compression necessary for fine-tuning very large scale models with limited
resources. This paper introduces Tensor Train Low-Rank Approximation (TT-LoRA),
a novel parameter-efficient fine-tuning (PEFT) approach that extends LoRETTA
with optimized tensor train (TT) decomposition integration. By eliminating
Adapters and traditional LoRA-based structures, TT-LoRA achieves greater model
compression without compromising downstream task performance, along with
reduced inference latency and computational overhead. We conduct an exhaustive
parameter search to establish benchmarks that highlight the trade-off between
model compression and performance. Our results demonstrate significant
compression of LLMs while maintaining comparable performance to larger models,
facilitating their deployment on resource-constraint platforms.

中文翻译:
近年来，大型语言模型（LLMs）在问答、情感分析、文本摘要和机器翻译等自然语言处理任务中展现出卓越能力。然而，模型规模的持续膨胀导致计算资源需求激增，制约了其研究与应用推广。为此，研究者开发了包括低秩近似（LoRA）和适配器（Adapters）在内的多种参数高效微调策略。尽管这些方法潜力显著，但其压缩能力仍存在局限：LoRA难以适应现代大规模LLMs可训练参数量的增长需求；基于张量火车分解的LoRETTA方法亦未能在有限资源下实现超大规模模型微调所需的压缩水平。本文提出张量火车低秩近似（TT-LoRA）这一新型参数高效微调方法，通过优化张量火车分解集成扩展了LoRETTA框架。该方法摒弃适配器和传统LoRA结构，在不损失下游任务性能的前提下实现更高模型压缩率，同时降低推理延迟与计算开销。我们通过详尽参数搜索建立基准测试，揭示模型压缩与性能间的权衡关系。实验结果表明，该方法在保持与大型模型相当性能的同时，显著压缩了LLMs规模，为资源受限平台的部署提供了可行性。
