# GreekBART: The First Pretrained Greek Sequence-to-Sequence Model

链接: http://arxiv.org/abs/2304.00869v1

原文摘要:
The era of transfer learning has revolutionized the fields of Computer Vision
and Natural Language Processing, bringing powerful pretrained models with
exceptional performance across a variety of tasks. Specifically, Natural
Language Processing tasks have been dominated by transformer-based language
models. In Natural Language Inference and Natural Language Generation tasks,
the BERT model and its variants, as well as the GPT model and its successors,
demonstrated exemplary performance. However, the majority of these models are
pretrained and assessed primarily for the English language or on a multilingual
corpus. In this paper, we introduce GreekBART, the first Seq2Seq model based on
BART-base architecture and pretrained on a large-scale Greek corpus. We
evaluate and compare GreekBART against BART-random, Greek-BERT, and XLM-R on a
variety of discriminative tasks. In addition, we examine its performance on two
NLG tasks from GreekSUM, a newly introduced summarization dataset for the Greek
language. The model, the code, and the new summarization dataset will be
publicly available.

中文翻译:
迁移学习时代彻底革新了计算机视觉与自然语言处理领域，催生出在各种任务中表现卓越的强大预训练模型。尤其在自然语言处理任务中，基于Transformer架构的语言模型已成为主导力量——无论是自然语言推理任务中的BERT及其变体，还是自然语言生成任务中的GPT及其后继模型，均展现出标杆性性能。然而这些模型大多以英语或混合语料库进行预训练与评估。本文首次提出基于BART-base架构、在大规模希腊语语料上预训练的Seq2Seq模型GreekBART，通过在多项判别式任务上与BART-random、Greek-BERT及XLM-R进行系统对比评估，并基于新构建的希腊语摘要数据集GreekSUM测试其生成式任务表现。该模型、相关代码及新摘要数据集将全面开源。
