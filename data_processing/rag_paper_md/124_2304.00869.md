# GreekBART: The First Pretrained Greek Sequence-to-Sequence Model

链接: http://arxiv.org/abs/2304.00869v1

原文摘要:
The era of transfer learning has revolutionized the fields of Computer Vision
and Natural Language Processing, bringing powerful pretrained models with
exceptional performance across a variety of tasks. Specifically, Natural
Language Processing tasks have been dominated by transformer-based language
models. In Natural Language Inference and Natural Language Generation tasks,
the BERT model and its variants, as well as the GPT model and its successors,
demonstrated exemplary performance. However, the majority of these models are
pretrained and assessed primarily for the English language or on a multilingual
corpus. In this paper, we introduce GreekBART, the first Seq2Seq model based on
BART-base architecture and pretrained on a large-scale Greek corpus. We
evaluate and compare GreekBART against BART-random, Greek-BERT, and XLM-R on a
variety of discriminative tasks. In addition, we examine its performance on two
NLG tasks from GreekSUM, a newly introduced summarization dataset for the Greek
language. The model, the code, and the new summarization dataset will be
publicly available.

中文翻译:
迁移学习时代彻底改变了计算机视觉和自然语言处理领域，催生出在各种任务中表现卓越的强大预训练模型。尤其在自然语言处理任务中，基于Transformer架构的语言模型已成为主流。在自然语言推理与生成任务中，BERT模型及其变体，以及GPT模型及其后续版本均展现出标杆性性能。然而这些模型大多仅针对英语进行预训练和评估，或基于多语言语料库开展研究。本文首次提出了GreekBART——基于BART-base架构并在大规模希腊语语料库上预训练的序列到序列模型。我们在多项判别式任务中将GreekBART与BART-random、Greek-BERT及XLM-R进行对比评估，并针对新发布的希腊语摘要数据集GreekSUM中的两项自然语言生成任务测试其性能。该模型、相关代码及新型摘要数据集将全部开源发布。
