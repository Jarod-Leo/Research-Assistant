# Inverse Dynamics Pretraining Learns Good Representations for Multitask Imitation

链接: http://arxiv.org/abs/2305.16985v1

原文摘要:
In recent years, domains such as natural language processing and image
recognition have popularized the paradigm of using large datasets to pretrain
representations that can be effectively transferred to downstream tasks. In
this work we evaluate how such a paradigm should be done in imitation learning,
where both pretraining and finetuning data are trajectories collected by
experts interacting with an unknown environment. Namely, we consider a setting
where the pretraining corpus consists of multitask demonstrations and the task
for each demonstration is set by an unobserved latent context variable. The
goal is to use the pretraining corpus to learn a low dimensional representation
of the high dimensional (e.g., visual) observation space which can be
transferred to a novel context for finetuning on a limited dataset of
demonstrations. Among a variety of possible pretraining objectives, we argue
that inverse dynamics modeling -- i.e., predicting an action given the
observations appearing before and after it in the demonstration -- is
well-suited to this setting. We provide empirical evidence of this claim
through evaluations on a variety of simulated visuomotor manipulation problems.
While previous work has attempted various theoretical explanations regarding
the benefit of inverse dynamics modeling, we find that these arguments are
insufficient to explain the empirical advantages often observed in our
settings, and so we derive a novel analysis using a simple but general
environment model.

中文翻译:
近年来，自然语言处理和图像识别等领域已普遍采用"利用大规模数据集预训练表征，再迁移至下游任务"的研究范式。本研究探讨了模仿学习领域应如何实施这一范式——在该场景中，预训练与微调数据均为专家与未知环境交互产生的轨迹数据。具体而言，我们设定预训练语料库由多任务演示轨迹构成，每条轨迹对应的任务由未观测的潜在上下文变量决定。研究目标是通过预训练语料库，将高维（如视觉）观测空间编码为低维表征，使其能迁移至新上下文环境，并基于有限演示数据集进行微调。在多种可能的预训练目标中，我们认为逆向动力学建模（即根据演示轨迹中动作前后的观测数据预测该动作）特别适合此场景。我们通过多种视觉运动操控模拟实验验证了这一观点。尽管前人研究曾尝试从理论层面解释逆向动力学建模的优势，但我们发现这些解释尚不足以说明本实验环境中观察到的实证优势，因此我们基于简单通用的环境模型提出了新的理论分析框架。

（翻译说明：采用学术论文摘要的标准句式结构，通过拆分英文长句、调整语序（如将"Namely"转化为具体化表述）、转换被动语态（如"are collected by"译为主动式"产生"）、专业术语统一（如"pretraining/finetuning"统一为"预训练/微调"）等策略，在保持专业性的同时确保中文表达流畅。关键概念如"inverse dynamics modeling"采用领域内常用译法"逆向动力学建模"，并对复杂理论表述进行逻辑重组，如最后长句通过分号实现层次化呈现。）
