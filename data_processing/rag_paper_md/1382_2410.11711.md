# Zero-shot Model-based Reinforcement Learning using Large Language Models

链接: http://arxiv.org/abs/2410.11711v1

原文摘要:
The emerging zero-shot capabilities of Large Language Models (LLMs) have led
to their applications in areas extending well beyond natural language
processing tasks. In reinforcement learning, while LLMs have been extensively
used in text-based environments, their integration with continuous state spaces
remains understudied. In this paper, we investigate how pre-trained LLMs can be
leveraged to predict in context the dynamics of continuous Markov decision
processes. We identify handling multivariate data and incorporating the control
signal as key challenges that limit the potential of LLMs' deployment in this
setup and propose Disentangled In-Context Learning (DICL) to address them. We
present proof-of-concept applications in two reinforcement learning settings:
model-based policy evaluation and data-augmented off-policy reinforcement
learning, supported by theoretical analysis of the proposed methods. Our
experiments further demonstrate that our approach produces well-calibrated
uncertainty estimates. We release the code at
https://github.com/abenechehab/dicl.

中文翻译:
大型语言模型（LLM）涌现的零样本能力，已使其应用范围远超自然语言处理任务。在强化学习领域，尽管LLM在基于文本的环境中已得到广泛应用，但其与连续状态空间的结合仍鲜有研究。本文探讨如何利用预训练LLM在上下文中预测连续马尔可夫决策过程的动态特性。我们发现处理多变量数据与整合控制信号是限制LLM在此场景下部署潜力的关键挑战，并提出解耦上下文学习（DICL）方法予以解决。通过两个强化学习场景的概念验证应用——基于模型的策略评估和数据增强的离策略强化学习（辅以所提方法的理论分析），我们展示了该方法的有效性。实验进一步表明，我们的方法能生成校准良好的不确定性估计。代码已发布于https://github.com/abenechehab/dicl。
