# Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark

链接: http://arxiv.org/abs/2305.10036v1

原文摘要:
Large language models (LLMs) have demonstrated powerful capabilities in both
text understanding and generation. Companies have begun to offer Embedding as a
Service (EaaS) based on these LLMs, which can benefit various natural language
processing (NLP) tasks for customers. However, previous studies have shown that
EaaS is vulnerable to model extraction attacks, which can cause significant
losses for the owners of LLMs, as training these models is extremely expensive.
To protect the copyright of LLMs for EaaS, we propose an Embedding Watermark
method called EmbMarker that implants backdoors on embeddings. Our method
selects a group of moderate-frequency words from a general text corpus to form
a trigger set, then selects a target embedding as the watermark, and inserts it
into the embeddings of texts containing trigger words as the backdoor. The
weight of insertion is proportional to the number of trigger words included in
the text. This allows the watermark backdoor to be effectively transferred to
EaaS-stealer's model for copyright verification while minimizing the adverse
impact on the original embeddings' utility. Our extensive experiments on
various datasets show that our method can effectively protect the copyright of
EaaS models without compromising service quality.

中文翻译:
大型语言模型（LLM）在文本理解与生成方面展现出强大能力，企业已开始基于此类模型提供嵌入即服务（EaaS），为客户的各类自然语言处理（NLP）任务赋能。然而，已有研究表明EaaS易受模型提取攻击，由于训练这些模型成本极其高昂，此类攻击将给LLM所有者造成重大损失。为保护EaaS中LLM的版权，我们提出名为EmbMarker的嵌入水印方法，通过在嵌入向量中植入后门实现版权保护。该方法从通用文本语料库中筛选一组中频词构成触发词集，选定目标嵌入向量作为水印，将其注入包含触发词的文本嵌入中形成后门。注入权重与文本所含触发词数量成正比，既能有效将水印后门迁移至EaaS窃取者的模型中进行版权验证，又能最大限度降低对原始嵌入功能的影响。我们在多组数据集上的实验表明，该方法能在不影响服务质量的前提下有效保护EaaS模型版权。
