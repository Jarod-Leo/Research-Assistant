# Transformer-Patcher: One Mistake worth One Neuron

链接: http://arxiv.org/abs/2301.09785v1

原文摘要:
Large Transformer-based Pretrained Language Models (PLMs) dominate almost all
Natural Language Processing (NLP) tasks. Nevertheless, they still make mistakes
from time to time. For a model deployed in an industrial environment, fixing
these mistakes quickly and robustly is vital to improve user experiences.
Previous works formalize such problems as Model Editing (ME) and mostly focus
on fixing one mistake. However, the one-mistake-fixing scenario is not an
accurate abstraction of the real-world challenge. In the deployment of AI
services, there are ever-emerging mistakes, and the same mistake may recur if
not corrected in time. Thus a preferable solution is to rectify the mistakes as
soon as they appear nonstop. Therefore, we extend the existing ME into
Sequential Model Editing (SME) to help develop more practical editing methods.
Our study shows that most current ME methods could yield unsatisfying results
in this scenario. We then introduce Transformer-Patcher, a novel model editor
that can shift the behavior of transformer-based models by simply adding and
training a few neurons in the last Feed-Forward Network layer. Experimental
results on both classification and generation tasks show that
Transformer-Patcher can successively correct up to thousands of errors
(Reliability) and generalize to their equivalent inputs (Generality) while
retaining the model's accuracy on irrelevant inputs (Locality). Our method
outperforms previous fine-tuning and HyperNetwork-based methods and achieves
state-of-the-art performance for Sequential Model Editing (SME). The code is
available at https://github.com/ZeroYuHuang/Transformer-Patcher.

中文翻译:
基于Transformer的大型预训练语言模型（PLMs）几乎主导了所有自然语言处理（NLP）任务。然而，它们仍会不时出现错误。对于部署在工业环境中的模型而言，快速且稳健地修复这些错误对提升用户体验至关重要。先前研究将此类问题形式化为模型编辑（Model Editing, ME），并主要集中于修正单一错误。但单错误修正场景并非现实挑战的准确抽象。在AI服务部署中，错误不断涌现，若未及时纠正，同一错误可能反复发生。因此，更优的解决方案是持续即时地修正错误。为此，我们将现有ME扩展为序列化模型编辑（Sequential Model Editing, SME），以开发更具实用性的编辑方法。

研究发现，当前多数ME方法在此场景下表现欠佳。我们继而提出Transformer-Patcher——一种新型模型编辑器，仅需在最后的前馈网络层中增训少量神经元即可调整基于Transformer模型的行为。分类与生成任务的实验结果表明，Transformer-Patcher能连续修正数千个错误（可靠性），并泛化至等效输入（泛化性），同时保持模型对无关输入的准确性（局部性）。该方法优于先前的微调与基于超网络的方法，在序列化模型编辑（SME）中实现了最先进的性能。代码已发布于https://github.com/ZeroYuHuang/Transformer-Patcher。
