# Learning from Predictions: Fusing Training and Autoregressive Inference for Long-Term Spatiotemporal Forecasts

链接: http://arxiv.org/abs/2302.11101v1

原文摘要:
Recurrent Neural Networks (RNNs) have become an integral part of modeling and
forecasting frameworks in areas like natural language processing and
high-dimensional dynamical systems such as turbulent fluid flows. To improve
the accuracy of predictions, RNNs are trained using the Backpropagation Through
Time (BPTT) method to minimize prediction loss. During testing, RNNs are often
used in autoregressive scenarios where the output of the network is fed back
into the input. However, this can lead to the exposure bias effect, as the
network was trained to receive ground-truth data instead of its own
predictions. This mismatch between training and testing is compounded when the
state distributions are different, and the train and test losses are measured.
To address this, previous studies have proposed solutions for language
processing networks with probabilistic predictions. Building on these advances,
we propose the Scheduled Autoregressive BPTT (BPTT-SA) algorithm for predicting
complex systems. Our results show that BPTT-SA effectively reduces iterative
error propagation in Convolutional RNNs and Convolutional Autoencoder RNNs, and
demonstrate its capabilities in long-term prediction of high-dimensional fluid
flows.

中文翻译:
循环神经网络（RNN）已成为自然语言处理及湍流等高维动力系统建模与预测框架的核心组成部分。为提升预测精度，RNN通过时间反向传播（BPTT）算法进行训练以最小化预测误差。然而在测试阶段，当网络以自回归模式运行时（即输出反馈为输入），会引发暴露偏差问题——因为训练时网络接收的是真实数据而非自身预测结果。这种训练与测试的差异在状态分布不同时会被放大，进而导致训练损失与测试损失出现偏差。针对此问题，先前研究已为概率预测型语言处理网络提出了解决方案。基于这些进展，我们提出适用于复杂系统预测的"计划自回归BPTT"算法（BPTT-SA）。实验表明，BPTT-SA能有效抑制卷积RNN和卷积自编码器RNN中的迭代误差传播，并在高维流体流动的长期预测任务中展现出卓越性能。
