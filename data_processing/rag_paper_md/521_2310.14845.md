# ULTRA-DP: Unifying Graph Pre-training with Multi-task Graph Dual Prompt

链接: http://arxiv.org/abs/2310.14845v1

原文摘要:
Recent research has demonstrated the efficacy of pre-training graph neural
networks (GNNs) to capture the transferable graph semantics and enhance the
performance of various downstream tasks. However, the semantic knowledge
learned from pretext tasks might be unrelated to the downstream task, leading
to a semantic gap that limits the application of graph pre-training. To reduce
this gap, traditional approaches propose hybrid pre-training to combine various
pretext tasks together in a multi-task learning fashion and learn multi-grained
knowledge, which, however, cannot distinguish tasks and results in some
transferable task-specific knowledge distortion by each other. Moreover, most
GNNs cannot distinguish nodes located in different parts of the graph, making
them fail to learn position-specific knowledge and lead to suboptimal
performance. In this work, inspired by the prompt-based tuning in natural
language processing, we propose a unified framework for graph hybrid
pre-training which injects the task identification and position identification
into GNNs through a prompt mechanism, namely multi-task graph dual prompt
(ULTRA-DP). Based on this framework, we propose a prompt-based transferability
test to find the most relevant pretext task in order to reduce the semantic
gap. To implement the hybrid pre-training tasks, beyond the classical edge
prediction task (node-node level), we further propose a novel pre-training
paradigm based on a group of $k$-nearest neighbors (node-group level). The
combination of them across different scales is able to comprehensively express
more structural semantics and derive richer multi-grained knowledge. Extensive
experiments show that our proposed ULTRA-DP can significantly enhance the
performance of hybrid pre-training methods and show the generalizability to
other pre-training tasks and backbone architectures.

中文翻译:
近期研究表明，图神经网络（GNNs）的预训练能有效捕获可迁移的图语义信息，从而提升各类下游任务的性能。然而，从预训练任务中习得的语义知识可能与下游任务无关，这种语义鸿沟限制了图预训练的应用。为缩小这一差距，传统方法采用混合预训练策略，通过多任务学习框架整合多种预训练任务以获取多粒度知识，但该方法无法区分任务特性，导致部分可迁移的任务特定知识在交互过程中产生失真。此外，多数GNN模型难以区分图中不同位置的节点，致使无法学习位置特定知识，最终影响模型性能。

本研究受自然语言处理中基于提示微调技术的启发，提出一个统一的图混合预训练框架——ULTRA-DP（多任务图双提示框架），通过提示机制将任务标识与位置标识注入GNN模型。基于该框架，我们设计了基于提示的可迁移性测试，用于筛选最相关的预训练任务以减小语义差距。在混合预训练任务实现方面，除经典的边预测任务（节点-节点层级）外，我们创新性地提出基于k近邻节点组（节点-群体层级）的新型预训练范式。二者在不同尺度上的组合能够全面表达更丰富的结构语义，从而衍生出更具多样性的多粒度知识。大量实验证明，ULTRA-DP能显著提升混合预训练方法的性能，并展现出对其他预训练任务及骨干架构的强泛化能力。

（注：根据学术翻译规范，对部分术语进行了标准化处理：
1. "pretext tasks"译为"预训练任务"而非"借口任务"
2. "multi-grained knowledge"译为"多粒度知识"以保持计算机领域术语一致性
3. "prompt mechanism"统一译为"提示机制"（NLP领域通用译法）
4. 保留ULTRA-DP等算法名称的英文缩写形式，符合计算机领域论文惯例）
