# ULTRA-DP: Unifying Graph Pre-training with Multi-task Graph Dual Prompt

链接: http://arxiv.org/abs/2310.14845v1

原文摘要:
Recent research has demonstrated the efficacy of pre-training graph neural
networks (GNNs) to capture the transferable graph semantics and enhance the
performance of various downstream tasks. However, the semantic knowledge
learned from pretext tasks might be unrelated to the downstream task, leading
to a semantic gap that limits the application of graph pre-training. To reduce
this gap, traditional approaches propose hybrid pre-training to combine various
pretext tasks together in a multi-task learning fashion and learn multi-grained
knowledge, which, however, cannot distinguish tasks and results in some
transferable task-specific knowledge distortion by each other. Moreover, most
GNNs cannot distinguish nodes located in different parts of the graph, making
them fail to learn position-specific knowledge and lead to suboptimal
performance. In this work, inspired by the prompt-based tuning in natural
language processing, we propose a unified framework for graph hybrid
pre-training which injects the task identification and position identification
into GNNs through a prompt mechanism, namely multi-task graph dual prompt
(ULTRA-DP). Based on this framework, we propose a prompt-based transferability
test to find the most relevant pretext task in order to reduce the semantic
gap. To implement the hybrid pre-training tasks, beyond the classical edge
prediction task (node-node level), we further propose a novel pre-training
paradigm based on a group of $k$-nearest neighbors (node-group level). The
combination of them across different scales is able to comprehensively express
more structural semantics and derive richer multi-grained knowledge. Extensive
experiments show that our proposed ULTRA-DP can significantly enhance the
performance of hybrid pre-training methods and show the generalizability to
other pre-training tasks and backbone architectures.

中文翻译:
近期研究表明，预训练图神经网络（GNNs）能有效捕获可迁移的图语义信息，显著提升下游任务性能。然而，从预训练任务中学得的语义知识可能与下游任务无关，这种语义鸿沟限制了图预训练的应用。传统方法通过多任务学习框架混合多种预训练任务来获取多层次知识，但无法区分任务间的差异，导致可迁移的任务特定知识相互干扰。此外，多数GNN模型难以识别图中不同位置的节点，致使无法学习位置特异性知识，影响模型性能。

本研究受自然语言处理中基于提示的调优技术启发，提出统一框架ULTRA-DP（多任务图双提示），通过提示机制将任务标识与位置标识注入GNN模型。基于该框架，我们设计了基于提示的可迁移性测试，用于筛选最相关的预训练任务以缩小语义鸿沟。在实现混合预训练任务时，除经典的边预测任务（节点-节点层级）外，创新性地提出基于k近邻节点组（节点-群组层级）的新型预训练范式。二者跨尺度的组合能全面表达更丰富的结构语义，衍生出多层次知识。

大量实验证明，ULTRA-DP能显著提升混合预训练方法的性能，并展现出对其他预训练任务和骨干架构的强泛化能力。
