# Bridging the Bosphorus: Advancing Turkish Large Language Models through Strategies for Low-Resource Language Adaptation and Benchmarking

链接: http://arxiv.org/abs/2405.04685v1

原文摘要:
Large Language Models (LLMs) are becoming crucial across various fields,
emphasizing the urgency for high-quality models in underrepresented languages.
This study explores the unique challenges faced by low-resource languages, such
as data scarcity, model selection, evaluation, and computational limitations,
with a special focus on Turkish. We conduct an in-depth analysis to evaluate
the impact of training strategies, model choices, and data availability on the
performance of LLMs designed for underrepresented languages. Our approach
includes two methodologies: (i) adapting existing LLMs originally pretrained in
English to understand Turkish, and (ii) developing a model from the ground up
using Turkish pretraining data, both supplemented with supervised fine-tuning
on a novel Turkish instruction-tuning dataset aimed at enhancing reasoning
capabilities. The relative performance of these methods is evaluated through
the creation of a new leaderboard for Turkish LLMs, featuring benchmarks that
assess different reasoning and knowledge skills. Furthermore, we conducted
experiments on data and model scaling, both during pretraining and fine-tuning,
simultaneously emphasizing the capacity for knowledge transfer across languages
and addressing the challenges of catastrophic forgetting encountered during
fine-tuning on a different language. Our goal is to offer a detailed guide for
advancing the LLM framework in low-resource linguistic contexts, thereby making
natural language processing (NLP) benefits more globally accessible.

中文翻译:
大语言模型（LLMs）正逐渐成为各领域的关键技术，这凸显了在资源匮乏语言中开发高质量模型的紧迫性。本研究以土耳其语为重点案例，深入探讨低资源语言面临的独特挑战，包括数据稀缺性、模型选择、评估方法及计算资源限制等问题。我们通过系统分析，评估了训练策略、模型选择和数据可用性对面向弱势语言的大语言模型性能的影响。研究采用两种方法论：(i) 对以英语预训练的现有LLMs进行适配以理解土耳其语；(ii) 直接使用土耳其语预训练数据从头构建模型，两种方法均辅以基于新型土耳其语指令微调数据集的有监督微调，旨在增强模型推理能力。通过建立土耳其语LLMs评估排行榜（包含测试不同推理与知识技能的基准），我们对这些方法的相对性能进行了量化评估。此外，我们在预训练和微调阶段同步开展了数据与模型规模扩展实验，既关注跨语言知识迁移能力，也着力解决不同语言微调过程中出现的灾难性遗忘问题。本研究旨在为低资源语言环境下的LLM框架发展提供详细指南，从而推动自然语言处理（NLP）技术在全球范围内的普惠化发展。
