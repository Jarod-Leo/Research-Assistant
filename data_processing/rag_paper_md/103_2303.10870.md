# Multi-task Transformer with Relation-attention and Type-attention for Named Entity Recognition

链接: http://arxiv.org/abs/2303.10870v1

原文摘要:
Named entity recognition (NER) is an important research problem in natural
language processing. There are three types of NER tasks, including flat, nested
and discontinuous entity recognition. Most previous sequential labeling models
are task-specific, while recent years have witnessed the rising of generative
models due to the advantage of unifying all NER tasks into the seq2seq model
framework. Although achieving promising performance, our pilot studies
demonstrate that existing generative models are ineffective at detecting entity
boundaries and estimating entity types. This paper proposes a multi-task
Transformer, which incorporates an entity boundary detection task into the
named entity recognition task. More concretely, we achieve entity boundary
detection by classifying the relations between tokens within the sentence. To
improve the accuracy of entity-type mapping during decoding, we adopt an
external knowledge base to calculate the prior entity-type distributions and
then incorporate the information into the model via the self and
cross-attention mechanisms. We perform experiments on an extensive set of NER
benchmarks, including two flat, three nested, and three discontinuous NER
datasets. Experimental results show that our approach considerably improves the
generative NER model's performance.

中文翻译:
命名实体识别（NER）是自然语言处理领域的重要研究课题。NER任务主要分为三类：扁平实体识别、嵌套实体识别和非连续实体识别。传统序列标注模型通常针对特定任务设计，而近年来生成式模型因其能将所有NER任务统一到序列到序列（seq2seq）模型框架中的优势逐渐兴起。尽管现有生成模型表现出色，但我们的初步研究表明，这些模型在实体边界检测和实体类型预测方面存在不足。本文提出一种多任务Transformer模型，通过将实体边界检测任务融入命名实体识别任务来提升性能。具体而言，我们通过分类句子内部词元之间的关系来实现实体边界检测。为提高解码过程中实体类型映射的准确性，我们引入外部知识库计算先验实体类型分布，并通过自注意力与交叉注意力机制将这些信息整合到模型中。我们在涵盖两个扁平实体、三个嵌套实体和三个非连续实体数据集的NER基准测试集上进行了实验。结果表明，该方法显著提升了生成式NER模型的性能。
