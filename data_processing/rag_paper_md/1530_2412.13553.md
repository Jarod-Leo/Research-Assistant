# Combining Aggregated Attention and Transformer Architecture for Accurate and Efficient Performance of Spiking Neural Networks

链接: http://arxiv.org/abs/2412.13553v1

原文摘要:
Spiking Neural Networks have attracted significant attention in recent years
due to their distinctive low-power characteristics. Meanwhile, Transformer
models, known for their powerful self-attention mechanisms and parallel
processing capabilities, have demonstrated exceptional performance across
various domains, including natural language processing and computer vision.
Despite the significant advantages of both SNNs and Transformers, directly
combining the low-power benefits of SNNs with the high performance of
Transformers remains challenging. Specifically, while the sparse computing mode
of SNNs contributes to reduced energy consumption, traditional attention
mechanisms depend on dense matrix computations and complex softmax operations.
This reliance poses significant challenges for effective execution in low-power
scenarios. Given the tremendous success of Transformers in deep learning, it is
a necessary step to explore the integration of SNNs and Transformers to harness
the strengths of both. In this paper, we propose a novel model architecture,
Spike Aggregation Transformer (SAFormer), that integrates the low-power
characteristics of SNNs with the high-performance advantages of Transformer
models. The core contribution of SAFormer lies in the design of the Spike
Aggregated Self-Attention (SASA) mechanism, which significantly simplifies the
computation process by calculating attention weights using only the spike
matrices query and key, thereby effectively reducing energy consumption.
Additionally, we introduce a Depthwise Convolution Module (DWC) to enhance the
feature extraction capabilities, further improving overall accuracy. We
evaluated and demonstrated that SAFormer outperforms state-of-the-art SNNs in
both accuracy and energy consumption, highlighting its significant advantages
in low-power and high-performance computing.

中文翻译:
脉冲神经网络（SNNs）凭借其独特的低功耗特性近年来备受关注。与此同时，以强大自注意力机制和并行处理能力著称的Transformer模型在自然语言处理、计算机视觉等多个领域展现出卓越性能。尽管SNNs与Transformer均具有显著优势，但如何将SNNs的低功耗特性与Transformer的高性能直接结合仍面临挑战。具体而言，SNNs的稀疏计算模式虽有助于降低能耗，但传统注意力机制依赖密集矩阵运算和复杂的softmax操作，这种特性对低功耗场景下的高效执行提出了重大挑战。鉴于Transformer在深度学习领域的巨大成功，探索SNNs与Transformer的融合以发挥两者优势已成为必要的研究方向。

本文提出了一种新型模型架构——脉冲聚合Transformer（SAFormer），成功将SNNs的低功耗特性与Transformer模型的高性能优势相结合。SAFormer的核心贡献在于设计了脉冲聚合自注意力机制（SASA），该机制仅通过脉冲矩阵query和key计算注意力权重，大幅简化了运算流程，从而有效降低能耗。此外，我们还引入了深度卷积模块（DWC）以增强特征提取能力，进一步提升整体精度。经实验验证，SAFormer在精度与能耗方面均优于当前最先进的SNNs模型，充分展现了其在低功耗高性能计算领域的显著优势。
