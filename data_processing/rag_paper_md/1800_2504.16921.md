# IberBench: LLM Evaluation on Iberian Languages

链接: http://arxiv.org/abs/2504.16921v1

原文摘要:
Large Language Models (LLMs) remain difficult to evaluate comprehensively,
particularly for languages other than English, where high-quality data is often
limited. Existing benchmarks and leaderboards are predominantly
English-centric, with only a few addressing other languages. These benchmarks
fall short in several key areas: they overlook the diversity of language
varieties, prioritize fundamental Natural Language Processing (NLP)
capabilities over tasks of industrial relevance, and are static. With these
aspects in mind, we present IberBench, a comprehensive and extensible benchmark
designed to assess LLM performance on both fundamental and industry-relevant
NLP tasks, in languages spoken across the Iberian Peninsula and Ibero-America.
IberBench integrates 101 datasets from evaluation campaigns and recent
benchmarks, covering 22 task categories such as sentiment and emotion analysis,
toxicity detection, and summarization. The benchmark addresses key limitations
in current evaluation practices, such as the lack of linguistic diversity and
static evaluation setups by enabling continual updates and community-driven
model and dataset submissions moderated by a committee of experts. We evaluate
23 LLMs ranging from 100 million to 14 billion parameters and provide empirical
insights into their strengths and limitations. Our findings indicate that (i)
LLMs perform worse on industry-relevant tasks than in fundamental ones, (ii)
performance is on average lower for Galician and Basque, (iii) some tasks show
results close to random, and (iv) in other tasks LLMs perform above random but
below shared task systems. IberBench offers open-source implementations for the
entire evaluation pipeline, including dataset normalization and hosting,
incremental evaluation of LLMs, and a publicly accessible leaderboard.

中文翻译:
大型语言模型（LLMs）的全面评估仍面临挑战，尤其在非英语语言领域，高质量数据往往匮乏。现有基准测试和排行榜主要聚焦英语，仅少数涉及其他语言。这些基准存在若干关键缺陷：忽视语言多样性、偏重基础自然语言处理（NLP）能力而非工业相关任务、且采用静态评估模式。为此，我们推出IberBench——一个全面可扩展的基准框架，旨在评估LLMs在伊比利亚半岛及拉丁美洲地区语言中处理基础与工业相关NLP任务的表现。

IberBench整合了来自评估活动与近期基准的101个数据集，涵盖情感分析、毒性检测、文本摘要等22类任务。该基准通过支持持续更新和专家委员会审核的社区驱动模型/数据集提交，解决了当前评估实践中语言多样性缺失和静态评估设置等核心局限。我们对23个参数量从1亿到140亿不等的LLMs进行了评估，实证揭示了其优势与不足。主要发现包括：（1）LLMs在工业相关任务表现逊于基础任务；（2）加利西亚语和巴斯克语的平均表现较低；（3）部分任务结果接近随机水平；（4）某些任务中LLMs表现虽优于随机但不及共享任务系统。

IberBench提供全流程开源实现方案，包含数据集标准化与托管、LLMs增量评估及公开访问的排行榜系统。
