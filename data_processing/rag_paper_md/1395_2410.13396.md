# Linguistically Grounded Analysis of Language Models using Shapley Head Values

链接: http://arxiv.org/abs/2410.13396v1

原文摘要:
Understanding how linguistic knowledge is encoded in language models is
crucial for improving their generalisation capabilities. In this paper, we
investigate the processing of morphosyntactic phenomena, by leveraging a
recently proposed method for probing language models via Shapley Head Values
(SHVs). Using the English language BLiMP dataset, we test our approach on two
widely used models, BERT and RoBERTa, and compare how linguistic constructions
such as anaphor agreement and filler-gap dependencies are handled. Through
quantitative pruning and qualitative clustering analysis, we demonstrate that
attention heads responsible for processing related linguistic phenomena cluster
together. Our results show that SHV-based attributions reveal distinct patterns
across both models, providing insights into how language models organize and
process linguistic information. These findings support the hypothesis that
language models learn subnetworks corresponding to linguistic theory, with
potential implications for cross-linguistic model analysis and interpretability
in Natural Language Processing (NLP).

中文翻译:
理解语言模型如何编码语言学知识对于提升其泛化能力至关重要。本文通过最新提出的Shapley头值（SHV）探测方法，研究了模型对形态句法现象的处理机制。基于英语BLiMP数据集，我们对BERT和RoBERTa两大主流模型进行测试，比较了照应词一致性与填充语-空缺依赖等语言结构的处理方式。通过定量剪枝与定性聚类分析，我们发现处理相关语言现象的注意力头会形成聚类。研究结果表明，基于SHV的归因分析揭示了两种模型间的差异化模式，为理解语言模型组织和处理语言信息的方式提供了新视角。这些发现支持了"语言模型会形成与语言学理论对应的子网络"的假设，对跨语言模型分析和自然语言处理可解释性研究具有潜在启示。
