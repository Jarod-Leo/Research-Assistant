# PrivacyScalpel: Enhancing LLM Privacy via Interpretable Feature Intervention with Sparse Autoencoders

链接: http://arxiv.org/abs/2503.11232v1

原文摘要:
Large Language Models (LLMs) have demonstrated remarkable capabilities in
natural language processing but also pose significant privacy risks by
memorizing and leaking Personally Identifiable Information (PII). Existing
mitigation strategies, such as differential privacy and neuron-level
interventions, often degrade model utility or fail to effectively prevent
leakage. To address this challenge, we introduce PrivacyScalpel, a novel
privacy-preserving framework that leverages LLM interpretability techniques to
identify and mitigate PII leakage while maintaining performance. PrivacyScalpel
comprises three key steps: (1) Feature Probing, which identifies layers in the
model that encode PII-rich representations, (2) Sparse Autoencoding, where a
k-Sparse Autoencoder (k-SAE) disentangles and isolates privacy-sensitive
features,
  and (3) Feature-Level Interventions, which employ targeted ablation and
vector steering to suppress PII leakage.
  Our empirical evaluation on Gemma2-2b and Llama2-7b, fine-tuned on the Enron
dataset, shows that PrivacyScalpel significantly reduces email leakage from
5.15\% to as low as 0.0\%, while maintaining over 99.4\% of the original
model's utility. Notably, our method outperforms neuron-level interventions in
privacy-utility trade-offs, demonstrating that acting on sparse, monosemantic
features is more effective than manipulating polysemantic neurons. Beyond
improving LLM privacy, our approach offers insights into the mechanisms
underlying PII memorization, contributing to the broader field of model
interpretability and secure AI deployment.

中文翻译:
大型语言模型（LLMs）在自然语言处理领域展现出卓越能力，但也因记忆和泄露个人身份信息（PII）而带来重大隐私风险。现有缓解策略如差分隐私和神经元级干预常导致模型效用下降或无法有效阻止泄露。为此，我们提出PrivacyScalpel——一种创新隐私保护框架，通过LLM可解释性技术识别并消除PII泄露，同时保持模型性能。该框架包含三个核心步骤：（1）特征探测：定位模型中编码高密度PII的表征层；（2）稀疏自编码：利用k稀疏自编码器（k-SAE）解耦并隔离隐私敏感特征；（3）特征级干预：采用靶向消融和向量导向技术抑制PII泄露。

基于Enron数据集微调的Gemma2-2b和Llama2-7b实证评估表明，PrivacyScalpel将邮件泄露率从5.15%显著降至0.0%，同时保留原模型99.4%以上的效用。值得注意的是，本方法在隐私-效用权衡上优于神经元级干预，证实作用于稀疏单义特征比操作多义神经元更有效。除提升LLM隐私保护外，该方法揭示了PII记忆机制，为模型可解释性和AI安全部署研究提供了新见解。
