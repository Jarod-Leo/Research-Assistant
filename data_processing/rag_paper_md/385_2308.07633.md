# A Survey on Model Compression for Large Language Models

链接: http://arxiv.org/abs/2308.07633v1

原文摘要:
Large Language Models (LLMs) have transformed natural language processing
tasks successfully. Yet, their large size and high computational needs pose
challenges for practical use, especially in resource-limited settings. Model
compression has emerged as a key research area to address these challenges.
This paper presents a survey of model compression techniques for LLMs. We cover
methods like quantization, pruning, and knowledge distillation, highlighting
recent advancements. We also discuss benchmarking strategies and evaluation
metrics crucial for assessing compressed LLMs. This survey offers valuable
insights for researchers and practitioners, aiming to enhance efficiency and
real-world applicability of LLMs while laying a foundation for future
advancements.

中文翻译:
大型语言模型（LLMs）已成功革新了自然语言处理任务。然而，其庞大的参数量与高计算需求为实际应用带来了挑战，尤其在资源受限的环境中。模型压缩技术由此成为解决这些问题的关键研究方向。本文系统综述了针对LLMs的模型压缩方法，涵盖量化、剪枝、知识蒸馏等技术，并重点阐述了最新研究进展。同时，我们探讨了评估压缩模型性能的基准测试策略与关键指标。本综述为研究者与实践者提供了提升LLMs效率与实用性的重要参考，也为该领域的未来发展奠定了理论基础。
