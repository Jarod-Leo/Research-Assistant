# PromptOptMe: Error-Aware Prompt Compression for LLM-based MT Evaluation Metrics

链接: http://arxiv.org/abs/2412.16120v1

原文摘要:
Evaluating the quality of machine-generated natural language content is a
challenging task in Natural Language Processing (NLP). Recently, large language
models (LLMs) like GPT-4 have been employed for this purpose, but they are
computationally expensive due to the extensive token usage required by complex
evaluation prompts. In this paper, we propose a prompt optimization approach
that uses a smaller, fine-tuned language model to compress input data for
evaluation prompt, thus reducing token usage and computational cost when using
larger LLMs for downstream evaluation. Our method involves a two-stage
fine-tuning process: supervised fine-tuning followed by preference optimization
to refine the model's outputs based on human preferences. We focus on Machine
Translation (MT) evaluation and utilize the GEMBA-MQM metric as a starting
point. Our results show a $2.37\times$ reduction in token usage without any
loss in evaluation quality. This work makes state-of-the-art LLM-based metrics
like GEMBA-MQM more cost-effective and efficient, enhancing their accessibility
for broader use.

中文翻译:
评估机器生成自然语言内容的质量是自然语言处理（NLP）领域的一项挑战性任务。近期，GPT-4等大型语言模型（LLM）被用于此项工作，但由于复杂评估提示需要消耗大量token，这类方法存在计算成本高昂的问题。本文提出一种提示优化方法，通过微调较小规模的语言模型来压缩评估提示的输入数据，从而降低下游评估中使用大型LLM时的token消耗与计算成本。该方法采用两阶段微调流程：先进行监督式微调，再通过偏好优化根据人类反馈精炼模型输出。我们以机器翻译（MT）评估为应用场景，以GEMBA-MQM指标为基准，实验结果表明在保持评估质量不变的前提下实现了2.37倍的token使用量缩减。这项研究使得GEMBA-MQM等基于LLM的先进评估指标更具成本效益，有助于推动其更广泛的应用。
