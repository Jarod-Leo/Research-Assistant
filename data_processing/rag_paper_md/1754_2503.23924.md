# Model Hemorrhage and the Robustness Limits of Large Language Models

链接: http://arxiv.org/abs/2503.23924v1

原文摘要:
Large language models (LLMs) demonstrate strong performance across natural
language processing tasks, yet undergo significant performance degradation when
modified for deployment through quantization, pruning, or decoding strategy
adjustments. We define this phenomenon as model hemorrhage - performance
decline caused by parameter alterations and architectural changes. Through
systematic analysis of various LLM frameworks, we identify key vulnerability
patterns: layer expansion frequently disrupts attention mechanisms, compression
techniques induce information loss cascades, and decoding adjustments amplify
prediction divergences. Our investigation reveals transformer architectures
exhibit inherent robustness thresholds that determine hemorrhage severity
across modification types. We propose three mitigation strategies:
gradient-aware pruning preserves critical weight pathways, dynamic quantization
scaling maintains activation integrity, and decoding calibration aligns
generation trajectories with original model distributions. This work
establishes foundational metrics for evaluating model stability during
adaptation, providing practical guidelines for maintaining performance while
enabling efficient LLM deployment. Our findings advance understanding of neural
network resilience under architectural transformations, particularly for
large-scale language models.

中文翻译:
大规模语言模型在自然语言处理任务中展现出卓越性能，但在通过量化、剪枝或解码策略调整等修改进行部署时，常出现显著的性能下降。我们将这种现象定义为"模型失血"——由参数变更和架构改动引发的性能衰退。通过对多种LLM框架的系统分析，我们识别出关键脆弱性模式：层扩展频繁破坏注意力机制、压缩技术引发信息损失级联、解码调整放大预测分歧。研究发现Transformer架构存在决定不同修改类型下失血严重程度的内在鲁棒性阈值。我们提出三种缓解策略：梯度感知剪枝保留关键权重路径、动态量化缩放维持激活完整性、解码校准使生成轨迹与原始模型分布对齐。本研究建立了评估模型适应过程中稳定性的基础指标，为在实现高效部署的同时保持性能提供了实用指南。这些发现深化了对神经网络在架构变换下鲁棒性的理解，尤其针对大规模语言模型具有重要意义。
