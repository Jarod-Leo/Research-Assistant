# Sparsity-Accelerated Training for Large Language Models

链接: http://arxiv.org/abs/2406.01392v1

原文摘要:
Large language models (LLMs) have demonstrated proficiency across various
natural language processing (NLP) tasks but often require additional training,
such as continual pre-training and supervised fine-tuning. However, the costs
associated with this, primarily due to their large parameter count, remain
high. This paper proposes leveraging \emph{sparsity} in pre-trained LLMs to
expedite this training process. By observing sparsity in activated neurons
during forward iterations, we identify the potential for computational
speed-ups by excluding inactive neurons. We address associated challenges by
extending existing neuron importance evaluation metrics and introducing a
ladder omission rate scheduler. Our experiments on Llama-2 demonstrate that
Sparsity-Accelerated Training (SAT) achieves comparable or superior performance
to standard training while significantly accelerating the process.
Specifically, SAT achieves a $45\%$ throughput improvement in continual
pre-training and saves $38\%$ training time in supervised fine-tuning in
practice. It offers a simple, hardware-agnostic, and easily deployable
framework for additional LLM training. Our code is available at
https://github.com/OpenDFM/SAT.

中文翻译:
大型语言模型（LLMs）已在多种自然语言处理（NLP）任务中展现出卓越能力，但通常需通过持续预训练和有监督微调等额外训练来优化性能。然而，受限于庞大的参数量，此类训练的成本依然居高不下。本文提出利用预训练LLMs中的\emph{稀疏性}来加速训练过程。通过观察前向传播过程中神经元的激活稀疏性，我们发现可通过跳过非活跃神经元实现计算加速。针对相关挑战，我们扩展了现有神经元重要性评估指标，并引入阶梯式省略率调度器。基于Llama-2的实验表明，稀疏性加速训练（SAT）在显著提升训练速度的同时，性能与标准训练相当或更优。具体而言，SAT在持续预训练中实现了45\%的吞吐量提升，在有监督微调中节省了38\%的训练时间。该方法提供了一种简单、硬件无关且易于部署的LLM附加训练框架。代码已开源于https://github.com/OpenDFM/SAT。
