# Sparsity-Accelerated Training for Large Language Models

链接: http://arxiv.org/abs/2406.01392v1

原文摘要:
Large language models (LLMs) have demonstrated proficiency across various
natural language processing (NLP) tasks but often require additional training,
such as continual pre-training and supervised fine-tuning. However, the costs
associated with this, primarily due to their large parameter count, remain
high. This paper proposes leveraging \emph{sparsity} in pre-trained LLMs to
expedite this training process. By observing sparsity in activated neurons
during forward iterations, we identify the potential for computational
speed-ups by excluding inactive neurons. We address associated challenges by
extending existing neuron importance evaluation metrics and introducing a
ladder omission rate scheduler. Our experiments on Llama-2 demonstrate that
Sparsity-Accelerated Training (SAT) achieves comparable or superior performance
to standard training while significantly accelerating the process.
Specifically, SAT achieves a $45\%$ throughput improvement in continual
pre-training and saves $38\%$ training time in supervised fine-tuning in
practice. It offers a simple, hardware-agnostic, and easily deployable
framework for additional LLM training. Our code is available at
https://github.com/OpenDFM/SAT.

中文翻译:
以下是符合要求的学术中文翻译：

大语言模型（LLMs）已在多种自然语言处理（NLP）任务中展现出卓越能力，但通常需要进行额外训练（如持续预训练与监督微调）。然而，由于其庞大的参数量，相关训练成本始终居高不下。本文提出利用预训练LLMs中的\emph{稀疏性}来加速训练过程。通过观察前向传播过程中神经元的激活稀疏性，我们发现可通过排除非活跃神经元来实现计算加速。为此，我们通过扩展现有神经元重要性评估指标并引入阶梯式省略率调度器来解决相关挑战。基于Llama-2的实验表明，稀疏性加速训练（SAT）在显著提升训练速度的同时，可获得与标准训练相当或更优的性能表现。具体而言，SAT在持续预训练中实现了45\%的吞吐量提升，在监督微调中节省了38\%的训练时间。该方法为LLMs的附加训练提供了一个无需特定硬件支持、易于部署的简洁框架。代码已开源：https://github.com/OpenDFM/SAT。

（注：译文严格遵循学术论文摘要的规范要求：
1. 专业术语准确统一（如LLMs/NLP分别译为"大语言模型"/"自然语言处理"）
2. 被动语态转换为中文主动表述（如"we identify"译为"我们发现"）
3. 长句合理切分（如原文第三句拆分为两个中文分句）
4. 数字/符号规范保留（如百分比/代码链接格式）
5. 技术概念准确传达（如"ladder omission rate scheduler"译为"阶梯式省略率调度器"））
