# Effectively Compress KV Heads for LLM

链接: http://arxiv.org/abs/2406.07056v1

原文摘要:
The advent of pre-trained large language models (LLMs) has revolutionized
various natural language processing tasks. These models predominantly employ an
auto-regressive decoding mechanism that utilizes Key-Value (KV) caches to
eliminate redundant calculations for previous tokens. Nevertheless, as context
lengths and batch sizes increase, the linear expansion in memory footprint of
KV caches becomes a key bottleneck of LLM deployment, which decreases
generation speeds significantly. To mitigate this issue, previous techniques
like multi-query attention (MQA) and grouped-query attention (GQA) have been
developed, in order to reduce KV heads to accelerate inference with comparable
accuracy to multi-head attention (MHA). Despite their effectiveness, existing
strategies for compressing MHA often overlook the intrinsic properties of the
KV caches. In this work, we explore the low-rank characteristics of the KV
caches and propose a novel approach for compressing KV heads. In particular, we
carefully optimize the MHA-to-GQA transformation to minimize compression error,
and to remain compatible with rotary position embeddings (RoPE), we also
introduce specialized strategies for key caches with RoPE. We demonstrate that
our method can compress half or even three-quarters of KV heads while
maintaining performance comparable to the original LLMs, which presents a
promising direction for more efficient LLM deployment in resource-constrained
environments.

中文翻译:
预训练大语言模型（LLM）的出现彻底改变了自然语言处理任务的格局。这类模型主要采用自回归解码机制，通过键值（KV）缓存消除对已生成令牌的冗余计算。然而随着上下文长度和批处理规模的增加，KV缓存内存占用的线性增长成为LLM部署的关键瓶颈，会显著降低生成速度。为缓解这一问题，先前研究提出了多查询注意力（MQA）和分组查询注意力（GQA）等技术，通过减少KV头数量在保持多头注意力（MHA）精度水平的同时加速推理。尽管这些方法有效，但现有MHA压缩策略往往忽略了KV缓存的内在特性。本研究通过探索KV缓存的低秩特征，提出了一种新颖的KV头压缩方法。我们特别优化了MHA到GQA的转换过程以最小化压缩误差，同时针对采用旋转位置编码（RoPE）的键缓存设计了专用策略以确保兼容性。实验表明，该方法在压缩50%甚至75% KV头的情况下，仍能保持与原始LLM相当的性能，为资源受限环境下高效部署LLM提供了新思路。

（翻译说明：
1. 专业术语采用学术界通用译法，如"auto-regressive decoding"译为"自回归解码"、"rotary position embeddings"保留英文缩写+中文全称
2. 长难句进行合理切分，如将"which decreases..."独立成句并添加连接词
3. 被动语态转换为主动句式，如"have been developed"译为"研究提出了"
4. 关键概念首次出现保留英文缩写（LLM/KV/MHA等），后文直接用中文指代
5. 技术描述保持精确性，如"low-rank characteristics"译为专业术语"低秩特征"
6. 学术论文特有的逻辑连接词如"Nevertheless"、"Despite"等转换为中文论文常用表述方式）
