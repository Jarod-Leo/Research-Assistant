# Effectively Compress KV Heads for LLM

链接: http://arxiv.org/abs/2406.07056v1

原文摘要:
The advent of pre-trained large language models (LLMs) has revolutionized
various natural language processing tasks. These models predominantly employ an
auto-regressive decoding mechanism that utilizes Key-Value (KV) caches to
eliminate redundant calculations for previous tokens. Nevertheless, as context
lengths and batch sizes increase, the linear expansion in memory footprint of
KV caches becomes a key bottleneck of LLM deployment, which decreases
generation speeds significantly. To mitigate this issue, previous techniques
like multi-query attention (MQA) and grouped-query attention (GQA) have been
developed, in order to reduce KV heads to accelerate inference with comparable
accuracy to multi-head attention (MHA). Despite their effectiveness, existing
strategies for compressing MHA often overlook the intrinsic properties of the
KV caches. In this work, we explore the low-rank characteristics of the KV
caches and propose a novel approach for compressing KV heads. In particular, we
carefully optimize the MHA-to-GQA transformation to minimize compression error,
and to remain compatible with rotary position embeddings (RoPE), we also
introduce specialized strategies for key caches with RoPE. We demonstrate that
our method can compress half or even three-quarters of KV heads while
maintaining performance comparable to the original LLMs, which presents a
promising direction for more efficient LLM deployment in resource-constrained
environments.

中文翻译:
预训练大语言模型（LLM）的出现彻底改变了各类自然语言处理任务。这些模型主要采用自回归解码机制，利用键值（KV）缓存消除对历史令牌的冗余计算。然而随着上下文长度和批量大小的增加，KV缓存内存占用的线性扩张成为LLM部署的关键瓶颈，会显著降低生成速度。为缓解此问题，先前研究提出了多查询注意力（MQA）和分组查询注意力（GQA）等技术，通过减少KV头数量在保持与多头注意力（MHA）相当精度的同时加速推理。尽管这些方法有效，现有MHA压缩策略往往忽略了KV缓存的内在特性。本研究基于KV缓存的低秩特性，提出了一种新颖的KV头压缩方法。我们精心优化了MHA到GQA的转换过程以最小化压缩误差，并针对采用旋转位置编码（RoPE）的键缓存设计了专用策略以保持兼容性。实验表明，该方法能压缩半数甚至四分之三KV头的同时保持与原模型相当的性能，为资源受限环境下高效部署LLM提供了可行方向。
