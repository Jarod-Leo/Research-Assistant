# ReAGent: Towards A Model-agnostic Feature Attribution Method for Generative Language Models

链接: http://arxiv.org/abs/2402.00794v1

原文摘要:
Feature attribution methods (FAs), such as gradients and attention, are
widely employed approaches to derive the importance of all input features to
the model predictions. Existing work in natural language processing has mostly
focused on developing and testing FAs for encoder-only language models (LMs) in
classification tasks. However, it is unknown if it is faithful to use these FAs
for decoder-only models on text generation, due to the inherent differences
between model architectures and task settings respectively. Moreover, previous
work has demonstrated that there is no `one-wins-all' FA across models and
tasks. This makes the selection of a FA computationally expensive for large LMs
since input importance derivation often requires multiple forward and backward
passes including gradient computations that might be prohibitive even with
access to large compute. To address these issues, we present a model-agnostic
FA for generative LMs called Recursive Attribution Generator (ReAGent). Our
method updates the token importance distribution in a recursive manner. For
each update, we compute the difference in the probability distribution over the
vocabulary for predicting the next token between using the original input and
using a modified version where a part of the input is replaced with RoBERTa
predictions. Our intuition is that replacing an important token in the context
should have resulted in a larger change in the model's confidence in predicting
the token than replacing an unimportant token. Our method can be universally
applied to any generative LM without accessing internal model weights or
additional training and fine-tuning, as most other FAs require. We extensively
compare the faithfulness of ReAGent with seven popular FAs across six
decoder-only LMs of various sizes. The results show that our method
consistently provides more faithful token importance distributions.

中文翻译:
特征归因方法（FAs），如梯度和注意力机制，是广泛用于推导所有输入特征对模型预测重要性的技术。现有自然语言处理研究主要集中于针对仅编码器语言模型（LMs）在分类任务中开发和测试FAs。然而，由于模型架构和任务设置的内在差异，尚不清楚这些FAs是否适用于仅解码器模型在文本生成任务中的忠实性评估。此外，先前研究表明，不存在适用于所有模型和任务的"通用最优"FA。这使得对于大型语言模型而言，选择FA的计算成本极高——因为输入重要性推导通常需要多次前向和反向传播（包括梯度计算），即使拥有强大算力也可能难以实现。

为解决这些问题，我们提出了一种与模型无关的生成式语言模型FA方法，称为递归归因生成器（ReAGent）。该方法以递归方式更新令牌重要性分布：每次更新时，我们计算原始输入与部分输入被RoBERTa预测替换后，模型在预测下一令牌时词汇表概率分布的差异。其核心直觉是——替换上下文中的重要令牌应比替换不重要令牌导致模型预测置信度发生更大变化。该方法无需访问模型内部权重或进行额外训练微调（如大多数其他FA所需），即可普遍适用于任何生成式语言模型。我们系统比较了ReAGent与七种主流FAs在六种不同规模的仅解码器模型上的忠实性表现，结果表明我们的方法能持续提供更可靠的令牌重要性分布。
