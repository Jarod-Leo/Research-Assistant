# CPRM: A LLM-based Continual Pre-training Framework for Relevance Modeling in Commercial Search

链接: http://arxiv.org/abs/2412.01269v2

原文摘要:
Relevance modeling between queries and items stands as a pivotal component in
commercial search engines, directly affecting the user experience. Given the
remarkable achievements of large language models (LLMs) in various natural
language processing (NLP) tasks, LLM-based relevance modeling is gradually
being adopted within industrial search systems. Nevertheless, foundational LLMs
lack domain-specific knowledge and do not fully exploit the potential of
in-context learning. Furthermore, structured item text remains underutilized,
and there is a shortage in the supply of corresponding queries and background
knowledge. We thereby propose CPRM (Continual Pre-training for Relevance
Modeling), a framework designed for the continual pre-training of LLMs to
address these issues. Our CPRM framework includes three modules: 1) employing
both queries and multi-field item to jointly pre-train for enhancing domain
knowledge, 2) applying in-context pre-training, a novel approach where LLMs are
pre-trained on a sequence of related queries or items, and 3) conducting
reading comprehension on items to produce associated domain knowledge and
background information (e.g., generating summaries and corresponding queries)
to further strengthen LLMs. Results on offline experiments and online A/B
testing demonstrate that our model achieves convincing performance compared to
strong baselines.

中文翻译:
查询与商品之间的相关性建模是商业搜索引擎的核心环节，其效果直接影响用户体验。鉴于大语言模型（LLMs）在各类自然语言处理任务中的卓越表现，基于LLM的相关性建模正逐步应用于工业搜索系统。然而，基础大语言模型存在领域知识匮乏、上下文学习潜力未充分挖掘等问题；同时结构化商品文本利用率不足，且缺乏对应查询与背景知识的供给。为此，我们提出CPRM（Continual Pre-training for Relevance Modeling）框架，通过持续预训练解决上述问题。该框架包含三大模块：1）采用查询与多字段商品联合预训练以增强领域知识；2）创新性地实施上下文预训练，使模型在相关查询或商品序列上进行预训练；3）对商品进行阅读理解，生成关联领域知识与背景信息（如摘要及对应查询）以强化模型。离线和在线A/B测试结果表明，相较于强基线模型，我们的方案展现出显著优势。
