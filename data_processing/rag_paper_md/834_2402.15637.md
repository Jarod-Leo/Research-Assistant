# Addressing Order Sensitivity of In-Context Demonstration Examples in Causal Language Models

链接: http://arxiv.org/abs/2402.15637v1

原文摘要:
In-context learning has become a popular paradigm in natural language
processing. However, its performance can be significantly influenced by the
order of in-context demonstration examples. In this paper, we found that causal
language models (CausalLMs) are more sensitive to this order compared to prefix
language models (PrefixLMs). We attribute this phenomenon to the
auto-regressive attention masks within CausalLMs, which restrict each token
from accessing information from subsequent tokens. This results in different
receptive fields for samples at different positions, thereby leading to
representation disparities across positions. To tackle this challenge, we
introduce an unsupervised fine-tuning method, termed the Information-Augmented
and Consistency-Enhanced approach. This approach utilizes contrastive learning
to align representations of in-context examples across different positions and
introduces a consistency loss to ensure similar representations for inputs with
different permutations. This enhances the model's predictive consistency across
permutations. Experimental results on five benchmarks suggest that our proposed
method can reduce the sensitivity of CausalLMs to the order of in-context
examples and exhibit robust generalizability, particularly when demonstrations
are sourced from a candidate pool different from that used in the training
phase, or when the number of in-context examples differs from what is used
during training.

中文翻译:
上下文学习已成为自然语言处理中的一种流行范式。然而，其性能可能显著受到上下文示例顺序的影响。本文发现，与前缀语言模型（PrefixLMs）相比，因果语言模型（CausalLMs）对此顺序更为敏感。我们将此现象归因于因果语言模型中的自回归注意力掩码机制，该机制限制每个词元获取后续词元的信息，导致不同位置的样本具有不同的感受野，从而产生跨位置的表示差异。为解决这一问题，我们提出了一种名为"信息增强与一致性优化"的无监督微调方法。该方法通过对比学习对齐不同位置上下文示例的表示，并引入一致性损失确保不同排列输入的表示相似性，从而增强模型在排列组合间的预测一致性。在五个基准测试上的实验结果表明，所提方法能有效降低因果语言模型对上下文示例顺序的敏感性，并展现出强泛化能力——尤其在演示样本来自与训练阶段不同的候选池，或上下文示例数量与训练时不同的情况下表现稳健。
