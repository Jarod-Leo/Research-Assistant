# Interpreting Bias in Large Language Models: A Feature-Based Approach

链接: http://arxiv.org/abs/2406.12347v1

原文摘要:
Large Language Models (LLMs) such as Mistral and LLaMA have showcased
remarkable performance across various natural language processing (NLP) tasks.
Despite their success, these models inherit social biases from the diverse
datasets on which they are trained. This paper investigates the propagation of
biases within LLMs through a novel feature-based analytical approach. Drawing
inspiration from causal mediation analysis, we hypothesize the evolution of
bias-related features and validate them using interpretability techniques like
activation and attribution patching. Our contributions are threefold: (1) We
introduce and empirically validate a feature-based method for bias analysis in
LLMs, applied to LLaMA-2-7B, LLaMA-3-8B, and Mistral-7B-v0.3 with templates
from a professions dataset. (2) We extend our method to another form of gender
bias, demonstrating its generalizability. (3) We differentiate the roles of
MLPs and attention heads in bias propagation and implement targeted debiasing
using a counterfactual dataset. Our findings reveal the complex nature of bias
in LLMs and emphasize the necessity for tailored debiasing strategies, offering
a deeper understanding of bias mechanisms and pathways for effective
mitigation.

中文翻译:
Mistral和LLaMA等大语言模型（LLM）在各类自然语言处理任务中展现出卓越性能。然而这些模型在训练过程中从多样化的数据集中继承了社会偏见。本文通过一种新颖的基于特征的分析方法，探究了LLM中偏见的传播机制。受因果中介分析启发，我们假设偏见相关特征的演化过程，并利用激活修补和归因修补等可解释性技术进行验证。主要贡献包括：（1）提出并实证验证了针对LLM的基于特征偏见分析方法，应用于LLaMA-2-7B、LLaMA-3-8B和Mistral-7B-v0.3模型，测试数据来自职业数据集模板；（2）将该方法扩展至另一种性别偏见形式，证明其普适性；（3）区分了MLP层与注意力头在偏见传播中的不同作用，基于反事实数据集实现了针对性去偏。研究结果揭示了LLM偏见的复杂性，强调需要定制化去偏策略，为理解偏见机制和开发有效缓解路径提供了新见解。
