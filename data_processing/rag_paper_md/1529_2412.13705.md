# Mitigating Adversarial Attacks in LLMs through Defensive Suffix Generation

链接: http://arxiv.org/abs/2412.13705v1

原文摘要:
Large language models (LLMs) have exhibited outstanding performance in
natural language processing tasks. However, these models remain susceptible to
adversarial attacks in which slight input perturbations can lead to harmful or
misleading outputs. A gradient-based defensive suffix generation algorithm is
designed to bolster the robustness of LLMs. By appending carefully optimized
defensive suffixes to input prompts, the algorithm mitigates adversarial
influences while preserving the models' utility. To enhance adversarial
understanding, a novel total loss function ($L_{\text{total}}$) combining
defensive loss ($L_{\text{def}}$) and adversarial loss ($L_{\text{adv}}$)
generates defensive suffixes more effectively. Experimental evaluations
conducted on open-source LLMs such as Gemma-7B, mistral-7B, Llama2-7B, and
Llama2-13B show that the proposed method reduces attack success rates (ASR) by
an average of 11\% compared to models without defensive suffixes. Additionally,
the perplexity score of Gemma-7B decreased from 6.57 to 3.93 when applying the
defensive suffix generated by openELM-270M. Furthermore, TruthfulQA evaluations
demonstrate consistent improvements with Truthfulness scores increasing by up
to 10\% across tested configurations. This approach significantly enhances the
security of LLMs in critical applications without requiring extensive
retraining.

中文翻译:
大型语言模型（LLM）在自然语言处理任务中展现出卓越性能，但这些模型仍易受对抗性攻击影响——微小的输入扰动即可导致有害或误导性输出。本文设计了一种基于梯度的防御性后缀生成算法，通过向输入提示词附加精心优化的防御后缀，在保持模型实用性的同时有效抵御对抗干扰。为提升对抗理解能力，创新性地提出融合防御损失（$L_{\text{def}}$）与对抗损失（$L_{\text{adv}}$）的复合损失函数（$L_{\text{total}}$），显著优化了防御后缀生成效果。在Gemma-7B、mistral-7B、Llama2-7B和Llama2-13B等开源模型上的实验表明：相比无防御后缀模型，该方法平均降低11%的攻击成功率（ASR）；当采用openELM-270M生成的防御后缀时，Gemma-7B的困惑度从6.57降至3.93。TruthfulQA评估显示，各测试配置下的真实性评分最高提升10%。该方法无需大规模重训练即可显著增强关键应用中LLM的安全性。
