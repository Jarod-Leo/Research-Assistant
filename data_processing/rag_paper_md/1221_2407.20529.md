# Can LLMs be Fooled? Investigating Vulnerabilities in LLMs

链接: http://arxiv.org/abs/2407.20529v1

原文摘要:
The advent of Large Language Models (LLMs) has garnered significant
popularity and wielded immense power across various domains within Natural
Language Processing (NLP). While their capabilities are undeniably impressive,
it is crucial to identify and scrutinize their vulnerabilities especially when
those vulnerabilities can have costly consequences. One such LLM, trained to
provide a concise summarization from medical documents could unequivocally leak
personal patient data when prompted surreptitiously. This is just one of many
unfortunate examples that have been unveiled and further research is necessary
to comprehend the underlying reasons behind such vulnerabilities. In this
study, we delve into multiple sections of vulnerabilities which are
model-based, training-time, inference-time vulnerabilities, and discuss
mitigation strategies including "Model Editing" which aims at modifying LLMs
behavior, and "Chroma Teaming" which incorporates synergy of multiple teaming
strategies to enhance LLMs' resilience. This paper will synthesize the findings
from each vulnerability section and propose new directions of research and
development. By understanding the focal points of current vulnerabilities, we
can better anticipate and mitigate future risks, paving the road for more
robust and secure LLMs.

中文翻译:
大型语言模型（LLM）的出现已在自然语言处理（NLP）各领域引发广泛关注并展现出强大能力。尽管其性能令人瞩目，但识别与剖析其脆弱性至关重要——尤其是当这些漏洞可能造成高昂代价时。例如，某款经过训练可从医疗文档生成摘要的LLM，在受到隐蔽诱导时竟会泄露患者隐私数据。这仅是众多已曝光案例之一，亟需深入研究以理解此类漏洞的根源机制。  

本研究系统探究了多维度脆弱性：基于模型架构的缺陷、训练阶段的漏洞及推理环节的风险，并提出"模型编辑"（通过参数修正调整LLM行为）与"色彩协同"（融合多重协作策略增强模型鲁棒性）等应对方案。本文整合各脆弱性维度的研究发现，指明未来研发的新方向。通过厘清当前漏洞的核心症结，我们能够更精准地预测与防范潜在风险，为构建更稳健、安全的LLM铺平道路。
