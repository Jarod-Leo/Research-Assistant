# Exploring Effectiveness of GPT-3 in Grammatical Error Correction: A Study on Performance and Controllability in Prompt-Based Methods

链接: http://arxiv.org/abs/2305.18156v1

原文摘要:
Large-scale pre-trained language models such as GPT-3 have shown remarkable
performance across various natural language processing tasks. However, applying
prompt-based methods with GPT-3 for Grammatical Error Correction (GEC) tasks
and their controllability remains underexplored. Controllability in GEC is
crucial for real-world applications, particularly in educational settings,
where the ability to tailor feedback according to learner levels and specific
error types can significantly enhance the learning process. This paper
investigates the performance and controllability of prompt-based methods with
GPT-3 for GEC tasks using zero-shot and few-shot setting. We explore the impact
of task instructions and examples on GPT-3's output, focusing on controlling
aspects such as minimal edits, fluency edits, and learner levels. Our findings
demonstrate that GPT-3 could effectively perform GEC tasks, outperforming
existing supervised and unsupervised approaches. We also showed that GPT-3
could achieve controllability when appropriate task instructions and examples
are given.

中文翻译:
诸如GPT-3等大规模预训练语言模型已在各类自然语言处理任务中展现出卓越性能。然而，如何将基于提示的方法应用于GPT-3进行语法错误修正（GEC）任务及其可控性研究仍显不足。GEC的可控性在实际应用中至关重要，特别是在教育场景下，根据学习者水平和特定错误类型定制反馈的能力能显著提升学习效果。本文通过零样本和小样本设置，研究了基于提示的GPT-3方法在GEC任务中的性能与可控性。我们重点探究任务指令和示例对GPT-3输出的影响，包括最小化编辑、流畅性编辑和学习者水平等控制维度。研究发现GPT-3能有效执行GEC任务，其表现优于现有监督与非监督方法。实验同时证明，当提供恰当的任务指令和示例时，GPT-3可实现可控的语法纠错。
