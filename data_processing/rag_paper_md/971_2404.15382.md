# Feature Distribution Shift Mitigation with Contrastive Pretraining for Intrusion Detection

链接: http://arxiv.org/abs/2404.15382v1

原文摘要:
In recent years, there has been a growing interest in using Machine Learning
(ML), especially Deep Learning (DL) to solve Network Intrusion Detection (NID)
problems. However, the feature distribution shift problem remains a difficulty,
because the change in features' distributions over time negatively impacts the
model's performance. As one promising solution, model pretraining has emerged
as a novel training paradigm, which brings robustness against feature
distribution shift and has proven to be successful in Computer Vision (CV) and
Natural Language Processing (NLP). To verify whether this paradigm is
beneficial for NID problem, we propose SwapCon, a ML model in the context of
NID, which compresses shift-invariant feature information during the
pretraining stage and refines during the finetuning stage. We exemplify the
evidence of feature distribution shift using the Kyoto2006+ dataset. We
demonstrate how pretraining a model with the proper size can increase
robustness against feature distribution shifts by over 8%. Moreover, we show
how an adequate numerical embedding strategy also enhances the performance of
pretrained models. Further experiments show that the proposed SwapCon model
also outperforms eXtreme Gradient Boosting (XGBoost) and K-Nearest Neighbor
(KNN) based models by a large margin.

中文翻译:
近年来，利用机器学习（ML），尤其是深度学习（DL）解决网络入侵检测（NID）问题的研究日益受到关注。然而，特征分布偏移问题仍是难点，因为特征分布随时间变化会显著降低模型性能。作为一种有前景的解决方案，模型预训练已成为新型训练范式，它能增强模型对特征分布偏移的鲁棒性，并已在计算机视觉（CV）和自然语言处理（NLP）领域取得成功。为验证该范式对NID问题的适用性，我们提出SwapCon模型——一种面向NID的ML模型，通过在预训练阶段压缩偏移不变特征信息，并在微调阶段进行优化。基于Kyoto2006+数据集，我们实证了特征分布偏移现象，并证明适当规模的预训练可使模型对特征分布偏移的鲁棒性提升超8%。研究还表明，合理的数值嵌入策略能进一步提升预训练模型性能。对比实验显示，SwapCon模型在性能上显著优于基于XGBoost和KNN的模型。
