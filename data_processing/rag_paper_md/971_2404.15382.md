# Feature Distribution Shift Mitigation with Contrastive Pretraining for Intrusion Detection

链接: http://arxiv.org/abs/2404.15382v1

原文摘要:
In recent years, there has been a growing interest in using Machine Learning
(ML), especially Deep Learning (DL) to solve Network Intrusion Detection (NID)
problems. However, the feature distribution shift problem remains a difficulty,
because the change in features' distributions over time negatively impacts the
model's performance. As one promising solution, model pretraining has emerged
as a novel training paradigm, which brings robustness against feature
distribution shift and has proven to be successful in Computer Vision (CV) and
Natural Language Processing (NLP). To verify whether this paradigm is
beneficial for NID problem, we propose SwapCon, a ML model in the context of
NID, which compresses shift-invariant feature information during the
pretraining stage and refines during the finetuning stage. We exemplify the
evidence of feature distribution shift using the Kyoto2006+ dataset. We
demonstrate how pretraining a model with the proper size can increase
robustness against feature distribution shifts by over 8%. Moreover, we show
how an adequate numerical embedding strategy also enhances the performance of
pretrained models. Further experiments show that the proposed SwapCon model
also outperforms eXtreme Gradient Boosting (XGBoost) and K-Nearest Neighbor
(KNN) based models by a large margin.

中文翻译:
近年来，利用机器学习（ML）尤其是深度学习（DL）解决网络入侵检测（NID）问题的研究日益受到关注。然而，特征分布偏移问题仍是难点——特征分布随时间变化会显著降低模型性能。作为一种具有前景的解决方案，模型预训练已成为新型训练范式，其能增强模型对特征分布偏移的鲁棒性，并已在计算机视觉（CV）和自然语言处理（NLP）领域验证了有效性。为验证该范式对NID问题的适用性，我们提出SwapCon模型：该模型在预训练阶段压缩偏移不变的特征信息，在微调阶段进行特征优化。基于Kyoto2006+数据集，我们实证了特征分布偏移现象的存在。实验表明，采用适当规模的预训练模型可使特征分布偏移的鲁棒性提升8%以上。同时，合理的数值嵌入策略也能提升预训练模型性能。对比实验进一步证明，所提出的SwapCon模型在性能上显著优于基于XGBoost和K近邻（KNN）的基准模型。
