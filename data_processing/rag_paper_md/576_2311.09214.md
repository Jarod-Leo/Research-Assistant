# Mind's Mirror: Distilling Self-Evaluation Capability and Comprehensive Thinking from Large Language Models

链接: http://arxiv.org/abs/2311.09214v1

原文摘要:
Large language models (LLMs) have achieved remarkable advancements in natural
language processing. However, the massive scale and computational demands of
these models present formidable challenges when considering their practical
deployment in resource-constrained environments. While techniques such as
chain-of-thought (CoT) distillation have displayed promise in distilling LLMs
into small language models (SLMs), there is a risk that distilled SLMs may
still inherit flawed reasoning and hallucinations from LLMs. To address these
issues, we propose a twofold methodology: First, we introduce a novel method
for distilling the self-evaluation capability from LLMs into SLMs, aiming to
mitigate the adverse effects of flawed reasoning and hallucinations inherited
from LLMs. Second, we advocate for distilling more comprehensive thinking by
incorporating multiple distinct CoTs and self-evaluation outputs, to ensure a
more thorough and robust knowledge transfer into SLMs. Experiments on three NLP
benchmarks demonstrate that our method significantly improves the performance
of distilled SLMs, offering a new perspective for developing more effective and
efficient SLMs in resource-constrained environments.

中文翻译:
大型语言模型（LLM）在自然语言处理领域取得了显著进展。然而，这些模型庞大的参数量与计算需求为其在资源受限环境中的实际部署带来了严峻挑战。尽管思维链（CoT）蒸馏等技术已展现出将LLM压缩为小型语言模型（SLM）的潜力，但蒸馏后的SLM仍可能继承LLM存在的错误推理与幻觉问题。针对这些挑战，我们提出双重解决方案：首先，我们创新性地将LLM的自我评估能力蒸馏至SLM，旨在缓解从LLM继承的错误推理与幻觉的负面影响；其次，我们主张通过整合多条差异化思维链及自我评估输出来实现更全面的思维蒸馏，从而确保向SLM传递更彻底、更鲁棒的知识。在三个NLP基准测试上的实验表明，该方法显著提升了蒸馏后SLM的性能，为资源受限环境下开发更高效能的小型语言模型提供了新思路。
