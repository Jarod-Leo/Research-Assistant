# A Transformer-based Framework For Multi-variate Time Series: A Remaining Useful Life Prediction Use Case

链接: http://arxiv.org/abs/2308.09884v1

原文摘要:
In recent times, Large Language Models (LLMs) have captured a global
spotlight and revolutionized the field of Natural Language Processing. One of
the factors attributed to the effectiveness of LLMs is the model architecture
used for training, transformers. Transformer models excel at capturing
contextual features in sequential data since time series data are sequential,
transformer models can be leveraged for more efficient time series data
prediction. The field of prognostics is vital to system health management and
proper maintenance planning. A reliable estimation of the remaining useful life
(RUL) of machines holds the potential for substantial cost savings. This
includes avoiding abrupt machine failures, maximizing equipment usage, and
serving as a decision support system (DSS). This work proposed an
encoder-transformer architecture-based framework for multivariate time series
prediction for a prognostics use case. We validated the effectiveness of the
proposed framework on all four sets of the C-MAPPS benchmark dataset for the
remaining useful life prediction task. To effectively transfer the knowledge
and application of transformers from the natural language domain to time
series, three model-specific experiments were conducted. Also, to enable the
model awareness of the initial stages of the machine life and its degradation
path, a novel expanding window method was proposed for the first time in this
work, it was compared with the sliding window method, and it led to a large
improvement in the performance of the encoder transformer model. Finally, the
performance of the proposed encoder-transformer model was evaluated on the test
dataset and compared with the results from 13 other state-of-the-art (SOTA)
models in the literature and it outperformed them all with an average
performance increase of 137.65% over the next best model across all the
datasets.

中文翻译:
近年来，大型语言模型（LLMs）在全球范围内引发关注，并彻底改变了自然语言处理领域。LLMs高效性的关键因素之一在于其所采用的训练模型架构——Transformer。由于时间序列数据具有顺序性特征，Transformer模型在捕捉序列数据中的上下文特征方面表现卓越，因此可被更高效地应用于时间序列数据预测。预测技术对系统健康管理和维护规划至关重要，准确预估机器剩余使用寿命（RUL）能带来显著的成本节约，包括避免突发故障、最大化设备利用率以及作为决策支持系统（DSS）发挥作用。

本研究提出了一种基于编码器-Transformer架构的多变量时间序列预测框架，用于寿命预测场景。我们在C-MAPPS基准数据集全部四个子集上验证了该框架在剩余使用寿命预测任务中的有效性。为有效实现Transformer模型从自然语言领域到时序预测的知识迁移与应用转化，研究进行了三项针对性实验。此外，为使模型感知机器生命周期初始阶段及其退化轨迹，本文首次提出了一种创新的扩展窗口方法，通过与滑动窗口法的对比实验表明，该方法显著提升了编码器-Transformer模型的性能表现。最终，所提模型在测试数据集上的评估结果显示：相较于文献中其他13种最先进（SOTA）模型，该框架实现了全面超越，平均性能提升幅度达137.65%。
