# Elo Uncovered: Robustness and Best Practices in Language Model Evaluation

链接: http://arxiv.org/abs/2311.17295v1

原文摘要:
In Natural Language Processing (NLP), the Elo rating system, originally
designed for ranking players in dynamic games such as chess, is increasingly
being used to evaluate Large Language Models (LLMs) through "A vs B" paired
comparisons. However, while popular, the system's suitability for assessing
entities with constant skill levels, such as LLMs, remains relatively
unexplored. We study two fundamental axioms that evaluation methods should
adhere to: reliability and transitivity. We conduct extensive evaluation of Elo
behaviour, illustrating that individual Elo computations exhibit volatility and
delving into the impact of varying the Elo rating system's hyperparameters. We
show that these axioms are not always satisfied raising questions about the
reliability of current comparative evaluations of LLMs. If the current use of
Elo scores is intended to substitute the costly head-to-head comparison of
LLMs, it is crucial to ensure the ranking is as robust as possible. Guided by
the axioms, our findings offer concrete guidelines for enhancing the
reliability of LLM evaluation methods, suggesting a need for reassessment of
existing comparative approaches.

中文翻译:
在自然语言处理（NLP）领域，原本为国际象棋等动态博弈设计的Elo评分系统，正日益频繁地通过"A vs B"配对比较方式用于评估大语言模型（LLMs）。然而尽管该方法广受欢迎，该系统对LLMs这类能力恒定型实体的适用性仍缺乏深入研究。我们基于评估方法应满足的两个核心公理——可靠性与传递性展开研究，通过大量实验分析Elo系统的行为特征，不仅揭示了单个Elo计算存在波动性，还深入探究了系统超参数变化产生的影响。研究表明这些公理并非总能得到满足，这对当前LLMs比较评估的可靠性提出了质疑。若采用Elo评分旨在替代成本高昂的LLMs直接对抗测试，则必须确保排名体系具备最高稳健性。基于公理指导的研究结论为提升LLM评估方法可靠性提供了具体改进方向，同时表明现有比较评估体系亟待重新审视。
