# Efficient Vision Transformer for Accurate Traffic Sign Detection

链接: http://arxiv.org/abs/2311.01429v1

原文摘要:
This research paper addresses the challenges associated with traffic sign
detection in self-driving vehicles and driver assistance systems. The
development of reliable and highly accurate algorithms is crucial for the
widespread adoption of traffic sign recognition and detection (TSRD) in diverse
real-life scenarios. However, this task is complicated by suboptimal traffic
images affected by factors such as camera movement, adverse weather conditions,
and inadequate lighting. This study specifically focuses on traffic sign
detection methods and introduces the application of the Transformer model,
particularly the Vision Transformer variants, to tackle this task. The
Transformer's attention mechanism, originally designed for natural language
processing, offers improved parallel efficiency. Vision Transformers have
demonstrated success in various domains, including autonomous driving, object
detection, healthcare, and defense-related applications. To enhance the
efficiency of the Transformer model, the research proposes a novel strategy
that integrates a locality inductive bias and a transformer module. This
includes the introduction of the Efficient Convolution Block and the Local
Transformer Block, which effectively capture short-term and long-term
dependency information, thereby improving both detection speed and accuracy.
Experimental evaluations demonstrate the significant advancements achieved by
this approach, particularly when applied to the GTSDB dataset.

中文翻译:
本研究针对自动驾驶车辆与驾驶员辅助系统中交通标志检测所面临的挑战展开探讨。开发可靠且高精度的算法对于交通标志识别与检测（TSRD）技术在各种现实场景中的广泛应用至关重要。然而，该任务因受相机移动、恶劣天气条件及光照不足等因素影响的次优交通图像而变得复杂。本研究特别聚焦于交通标志检测方法，并引入Transformer模型（尤其是视觉Transformer变体）来解决这一任务。该模型最初为自然语言处理设计的注意力机制具有更优的并行效率，其视觉版本已在自动驾驶、目标检测、医疗健康和国防应用等多个领域取得成功。为提升Transformer模型的效能，研究提出了一种整合局部归纳偏置与Transformer模块的创新策略，包括引入高效卷积块和局部Transformer块，有效捕获短期与长期依赖信息，从而同步提升检测速度与准确率。实验评估表明，该方法在GTSDB数据集上取得了显著进展。
