# Efficient Vision Transformer for Accurate Traffic Sign Detection

链接: http://arxiv.org/abs/2311.01429v1

原文摘要:
This research paper addresses the challenges associated with traffic sign
detection in self-driving vehicles and driver assistance systems. The
development of reliable and highly accurate algorithms is crucial for the
widespread adoption of traffic sign recognition and detection (TSRD) in diverse
real-life scenarios. However, this task is complicated by suboptimal traffic
images affected by factors such as camera movement, adverse weather conditions,
and inadequate lighting. This study specifically focuses on traffic sign
detection methods and introduces the application of the Transformer model,
particularly the Vision Transformer variants, to tackle this task. The
Transformer's attention mechanism, originally designed for natural language
processing, offers improved parallel efficiency. Vision Transformers have
demonstrated success in various domains, including autonomous driving, object
detection, healthcare, and defense-related applications. To enhance the
efficiency of the Transformer model, the research proposes a novel strategy
that integrates a locality inductive bias and a transformer module. This
includes the introduction of the Efficient Convolution Block and the Local
Transformer Block, which effectively capture short-term and long-term
dependency information, thereby improving both detection speed and accuracy.
Experimental evaluations demonstrate the significant advancements achieved by
this approach, particularly when applied to the GTSDB dataset.

中文翻译:
本研究探讨了自动驾驶汽车与驾驶员辅助系统中交通标志检测所面临的挑战。开发可靠且高精度的算法对于交通标志识别与检测（TSRD）技术在不同现实场景中的广泛应用至关重要。然而，受摄像机移动、恶劣天气条件及光照不足等因素影响的低质量交通图像，使得这一任务变得尤为复杂。本研究聚焦于交通标志检测方法，创新性地引入Transformer模型（特别是视觉Transformer变体）来解决该问题。该模型最初为自然语言处理设计的注意力机制具有更优的并行效率，其视觉版本已在自动驾驶、目标检测、医疗健康和国防应用等多个领域取得成功。为提升Transformer模型的效能，研究提出了一种整合局部归纳偏置与Transformer模块的新策略：通过引入高效卷积块和局部Transformer块，有效捕获短期与长期依赖信息，从而同步提升检测速度与精度。实验评估表明，该方法在GTSDB数据集上取得了显著性能突破。

（翻译说明：
1. 专业术语处理："Transformer model"保留英文原名并补充中文说明，"Efficient Convolution Block"等专业组件采用意译+原名标注
2. 长句拆分：将原文复合句按中文表达习惯分解为多个短句，如将实验评估部分拆分为方法说明与结果呈现
3. 被动语态转换："is complicated by"等被动结构转为主动式表达
4. 逻辑显化：通过"创新性地""从而"等连接词强化技术路线的逻辑关系
5. 文化适配："adverse weather conditions"译为符合中文交通语境表达的"恶劣天气条件"
6. 数据规范：学术名称"GTSDB"保留英文缩写并补充"数据集"说明）
