# A Small-Scale Switch Transformer and NLP-based Model for Clinical Narratives Classification

链接: http://arxiv.org/abs/2303.12892v1

原文摘要:
Transformer-based models have shown outstanding results in natural language
processing but face challenges in applications like classifying small-scale
clinical texts, especially with constrained computational resources. This study
presents a customized Mixture of Expert (MoE) Transformer models for
classifying small-scale French clinical texts at CHU Sainte-Justine Hospital.
The MoE-Transformer addresses the dual challenges of effective training with
limited data and low-resource computation suitable for in-house hospital use.
Despite the success of biomedical pre-trained models such as CamemBERT-bio,
DrBERT, and AliBERT, their high computational demands make them impractical for
many clinical settings. Our MoE-Transformer model not only outperforms
DistillBERT, CamemBERT, FlauBERT, and Transformer models on the same dataset
but also achieves impressive results: an accuracy of 87\%, precision of 87\%,
recall of 85\%, and F1-score of 86\%. While the MoE-Transformer does not
surpass the performance of biomedical pre-trained BERT models, it can be
trained at least 190 times faster, offering a viable alternative for settings
with limited data and computational resources. Although the MoE-Transformer
addresses challenges of generalization gaps and sharp minima, demonstrating
some limitations for efficient and accurate clinical text classification, this
model still represents a significant advancement in the field. It is
particularly valuable for classifying small French clinical narratives within
the privacy and constraints of hospital-based computational resources.

中文翻译:
基于Transformer的模型在自然语言处理领域表现卓越，但在小规模临床文本分类等应用中面临挑战，特别是在计算资源受限的情况下。本研究为圣贾斯汀医院的小规模法语临床文本分类定制了混合专家（MoE）Transformer模型。该MoE-Transformer模型有效解决了有限数据下的高效训练与适合医院内部使用的低资源计算双重难题。尽管CamemBERT-bio、DrBERT和AliBERT等生物医学预训练模型表现优异，但其高计算需求使得它们难以适用于多数临床场景。我们的MoE-Transformer模型不仅在同一数据集上超越了DistillBERT、CamemBERT、FlauBERT及传统Transformer模型，更取得了87%准确率、87%精确率、85%召回率和86% F1分数的优异表现。虽然其性能尚未超越生物医学预训练BERT模型，但训练速度至少提升190倍，为数据与计算资源受限的场景提供了可行替代方案。尽管该模型在泛化差距和尖锐最小值问题上展现出一定局限性，未能完全实现高效精准的临床文本分类，但其仍代表了该领域的重要进展。对于需要在医院内部计算资源隐私限制下处理法语临床短文本的场景，这一模型具有特殊价值。
