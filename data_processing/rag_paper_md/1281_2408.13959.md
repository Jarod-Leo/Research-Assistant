# Bidirectional Awareness Induction in Autoregressive Seq2Seq Models

链接: http://arxiv.org/abs/2408.13959v1

原文摘要:
Autoregressive Sequence-To-Sequence models are the foundation of many Deep
Learning achievements in major research fields such as Vision and Natural
Language Processing. Despite that, they still present significant limitations.
For instance, when errors occur in the early steps of the prediction, the whole
output is severely affected. Such reliance on previously predicted tokens and
the inherent computational unfriendliness of sequential algorithms, motivated
researchers to explore different architectures and methods in the search for
bidirectional approaches. In this work, we introduce the Bidirectional
Awareness Induction (BAI), a training method that leverages a subset of
elements in the network, the Pivots, to perform bidirectional learning without
breaking the autoregressive constraints. To showcase its flexibility, we apply
the method to three architectures, the Transformer, ExpansionNet v2 and GPT,
then perform experiments over three tasks. Experimental results showcase BAI's
effectiveness on all selected tasks and architectures. In particular, we
observed an increase of up to 2.4 CIDEr in Image-Captioning, 4.96 BLEU in
Neural Machine Translation, and 1.16 ROUGE in Text Summarization compared to
the respective baselines. Notably, BAI not only has a positive impact on models
trained from scratch but on pre-trained models as well. Such an aspect,
combined with the absence of architectural requirements synergizes well with
the current trend of LLMs.

中文翻译:
自回归序列到序列模型是计算机视觉与自然语言处理等核心研究领域中多项深度学习成果的基石。然而，这类模型仍存在显著局限：例如预测早期步骤出现错误时，整体输出会受到严重影响。这种对已预测标记的依赖性以及序列算法固有的计算低效性，促使研究者探索不同架构与方法以实现双向建模。本文提出双向感知诱导（BAI）训练方法，通过利用网络中被称为"枢轴"的元素子集，在不破坏自回归约束的前提下实现双向学习。为验证其灵活性，我们将该方法应用于Transformer、ExpansionNet v2和GPT三种架构，并在三项任务上进行实验。结果表明BAI在所有选定任务和架构中均表现优异：图像描述任务CIDEr指标最高提升2.4分，神经机器翻译BLEU值提升4.96分，文本摘要ROUGE分数提高1.16分。值得注意的是，BAI不仅对从头训练的模型有效，对预训练模型同样具有积极影响。这一特性加之无需特定架构要求的优势，与当前大语言模型的发展趋势形成了良好协同。
