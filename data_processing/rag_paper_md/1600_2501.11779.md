# Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference

链接: http://arxiv.org/abs/2501.11779v1

原文摘要:
We introduce Glinthawk, an architecture for offline Large Language Model
(LLM) inference. By leveraging a two-tiered structure, Glinthawk optimizes the
utilization of the high-end accelerators ("Tier 1") by offloading the attention
mechanism to lower-end compute tier ("Tier 2"). This separation allows the
memory demand of the attention, known as the key-value cache, to scale
independently from the model weights, enabling larger batch sizes and more
efficient accelerator usage. Prototyped with NVIDIA T4 GPUs and standard CPU
VMs, Glinthawk improves throughput by $5.9\times$ and reduces cost of
generation by $2.8\times$, compared to paged attention baselines. For long
sequence lengths, it achieves $16.3\times$ throughput improvement at
$2.4\times$ less cost. Our evaluation shows that this architecture can tolerate
moderate network latency with minimal performance degradation, making it highly
effective for latency-tolerant, throughput-focused applications such as batch
processing. The prototype is publicly available at
https://github.com/microsoft/glinthawk.

中文翻译:
我们介绍了Glinthawk——一种专为离线大型语言模型(LLM)推理设计的架构。该架构采用双层结构，通过将注意力机制卸载至低端计算层("第二层")，优化了高端加速器("第一层")的利用率。这种分离设计使得注意力机制的内存需求(即键值缓存)能够独立于模型权重进行扩展，从而实现更大的批处理规模和更高效的加速器使用。基于NVIDIA T4 GPU和标准CPU虚拟机的原型测试表明，相较于分页注意力基线，Glinthawk将吞吐量提升5.9倍，同时将生成成本降低2.8倍。在处理长序列时，其吞吐量提升达16.3倍，成本反而减少2.4倍。评估显示该架构能承受适度网络延迟且性能衰减极小，非常适合批处理等对延迟容忍度高、注重吞吐量的应用场景。原型代码已开源发布于https://github.com/microsoft/glinthawk。
