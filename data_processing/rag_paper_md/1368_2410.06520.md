# A Novel LLM-based Two-stage Summarization Approach for Long Dialogues

链接: http://arxiv.org/abs/2410.06520v1

原文摘要:
Long document summarization poses a significant challenge in natural language
processing due to input lengths that exceed the capacity of most
state-of-the-art pre-trained language models. This study proposes a
hierarchical framework that segments and condenses information from long
documents, subsequently fine-tuning the processed text with an abstractive
summarization model. Unsupervised topic segmentation methods identify
semantically appropriate breakpoints. The condensation stage utilizes an
unsupervised generation model to generate condensed data, and our current
experiments employ ChatGPT(v3.5). The summarization stage fine-tunes the
abstractive summarization model on the condensed data to generate the final
results. This framework enables long documents to be processed on models even
when the document length exceeds the model's maximum input size. The exclusion
of the entire document from the summarization model reduces the time and
computational resources required for training, making the framework suitable
for contexts with constrained local computational resources.

中文翻译:
长文档摘要任务在自然语言处理中面临重大挑战，主要源于文档长度常超出当前主流预训练语言模型的最大输入限制。本研究提出一种分层处理框架：首先对长文档进行语义分段与信息浓缩，随后通过抽象式摘要模型对处理后的文本进行微调生成摘要。框架采用无监督主题分割算法识别语义边界点，在浓缩阶段利用无监督生成模型（当前实验选用ChatGPT v3.5）生成精简文本，最后通过抽象式摘要模型在浓缩数据上微调输出最终结果。该框架突破了模型输入长度的物理限制，使超长文档得以分段处理；同时通过避免将完整文档直接输入摘要模型，显著降低了训练所需的时间与计算资源，特别适用于本地计算资源受限的应用场景。
