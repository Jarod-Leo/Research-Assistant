# Context-Alignment: Activating and Enhancing LLM Capabilities in Time Series

链接: http://arxiv.org/abs/2501.03747v1

原文摘要:
Recently, leveraging pre-trained Large Language Models (LLMs) for time series
(TS) tasks has gained increasing attention, which involves activating and
enhancing LLMs' capabilities. Many methods aim to activate LLMs' capabilities
based on token-level alignment but overlook LLMs' inherent strength on natural
language processing -- their deep understanding of linguistic logic and
structure rather than superficial embedding processing. We propose
Context-Alignment, a new paradigm that aligns TS with a linguistic component in
the language environments familiar to LLMs to enable LLMs to contextualize and
comprehend TS data, thereby activating their capabilities. Specifically, such
context-level alignment comprises structural alignment and logical alignment,
which is achieved by a Dual-Scale Context-Alignment GNNs (DSCA-GNNs) applied to
TS-language multimodal inputs. Structural alignment utilizes dual-scale nodes
to describe hierarchical structure in TS-language, enabling LLMs treat long TS
data as a whole linguistic component while preserving intrinsic token features.
Logical alignment uses directed edges to guide logical relationships, ensuring
coherence in the contextual semantics. Demonstration examples prompt are
employed to construct Demonstration Examples based Context-Alignment (DECA)
following DSCA-GNNs framework. DECA can be flexibly and repeatedly integrated
into various layers of pre-trained LLMs to improve awareness of logic and
structure, thereby enhancing performance. Extensive experiments show the
effectiveness of DECA and the importance of Context-Alignment across tasks,
particularly in few-shot and zero-shot forecasting, confirming that
Context-Alignment provide powerful prior knowledge on context.

中文翻译:
近年来，利用预训练大语言模型（LLMs）处理时间序列任务的研究日益受到关注，其核心在于激活并增强LLMs的潜在能力。现有方法多基于令牌级对齐来激活LLMs能力，却忽视了LLMs在自然语言处理中的核心优势——对语言逻辑与结构的深层理解能力，而非浅层的嵌入处理。我们提出"语境对齐"新范式，通过将时间序列与LLMs熟悉的语言环境中的语言成分对齐，使LLMs能够将时序数据语境化并深度理解，从而激活其能力。具体而言，这种语境级对齐包含结构对齐与逻辑对齐，由我们提出的双尺度语境对齐图神经网络（DSCA-GNNs）在时序-语言多模态输入上实现：结构对齐采用双尺度节点描述时序-语言的层次结构，使LLMs能将长时序数据视为完整语言成分同时保留内在令牌特征；逻辑对齐通过有向边引导逻辑关系，确保上下文语义的连贯性。基于DSCA-GNNs框架，我们构建了示范样例提示策略DECA（基于示范样例的语境对齐），该策略可灵活、重复地集成到预训练LLMs的各个层级，提升模型对逻辑与结构的认知，从而增强性能。大量实验验证了DECA的有效性及语境对齐在不同任务中的重要性，尤其在少样本与零样本预测任务中表现突出，证实语境对齐能为模型提供强大的上下文先验知识。
