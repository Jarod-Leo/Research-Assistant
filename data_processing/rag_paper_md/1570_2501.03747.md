# Context-Alignment: Activating and Enhancing LLM Capabilities in Time Series

链接: http://arxiv.org/abs/2501.03747v1

原文摘要:
Recently, leveraging pre-trained Large Language Models (LLMs) for time series
(TS) tasks has gained increasing attention, which involves activating and
enhancing LLMs' capabilities. Many methods aim to activate LLMs' capabilities
based on token-level alignment but overlook LLMs' inherent strength on natural
language processing -- their deep understanding of linguistic logic and
structure rather than superficial embedding processing. We propose
Context-Alignment, a new paradigm that aligns TS with a linguistic component in
the language environments familiar to LLMs to enable LLMs to contextualize and
comprehend TS data, thereby activating their capabilities. Specifically, such
context-level alignment comprises structural alignment and logical alignment,
which is achieved by a Dual-Scale Context-Alignment GNNs (DSCA-GNNs) applied to
TS-language multimodal inputs. Structural alignment utilizes dual-scale nodes
to describe hierarchical structure in TS-language, enabling LLMs treat long TS
data as a whole linguistic component while preserving intrinsic token features.
Logical alignment uses directed edges to guide logical relationships, ensuring
coherence in the contextual semantics. Demonstration examples prompt are
employed to construct Demonstration Examples based Context-Alignment (DECA)
following DSCA-GNNs framework. DECA can be flexibly and repeatedly integrated
into various layers of pre-trained LLMs to improve awareness of logic and
structure, thereby enhancing performance. Extensive experiments show the
effectiveness of DECA and the importance of Context-Alignment across tasks,
particularly in few-shot and zero-shot forecasting, confirming that
Context-Alignment provide powerful prior knowledge on context.

中文翻译:
近年来，利用预训练大语言模型（LLMs）处理时间序列（TS）任务日益受到关注，其核心在于激活并增强LLMs的潜在能力。现有方法多基于词元级对齐来激活LLMs能力，却忽视了LLMs在自然语言处理中的本质优势——对语言逻辑与结构的深层理解，而非浅层嵌入处理。本文提出语境对齐新范式，通过将时间序列与LLMs熟悉的语言环境中的语言成分相融合，使LLMs能够将时序数据语境化并深度理解，从而激活其能力。具体而言，这种语境级对齐包含结构对齐与逻辑对齐，由双尺度语境对齐图神经网络（DSCA-GNNs）实现，该网络作用于时序-语言多模态输入。结构对齐采用双尺度节点描述时序-语言的层次结构，使LLMs能将长时序数据视为完整语言成分同时保留内在词元特征；逻辑对齐通过有向边引导逻辑关系，确保上下文语义连贯。基于DSCA-GNNs框架构建的示范样本提示法（DECA）采用演示案例进行语境对齐。DECA可灵活、重复地集成至预训练LLMs的各个层级，提升模型对逻辑与结构的感知能力，进而优化性能。大量实验验证了DECA的有效性及语境对齐在不同任务中的重要性，尤其在少样本与零样本预测场景中表现突出，证实语境对齐能为模型提供强有力的上下文先验知识。
