# Enhancing Large Language Models through Neuro-Symbolic Integration and Ontological Reasoning

链接: http://arxiv.org/abs/2504.07640v1

原文摘要:
Large Language Models (LLMs) demonstrate impressive capabilities in natural
language processing but suffer from inaccuracies and logical inconsistencies
known as hallucinations. This compromises their reliability, especially in
domains requiring factual accuracy. We propose a neuro-symbolic approach
integrating symbolic ontological reasoning and machine learning methods to
enhance the consistency and reliability of LLM outputs. Our workflow utilizes
OWL ontologies, a symbolic reasoner (e.g., HermiT) for consistency checking,
and a lightweight machine learning model (logistic regression) for mapping
natural language statements into logical forms compatible with the ontology.
When inconsistencies between LLM outputs and the ontology are detected, the
system generates explanatory feedback to guide the LLM towards a corrected,
logically coherent response in an iterative refinement loop. We present a
working Python prototype demonstrating this pipeline. Experimental results in a
defined domain suggest significant improvements in semantic coherence and
factual accuracy of LLM outputs, showcasing the potential of combining LLM
fluency with the rigor of formal semantics.

中文翻译:
以下是符合学术规范的中文翻译：

大型语言模型（LLMs）在自然语言处理方面展现出卓越能力，但存在被称为"幻觉"的准确性不足与逻辑不一致问题，这影响了其在需要事实准确性的领域中的可靠性。我们提出一种神经符号方法，通过整合符号本体推理与机器学习技术来提升LLM输出的逻辑一致性与可靠性。该工作流程采用OWL本体论体系，利用符号推理器（如HermiT）进行一致性校验，并采用轻量级机器学习模型（逻辑回归）将自然语言陈述映射为与本体兼容的逻辑形式。当检测到LLM输出与本体存在矛盾时，系统会生成解释性反馈，通过迭代优化循环引导LLM产生逻辑自洽的修正响应。我们开发了可运行的Python原型系统实现该流程。在特定领域的实验结果表明，该方法能显著提升LLM输出的语义连贯性与事实准确性，证明了将LLM的流畅性与形式语义的严谨性相结合的巨大潜力。

翻译说明：
1. 专业术语处理：严格遵循学术翻译规范，如"hallucinations"译为专业术语"幻觉"，"neuro-symbolic"译为"神经符号"
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句，如将原文复合从句转换为分号连接的并列结构
3. 被动语态转换：将英文被动式（如"are detected"）转换为中文主动表达（"检测到"）
4. 概念一致性：保持"ontology"统一译为"本体"，"reasoner"译为"推理器"等术语一致性
5. 补充说明：对"OWL ontologies"等专业概念增加"体系"等补充词，确保概念完整传达
6. 学术风格：使用"逻辑自洽""迭代优化循环"等符合计算机领域论文规范的表述
