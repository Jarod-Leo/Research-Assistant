# Enhancing Large Language Models through Neuro-Symbolic Integration and Ontological Reasoning

链接: http://arxiv.org/abs/2504.07640v1

原文摘要:
Large Language Models (LLMs) demonstrate impressive capabilities in natural
language processing but suffer from inaccuracies and logical inconsistencies
known as hallucinations. This compromises their reliability, especially in
domains requiring factual accuracy. We propose a neuro-symbolic approach
integrating symbolic ontological reasoning and machine learning methods to
enhance the consistency and reliability of LLM outputs. Our workflow utilizes
OWL ontologies, a symbolic reasoner (e.g., HermiT) for consistency checking,
and a lightweight machine learning model (logistic regression) for mapping
natural language statements into logical forms compatible with the ontology.
When inconsistencies between LLM outputs and the ontology are detected, the
system generates explanatory feedback to guide the LLM towards a corrected,
logically coherent response in an iterative refinement loop. We present a
working Python prototype demonstrating this pipeline. Experimental results in a
defined domain suggest significant improvements in semantic coherence and
factual accuracy of LLM outputs, showcasing the potential of combining LLM
fluency with the rigor of formal semantics.

中文翻译:
大型语言模型（LLM）在自然语言处理方面展现出卓越能力，但其存在的错误与逻辑矛盾——即"幻觉"问题——削弱了可靠性，尤其在需要事实准确性的领域更为突出。为此，我们提出一种融合符号化本体推理与机器学习的神经符号化方法，以提升LLM输出的逻辑一致性与可靠性。该工作流程采用OWL本体论体系，通过符号推理器（如HermiT）进行一致性校验，并运用轻量级机器学习模型（逻辑回归）将自然语言陈述映射为符合本体论规范的逻辑表达式。当检测到LLM输出与本体论存在矛盾时，系统会生成解释性反馈，在迭代优化循环中引导LLM生成逻辑自洽的修正结果。我们开发了可运行的Python原型系统来验证该流程。在特定领域的实验结果表明，该方法能显著提升LLM输出的语义连贯性与事实准确性，充分展现了将LLM的流畅性与形式语义的严谨性相结合的巨大潜力。
