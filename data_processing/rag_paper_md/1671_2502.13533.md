# Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models

链接: http://arxiv.org/abs/2502.13533v1

原文摘要:
Large Language Models (LLMs) have significantly advanced natural language
processing with exceptional task generalization capabilities. Low-Rank Adaption
(LoRA) offers a cost-effective fine-tuning solution, freezing the original
model parameters and training only lightweight, low-rank adapter matrices.
However, the memory footprint of LoRA is largely dominated by the original
model parameters. To mitigate this, we propose LoRAM, a memory-efficient LoRA
training scheme founded on the intuition that many neurons in
over-parameterized LLMs have low training utility but are essential for
inference. LoRAM presents a unique twist: it trains on a pruned (small) model
to obtain pruned low-rank matrices, which are then recovered and utilized with
the original (large) model for inference. Additionally, minimal-cost continual
pre-training, performed by the model publishers in advance, aligns the
knowledge discrepancy between pruned and original models. Our extensive
experiments demonstrate the efficacy of LoRAM across various pruning strategies
and downstream tasks. For a model with 70 billion parameters, LoRAM enables
training on a GPU with only 20G HBM, replacing an A100-80G GPU for LoRA
training and 15 GPUs for full fine-tuning. Specifically, QLoRAM implemented by
structured pruning combined with 4-bit quantization, for LLaMA-3.1-70B
(LLaMA-2-70B), reduces the parameter storage cost that dominates the memory
usage in low-rank matrix training by 15.81$\times$ (16.95$\times$), while
achieving dominant performance gains over both the original LLaMA-3.1-70B
(LLaMA-2-70B) and LoRA-trained LLaMA-3.1-8B (LLaMA-2-13B). Code is available at
https://github.com/junzhang-zj/LoRAM.

中文翻译:
大型语言模型（LLMs）凭借卓越的任务泛化能力显著推动了自然语言处理的发展。低秩适应（LoRA）提供了一种经济高效的微调方案，通过冻结原始模型参数并仅训练轻量级的低秩适配矩阵来实现。然而，LoRA的内存占用主要由原始模型参数主导。为缓解这一问题，我们提出LoRAM——一种基于以下洞察的高效内存LoRA训练方案：过参数化LLM中许多神经元虽训练效用低但对推理至关重要。LoRAM采用独特思路：在剪枝后（小型）模型上训练以获得剪枝低秩矩阵，随后将其恢复并与原始（大型）模型共同用于推理。此外，模型发布方预先执行的最小成本持续预训练可消除剪枝模型与原始模型间的知识差异。大量实验证明，LoRAM在不同剪枝策略和下游任务中均表现优异。对于700亿参数模型，LoRAM仅需20G HBM的GPU即可完成训练，替代了LoRA训练所需的A100-80G GPU和全参数微调所需的15块GPU。具体而言，QLoRAM（结构化剪枝结合4位量化实现）在LLaMA-3.1-70B（LLaMA-2-70B）上，将主导低秩矩阵训练内存消耗的参数存储成本降低15.81倍（16.95倍），同时性能显著超越原始LLaMA-3.1-70B（LLaMA-2-70B）和LoRA训练的LLaMA-3.1-8B（LLaMA-2-13B）。代码已开源：https://github.com/junzhang-zj/LoRAM。
