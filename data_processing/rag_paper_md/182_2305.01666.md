# BrainNPT: Pre-training of Transformer networks for brain network classification

链接: http://arxiv.org/abs/2305.01666v1

原文摘要:
Deep learning methods have advanced quickly in brain imaging analysis over
the past few years, but they are usually restricted by the limited labeled
data. Pre-trained model on unlabeled data has presented promising improvement
in feature learning in many domains, including natural language processing and
computer vision. However, this technique is under-explored in brain network
analysis. In this paper, we focused on pre-training methods with Transformer
networks to leverage existing unlabeled data for brain functional network
classification. First, we proposed a Transformer-based neural network, named as
BrainNPT, for brain functional network classification. The proposed method
leveraged <cls> token as a classification embedding vector for the Transformer
model to effectively capture the representation of brain network. Second, we
proposed a pre-training framework for BrainNPT model to leverage unlabeled
brain network data to learn the structure information of brain networks. The
results of classification experiments demonstrated the BrainNPT model without
pre-training achieved the best performance with the state-of-the-art models,
and the BrainNPT model with pre-training strongly outperformed the
state-of-the-art models. The pre-training BrainNPT model improved 8.75% of
accuracy compared with the model without pre-training. We further compared the
pre-training strategies, analyzed the influence of the parameters of the model,
and interpreted the trained model.

中文翻译:
深度学习方法近年来在脑影像分析领域发展迅速，但常受限于标注数据的稀缺。基于无标注数据的预训练模型已在自然语言处理和计算机视觉等多个领域展现出卓越的特征学习能力，然而该技术在脑网络分析中的应用尚未充分探索。本研究聚焦Transformer网络的预训练方法，旨在利用现有无标注数据提升脑功能网络分类性能。首先，我们提出名为BrainNPT的基于Transformer的神经网络架构，通过引入<cls>标记作为分类嵌入向量，使模型能有效捕捉脑网络表征特征。其次，我们设计了BrainNPT模型的预训练框架，利用无标注脑网络数据学习网络结构信息。分类实验表明：未经预训练的BrainNPT模型已达到最先进模型的性能上限，而经过预训练的BrainNPT模型则显著超越所有基线模型，相较未预训练版本准确率提升8.75%。研究进一步对比了不同预训练策略，分析了模型参数影响，并对训练完成的模型进行了解释性分析。
