# Better Explain Transformers by Illuminating Important Information

链接: http://arxiv.org/abs/2401.09972v2

原文摘要:
Transformer-based models excel in various natural language processing (NLP)
tasks, attracting countless efforts to explain their inner workings. Prior
methods explain Transformers by focusing on the raw gradient and attention as
token attribution scores, where non-relevant information is often considered
during explanation computation, resulting in confusing results. In this work,
we propose highlighting the important information and eliminating irrelevant
information by a refined information flow on top of the layer-wise relevance
propagation (LRP) method. Specifically, we consider identifying syntactic and
positional heads as important attention heads and focus on the relevance
obtained from these important heads. Experimental results demonstrate that
irrelevant information does distort output attribution scores and then should
be masked during explanation computation. Compared to eight baselines on both
classification and question-answering datasets, our method consistently
outperforms with over 3\% to 33\% improvement on explanation metrics, providing
superior explanation performance. Our anonymous code repository is available
at: https://github.com/LinxinS97/Mask-LRP

中文翻译:
基于Transformer的模型在各类自然语言处理任务中表现卓越，这促使研究者们不断探索其内部工作机制。现有解释方法主要通过原始梯度和注意力权重作为词元归因分数，但在计算过程中常包含不相关信息，导致解释结果难以理解。本研究提出在层级相关性传播（LRP）方法基础上，通过优化信息流来突出重要信息并过滤无关信息。具体而言，我们将句法头和位置头识别为关键注意力头，并聚焦于这些重要头产生的相关性。实验结果表明，无关信息确实会扭曲输出归因分数，因此在解释计算过程中应当被屏蔽。在分类和问答数据集上与八种基线方法对比中，本方法在解释指标上始终优于基线模型，提升幅度达3%至33%，展现出更优越的解释性能。匿名代码仓库已发布于：https://github.com/LinxinS97/Mask-LRP
