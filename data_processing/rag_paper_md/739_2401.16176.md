# A Survey on Structure-Preserving Graph Transformers

链接: http://arxiv.org/abs/2401.16176v1

原文摘要:
The transformer architecture has shown remarkable success in various domains,
such as natural language processing and computer vision. When it comes to graph
learning, transformers are required not only to capture the interactions
between pairs of nodes but also to preserve graph structures connoting the
underlying relations and proximity between them, showing the expressive power
to capture different graph structures. Accordingly, various
structure-preserving graph transformers have been proposed and widely used for
various tasks, such as graph-level tasks in bioinformatics and
chemoinformatics. However, strategies related to graph structure preservation
have not been well organized and systematized in the literature. In this paper,
we provide a comprehensive overview of structure-preserving graph transformers
and generalize these methods from the perspective of their design objective.
First, we divide strategies into four main groups: node feature modulation,
context node sampling, graph rewriting, and transformer architecture
improvements. We then further divide the strategies according to the coverage
and goals of graph structure preservation. Furthermore, we also discuss
challenges and future directions for graph transformer models to preserve the
graph structure and understand the nature of graphs.

中文翻译:
Transformer架构在自然语言处理和计算机视觉等多个领域展现出卓越成效。针对图学习任务，该架构不仅需捕捉节点间的交互关系，更需保持蕴含底层关联性与邻近性的图结构特征，以展现其表达多样化图结构的能力。为此，研究者相继提出各类结构保持型图Transformer模型，并广泛应用于生物信息学与化学信息学等领域的图级别任务。然而现有文献尚未对图结构保持策略进行系统化梳理。本文从设计目标出发，对结构保持型图Transformer进行全面综述与方法归纳：首先将策略划分为节点特征调制、上下文节点采样、图重构和Transformer架构优化四大类；进而根据图结构保持的覆盖范围与目标进行二级分类；最后探讨了当前图Transformer模型在结构保持与图本质理解方面面临的挑战及未来发展方向。
