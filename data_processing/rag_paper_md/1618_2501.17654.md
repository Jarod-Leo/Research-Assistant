# Exploring Vision Language Models for Multimodal and Multilingual Stance Detection

链接: http://arxiv.org/abs/2501.17654v1

原文摘要:
Social media's global reach amplifies the spread of information, highlighting
the need for robust Natural Language Processing tasks like stance detection
across languages and modalities. Prior research predominantly focuses on
text-only inputs, leaving multimodal scenarios, such as those involving both
images and text, relatively underexplored. Meanwhile, the prevalence of
multimodal posts has increased significantly in recent years. Although
state-of-the-art Vision-Language Models (VLMs) show promise, their performance
on multimodal and multilingual stance detection tasks remains largely
unexamined. This paper evaluates state-of-the-art VLMs on a newly extended
dataset covering seven languages and multimodal inputs, investigating their use
of visual cues, language-specific performance, and cross-modality interactions.
Our results show that VLMs generally rely more on text than images for stance
detection and this trend persists across languages. Additionally, VLMs rely
significantly more on text contained within the images than other visual
content. Regarding multilinguality, the models studied tend to generate
consistent predictions across languages whether they are explicitly
multilingual or not, although there are outliers that are incongruous with
macro F1, language support, and model size.

中文翻译:
社交媒体的全球影响力放大了信息传播的范围，凸显了跨语言与多模态场景下立场检测等自然语言处理任务的重要性。现有研究主要集中于纯文本输入，而对图像-文本等多模态情境的探索相对不足。与此同时，近年来多模态帖子的数量呈现显著增长趋势。尽管前沿的视觉语言模型（VLMs）展现出潜力，但其在多模态与多语言立场检测任务中的表现尚未得到充分验证。本文通过在覆盖七种语言和多模态输入的新扩展数据集上评估先进VLMs，系统探究了模型对视觉线索的利用、语言特异性表现及跨模态交互机制。研究发现：VLMs普遍更依赖文本而非图像进行立场判断，这一趋势在不同语言中保持一致；模型对图像内嵌文本的依赖度显著高于其他视觉内容。在多语言性方面，无论是否显式支持多语言处理，被评估模型倾向于生成跨语言一致的预测结果，但存在与宏观F1分数、语言支持度和模型规模不相符的异常案例。
