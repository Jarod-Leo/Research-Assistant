# Unified CNNs and transformers underlying learning mechanism reveals multi-head attention modus vivendi

链接: http://arxiv.org/abs/2501.12900v1

原文摘要:
Convolutional neural networks (CNNs) evaluate short-range correlations in
input images which progress along the layers, whereas vision transformer (ViT)
architectures evaluate long-range correlations, using repeated transformer
encoders composed of fully connected layers. Both are designed to solve complex
classification tasks but from different perspectives. This study demonstrates
that CNNs and ViT architectures stem from a unified underlying learning
mechanism, which quantitatively measures the single-nodal performance (SNP) of
each node in feedforward (FF) and multi-head attention (MHA) sub-blocks. Each
node identifies small clusters of possible output labels, with additional noise
represented as labels outside these clusters. These features are progressively
sharpened along the transformer encoders, enhancing the signal-to-noise ratio.
This unified underlying learning mechanism leads to two main findings. First,
it enables an efficient applied nodal diagonal connection (ANDC) pruning
technique without affecting the accuracy. Second, based on the SNP, spontaneous
symmetry breaking occurs among the MHA heads, such that each head focuses its
attention on a subset of labels through cooperation among its SNPs.
Consequently, each head becomes an expert in recognizing its designated labels,
representing a quantitative MHA modus vivendi mechanism. This statistical
mechanics inspired viewpoint enables to reveal macroscopic behavior of the
entire network from the microscopic performance of each node. These results are
based on a compact convolutional transformer architecture trained on the
CIFAR-100 and Flowers-102 datasets and call for their extension to other
architectures and applications, such as natural language processing.

中文翻译:
卷积神经网络（CNN）通过逐层处理输入图像的短程相关性，而视觉变换器（ViT）架构则利用由全连接层构成的重复变换器编码器来评估长程相关性。两者虽设计初衷不同，但均致力于解决复杂分类任务。本研究表明，CNN与ViT架构源于统一的基础学习机制——该机制通过定量测量前馈（FF）子块和多头注意力（MHA）子块中每个节点的单节点性能（SNP）实现学习。每个节点会识别出若干可能的输出标签小簇，并将簇外标签视为噪声表征。这些特征沿变换器编码器逐步锐化，从而提升信噪比。

这一统一机制带来两项重要发现：首先，基于SNP可实现高效的节点对角连接（ANDC）剪枝技术，且不影响模型精度；其次，MHA头部会自发产生对称性破缺现象——通过SNP间的协同作用，每个头部会专注于特定标签子集的注意力分配。这使得每个头部成为识别其专属标签的"专家"，形成定量化的MHA共存机制。这种受统计力学启发的视角，实现了从微观节点性能到宏观网络行为的跨尺度解析。研究结论基于在CIFAR-100和Flowers-102数据集上训练的紧凑卷积变换器架构得出，未来可扩展至自然语言处理等其他架构与应用场景。
