# Extracting Sentence Embeddings from Pretrained Transformer Models

链接: http://arxiv.org/abs/2408.08073v1

原文摘要:
Pre-trained transformer models shine in many natural language processing
tasks and therefore are expected to bear the representation of the input
sentence or text meaning. These sentence-level embeddings are also important in
retrieval-augmented generation. But do commonly used plain averaging or prompt
templates sufficiently capture and represent the underlying meaning? After
providing a comprehensive review of existing sentence embedding extraction and
refinement methods, we thoroughly test different combinations and our original
extensions of the most promising ones on pretrained models. Namely, given 110 M
parameters, BERT's hidden representations from multiple layers, and many
tokens, we try diverse ways to extract optimal sentence embeddings. We test
various token aggregation and representation post-processing techniques. We
also test multiple ways of using a general Wikitext dataset to complement
BERT's sentence embeddings. All methods are tested on eight Semantic Textual
Similarity (STS), six short text clustering, and twelve classification tasks.
We also evaluate our representation-shaping techniques on other static models,
including random token representations. Proposed representation extraction
methods improve the performance on STS and clustering tasks for all models
considered. Very high improvements for static token-based models, especially
random embeddings for STS tasks, almost reach the performance of BERT-derived
representations. Our work shows that the representation-shaping techniques
significantly improve sentence embeddings extracted from BERT-based and simple
baseline models.

中文翻译:
预训练的Transformer模型在众多自然语言处理任务中表现卓越，因此被认为能够有效承载输入句子或文本的语义表征。这类句子级嵌入在检索增强生成任务中同样至关重要。然而，常用的简单平均或提示模板是否足以捕捉和呈现深层语义？本文在系统梳理现有句子嵌入提取与优化方法的基础上，对预训练模型上最具潜力的多种组合方案及我们提出的原创扩展进行了全面测试。具体而言，针对参数量达1.1亿的BERT模型，我们探索了从其多层隐藏表征和多样化token中提取最优句子嵌入的多种途径：测试了不同token聚合与表征后处理技术，并尝试利用通用Wikitext数据集增强BERT句子嵌入的多种方法。所有方法均在8项语义文本相似度（STS）任务、6项短文本聚类任务和12项分类任务上进行验证。我们还评估了这些表征塑造技术在其他静态模型（包括随机token表征）上的适用性。实验表明，所提出的表征提取方法显著提升了所有测试模型在STS和聚类任务上的表现——特别是基于静态token的模型在STS任务上取得极大提升，随机嵌入的表现甚至接近BERT衍生表征的水平。本研究证实，表征塑造技术能显著改善基于BERT的模型及简单基线模型提取的句子嵌入质量。
