# Narrowing the Gap between Supervised and Unsupervised Sentence Representation Learning with Large Language Model

链接: http://arxiv.org/abs/2309.06453v1

原文摘要:
Sentence Representation Learning (SRL) is a fundamental task in Natural
Language Processing (NLP), with the Contrastive Learning of Sentence Embeddings
(CSE) being the mainstream technique due to its superior performance. An
intriguing phenomenon in CSE is the significant performance gap between
supervised and unsupervised methods, with their only difference lying in the
training data. Previous works attribute this performance gap to differences in
two representation properties (alignment and uniformity). However, since
alignment and uniformity only measure the results, they fail to answer "What
aspects of the training data contribute to the performance gap?" and "How can
the performance gap be narrowed?", In this paper, we conduct empirical
experiments to answer these "What" and "How" questions. We first answer the
"What" question by thoroughly comparing the behavior of supervised and
unsupervised CSE during their respective training processes. From the
comparison, we identify the similarity pattern as a key factor to the
performance gap, and introduce a metric, called Relative Fitting Difficulty
(RFD), to measure the complexity of the similarity pattern. Then, based on the
insights gained from the "What" question, we tackle the "How" question by
increasing the pattern complexity of the training data. We achieve this by
leveraging the In-Context Learning (ICL) capability of the Large Language Model
(LLM) to generate data that simulates complex patterns. By utilizing the
hierarchical patterns in the LLM-generated data, we effectively narrow the gap
between supervised and unsupervised CSE. We release our codes and appendix at
https://github.com/BDBC-KG-NLP/NGCSE.

中文翻译:
句子表示学习（Sentence Representation Learning, SRL）是自然语言处理（NLP）中的基础任务，其中基于对比学习的句子嵌入（Contrastive Learning of Sentence Embeddings, CSE）因其卓越性能成为主流技术。CSE领域存在一个引人注目的现象：监督式与无监督方法之间存在显著性能差距，而两者差异仅体现在训练数据上。现有研究将这种差距归因于两种表示属性（对齐性和均匀性）的差异。然而由于这两种属性仅能衡量结果，它们无法回答"训练数据的哪些方面导致了性能差距"以及"如何缩小这一差距"这两个核心问题。

本文通过实证研究系统解答了上述"什么"和"如何"的问题。首先，我们通过全面对比监督式与无监督CSE在训练过程中的行为特征，发现相似性模式是导致性能差距的关键因素，并提出了"相对拟合难度"（Relative Fitting Difficulty, RFD）这一指标来量化相似性模式的复杂度。基于对"什么"问题的研究发现，我们通过提升训练数据的模式复杂度来解决"如何"问题——利用大语言模型（LLM）的上下文学习（In-Context Learning, ICL）能力生成具有复杂模式的数据。通过挖掘LLM生成数据中的层次化模式，我们有效缩小了监督式与无监督CSE之间的性能差距。相关代码与附录已发布于https://github.com/BDBC-KG-NLP/NGCSE。
