# Narrowing the Gap between Supervised and Unsupervised Sentence Representation Learning with Large Language Model

链接: http://arxiv.org/abs/2309.06453v1

原文摘要:
Sentence Representation Learning (SRL) is a fundamental task in Natural
Language Processing (NLP), with the Contrastive Learning of Sentence Embeddings
(CSE) being the mainstream technique due to its superior performance. An
intriguing phenomenon in CSE is the significant performance gap between
supervised and unsupervised methods, with their only difference lying in the
training data. Previous works attribute this performance gap to differences in
two representation properties (alignment and uniformity). However, since
alignment and uniformity only measure the results, they fail to answer "What
aspects of the training data contribute to the performance gap?" and "How can
the performance gap be narrowed?", In this paper, we conduct empirical
experiments to answer these "What" and "How" questions. We first answer the
"What" question by thoroughly comparing the behavior of supervised and
unsupervised CSE during their respective training processes. From the
comparison, we identify the similarity pattern as a key factor to the
performance gap, and introduce a metric, called Relative Fitting Difficulty
(RFD), to measure the complexity of the similarity pattern. Then, based on the
insights gained from the "What" question, we tackle the "How" question by
increasing the pattern complexity of the training data. We achieve this by
leveraging the In-Context Learning (ICL) capability of the Large Language Model
(LLM) to generate data that simulates complex patterns. By utilizing the
hierarchical patterns in the LLM-generated data, we effectively narrow the gap
between supervised and unsupervised CSE. We release our codes and appendix at
https://github.com/BDBC-KG-NLP/NGCSE.

中文翻译:
句子表示学习（Sentence Representation Learning, SRL）是自然语言处理（NLP）领域的基础任务，其中基于对比学习的句子嵌入（Contrastive Learning of Sentence Embeddings, CSE）因其卓越性能成为主流技术。CSE研究中存在一个有趣现象：监督式与无监督方法性能差异显著，而两者唯一区别仅在于训练数据。现有研究将这种性能差距归因于两种表示特性（对齐性和均匀性）的差异。然而由于对齐性和均匀性仅能衡量结果，它们无法回答"训练数据的哪些特性导致了性能差距"以及"如何缩小这种差距"这两个核心问题。

本文通过实证研究系统解答了上述"归因"与"优化"问题。针对"归因"问题，我们通过全面对比监督式与无监督CSE在训练过程中的行为差异，发现相似性模式是关键影响因素，并提出了"相对拟合难度"（Relative Fitting Difficulty, RFD）指标来量化相似性模式的复杂度。基于"归因"研究的发现，我们通过提升训练数据的模式复杂度来应对"优化"问题：利用大语言模型（LLM）的上下文学习（In-Context Learning, ICL）能力生成具有复杂模式的数据，通过挖掘LLM生成数据中的层次化模式，有效缩小了监督式与无监督CSE的性能差距。相关代码及附录已开源在https://github.com/BDBC-KG-NLP/NGCSE。

（注：根据学术论文翻译规范，对原文进行了以下优化处理：
1. 专业术语采用"监督式/无监督"的规范译法
2. 将英文被动语态转换为中文主动表述
3. 对长难句进行合理切分，符合中文表达习惯
4. 关键概念首次出现时保留英文缩写
5. 技术路径描述采用"通过...实现..."的典型中文科研论文句式）
