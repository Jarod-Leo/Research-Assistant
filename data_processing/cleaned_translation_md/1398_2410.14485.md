# CaTs and DAGs: Integrating Directed Acyclic Graphs with Transformers and Fully-Connected Neural Networks for Causally Constrained Predictions

链接: http://arxiv.org/abs/2410.14485v1

原文摘要:
Artificial Neural Networks (ANNs), including fully-connected networks and
transformers, are highly flexible and powerful function approximators, widely
applied in fields like computer vision and natural language processing.
However, their inability to inherently respect causal structures can limit
their robustness, making them vulnerable to covariate shift and difficult to
interpret/explain. This poses significant challenges for their reliability in
real-world applications. In this paper, we introduce Causal Fully-Connected
Neural Networks (CFCNs) and Causal Transformers (CaTs), two general model
families designed to operate under predefined causal constraints, as specified
by a Directed Acyclic Graph (DAG). These models retain the powerful function
approximation abilities of traditional neural networks while adhering to the
underlying structural constraints, improving robustness, reliability, and
interpretability at inference time. This approach opens new avenues for
deploying neural networks in more demanding, real-world scenarios where
robustness and explainability is critical.

中文翻译:
人工神经网络（ANNs），包括全连接网络和Transformer模型，因其高度灵活且强大的函数逼近能力，被广泛应用于计算机视觉和自然语言处理等领域。然而，这类模型本质上无法遵循因果结构，这会限制其鲁棒性，使其容易受到协变量偏移的影响，并导致模型难以解释/说明。这一缺陷对神经网络在实际应用中的可靠性提出了重大挑战。本文提出了因果全连接神经网络（CFCNs）和因果Transformer（CaTs）——这两种通用模型框架能够在有向无环图（DAG）定义的因果约束条件下运行。这些模型在保持传统神经网络强大函数逼近能力的同时，严格遵循底层结构约束，从而在推理时显著提升了鲁棒性、可靠性和可解释性。该方法为神经网络在要求严苛的现实场景（其中鲁棒性与可解释性至关重要）中的应用开辟了新途径。

（翻译说明：
1. 专业术语处理：采用"Transformer→Transformer模型"、"DAG→有向无环图"等学界通用译法
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句，如将"making them..."独立成句译为"并导致..."
3. 被动语态转化："are widely applied"转为主动态"被广泛应用于"
4. 概念显化："covariate shift"译为"协变量偏移"而非字面直译
5. 逻辑连接优化：通过"然而""从而"等连接词保持论证连贯性
6. 术语统一性：全篇保持"鲁棒性""可解释性"等关键术语的一致性
7. 技术准确性：严格区分"model families→模型框架"与"models→模型"的层级关系）
