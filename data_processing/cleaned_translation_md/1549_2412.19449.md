# Feature Alignment-Based Knowledge Distillation for Efficient Compression of Large Language Models

链接: http://arxiv.org/abs/2412.19449v1

原文摘要:
This study proposes a knowledge distillation algorithm based on large
language models and feature alignment, aiming to effectively transfer the
knowledge of large pre-trained models into lightweight student models, thereby
reducing computational costs while maintaining high model performance.
Different from the traditional soft label distillation method, this method
introduces a multi-layer feature alignment strategy to deeply align the
intermediate features and attention mechanisms of the teacher model and the
student model, maximally retaining the semantic expression ability and context
modeling ability of the teacher model. In terms of method design, a multi-task
loss function is constructed, including feature matching loss, attention
alignment loss, and output distribution matching loss, to ensure multi-level
information transfer through joint optimization. The experiments were
comprehensively evaluated on the GLUE data set and various natural language
processing tasks. The results show that the proposed model performs very close
to the state-of-the-art GPT-4 model in terms of evaluation indicators such as
perplexity, BLEU, ROUGE, and CER. At the same time, it far exceeds baseline
models such as DeBERTa, XLNet, and GPT-3, showing significant performance
improvements and computing efficiency advantages. Research results show that
the feature alignment distillation strategy is an effective model compression
method that can significantly reduce computational overhead and storage
requirements while maintaining model capabilities. Future research can be
further expanded in the directions of self-supervised learning, cross-modal
feature alignment, and multi-task transfer learning to provide more flexible
and efficient solutions for the deployment and optimization of deep learning
models.

中文翻译:
本研究提出了一种基于大语言模型与特征对齐的知识蒸馏算法，旨在将大规模预训练模型的知识高效迁移至轻量级学生模型，在保持模型高性能的同时显著降低计算成本。区别于传统的软标签蒸馏方法，本方法引入多层特征对齐策略，通过深度对齐教师模型与学生模型的中间层特征及注意力机制，最大限度保留教师模型的语义表达能力与上下文建模能力。在方法设计上，构建了包含特征匹配损失、注意力对齐损失和输出分布匹配损失的多任务损失函数，通过联合优化确保多层次信息传递。实验在GLUE数据集及多项自然语言处理任务上进行了综合评估，结果表明：所提模型在困惑度、BLEU、ROUGE、CER等评价指标上表现与当前最先进的GPT-4模型极为接近，同时大幅超越DeBERTa、XLNet、GPT-3等基线模型，展现出显著的性能提升与计算效率优势。研究结果表明，特征对齐蒸馏策略是一种有效的模型压缩方法，能在保持模型能力的同时显著降低计算开销与存储需求。未来研究可进一步向自监督学习、跨模态特征对齐及多任务迁移学习等方向拓展，为深度学习模型的部署与优化提供更灵活高效的解决方案。
