# Scaling Vision-Language Models with Sparse Mixture of Experts

链接: http://arxiv.org/abs/2303.07226v1

原文摘要:
The field of natural language processing (NLP) has made significant strides
in recent years, particularly in the development of large-scale vision-language
models (VLMs). These models aim to bridge the gap between text and visual
information, enabling a more comprehensive understanding of multimedia data.
However, as these models become larger and more complex, they also become more
challenging to train and deploy. One approach to addressing this challenge is
the use of sparsely-gated mixture-of-experts (MoE) techniques, which divide the
model into smaller, specialized sub-models that can jointly solve a task. In
this paper, we explore the effectiveness of MoE in scaling vision-language
models, demonstrating its potential to achieve state-of-the-art performance on
a range of benchmarks over dense models of equivalent computational cost. Our
research offers valuable insights into stabilizing the training of MoE models,
understanding the impact of MoE on model interpretability, and balancing the
trade-offs between compute performance when scaling VLMs. We hope our work will
inspire further research into the use of MoE for scaling large-scale
vision-language models and other multimodal machine learning applications.

中文翻译:
近年来，自然语言处理（NLP）领域取得了显著进展，尤其是在大规模视觉语言模型（VLMs）的开发方面。这类模型致力于弥合文本与视觉信息之间的鸿沟，从而实现对多媒体数据更全面的理解。然而，随着模型规模扩大和复杂度提升，其训练与部署也面临更大挑战。稀疏门控专家混合（MoE）技术通过将模型分解为多个小型专业化子模型来协同完成任务，为应对这一挑战提供了可行方案。本文探究了MoE在扩展视觉语言模型规模方面的有效性，证明其在同等计算成本下，相较于密集模型能在一系列基准测试中实现最先进的性能。我们的研究为稳定MoE模型训练、理解MoE对模型可解释性的影响，以及权衡视觉语言模型扩展过程中的计算性能提供了重要见解。期待这项工作能推动MoE技术在大规模视觉语言模型及其他多模态机器学习应用中的进一步研究。  

（翻译说明：  
1. 专业术语处理："sparsely-gated mixture-of-experts"采用学界通用译法"稀疏门控专家混合"，"VLMs"保留英文缩写但首次出现时标注全称  
2. 长句拆分：将原文复合句按中文表达习惯分解为多个短句，如将"demonstrating..."独立成句  
3. 被动语态转换："can be trained"等被动结构转为主动式"面临挑战"  
4. 学术风格保持：使用"探究""权衡""见解"等符合论文摘要特征的词汇  
5. 逻辑衔接强化：通过"从而""然而""为应对"等连接词保持论证连贯性）
