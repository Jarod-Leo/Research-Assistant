# A Comparative Study between Full-Parameter and LoRA-based Fine-Tuning on Chinese Instruction Data for Instruction Following Large Language Model

链接: http://arxiv.org/abs/2304.08109v2

原文摘要:
Recently, the instruction-tuning of large language models is a crucial area
of research in the field of natural language processing. Due to resource and
cost limitations, several researchers have employed parameter-efficient tuning
techniques, such as LoRA, for instruction tuning, and have obtained encouraging
results In comparison to full-parameter fine-tuning, LoRA-based tuning
demonstrates salient benefits in terms of training costs. In this study, we
undertook experimental comparisons between full-parameter fine-tuning and
LoRA-based tuning methods, utilizing LLaMA as the base model. The experimental
results show that the selection of the foundational model, training dataset
scale, learnable parameter quantity, and model training cost are all important
factors. We hope that the experimental conclusions of this paper can provide
inspiration for training large language models, especially in the field of
Chinese, and help researchers find a better trade-off strategy between training
cost and model performance. To facilitate the reproduction of the paper's
results, the dataset, model and code will be released.

中文翻译:
近年来，大型语言模型的指令微调已成为自然语言处理领域的关键研究方向。由于资源和成本限制，部分研究者采用LoRA等参数高效微调技术进行指令调优，并取得了令人鼓舞的成果。与全参数微调相比，基于LoRA的调优方法在训练成本方面展现出显著优势。本研究以LLaMA为基础模型，对全参数微调与基于LoRA的微调方法进行了实验对比。结果表明：基础模型的选择、训练数据集规模、可训练参数量以及模型训练成本均为重要影响因素。我们希望本文的实验结论能为大语言模型训练（尤其是中文领域）提供启发，帮助研究者在训练成本与模型性能之间找到更优的平衡策略。为便于复现论文结果，我们将公开数据集、模型及代码。

（翻译说明：
1. 专业术语处理："instruction-tuning"译为"指令微调"，"parameter-efficient tuning"译为"参数高效微调"，"LoRA"保留英文缩写
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句，如将"Due to..."原因状语从句转换为"由于..."前置分句
3. 语态转换：将被动语态"have been employed"等转换为主动表述"采用"
4. 逻辑显化：通过"结果表明："明确标示研究结论的引出
5. 补充说明：在"大语言模型训练"后添加括号注释"尤其是中文领域"，使专业背景更清晰
6. 术语统一："fine-tuning"与"tuning"统一译为"微调"，保持全文一致性）
