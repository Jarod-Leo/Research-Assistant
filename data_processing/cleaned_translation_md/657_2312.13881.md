# Diversifying Knowledge Enhancement of Biomedical Language Models using Adapter Modules and Knowledge Graphs

链接: http://arxiv.org/abs/2312.13881v1

原文摘要:
Recent advances in natural language processing (NLP) owe their success to
pre-training language models on large amounts of unstructured data. Still,
there is an increasing effort to combine the unstructured nature of LMs with
structured knowledge and reasoning. Particularly in the rapidly evolving field
of biomedical NLP, knowledge-enhanced language models (KELMs) have emerged as
promising tools to bridge the gap between large language models and
domain-specific knowledge, considering the available biomedical knowledge
graphs (KGs) curated by experts over the decades. In this paper, we develop an
approach that uses lightweight adapter modules to inject structured biomedical
knowledge into pre-trained language models (PLMs). We use two large KGs, the
biomedical knowledge system UMLS and the novel biochemical ontology OntoChem,
with two prominent biomedical PLMs, PubMedBERT and BioLinkBERT. The approach
includes partitioning knowledge graphs into smaller subgraphs, fine-tuning
adapter modules for each subgraph, and combining the knowledge in a fusion
layer. We test the performance on three downstream tasks: document
classification,question answering, and natural language inference. We show that
our methodology leads to performance improvements in several instances while
keeping requirements in computing power low. Finally, we provide a detailed
interpretation of the results and report valuable insights for future work.

中文翻译:
近期自然语言处理（NLP）领域的突破性进展，主要得益于海量非结构化数据上的语言模型预训练。然而学界正日益关注如何将语言模型的非结构化特性与结构化知识及推理能力相结合。尤其在快速发展的生物医学NLP领域，考虑到专家数十年构建的生物医学知识图谱（KGs）资源，知识增强型语言模型（KELMs）已成为弥合大语言模型与领域专业知识的重要工具。本文提出一种通过轻量级适配器模块将结构化生物医学知识注入预训练语言模型（PLMs）的方法，选用两大知识图谱（生物医学知识系统UMLS和新型生化本体OntoChem）与两个主流生物医学PLMs（PubMedBERT和BioLinkBERT）进行实验。该方法包括：将知识图谱划分为子图、为每个子图微调适配器模块、通过融合层整合知识。我们在文档分类、问答系统和自然语言推理三个下游任务上测试性能，结果表明该方法在多个场景下能提升模型表现，同时保持较低算力需求。最后我们对实验结果进行了深度解析，并为后续研究提供了有价值的见解。
