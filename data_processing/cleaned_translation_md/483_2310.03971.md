# Quantized Transformer Language Model Implementations on Edge Devices

链接: http://arxiv.org/abs/2310.03971v1

原文摘要:
Large-scale transformer-based models like the Bidirectional Encoder
Representations from Transformers (BERT) are widely used for Natural Language
Processing (NLP) applications, wherein these models are initially pre-trained
with a large corpus with millions of parameters and then fine-tuned for a
downstream NLP task. One of the major limitations of these large-scale models
is that they cannot be deployed on resource-constrained devices due to their
large model size and increased inference latency. In order to overcome these
limitations, such large-scale models can be converted to an optimized
FlatBuffer format, tailored for deployment on resource-constrained edge
devices. Herein, we evaluate the performance of such FlatBuffer transformed
MobileBERT models on three different edge devices, fine-tuned for Reputation
analysis of English language tweets in the RepLab 2013 dataset. In addition,
this study encompassed an evaluation of the deployed models, wherein their
latency, performance, and resource efficiency were meticulously assessed. Our
experiment results show that, compared to the original BERT large model, the
converted and quantized MobileBERT models have 160$\times$ smaller footprints
for a 4.1% drop in accuracy while analyzing at least one tweet per second on
edge devices. Furthermore, our study highlights the privacy-preserving aspect
of TinyML systems as all data is processed locally within a serverless
environment.

中文翻译:
基于Transformer的大规模模型（如双向编码器表示模型BERT）被广泛应用于自然语言处理（NLP）任务中。这类模型通常先通过包含数百万参数的大规模语料库进行预训练，再针对下游NLP任务进行微调。此类大型模型的主要局限在于其庞大的参数量和高推理延迟，导致难以部署在资源受限的设备上。为突破这些限制，可将大型模型转换为专为边缘设备优化的FlatBuffer格式。本研究评估了经FlatBuffer转换的MobileBERT模型在三种不同边缘设备上的性能表现，这些模型针对RepLab 2013数据集中英语推文的声誉分析任务进行了微调。研究还系统评估了部署模型的延迟、性能和资源效率。实验结果表明：与原始BERT大型模型相比，经过转换和量化的MobileBERT模型体积缩小160倍，在边缘设备上每秒至少处理一条推文的条件下，准确率仅下降4.1%。此外，研究凸显了TinyML系统的隐私保护优势——所有数据均在无服务器环境中实现本地化处理。
