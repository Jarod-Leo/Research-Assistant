# A Survey on Knowledge-Enhanced Pre-trained Language Models

链接: http://arxiv.org/abs/2212.13428v1

原文摘要:
Natural Language Processing (NLP) has been revolutionized by the use of
Pre-trained Language Models (PLMs) such as BERT. Despite setting new records in
nearly every NLP task, PLMs still face a number of challenges including poor
interpretability, weak reasoning capability, and the need for a lot of
expensive annotated data when applied to downstream tasks. By integrating
external knowledge into PLMs,
\textit{\underline{K}nowledge-\underline{E}nhanced \underline{P}re-trained
\underline{L}anguage \underline{M}odels} (KEPLMs) have the potential to
overcome the above-mentioned limitations. In this paper, we examine KEPLMs
systematically through a series of studies. Specifically, we outline the common
types and different formats of knowledge to be integrated into KEPLMs, detail
the existing methods for building and evaluating KEPLMS, present the
applications of KEPLMs in downstream tasks, and discuss the future research
directions. Researchers will benefit from this survey by gaining a quick and
comprehensive overview of the latest developments in this field.

中文翻译:
以下是符合要求的学术摘要中文翻译：

知识增强的预训练语言模型研究综述

预训练语言模型（如BERT）的应用为自然语言处理（NLP）领域带来了革命性变革。尽管这些模型在几乎所有NLP任务中都刷新了性能记录，但仍面临可解释性差、推理能力弱、下游任务需要大量昂贵标注数据等挑战。通过将外部知识整合到预训练模型中，知识增强的预训练语言模型（KEPLMs）有望突破上述局限。本文通过系统性的研究对KEPLMs进行全面考察：首先梳理了可用于知识增强的常见知识类型与不同表示格式，详细阐述了当前KEPLMs的构建方法与评估体系，总结了模型在下游任务中的应用情况，最后探讨了未来研究方向。本综述有助于研究人员快速把握该领域的最新发展动态，获得全面深入的认识。


2. 被动语态转换为中文主动表述（如"are presented"译为"总结了"）
3. 长难句合理切分（如将原文复合句拆解为符合中文表达习惯的短句）
4. 学术用语规范（如"systematically"译为"系统性"，"limitations"译为"局限"而非"限制"）
5. 保留关键英文缩写并在首次出现时标注全称）
