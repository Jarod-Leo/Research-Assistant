# Consistency Evaluation of News Article Summaries Generated by Large (and Small) Language Models

链接: http://arxiv.org/abs/2502.20647v1

原文摘要:
Text summarizing is a critical Natural Language Processing (NLP) task with
applications ranging from information retrieval to content generation. Large
Language Models (LLMs) have shown remarkable promise in generating fluent
abstractive summaries but they can produce hallucinated details not grounded in
the source text. Regardless of the method of generating a summary, high quality
automated evaluations remain an open area of investigation. This paper embarks
on an exploration of text summarization with a diverse set of techniques,
including TextRank, BART, Mistral-7B-Instruct, and OpenAI GPT-3.5-Turbo. The
generated summaries are evaluated using traditional metrics such as the
Recall-Oriented Understudy for Gisting Evaluation (ROUGE) Score and
Bidirectional Encoder Representations from Transformers (BERT) Score, as well
as LLM-powered evaluation methods that directly assess a generated summary's
consistency with the source text. We introduce a meta evaluation score which
directly assesses the performance of the LLM evaluation system (prompt +
model). We find that that all summarization models produce consistent summaries
when tested on the XL-Sum dataset, exceeding the consistency of the reference
summaries.

中文翻译:
文本摘要是一项关键的自然语言处理（NLP）任务，其应用场景涵盖信息检索到内容生成等多个领域。尽管大型语言模型（LLMs）在生成流畅的抽象摘要方面展现出显著潜力，但仍可能产生脱离原文的虚构内容。无论采用何种摘要生成方法，高质量的自动化评估始终是待探索的研究领域。本文通过TextRank、BART、Mistral-7B-Instruct和OpenAI GPT-3.5-Turbo等多种技术对文本摘要进行系统性探究，并采用传统评估指标（如面向召回率的ROUGE评分和基于Transformer双向编码表示的BERT评分）与基于LLM的评估方法（直接衡量生成摘要与原文的一致性）进行综合评价。我们提出了一种元评估分数，用于直接评估LLM评价系统（提示词+模型）的性能表现。实验发现，当在XL-Sum数据集上测试时，所有摘要模型生成的摘要一致性均超过参考摘要，表现出稳定的性能。

（翻译说明：通过以下处理确保专业性与可读性：
1. 术语标准化："abstractive summaries"译为"抽象摘要"，"hallucinated details"译为"虚构内容"
2. 长句拆分：将原文复合句分解为符合中文表达习惯的短句结构
3. 被动语态转化："are evaluated"译为主动态的"采用...进行综合评价"
4. 概念显化："meta evaluation score"增译为"元评估分数"并添加括号说明
5. 数据名称保留：XL-Sum等专业名称保持原文形式
6. 逻辑连接优化：使用"尽管...但仍"等连词保持论证连贯性）
