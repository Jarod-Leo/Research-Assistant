# IDoFew: Intermediate Training Using Dual-Clustering in Language Models for Few Labels Text Classification

链接: http://arxiv.org/abs/2401.04025v1

原文摘要:
Language models such as Bidirectional Encoder Representations from
Transformers (BERT) have been very effective in various Natural Language
Processing (NLP) and text mining tasks including text classification. However,
some tasks still pose challenges for these models, including text
classification with limited labels. This can result in a cold-start problem.
Although some approaches have attempted to address this problem through
single-stage clustering as an intermediate training step coupled with a
pre-trained language model, which generates pseudo-labels to improve
classification, these methods are often error-prone due to the limitations of
the clustering algorithms. To overcome this, we have developed a novel
two-stage intermediate clustering with subsequent fine-tuning that models the
pseudo-labels reliably, resulting in reduced prediction errors. The key novelty
in our model, IDoFew, is that the two-stage clustering coupled with two
different clustering algorithms helps exploit the advantages of the
complementary algorithms that reduce the errors in generating reliable
pseudo-labels for fine-tuning. Our approach has shown significant improvements
compared to strong comparative models.

中文翻译:
基于Transformer的双向编码器表征模型（BERT）等语言模型在文本分类等多种自然语言处理（NLP）和文本挖掘任务中表现卓越。然而在标签数据有限的文本分类场景中，这类模型仍面临冷启动问题的挑战。现有解决方案通常采用单阶段聚类作为中间训练步骤，结合预训练语言模型生成伪标签以提升分类性能，但由于聚类算法本身的局限性，这类方法往往存在较高错误率。为此，我们创新性地提出了一种双阶段中间聚类结合微调的新范式，通过可靠建模伪标签有效降低预测误差。IDoFew模型的核心创新在于：采用两种互补聚类算法构建的双阶段框架，能够充分发挥不同算法的协同优势，显著减少伪标签生成过程中的错误。实验表明，相较于现有强基线模型，本方法取得了显著性能提升。

（翻译说明：
1. 专业术语处理：采用"双向编码器表征模型"规范翻译BERT全称，保留"NLP"缩写形式符合中文文献惯例
2. 长句拆分重构：将原文复合长句分解为符合中文表达习惯的短句，如将"which generates..."从句转为独立分句
3. 被动语态转化："have been very effective"等被动结构转换为"表现卓越"等主动表述
4. 概念显化处理："cold-start problem"译为专业术语"冷启动问题"并补充"场景"等语境词
5. 逻辑连接优化：使用"为此"、"通过"等连接词保持论证链条清晰
6. 创新点突出：通过"核心创新在于"等强调句式凸显方法论贡献
7. 术语一致性：全篇保持"伪标签"、"微调"等关键术语统一）
