# Assessing Large Language Models in Agentic Multilingual National Bias

链接: http://arxiv.org/abs/2502.17945v1

原文摘要:
Large Language Models have garnered significant attention for their
capabilities in multilingual natural language processing, while studies on
risks associated with cross biases are limited to immediate context
preferences. Cross-language disparities in reasoning-based recommendations
remain largely unexplored, with a lack of even descriptive analysis. This study
is the first to address this gap. We test LLM's applicability and capability in
providing personalized advice across three key scenarios: university
applications, travel, and relocation. We investigate multilingual bias in
state-of-the-art LLMs by analyzing their responses to decision-making tasks
across multiple languages. We quantify bias in model-generated scores and
assess the impact of demographic factors and reasoning strategies (e.g.,
Chain-of-Thought prompting) on bias patterns. Our findings reveal that local
language bias is prevalent across different tasks, with GPT-4 and Sonnet
reducing bias for English-speaking countries compared to GPT-3.5 but failing to
achieve robust multilingual alignment, highlighting broader implications for
multilingual AI agents and applications such as education.

中文翻译:
大型语言模型因其在多语言自然语言处理方面的能力而备受关注，然而关于跨文化偏见的风险研究目前仅局限于即时语境偏好。基于推理的推荐建议中存在的跨语言差异至今尚未得到充分探索，甚至连描述性分析都极为匮乏。本研究首次填补了这一空白。我们通过大学申请、旅行规划和移民定居三个关键场景，测试了大语言模型在提供个性化建议时的适用性与能力。通过分析模型对多语言决策任务的响应，我们深入研究了前沿大语言模型中的多语言偏见问题。我们量化了模型生成评分中的偏见程度，并评估了人口统计因素与推理策略（如思维链提示）对偏见模式的影响。研究结果表明：本地语言偏见在不同任务中普遍存在，虽然GPT-4和Sonnet相比GPT-3.5降低了英语国家的偏见，但仍未能实现稳健的多语言对齐——这一发现对多语言AI智能体及教育等应用场景具有广泛启示意义。

（翻译说明：采用学术论文的规范表达，通过以下处理确保专业性与可读性：
1. 术语统一："Chain-of-Thought prompting"译为专业术语"思维链提示"
2. 长句拆分：将原文复合句分解为符合中文表达习惯的短句结构
3. 逻辑显化：通过破折号和括号补充原文隐含的递进关系
4. 动态对等："robust multilingual alignment"译为"稳健的多语言对齐"既保留计算机术语特征又确保易懂
5. 文化适配："university applications"根据中文语境译为"大学申请"而非字面翻译）
