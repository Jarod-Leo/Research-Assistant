# Ranking LLMs by compression

链接: http://arxiv.org/abs/2406.14171v1

原文摘要:
We conceptualize the process of understanding as information compression, and
propose a method for ranking large language models (LLMs) based on lossless
data compression. We demonstrate the equivalence of compression length under
arithmetic coding with cumulative negative log probabilities when using a large
language model as a prior, that is, the pre-training phase of the model is
essentially the process of learning the optimal coding length. At the same
time, the evaluation metric compression ratio can be obtained without actual
compression, which greatly saves overhead. In this paper, we use five large
language models as priors for compression, then compare their performance on
challenging natural language processing tasks, including sentence completion,
question answering, and coreference resolution. Experimental results show that
compression ratio and model performance are positively correlated, so it can be
used as a general metric to evaluate large language models.

中文翻译:
我们将理解过程概念化为信息压缩，并提出了一种基于无损数据压缩的大型语言模型（LLM）排序方法。通过论证算术编码压缩长度与累积负对数概率在采用大语言模型作为先验时的等价性，揭示了模型预训练阶段本质上是学习最优编码长度的过程。同时，评估指标压缩比可在无需实际压缩的情况下获得，这显著节省了计算开销。本文采用五种大语言模型作为压缩先验，进而比较它们在挑战性自然语言处理任务（包括句子补全、问答和共指消解）上的表现。实验结果表明压缩比与模型性能呈正相关，因此可作为评估大语言模型的通用指标。

（翻译说明：
1. 专业术语处理："arithmetic coding"译为"算术编码"，"coreference resolution"译为"共指消解"，保持NLP领域术语规范
2. 句式重构：将原文复合长句拆分为符合中文表达习惯的短句，如将"demonstrate..."长句分解为"通过论证...揭示了..."
3. 被动语态转换：将"can be obtained"等被动式转为中文主动表达"可获得"
4. 概念显化："prior"在统计学语境下译为"先验"，并通过增补"模型"明确指代关系
5. 逻辑连接：添加"进而"等连接词强化实验方法与结论间的递进关系
6. 术语一致性：全篇统一"large language model"的译法为"大语言模型"
7. 学术风格保持：使用"揭示""显著""呈正相关"等符合学术论文表达的词汇）
