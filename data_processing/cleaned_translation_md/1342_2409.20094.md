# Aggressive Post-Training Compression on Extremely Large Language Models

链接: http://arxiv.org/abs/2409.20094v1

原文摘要:
The increasing size and complexity of Large Language Models (LLMs) pose
challenges for their deployment on personal computers and mobile devices.
Aggressive post-training model compression is necessary to reduce the models'
size, but it often results in significant accuracy loss. To address this
challenge, we propose a novel network pruning technology that utilizes over 0.7
sparsity and less than 8 bits of quantization. Our approach enables the
compression of prevailing LLMs within a couple of hours while maintaining a
relatively small accuracy loss. In experimental evaluations, our method
demonstrates effectiveness and potential for practical deployment. By making
LLMs available on domestic devices, our work can facilitate a new era of
natural language processing applications with wide-ranging impacts.

中文翻译:
大型语言模型（LLMs）日益增长的规模和复杂性为其在个人计算机及移动设备上的部署带来了挑战。为实现模型体积的缩减，必须采用激进的训练后模型压缩技术，但这往往会导致显著的精度损失。为应对这一挑战，我们提出了一种新型网络剪枝技术，该技术采用超过0.7的稀疏度和低于8位的量化精度。我们的方法能在数小时内完成主流LLMs的压缩，同时保持相对较小的精度损失。实验评估表明，该方法展现出卓越的有效性及实际部署潜力。通过实现LLMs在家用设备上的运行，我们的工作将推动自然语言处理应用进入具有广泛影响力的新时代。

（翻译说明：采用技术论文的标准表述方式，通过以下处理确保专业性与可读性：
1. 术语统一："post-training"译为"训练后"，"quantization"译为"量化"
2. 被动语态转化：将英文被动结构转换为中文主动句式（如"it often results in"译为"但这往往会导致"）
3. 长句拆分：将原文复合句按中文表达习惯分解为短句
4. 概念显化："wide-ranging impacts"意译为"具有广泛影响力"而非字面直译
5. 数据规范：精确保留"0.7 sparsity"等技术参数表述）
