# MultiTok: Variable-Length Tokenization for Efficient LLMs Adapted from LZW Compression

链接: http://arxiv.org/abs/2410.21548v1

原文摘要:
Large language models have drastically changed the prospects of AI by
introducing technologies for more complex natural language processing. However,
current methodologies to train such LLMs require extensive resources including
but not limited to large amounts of data, expensive machinery, and lengthy
training. To solve this problem, this paper proposes a new tokenization method
inspired by universal Lempel-Ziv-Welch data compression that compresses
repetitive phrases into multi-word tokens. With MultiTok as a new tokenizing
tool, we show that language models are able to be trained notably more
efficiently while offering a similar accuracy on more succinct and compressed
training data. In fact, our results demonstrate that MultiTok achieves a
comparable performance to the BERT and GPT-2 standards as both a stand-alone
tokenizer and an add-on to existing tokenizers while also providing close to
2.5x faster training with more than 30% less training data.

中文翻译:
大型语言模型通过引入更复杂的自然语言处理技术，彻底改变了人工智能的发展前景。然而当前训练此类大模型的现有方法需要消耗大量资源，包括但不限于海量数据、昂贵硬件及冗长训练周期。为解决这一问题，本文受通用Lempel-Ziv-Welch数据压缩算法启发，提出一种将重复短语压缩为多词单元的新型分词方法。通过将MultiTok作为新型分词工具，我们证明语言模型能够在更精简的压缩训练数据上实现显著提升的训练效率，同时保持相近的准确率。实验结果表明：无论是作为独立分词器还是现有分词器的扩展组件，MultiTok在性能上均可比肩BERT和GPT-2标准，同时能实现近2.5倍的训练加速，并减少超过30%的训练数据需求。

（翻译说明：
1. 专业术语处理："tokenization method"译为"分词方法"，"multi-word tokens"译为"多词单元"符合NLP领域术语规范
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句，如第一句通过分号处理复杂信息
3. 被动语态转换："are able to be trained"转为主动式"能实现"
4. 数据呈现优化："close to 2.5x faster"译为"近2.5倍"符合中文倍数表达规范
5. 技术概念保留：保留"BERT/GPT-2"等专有名词不翻译，维持技术准确性
6. 逻辑连接词补充：添加"无论是...还是..."等连接结构增强行文连贯性）
