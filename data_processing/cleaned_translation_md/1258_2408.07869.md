# A Systematic Evaluation of Generated Time Series and Their Effects in Self-Supervised Pretraining

链接: http://arxiv.org/abs/2408.07869v1

原文摘要:
Self-supervised Pretrained Models (PTMs) have demonstrated remarkable
performance in computer vision and natural language processing tasks. These
successes have prompted researchers to design PTMs for time series data. In our
experiments, most self-supervised time series PTMs were surpassed by simple
supervised models. We hypothesize this undesired phenomenon may be caused by
data scarcity. In response, we test six time series generation methods, use the
generated data in pretraining in lieu of the real data, and examine the effects
on classification performance. Our results indicate that replacing a real-data
pretraining set with a greater volume of only generated samples produces
noticeable improvement.

中文翻译:
以下是符合学术规范的中文翻译：

自监督预训练模型（PTMs）在计算机视觉和自然语言处理任务中展现出卓越性能，这一成功促使研究者开始为时间序列数据设计PTM架构。然而实验表明，当前多数自监督时间序列PTMs的性能甚至不及简单的监督模型。我们推测该现象可能源于数据稀缺问题。为此，我们系统测试了六种时间序列生成方法，采用生成数据替代真实数据进行预训练，并评估其对分类性能的影响。实验结果表明：当使用更大规模的纯生成样本替代真实数据预训练集时，模型性能可获得显著提升。

（翻译说明：
1. 专业术语统一处理：PTMs全称与缩写对应，保持"预训练模型"标准译法
2. 被动语态转化："were surpassed"译为主动式"不及"更符合中文表达
3. 长句拆分：将原文复合句分解为符合中文阅读习惯的短句结构
4. 学术用语规范："hypothesize"译为"推测"而非字面"假设"，更贴近科研语境
5. 逻辑显化：增加"然而"等连接词明确转折关系
6. 概念准确："data scarcity"译为"数据稀缺"而非"数据缺乏"，符合机器学习领域术语）
