# Adaptive Optimization for Enhanced Efficiency in Large-Scale Language Model Training

链接: http://arxiv.org/abs/2412.04718v1

原文摘要:
With the rapid development of natural language processing technology,
large-scale language models (LLM) have achieved remarkable results in a variety
of tasks. However, how to effectively train these huge models and improve their
performance and computational efficiency remains an important challenge. This
paper proposes an improved method based on adaptive optimization algorithm,
aiming to improve the training efficiency and final performance of LLM. Through
comparative experiments on the SQuAD and GLUE data sets, the experimental
results show that compared with traditional optimization algorithms (such as
SGD, Momentum, AdaGrad, RMSProp and Adam), the adaptive optimization algorithm
we proposed has better accuracy and F1 score. Both have achieved significant
improvements, especially showed stronger training capabilities when processed
large-scale texts and complex tasks. The research results verify the advantages
of adaptive optimization algorithms in large-scale language model training and
provide new ideas and directions for future optimization methods.

中文翻译:
随着自然语言处理技术的快速发展，大规模语言模型（LLM）在多种任务中取得了显著成果。然而，如何有效训练这些庞大模型并提升其性能与计算效率仍是重要挑战。本文提出一种基于自适应优化算法的改进方法，旨在提高LLM的训练效率与最终性能。通过在SQuAD和GLUE数据集上的对比实验，结果表明相较于传统优化算法（如SGD、Momentum、AdaGrad、RMSProp和Adam），我们提出的自适应优化算法在准确率和F1分数上均取得显著提升，尤其在处理大规模文本和复杂任务时展现出更强的训练能力。研究成果验证了自适应优化算法在大规模语言模型训练中的优势，为未来优化方法提供了新思路与方向。


