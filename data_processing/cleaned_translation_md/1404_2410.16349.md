# Large Language Models in Computer Science Education: A Systematic Literature Review

链接: http://arxiv.org/abs/2410.16349v1

原文摘要:
Large language models (LLMs) are becoming increasingly better at a wide range
of Natural Language Processing tasks (NLP), such as text generation and
understanding. Recently, these models have extended their capabilities to
coding tasks, bridging the gap between natural languages (NL) and programming
languages (PL). Foundational models such as the Generative Pre-trained
Transformer (GPT) and LLaMA series have set strong baseline performances in
various NL and PL tasks. Additionally, several models have been fine-tuned
specifically for code generation, showing significant improvements in
code-related applications. Both foundational and fine-tuned models are
increasingly used in education, helping students write, debug, and understand
code. We present a comprehensive systematic literature review to examine the
impact of LLMs in computer science and computer engineering education. We
analyze their effectiveness in enhancing the learning experience, supporting
personalized education, and aiding educators in curriculum development. We
address five research questions to uncover insights into how LLMs contribute to
educational outcomes, identify challenges, and suggest directions for future
research.

中文翻译:
大型语言模型（LLMs）在文本生成与理解等自然语言处理（NLP）任务中表现日益卓越。近期，这类模型将其能力拓展至编程任务，弥合了自然语言（NL）与编程语言（PL）之间的鸿沟。以生成式预训练变换模型（GPT）和LLaMA系列为代表的基础模型，已在各类NL与PL任务中建立了强劲的基准性能。此外，多个专门针对代码生成进行微调的模型在代码相关应用中展现出显著提升。基础模型与微调模型正日益广泛应用于教育领域，助力学生编写、调试与理解代码。本文通过系统性文献综述，全面考察LLMs对计算机科学与计算机工程教育的影响：分析其在优化学习体验、支持个性化教育、辅助教师课程开发方面的有效性；针对五个研究问题展开探讨，揭示LLMs对教育成果的贡献机制，识别现存挑战，并为未来研究方向提出建议。  

（翻译说明：  
1. 专业术语保留英文缩写并首次出现时标注全称（如LLMs/Large Language Models）  
2. 采用学术文献惯用的主动语态（如"本文通过...考察"替代被动句式）  
3. 长句拆分重组（如将原文末段拆分为"分析...探讨...揭示"三个层次）  
4. 关键概念精准对应（如"fine-tuned"译为"微调"，"personalized education"译为"个性化教育"）  
5. 保持学术文本的简洁性与客观性，避免过度意译）
