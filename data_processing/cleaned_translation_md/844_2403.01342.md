# LM4OPT: Unveiling the Potential of Large Language Models in Formulating Mathematical Optimization Problems

链接: http://arxiv.org/abs/2403.01342v1

原文摘要:
In the rapidly evolving field of natural language processing, the translation
of linguistic descriptions into mathematical formulation of optimization
problems presents a formidable challenge, demanding intricate understanding and
processing capabilities from Large Language Models (LLMs). This study compares
prominent LLMs, including GPT-3.5, GPT-4, and Llama-2-7b, in zero-shot and
one-shot settings for this task. Our findings show GPT-4's superior
performance, particularly in the one-shot scenario. A central part of this
research is the introduction of `LM4OPT,' a progressive fine-tuning framework
for Llama-2-7b that utilizes noisy embeddings and specialized datasets.
However, this research highlights a notable gap in the contextual understanding
capabilities of smaller models such as Llama-2-7b compared to larger
counterparts, especially in processing lengthy and complex input contexts. Our
empirical investigation, utilizing the NL4Opt dataset, unveils that GPT-4
surpasses the baseline performance established by previous research, achieving
an F1-score of 0.63, solely based on the problem description in natural
language, and without relying on any additional named entity information.
GPT-3.5 follows closely, both outperforming the fine-tuned Llama-2-7b. These
findings not only benchmark the current capabilities of LLMs in a novel
application area but also lay the groundwork for future improvements in
mathematical formulation of optimization problems from natural language input.

中文翻译:
在自然语言处理这一快速发展的领域中，将语言描述转化为优化问题的数学公式是一项艰巨挑战，要求大语言模型（LLMs）具备复杂的理解与处理能力。本研究比较了GPT-3.5、GPT-4和Llama-2-7b等主流大语言模型在此任务中的零样本和单样本表现，发现GPT-4在单样本场景下表现尤为突出。研究核心是提出"LM4OPT"框架——一个基于噪声嵌入和专用数据集对Llama-2-7b进行渐进式微调的系统。然而，研究揭示了Llama-2-7b等较小模型与大型模型相比存在显著的上下文理解能力差距，尤其在处理冗长复杂输入时表现明显。基于NL4Opt数据集的实证研究表明，GPT-4仅凭自然语言问题描述（不依赖任何额外命名实体信息）就以0.63的F1分数超越了既有研究的基线水平，GPT-3.5紧随其后，两者均优于经过微调的Llama-2-7b。这些发现不仅为LLMs在新应用领域的当前能力提供了基准，更为未来从自然语言输入生成优化问题数学公式的改进奠定了基础。  

（翻译说明：  
1. 专业术语处理："zero-shot/one-shot"采用"零样本/单样本"的学界通用译法  
2. 技术概念显化：将"noisy embeddings"译为"噪声嵌入"并保留技术特征  
3. 长句拆分：将原文复合句按中文表达习惯分解为多个短句  
4. 被动语态转换："it is unveiled that"转为主动式"研究表明"  
5. 数据呈现优化：F1-score数值保留原格式，补充"分数"增强可读性  
6. 逻辑连接强化：使用"尤其""然而"等连接词保持论证脉络清晰）
