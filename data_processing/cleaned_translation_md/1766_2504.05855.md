# Enhancing Coreference Resolution with Pretrained Language Models: Bridging the Gap Between Syntax and Semantics

链接: http://arxiv.org/abs/2504.05855v1

原文摘要:
Large language models have made significant advancements in various natural
language processing tasks, including coreference resolution. However,
traditional methods often fall short in effectively distinguishing referential
relationships due to a lack of integration between syntactic and semantic
information. This study introduces an innovative framework aimed at enhancing
coreference resolution by utilizing pretrained language models. Our approach
combines syntax parsing with semantic role labeling to accurately capture finer
distinctions in referential relationships. By employing state-of-the-art
pretrained models to gather contextual embeddings and applying an attention
mechanism for fine-tuning, we improve the performance of coreference tasks.
Experimental results across diverse datasets show that our method surpasses
conventional coreference resolution systems, achieving notable accuracy in
disambiguating references. This development not only improves coreference
resolution outcomes but also positively impacts other natural language
processing tasks that depend on precise referential understanding.

中文翻译:
大型语言模型在各类自然语言处理任务中取得了显著进展，指代消解领域亦不例外。然而，传统方法由于未能有效整合句法与语义信息，往往难以精准区分指代关系。本研究提出一种创新框架，通过利用预训练语言模型来提升指代消解性能。该框架结合句法分析与语义角色标注技术，能更精确捕捉指代关系的细微差异。我们采用前沿的预训练模型获取上下文嵌入表示，并运用注意力机制进行微调，从而显著提升指代任务的性能表现。跨多数据集的实验结果表明，本方法在消解指代歧义方面准确率突出，性能优于传统指代消解系统。这一进展不仅优化了指代消解效果，同时对依赖精确指代理解的其他自然语言处理任务也产生了积极影响。

（翻译说明：采用学术论文摘要的标准表述方式，通过以下处理确保专业性与可读性：
1. 专业术语准确对应："coreference resolution"译为"指代消解"，"syntax parsing"译为"句法分析"
2. 长句拆分重组：将原文复合句按中文表达习惯分解为多个短句，如将"combines...labeling"独立成句
3. 逻辑显化处理：添加"亦不例外"等连接词强化段落连贯性
4. 被动语态转化："are employed"等被动结构转换为主动式"采用"
5. 概念准确传达："contextual embeddings"译为"上下文嵌入表示"既保留专业内涵又符合中文表达）
