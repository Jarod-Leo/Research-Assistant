# ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning

链接: http://arxiv.org/abs/2304.05613v1

原文摘要:
Over the last few years, large language models (LLMs) have emerged as the
most important breakthroughs in natural language processing (NLP) that
fundamentally transform research and developments in the field. ChatGPT
represents one of the most exciting LLM systems developed recently to showcase
impressive skills for language generation and highly attract public attention.
Among various exciting applications discovered for ChatGPT in English, the
model can process and generate texts for multiple languages due to its
multilingual training data. Given the broad adoption of ChatGPT for English in
different problems and areas, a natural question is whether ChatGPT can also be
applied effectively for other languages or it is necessary to develop more
language-specific technologies. The answer to this question requires a thorough
evaluation of ChatGPT over multiple tasks with diverse languages and large
datasets (i.e., beyond reported anecdotes), which is still missing or limited
in current research. Our work aims to fill this gap for the evaluation of
ChatGPT and similar LLMs to provide more comprehensive information for
multilingual NLP applications. While this work will be an ongoing effort to
include additional experiments in the future, our current paper evaluates
ChatGPT on 7 different tasks, covering 37 diverse languages with high, medium,
low, and extremely low resources. We also focus on the zero-shot learning
setting for ChatGPT to improve reproducibility and better simulate the
interactions of general users. Compared to the performance of previous models,
our extensive experimental results demonstrate a worse performance of ChatGPT
for different NLP tasks and languages, calling for further research to develop
better models and understanding for multilingual learning.

中文翻译:
近年来，大型语言模型（LLMs）已成为自然语言处理（NLP）领域最具突破性的技术，从根本上改变了该领域的研究与发展格局。作为近期最受瞩目的LLM系统之一，ChatGPT展现出令人惊叹的语言生成能力，引发了公众的广泛关注。尽管目前ChatGPT在英语应用中已展现出多样化的潜力，但由于其训练数据包含多语言语料，该模型同样具备处理与生成多语言文本的能力。鉴于ChatGPT在英语场景中的广泛应用，一个核心问题随之产生：该模型是否能同等高效地适用于其他语言？抑或我们需要开发更多针对特定语言的技术？要回答这个问题，必须基于多语言、多任务的大规模数据集（而非零散案例）对ChatGPT进行全面评估——而这正是当前研究中缺失或不足的环节。

本研究旨在填补这一空白，通过对ChatGPT及同类LLMs的系统评估，为多语言NLP应用提供更全面的参考依据。尽管未来将持续扩展实验范围，本文已针对7项不同任务展开评估，覆盖37种资源水平各异（从高资源到极低资源）的语言。我们特别聚焦ChatGPT的零样本学习场景，以提升结果可复现性，更真实地模拟普通用户的使用情境。与既有模型性能的对比显示，ChatGPT在不同NLP任务和语言中的表现普遍逊色于专用模型，这一发现呼吁学界进一步深入研究，以开发更优的多语言学习模型并深化相关理论认知。
