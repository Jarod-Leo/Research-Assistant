# OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models

链接: http://arxiv.org/abs/2406.01775v1

原文摘要:
The advent of large language models (LLMs) has revolutionized natural
language processing, enabling unprecedented capabilities in understanding and
generating human-like text. However, the computational cost and convergence
times associated with fine-tuning these models remain significant challenges.
Low-Rank Adaptation (LoRA) has emerged as a promising method to mitigate these
issues by introducing efficient fine-tuning techniques with a reduced number of
trainable parameters. In this paper, we present OLoRA, an enhancement to the
LoRA method that leverages orthonormal matrix initialization through QR
decomposition. OLoRA significantly accelerates the convergence of LLM training
while preserving the efficiency benefits of LoRA, such as the number of
trainable parameters and GPU memory footprint. Our empirical evaluations
demonstrate that OLoRA not only converges faster but also exhibits improved
performance compared to standard LoRA across a variety of language modeling
tasks. This advancement opens new avenues for more efficient and accessible
fine-tuning of LLMs, potentially enabling broader adoption and innovation in
natural language applications.

中文翻译:
大型语言模型（LLM）的出现彻底改变了自然语言处理领域，使其在理解和生成类人文本方面展现出前所未有的能力。然而，与微调这些模型相关的计算成本和收敛时间仍是重大挑战。低秩自适应（LoRA）通过引入可训练参数数量减少的高效微调技术，成为缓解这些问题的有效方法。本文提出OLoRA——一种基于QR分解正交矩阵初始化的LoRA增强方法。该方案在保持LoRA参数效率优势（如可训练参数数量和GPU内存占用）的同时，显著加速了LLM训练的收敛速度。实证研究表明，在多种语言建模任务中，OLoRA不仅收敛更快，其性能也优于标准LoRA。这一进展为LLM的高效微调开辟了新途径，有望推动自然语言应用更广泛的采纳与创新。

（翻译说明：
1. 专业术语处理："orthonormal matrix initialization"译为"正交矩阵初始化"，"QR decomposition"保留专业缩写"QR分解"
2. 技术概念显化："trainable parameters"统一译为"可训练参数"，"GPU memory footprint"译为"GPU内存占用"
3. 句式重构：将原文复合句拆分为符合中文表达习惯的短句，如将"leveraging...decomposition"转换为独立分句
4. 学术风格保持：使用"实证研究""开辟新途径"等符合论文摘要规范的表述
5. 逻辑衔接强化：通过"然而""通过""在...同时"等连接词保持论证连贯性）
