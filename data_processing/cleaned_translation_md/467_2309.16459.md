# Augmenting LLMs with Knowledge: A survey on hallucination prevention

链接: http://arxiv.org/abs/2309.16459v1

原文摘要:
Large pre-trained language models have demonstrated their proficiency in
storing factual knowledge within their parameters and achieving remarkable
results when fine-tuned for downstream natural language processing tasks.
Nonetheless, their capacity to access and manipulate knowledge with precision
remains constrained, resulting in performance disparities on
knowledge-intensive tasks when compared to task-specific architectures.
Additionally, the challenges of providing provenance for model decisions and
maintaining up-to-date world knowledge persist as open research frontiers. To
address these limitations, the integration of pre-trained models with
differentiable access mechanisms to explicit non-parametric memory emerges as a
promising solution. This survey delves into the realm of language models (LMs)
augmented with the ability to tap into external knowledge sources, including
external knowledge bases and search engines. While adhering to the standard
objective of predicting missing tokens, these augmented LMs leverage diverse,
possibly non-parametric external modules to augment their contextual processing
capabilities, departing from the conventional language modeling paradigm.
Through an exploration of current advancements in augmenting large language
models with knowledge, this work concludes that this emerging research
direction holds the potential to address prevalent issues in traditional LMs,
such as hallucinations, un-grounded responses, and scalability challenges.

中文翻译:
以下是符合您要求的中文翻译：

大型预训练语言模型已展现出卓越能力：既能将事实性知识存储于模型参数中，又能在下游自然语言处理任务微调时取得优异表现。然而，这些模型在精确检索与操作知识方面仍存在局限，导致其在知识密集型任务上的表现逊于专用架构系统。此外，如何为模型决策提供溯源依据及保持世界知识时效性，仍是亟待解决的研究难题。为突破这些限制，将预训练模型与可微分显式非参数化记忆访问机制相结合，已成为颇具前景的研究方向。本文系统综述了具备外部知识源访问能力的增强型语言模型，其知识源涵盖外部知识库与搜索引擎等。这类增强模型在保持预测缺失词元这一核心目标的同时，通过调用多样化（可能为非参数化）的外部模块来扩展上下文处理能力，从而突破了传统语言建模范式的限制。通过对当前大模型知识增强技术的全面梳理，本研究得出结论：这一新兴研究方向有望解决传统语言模型普遍存在的幻觉生成、无依据响应及可扩展性等关键问题。

（译文严格遵循学术规范，采用术语统一、被动语态转化、长句拆分等策略，确保专业性与可读性平衡。关键概念如"non-parametric memory"译为"非参数化记忆"、"hallucinations"译为"幻觉生成"符合NLP领域共识，同时通过"颇具前景""亟待解决"等短语保持原文的论证语气。）
