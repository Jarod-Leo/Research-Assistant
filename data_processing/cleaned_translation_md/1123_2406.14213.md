# Complexity of Symbolic Representation in Working Memory of Transformer Correlates with the Complexity of a Task

链接: http://arxiv.org/abs/2406.14213v1

原文摘要:
Even though Transformers are extensively used for Natural Language Processing
tasks, especially for machine translation, they lack an explicit memory to
store key concepts of processed texts. This paper explores the properties of
the content of symbolic working memory added to the Transformer model decoder.
Such working memory enhances the quality of model predictions in machine
translation task and works as a neural-symbolic representation of information
that is important for the model to make correct translations. The study of
memory content revealed that translated text keywords are stored in the working
memory, pointing to the relevance of memory content to the processed text.
Also, the diversity of tokens and parts of speech stored in memory correlates
with the complexity of the corpora for machine translation task.

中文翻译:
尽管Transformer模型已广泛应用于自然语言处理任务（尤其在机器翻译领域），但其缺乏显式记忆机制来存储已处理文本的关键概念。本文研究了在Transformer解码器中添加符号化工作记忆内容所呈现的特性。这种工作记忆不仅能提升机器翻译任务的预测质量，还形成了对模型正确翻译至关重要的神经符号化信息表征。通过对记忆内容的分析发现：工作记忆中存储了待译文本的关键词，这表明记忆内容与处理文本具有高度相关性。此外，记忆中所存词汇标记和词性的多样性程度，与机器翻译任务语料库的复杂度呈现显著相关性。
