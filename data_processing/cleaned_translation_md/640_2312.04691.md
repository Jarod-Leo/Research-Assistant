# Simul-LLM: A Framework for Exploring High-Quality Simultaneous Translation with Large Language Models

链接: http://arxiv.org/abs/2312.04691v1

原文摘要:
Large language models (LLMs) with billions of parameters and pretrained on
massive amounts of data are now capable of near or better than state-of-the-art
performance in a variety of downstream natural language processing tasks.
Neural machine translation (NMT) is one such task that LLMs have been applied
to with great success. However, little research has focused on applying LLMs to
the more difficult subset of NMT called simultaneous translation (SimulMT),
where translation begins before the entire source context is available to the
model. In this paper, we address key challenges facing LLMs fine-tuned for
SimulMT, validate classical SimulMT concepts and practices in the context of
LLMs, explore adapting LLMs that are fine-tuned for NMT to the task of SimulMT,
and introduce Simul-LLM, the first open-source fine-tuning and evaluation
pipeline development framework for LLMs focused on SimulMT.

中文翻译:
以下是符合要求的学术中文翻译：

具有数十亿参数且经过海量数据预训练的大语言模型（LLMs）当前在多种下游自然语言处理任务中已能实现接近或超越最先进水平的性能表现。神经机器翻译（NMT）正是LLMs取得显著成功的应用领域之一。然而，针对LLMs在NMT更复杂子任务——同步翻译（SimulMT，即在源文本未完全输入时即开始翻译）中的应用研究却鲜少涉及。本文系统研究了LLMs在SimulMT微调过程中的核心挑战：验证经典SimulMT理论在LLMs语境下的适用性，探索将NMT微调后的LLMs适配至SimulMT任务的方法，并首次提出专为SimulMT设计的开源框架Simul-LLM——该框架包含完整的LLMs微调与评估流程开发体系。

（翻译说明：
1. 专业术语处理：采用"大语言模型/LLMs"、"神经机器翻译/NMT"等学界通用译法，首现标注英文缩写
2. 长句拆分重构：将原文复合句分解为符合中文表达习惯的短句，如将"where..."从句独立处理
3. 被动语态转换："are now capable of..."译为主动式"当前已能实现..."
4. 概念显化："the more difficult subset"译为"更复杂子任务"以明确逻辑关系
5. 技术表述规范："fine-tuning"统一译为"微调"，"pipeline"译为"流程开发体系"符合计算机领域术语
6. 学术风格保持：使用"显著成功"、"系统研究"等学术用语，避免口语化表达）
