# Utilization of Pre-trained Language Model for Adapter-based Knowledge Transfer in Software Engineering

链接: http://arxiv.org/abs/2307.08540v1

原文摘要:
Software Engineering (SE) Pre-trained Language Models (PLMs), such as
CodeBERT, are pre-trained on large code corpora, and their learned knowledge
has shown success in transferring into downstream tasks (e.g., code clone
detection) through the fine-tuning of PLMs. In Natural Language Processing
(NLP), an alternative in transferring the knowledge of PLMs is explored through
the use of adapter, a compact and parameter efficient module that is inserted
into a PLM. Although the use of adapters has shown promising results in many
NLP-based downstream tasks, their application and exploration in SE-based
downstream tasks are limited.
  Here, we study the knowledge transfer using adapters on multiple down-stream
tasks including cloze test, code clone detection, and code summarization. These
adapters are trained on code corpora and are inserted into a PLM that is
pre-trained on English corpora or code corpora. We called these PLMs as NL-PLM
and C-PLM, respectively. We observed an improvement in results using NL-PLM
over a PLM that does not have adapters, and this suggested that adapters can
transfer and utilize useful knowledge from NL-PLM to SE tasks. The results are
sometimes on par with or exceed the results of C-PLM; while being more
efficient in terms of the number of parameters and training time.
Interestingly, adapters inserted into a C-PLM generally yield better results
than a traditional fine-tuned C-PLM. Our results open new directions to build
more compact models for SE tasks.

中文翻译:
以下是符合要求的学术中文翻译：

软件工程预训练语言模型（如CodeBERT）通过大规模代码语料库进行预训练，其习得的知识经微调后已成功迁移至下游任务（如代码克隆检测）。自然语言处理领域提出了一种替代性知识迁移方法——适配器（Adapter），即嵌入预训练模型中的紧凑型参数高效模块。尽管适配器在自然语言下游任务中表现优异，但其在软件工程下游任务中的应用探索仍属有限。

本研究探究了适配器在多项下游任务（完形填空测试、代码克隆检测、代码摘要生成）中的知识迁移效果。这些适配器经代码语料库训练后，被分别嵌入基于英语语料库（NL-PLM）和代码语料库（C-PLM）预训练的模型中。实验表明：1) 嵌入NL-PLM的适配器性能优于无适配器基线，证实其可有效迁移自然语言知识至软件工程任务；2) 其效果有时媲美甚至超越C-PLM，且具有参数量与训练时间的双重优势；3) 嵌入C-PLM的适配器通常优于传统微调方法。该发现为构建更紧凑的软件工程任务模型开辟了新途径。

（翻译严格遵循以下原则：
1. 专业术语准确统一（如PLM/Pretrained Language Model译为"预训练语言模型"）
2. 被动语态转换为主动句式（"are pre-trained"译为"通过...进行预训练"）
3. 长难句拆分重组（将原文最后复合句分解为三点发现）
4. 保留学术表述风格（使用"探究""表明""证实"等科研用语）
5. 逻辑关系显化（通过冒号、分号等标点明确层次关系）
6. 符合中文科技论文摘要惯例（首句背景导入，末句研究意义））
