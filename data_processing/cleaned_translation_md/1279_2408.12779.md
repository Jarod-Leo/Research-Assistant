# Investigating LLM Applications in E-Commerce

链接: http://arxiv.org/abs/2408.12779v1

原文摘要:
The emergence of Large Language Models (LLMs) has revolutionized natural
language processing in various applications especially in e-commerce. One
crucial step before the application of such LLMs in these fields is to
understand and compare the performance in different use cases in such tasks.
This paper explored the efficacy of LLMs in the e-commerce domain, focusing on
instruction-tuning an open source LLM model with public e-commerce datasets of
varying sizes and comparing the performance with the conventional models
prevalent in industrial applications. We conducted a comprehensive comparison
between LLMs and traditional pre-trained language models across specific tasks
intrinsic to the e-commerce domain, namely classification, generation,
summarization, and named entity recognition (NER). Furthermore, we examined the
effectiveness of the current niche industrial application of very large LLM,
using in-context learning, in e-commerce specific tasks. Our findings indicate
that few-shot inference with very large LLMs often does not outperform
fine-tuning smaller pre-trained models, underscoring the importance of
task-specific model optimization.Additionally, we investigated different
training methodologies such as single-task training, mixed-task training, and
LoRA merging both within domain/tasks and between different tasks. Through
rigorous experimentation and analysis, this paper offers valuable insights into
the potential effectiveness of LLMs to advance natural language processing
capabilities within the e-commerce industry.

中文翻译:
大型语言模型（LLMs）的崛起为自然语言处理领域带来了革命性变革，尤其在电子商务应用中表现显著。在将这些模型投入实际应用前，关键步骤是理解并比较其在不同任务场景中的性能表现。本研究探讨了LLMs在电商领域的效能，重点通过不同规模的公开电商数据集对开源LLM模型进行指令微调，并将其性能与工业界主流传统模型进行对比。我们针对电商领域核心任务——分类、生成、摘要和命名实体识别（NER）——开展了LLMs与传统预训练语言模型的全面性能比较。此外，我们还验证了当前业界超大规模LLM通过情境学习（in-context learning）在电商专项任务中的实际应用效果。研究发现：基于超大模型的少样本推理往往无法超越经过微调的小型预训练模型，这凸显了任务特异性优化的重要性。我们进一步探究了单任务训练、混合任务训练以及LoRA合并等训练方法在领域内/跨任务中的应用效果。通过严谨的实验与分析，本文为LLMs提升电商行业自然语言处理能力的潜在有效性提供了重要见解。  

（翻译说明：  
1. 专业术语处理："instruction-tuning"译为"指令微调"，"in-context learning"采用学界通用译法"情境学习"  
2. 长句拆分：将原文复合句按中文表达习惯分解为多个短句，如将"focusing on..."独立成句  
3. 被动语态转换："was conducted"等被动结构转为主动式"我们开展..."  
4. 概念显化处理："few-shot inference"译为"少样本推理"并补充说明性文字  
5. 技术术语保留：LoRA、NER等专业缩写首次出现时保留英文并加注中文全称）
