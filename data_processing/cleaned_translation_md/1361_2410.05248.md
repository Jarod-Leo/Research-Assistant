# SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe

链接: http://arxiv.org/abs/2410.05248v1

原文摘要:
To acquire instruction-following capabilities, large language models (LLMs)
undergo instruction tuning, where they are trained on instruction-response
pairs using next-token prediction (NTP). Efforts to improve instruction tuning
often focus on higher-quality supervised fine-tuning (SFT) datasets, typically
requiring data filtering with proprietary LLMs or human annotation. In this
paper, we take a different approach by proposing SFTMix, a novel Mixup-based
recipe that elevates LLM instruction tuning beyond the conventional NTP
paradigm, without relying on well-curated datasets. Observing that LLMs exhibit
uneven confidence across the semantic representation space, we argue that
examples with different confidence levels should play distinct roles in
instruction tuning--confident data is prone to overfitting, while unconfident
data is harder to generalize. Based on this insight, SFTMix leverages training
dynamics to identify examples with varying confidence levels, interpolates them
to bridge the confidence gap, and applies a Mixup-based regularization to
support learning on these additional, interpolated examples. By propagating
supervision signals across confidence regions and encouraging linear behavior
between them, SFTMix mitigates overfitting in confident examples while
enhancing generalization in unconfident ones. We demonstrate the effectiveness
of SFTMix in both instruction-following and healthcare-specific SFT tasks, with
consistent improvements across LLM families and SFT datasets of varying sizes
and qualities. Extensive analyses across six directions highlight SFTMix's
compatibility with data selection, adaptability to compute-constrained
scenarios, and scalability to broader applications.

中文翻译:
为获得指令跟随能力，大型语言模型（LLMs）需经过指令微调训练——通过下一词元预测（NTP）方法学习指令-响应对。现有改进方法通常聚焦于提升监督微调（SFT）数据集质量，往往依赖专有大模型进行数据过滤或人工标注。本文另辟蹊径，提出SFTMix这一基于Mixup的创新方案，无需依赖精标数据集即可突破传统NTP范式，实现更优的指令微调效果。我们发现LLMs在语义表征空间中存在置信度分布不均现象，主张不同置信度的样本应在指令微调中承担差异化角色：高置信样本易引发过拟合，而低置信样本更难泛化。基于此洞见，SFTMix通过训练动态识别不同置信度的样本，对其进行插值以弥合置信差距，并应用基于Mixup的正则化方法支持这些新增插值样本的学习。该方案通过在置信区域间传递监督信号并促进线性行为，有效抑制高置信样本的过拟合，同时提升低置信样本的泛化能力。我们在通用指令跟随和医疗领域SFT任务中验证了SFTMix的有效性，在不同规模/质量的LLM家族和SFT数据集上均取得稳定提升。六维度的深入分析表明：SFTMix兼具数据选择兼容性、算力受限场景适应性和更广泛应用的扩展潜力。

（翻译说明：采用学术论文的简洁风格，通过以下处理实现专业表达：
1. 术语统一："instruction tuning"译为"指令微调"，"supervised fine-tuning"保留英文缩写"SFT"并首次出现时标注全称
2. 复杂句式重构：将英语长句拆分为符合中文表达习惯的短句，如将"interpolates them to..."处理为分号连接的并列结构
3. 概念显化："training dynamics"译为"训练动态"而非字面直译，准确传达机器学习领域特定含义
4. 被动语态转化："are trained on"译为主动态的"学习"
5. 专业表述："linear behavior"保留"线性行为"的数学概念原意
6. 文化适配："recipe"根据计算机领域惯例译为"方案"而非字面"配方"）
