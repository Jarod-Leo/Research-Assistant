# Transformers are efficient hierarchical chemical graph learners

链接: http://arxiv.org/abs/2310.01704v1

原文摘要:
Transformers, adapted from natural language processing, are emerging as a
leading approach for graph representation learning. Contemporary graph
transformers often treat nodes or edges as separate tokens. This approach leads
to computational challenges for even moderately-sized graphs due to the
quadratic scaling of self-attention complexity with token count. In this paper,
we introduce SubFormer, a graph transformer that operates on subgraphs that
aggregate information by a message-passing mechanism. This approach reduces the
number of tokens and enhances learning long-range interactions. We demonstrate
SubFormer on benchmarks for predicting molecular properties from chemical
structures and show that it is competitive with state-of-the-art graph
transformers at a fraction of the computational cost, with training times on
the order of minutes on a consumer-grade graphics card. We interpret the
attention weights in terms of chemical structures. We show that SubFormer
exhibits limited over-smoothing and avoids over-squashing, which is prevalent
in traditional graph neural networks.

中文翻译:
以下是符合学术规范的中文翻译：

基于自然语言处理改进的Transformer模型正逐渐成为图表示学习的主流方法。当前图Transformer通常将节点或边视为独立标记，但由于自注意力机制复杂度与标记数量呈平方级增长，即使处理中等规模图数据也会面临计算挑战。本文提出SubFormer模型，这是一种基于子图结构的图Transformer，通过消息传递机制聚合信息。该方法既能减少标记数量，又能增强长程交互的学习能力。我们在化学结构分子性质预测基准测试中验证了SubFormer的性能，结果表明其计算成本仅为当前最优图Transformer的极小部分（在消费级显卡上训练时间仅需数分钟），却能达到相当的预测精度。我们通过化学结构对注意力权重进行了解释性分析，并证明SubFormer能有效缓解传统图神经网络中普遍存在的过度平滑现象，同时避免了信息过度压缩问题。

（翻译说明：
1. 专业术语处理："message-passing mechanism"译为"消息传递机制"，"over-smoothing/over-squashing"采用学界通用译法"过度平滑/信息过度压缩"
2. 句式重构：将英语长句拆分为符合中文表达习惯的短句，如将"show that..."从句转换为独立结果句
3. 数据呈现：括号补充说明训练硬件条件，符合中文论文表述惯例
4. 概念显化："a fraction of"具体化为"极小部分"，"on the order of minutes"量化为"数分钟"
5. 逻辑衔接：添加"既能...又能..."等连接词强化行文逻辑）
