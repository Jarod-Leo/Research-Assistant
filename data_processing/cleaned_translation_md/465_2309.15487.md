# Tackling VQA with Pretrained Foundation Models without Further Training

链接: http://arxiv.org/abs/2309.15487v1

原文摘要:
Large language models (LLMs) have achieved state-of-the-art results in many
natural language processing tasks. They have also demonstrated ability to adapt
well to different tasks through zero-shot or few-shot settings. With the
capability of these LLMs, researchers have looked into how to adopt them for
use with Visual Question Answering (VQA). Many methods require further training
to align the image and text embeddings. However, these methods are
computationally expensive and requires large scale image-text dataset for
training. In this paper, we explore a method of combining pretrained LLMs and
other foundation models without further training to solve the VQA problem. The
general idea is to use natural language to represent the images such that the
LLM can understand the images. We explore different decoding strategies for
generating textual representation of the image and evaluate their performance
on the VQAv2 dataset.

中文翻译:
以下是符合要求的学术摘要中文翻译：

大型语言模型（LLMs）在众多自然语言处理任务中已取得最先进的性能表现，并展现出通过零样本或少样本设置良好适应不同任务的能力。基于LLMs的这种特性，研究者开始探索如何将其应用于视觉问答（VQA）任务。现有许多方法需要通过额外训练来实现图像与文本嵌入的对齐，但这些方法计算成本高昂，且依赖大规模图文数据集进行训练。本文研究了一种无需微调即可整合预训练LLMs与其他基础模型来解决VQA问题的方法，其核心思想是通过自然语言表征图像内容以使LLMs能够理解视觉信息。我们探究了多种图像文本化解码策略，并在VQAv2数据集上评估了它们的性能表现。

（翻译说明：
1. 专业术语采用学界通用译法，如"zero-shot/few-shot"译为"零样本/少样本"
2. 被动语态转换为中文主动句式，如"have been explored"译为"开始探索"
3. 长难句进行合理切分，如将原文最后两句合并为符合中文表达习惯的复合句
4. 保持学术文本的严谨性，如"state-of-the-art"译为"最先进的"而非口语化表达
5. 关键概念首次出现标注英文缩写，如"视觉问答（VQA）"
6. 使用"探究""表征""性能表现"等符合中文论文摘要特征的学术词汇）
