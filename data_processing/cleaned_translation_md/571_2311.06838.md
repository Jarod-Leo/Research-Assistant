# GIELLM: Japanese General Information Extraction Large Language Model Utilizing Mutual Reinforcement Effect

链接: http://arxiv.org/abs/2311.06838v1

原文摘要:
Information Extraction (IE) stands as a cornerstone in natural language
processing, traditionally segmented into distinct sub-tasks. The advent of
Large Language Models (LLMs) heralds a paradigm shift, suggesting the
feasibility of a singular model addressing multiple IE subtasks. In this vein,
we introduce the General Information Extraction Large Language Model (GIELLM),
which integrates text Classification, Sentiment Analysis, Named Entity
Recognition, Relation Extraction, and Event Extraction using a uniform
input-output schema. This innovation marks the first instance of a model
simultaneously handling such a diverse array of IE subtasks. Notably, the
GIELLM leverages the Mutual Reinforcement Effect (MRE), enhancing performance
in integrated tasks compared to their isolated counterparts. Our experiments
demonstrate State-of-the-Art (SOTA) results in five out of six Japanese mixed
datasets, significantly surpassing GPT-3.5-Turbo. Further, an independent
evaluation using the novel Text Classification Relation and Event
Extraction(TCREE) dataset corroborates the synergistic advantages of MRE in
text and word classification. This breakthrough paves the way for most IE
subtasks to be subsumed under a singular LLM framework. Specialized fine-tune
task-specific models are no longer needed.

中文翻译:
信息抽取（Information Extraction, IE）作为自然语言处理的基石技术，传统上被划分为多个独立子任务。随着大语言模型（Large Language Models, LLMs）的出现，这一领域正迎来范式变革——单一模型处理多重IE子任务成为可能。为此，我们提出通用信息抽取大语言模型（GIELLM），通过统一的输入-输出架构整合文本分类、情感分析、命名实体识别、关系抽取和事件抽取五大功能。这一创新首次实现了单一模型同步处理如此多元的IE子任务组合。特别值得注意的是，GIELLM利用互增强效应（Mutual Reinforcement Effect, MRE），使得集成任务的性能表现显著优于孤立任务场景。实验数据显示，在六组日语混合数据集中，GIELLM有五项达到最先进水平（SOTA），性能大幅超越GPT-3.5-Turbo。此外，基于新型文本分类-关系-事件联合抽取（TCREE）数据集的独立评估进一步验证了MRE在文本分类与词语分类中的协同优势。这一突破性进展昭示着：绝大多数IE子任务有望纳入统一的大语言模型框架，特定任务的专用微调模型将不再必要。

（译文特点说明：
1. 专业术语采用"信息抽取""大语言模型"等学界通用译法
2. 技术概念如MRE采用"互增强效应"意译，括号保留英文原称
3. 模型名称GIELLM保留英文缩写，首次出现时标注全称
4. 长难句拆分重组，如将"using a uniform input-output schema"转化为"通过...架构"的前置状语
5. 被动语态转化，如"are no longer needed"译为"将不再必要"的主动表述
6. 学术用语规范化处理，如"paradigm shift"译为"范式变革"而非字面翻译）
