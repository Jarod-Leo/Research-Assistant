# Open or Closed LLM for Lesser-Resourced Languages? Lessons from Greek

链接: http://arxiv.org/abs/2501.12826v1

原文摘要:
Natural Language Processing (NLP) for lesser-resourced languages faces
persistent challenges, including limited datasets, inherited biases from
high-resource languages, and the need for domain-specific solutions. This study
addresses these gaps for Modern Greek through three key contributions. First,
we evaluate the performance of open-source (Llama-70b) and closed-source
(GPT-4o mini) large language models (LLMs) on seven core NLP tasks with dataset
availability, revealing task-specific strengths, weaknesses, and parity in
their performance. Second, we expand the scope of Greek NLP by reframing
Authorship Attribution as a tool to assess potential data usage by LLMs in
pre-training, with high 0-shot accuracy suggesting ethical implications for
data provenance. Third, we showcase a legal NLP case study, where a Summarize,
Translate, and Embed (STE) methodology outperforms the traditional TF-IDF
approach for clustering \emph{long} legal texts. Together, these contributions
provide a roadmap to advance NLP in lesser-resourced languages, bridging gaps
in model evaluation, task innovation, and real-world impact.

中文翻译:
针对资源相对匮乏语言的自然语言处理（NLP）研究长期面临三大挑战：数据集有限、继承自高资源语言的固有偏见，以及领域专用解决方案的缺失。本研究以现代希腊语为对象，通过三项核心创新应对这些挑战。首先，我们基于现有数据集评估开源模型（Llama-70b）与闭源模型（GPT-4o mini）在七项核心NLP任务中的表现，揭示了模型在特定任务中的优势、劣势及性能均衡性。其次，通过将"作者归属识别"重构为检测大语言模型预训练数据使用情况的工具（其零样本高准确率暗示了数据来源的伦理问题），我们拓展了希腊语NLP的研究维度。最后，在法律NLP案例研究中，我们提出的"摘要-翻译-嵌入"（STE）方法在长文本法律文件聚类任务中显著优于传统TF-IDF方法。这些成果共同为资源匮乏语言的NLP发展提供了技术路线图，在模型评估、任务创新与实际应用三个维度架设了进步阶梯。  


