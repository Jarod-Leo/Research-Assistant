# Federated Learning with Layer Skipping: Efficient Training of Large Language Models for Healthcare NLP

链接: http://arxiv.org/abs/2504.10536v1

原文摘要:
Federated learning (FL) enables collaborative model training across
organizations without sharing raw data, addressing crucial privacy concerns in
healthcare natural language processing (NLP). However, training large language
models (LLMs) in federated settings faces significant challenges, including
communication overhead and data heterogeneity. We propose Layer-Skipping
Federated Learning, where only selected layers of a pre-trained LLM are
fine-tuned across clients while others remain frozen. Applied to LLaMA 3.2-1B,
our approach reduces communication costs by approximately 70% while maintaining
performance within 2% of centralized training. We evaluate our method on
clinical NER and classification tasks using i2b2 and MIMIC-III datasets. Our
experiments demonstrate that Layer-Skipping FL outperforms competitive
baselines, handles non-IID clinical data distributions effectively, and shows
robustness when combined with differential privacy. This approach represents a
practical solution for privacy-preserving collaborative learning in healthcare
NLP.

中文翻译:
以下是符合您要求的中文翻译：

联邦学习（FL）使得跨机构协作训练模型成为可能，而无需共享原始数据，这有效解决了医疗自然语言处理（NLP）中的隐私保护核心问题。然而，在联邦环境下训练大语言模型（LLMs）仍面临重大挑战，包括通信开销和数据异构性。我们提出"层跳跃联邦学习"方法，该方法仅对预训练LLM中的选定层进行客户端微调，其余层则保持冻结状态。在LLaMA 3.2-1B模型上的应用表明，我们的方法可降低约70%的通信成本，同时性能与集中式训练的差距保持在2%以内。我们使用i2b2和MIMIC-III数据集对临床命名实体识别和分类任务进行评估。实验证明：层跳跃联邦学习不仅优于现有基线方法，能有效处理非独立同分布的临床数据，在与差分隐私结合时也展现出良好鲁棒性。该方法为医疗NLP领域的隐私保护协作学习提供了实用解决方案。

翻译说明：
1. 专业术语处理：采用"联邦学习""大语言模型""非独立同分布"等学界通用译法
2. 技术概念保留：Layer-Skipping FL译为"层跳跃联邦学习"以保持技术特征
3. 数据名称处理：i2b2和MIMIC-III作为专有名词保留不译
4. 句式重构：将英文长句拆分为符合中文表达习惯的短句（如第一句拆分）
5. 被动语态转换："are fine-tuned"译为主动式"进行...微调"
6. 数字规范：统一使用阿拉伯数字"70%"和"2%"
7. 学术风格保持：使用"该方法""实验证明"等学术用语
