# ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large Language Models

链接: http://arxiv.org/abs/2408.08554v1

原文摘要:
Large Language Models (LLMs) have revolutionized natural language processing
tasks. However, their practical application is constrained by substantial
memory and computational demands. Post-training quantization (PTQ) is
considered an effective method to accelerate LLM inference. Despite its growing
popularity in LLM model compression, PTQ deployment faces two major challenges.
First, low-bit quantization leads to performance degradation. Second,
restricted by the limited integer computing unit type on GPUs, quantized matrix
operations with different precisions cannot be effectively accelerated. To
address these issues, we introduce a novel arbitrary-bit quantization algorithm
and inference framework, ABQ-LLM. It achieves superior performance across
various quantization settings and enables efficient arbitrary-precision
quantized inference on the GPU. ABQ-LLM introduces several key innovations: (1)
a distribution correction method for transformer blocks to mitigate
distribution differences caused by full quantization of weights and
activations, improving performance at low bit-widths. (2) the bit balance
strategy to counteract performance degradation from asymmetric distribution
issues at very low bit-widths (e.g., 2-bit). (3) an innovative quantization
acceleration framework that reconstructs the quantization matrix multiplication
of arbitrary precision combinations based on BTC (Binary TensorCore)
equivalents, gets rid of the limitations of INT4/INT8 computing units. ABQ-LLM
can convert each component bit width gain into actual acceleration gain,
maximizing performance under mixed precision(e.g., W6A6, W2A8). Based on W2*A8
quantization configuration on LLaMA-7B model, it achieved a WikiText2
perplexity of 7.59 (2.17$\downarrow $ vs 9.76 in AffineQuant). Compared to
SmoothQuant, we realized 1.6$\times$ acceleration improvement and 2.7$\times$
memory compression gain.

中文翻译:
大型语言模型（LLMs）为自然语言处理任务带来了革命性变革，但其实际应用仍受限于巨大的内存与计算需求。训练后量化（PTQ）被视为加速LLM推理的有效方法。尽管PTQ在LLM模型压缩领域日益普及，但其部署面临两大挑战：一是低比特量化导致性能下降；二是受限于GPU有限的整数计算单元类型，不同精度的量化矩阵运算无法有效加速。为此，我们提出创新的任意比特量化算法与推理框架ABQ-LLM，该方案在多种量化配置下均能实现卓越性能，并支持GPU端的高效任意精度量化推理。ABQ-LLM包含以下核心创新：（1）针对Transformer模块的分布校正方法，通过缓解权重与激活值全量化引发的分布差异，提升低比特位宽下的性能；（2）比特平衡策略，用于抵消极低比特位宽（如2比特）下非对称分布问题导致的性能衰减；（3）基于BTC（二进制张量核）等效原理重构任意精度组合的量化矩阵乘法框架，突破INT4/INT8计算单元的限制。ABQ-LLM能将各组件的比特位宽增益转化为实际加速收益，在混合精度场景（如W6A6、W2A8）下实现性能最大化。在LLaMA-7B模型上采用W2*A8量化配置时，其WikiText2困惑度达7.59（较AffineQuant的9.76降低2.17），相比SmoothQuant实现了1.6倍加速提升与2.7倍内存压缩增益。
