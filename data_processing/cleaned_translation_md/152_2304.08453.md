# Improving Autoregressive NLP Tasks via Modular Linearized Attention

链接: http://arxiv.org/abs/2304.08453v1

原文摘要:
Various natural language processing (NLP) tasks necessitate models that are
efficient and small based on their ultimate application at the edge or in other
resource-constrained environments. While prior research has reduced the size of
these models, increasing computational efficiency without considerable
performance impacts remains difficult, especially for autoregressive tasks.
This paper proposes modular linearized attention (MLA), which combines multiple
efficient attention mechanisms, including cosFormer, to maximize inference
quality while achieving notable speedups. We validate this approach on several
autoregressive NLP tasks, including speech-to-text neural machine translation
(S2T NMT), speech-to-text simultaneous translation (SimulST), and
autoregressive text-to-spectrogram, noting efficiency gains on TTS and
competitive performance for NMT and SimulST during training and inference.

中文翻译:
以下是符合学术规范的中文翻译：

多种自然语言处理（NLP）任务需要根据其在边缘设备或其他资源受限环境中的最终应用场景，构建高效且轻量化的模型。尽管已有研究成功缩小了模型规模，但在保持性能不受显著影响的前提下提升计算效率仍具挑战性，尤其对于自回归任务而言。本文提出模块化线性注意力机制（MLA），通过整合包括cosFormer在内的多种高效注意力机制，在实现显著加速的同时最大化推理质量。我们在多项自回归NLP任务上验证了该方法的有效性，包括语音到文本神经机器翻译（S2T NMT）、语音到文本同步翻译（SimulST）以及自回归文本到声谱图转换任务，实验结果表明：在文本转语音（TTS）任务中取得效率提升，同时在NMT和SimulST任务的训练与推理阶段保持了具有竞争力的性能表现。

翻译说明：
1. 专业术语处理：采用"NLP/自回归/神经机器翻译"等学界通用译法，括号保留英文缩写（如MLA/S2T NMT）
2. 长句拆分：将原文复合长句按中文表达习惯分解为多个短句，如将"based on..."状语从句转为前置分句
3. 被动语态转换："has been reduced"译为主动式"成功缩小"
4. 概念显化："edge environments"译为"边缘设备"而非直译"边缘环境"
5. 逻辑显化：通过"实验结果表明"等过渡词明确结论部分
6. 技术表述："competitive performance"译为"具有竞争力的性能"符合中文论文表述惯例
