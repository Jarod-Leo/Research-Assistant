# RankingGPT: Empowering Large Language Models in Text Ranking with Progressive Enhancement

链接: http://arxiv.org/abs/2311.16720v1

原文摘要:
Text ranking is a critical task in information retrieval. Recent advances in
pre-trained language models (PLMs), especially large language models (LLMs),
present new opportunities for applying them to text ranking. While supervised
fine-tuning (SFT) with ranking data has been widely explored to better align
PLMs with text ranking goals, previous studies have focused primarily on
encoder-only and encoder-decoder PLMs. Research on leveraging decoder-only LLMs
for text ranking remains scarce. An exception to this is RankLLaMA, which uses
direct SFT to explore LLaMA's potential for text ranking. In this work, we
propose a two-stage progressive paradigm to better adapt LLMs to text ranking.
First, we conduct continual pre-training (CPT) of LLMs on a large
weakly-supervised corpus. Second, we perform SFT, and propose an improved
optimization strategy building upon RankLLaMA. Our experimental results on
multiple benchmarks show that our approach outperforms previous methods in both
in-domain and out-domain scenarios.

中文翻译:
文本排序是信息检索中的核心任务。预训练语言模型（PLMs）尤其是大语言模型（LLMs）的最新进展，为将其应用于文本排序提供了新的机遇。虽然基于排序数据的有监督微调（SFT）已被广泛探索以更好地使PLMs与文本排序目标对齐，但先前研究主要集中于仅编码器和编码器-解码器架构的PLMs。关于利用仅解码器LLMs进行文本排序的研究仍然匮乏，RankLLaMA是其中例外，它通过直接SFT探索了LLaMA在文本排序中的潜力。本研究提出两阶段渐进式范式以更好地使LLMs适配文本排序任务：首先在大型弱监督语料库上对LLMs进行持续预训练（CPT），随后实施SFT并提出基于RankLLaMA的改进优化策略。在多个基准测试上的实验结果表明，我们的方法在领域内和跨域场景中均优于现有方法。

（翻译说明：
1. 专业术语采用学术界通用译法，如"pre-trained language models"译为"预训练语言模型"，"supervised fine-tuning"译为"有监督微调"
2. 技术概念保持原文精确性，如"encoder-only/decoder-only"译为"仅编码器/仅解码器"
3. 长句按中文表达习惯拆分重组，如将原文最后复合句分解为因果逻辑的短句
4. 被动语态转换为主动句式，如"has been widely explored"译为"已被广泛探索"调整为"广泛探索"
5. 保持技术表述严谨性的同时提升可读性，如"weakly-supervised corpus"译为"弱监督语料库"而非字面直译
6. 模型名称RankLLaMA/LLaMA保留原名不翻译，符合技术文献惯例）
