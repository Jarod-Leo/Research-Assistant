# Raiders of the Lost Dependency: Fixing Dependency Conflicts in Python using LLMs

链接: http://arxiv.org/abs/2501.16191v1

原文摘要:
Fixing Python dependency issues is a tedious and error-prone task for
developers, who must manually identify and resolve environment dependencies and
version constraints of third-party modules and Python interpreters. Researchers
have attempted to automate this process by relying on large knowledge graphs
and database lookup tables. However, these traditional approaches face
limitations due to the variety of dependency error types, large sets of
possible module versions, and conflicts among transitive dependencies. This
study explores the potential of using large language models (LLMs) to
automatically fix dependency issues in Python programs. We introduce PLLM
(pronounced "plum"), a novel technique that employs retrieval-augmented
generation (RAG) to help an LLM infer Python versions and required modules for
a given Python file. PLLM builds a testing environment that iteratively (1)
prompts the LLM for module combinations, (2) tests the suggested changes, and
(3) provides feedback (error messages) to the LLM to refine the fix. This
feedback cycle leverages natural language processing (NLP) to intelligently
parse and interpret build error messages. We benchmark PLLM on the Gistable
HG2.9K dataset, a collection of challenging single-file Python gists. We
compare PLLM against two state-of-the-art automatic dependency inference
approaches, namely PyEGo and ReadPyE, w.r.t. the ability to resolve dependency
issues. Our results indicate that PLLM can fix more dependency issues than the
two baselines, with +218 (+15.97%) more fixes over ReadPyE and +281 (+21.58%)
over PyEGo. Our deeper analyses suggest that PLLM is particularly beneficial
for projects with many dependencies and for specific third-party numerical and
machine-learning modules. Our findings demonstrate the potential of LLM-based
approaches to iteratively resolve Python dependency issues.

中文翻译:
以下是符合您要求的中文翻译：

解决Python依赖问题对开发者而言是一项繁琐且易出错的任务，他们需要手动识别并解决第三方模块与Python解释器的环境依赖及版本约束。现有研究尝试通过大型知识图谱和数据库查询表实现自动化处理，但传统方法因依赖错误类型多样、可选模块版本庞大以及传递性依赖冲突等问题存在局限。本研究探索利用大语言模型（LLM）自动修复Python程序依赖问题的潜力，提出名为PLLM（发音同"plum"）的创新技术，该技术采用检索增强生成（RAG）机制辅助LLM推断Python版本及所需模块。PLLM构建的测试环境通过迭代循环：(1)向LLM获取模块组合建议，(2)测试修改方案，(3)将构建错误信息反馈给LLM以优化修复方案。该反馈循环利用自然语言处理（NLP）技术智能解析构建错误信息。我们在Gistable HG2.9K数据集（包含具有挑战性的单文件Python代码片段集合）上对PLLM进行基准测试，对比当前最先进的两种自动依赖推断方法PyEGo和ReadPyE的依赖解决能力。实验结果表明PLLM能修复更多依赖问题：相较ReadPyE多修复218个（+15.97%），较PyEGo多修复281个（+21.58%）。深入分析显示PLLM尤其适用于多依赖项目及特定第三方数值计算与机器学习模块。本研究证实了基于LLM的迭代方法在解决Python依赖问题方面的巨大潜力。

翻译说明：
1. 专业术语处理：LLM/RAG/NLP等专业缩写首次出现时保留英文缩写并添加中文全称
2. 技术概念转化："transitive dependencies"译为"传递性依赖"符合计算机领域术语
3. 数据呈现方式：百分比增幅采用中文习惯的"+15.97%"格式
4. 句式重构：将原文复合长句拆分为符合中文表达习惯的短句，如将"iteratively (1)...(2)...(3)..."处理为分号列举式结构
5. 文化适配："plum"采用音译"plum"加注发音说明的方式处理
6. 被动语态转换：将英文被动式转换为中文主动式，如"are prompted"译为"向...获取"
7. 逻辑连接词优化：使用"尤其适用于"替代原文较生硬的"particularly beneficial for"表达
