# Domain-specific Continued Pretraining of Language Models for Capturing Long Context in Mental Health

链接: http://arxiv.org/abs/2304.10447v1

原文摘要:
Pretrained language models have been used in various natural language
processing applications. In the mental health domain, domain-specific language
models are pretrained and released, which facilitates the early detection of
mental health conditions. Social posts, e.g., on Reddit, are usually long
documents. However, there are no domain-specific pretrained models for
long-sequence modeling in the mental health domain. This paper conducts
domain-specific continued pretraining to capture the long context for mental
health. Specifically, we train and release MentalXLNet and MentalLongformer
based on XLNet and Longformer. We evaluate the mental health classification
performance and the long-range ability of these two domain-specific pretrained
models. Our models are released in HuggingFace.

中文翻译:
预训练语言模型已广泛应用于各类自然语言处理任务。在心理健康领域，研究者预训练并发布了多个领域专用语言模型，这些模型有助于心理健康问题的早期识别。社交媒体帖子（如Reddit平台内容）通常属于长文本序列，然而当前心理健康领域尚缺乏针对长序列建模的领域专用预训练模型。本研究通过领域特异性持续预训练来捕捉心理健康文本的长上下文特征，具体而言，我们基于XLNet和Longformer架构训练并发布了MentalXLNet与MentalLongformer两个模型。实验评估了这两个领域专用预训练模型在心理健康分类任务中的表现及其长文本处理能力。相关模型已在HuggingFace平台开源发布。

（翻译说明：
1. 专业术语处理："pretrained language models"译为"预训练语言模型"，"domain-specific"统一译为"领域专用"，"HuggingFace"保留平台原名
2. 长句拆分：将原文复合句拆分为符合中文表达习惯的短句，如第一句拆分为两个逻辑单元
3. 被动语态转换："are pretrained and released"转为主动态"研究者预训练并发布了"
4. 概念显化处理："social posts"补充说明为"社交媒体帖子"并标注典型平台
5. 技术表述规范："continued pretraining"译为专业术语"持续预训练"，"long-range ability"意译为"长文本处理能力"
6. 机构名称保留：HuggingFace作为知名平台保留原名不译）
