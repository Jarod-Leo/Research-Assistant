# Massively Multilingual Shallow Fusion with Large Language Models

链接: http://arxiv.org/abs/2302.08917v1

原文摘要:
While large language models (LLM) have made impressive progress in natural
language processing, it remains unclear how to utilize them in improving
automatic speech recognition (ASR). In this work, we propose to train a single
multilingual language model (LM) for shallow fusion in multiple languages. We
push the limits of the multilingual LM to cover up to 84 languages by scaling
up using a mixture-of-experts LLM, i.e., generalist language model (GLaM). When
the number of experts increases, GLaM dynamically selects only two at each
decoding step to keep the inference computation roughly constant. We then apply
GLaM to a multilingual shallow fusion task based on a state-of-the-art
end-to-end model. Compared to a dense LM of similar computation during
inference, GLaM reduces the WER of an English long-tail test set by 4.4%
relative. In a multilingual shallow fusion task, GLaM improves 41 out of 50
languages with an average relative WER reduction of 3.85%, and a maximum
reduction of 10%. Compared to the baseline model, GLaM achieves an average WER
reduction of 5.53% over 43 languages.

中文翻译:
尽管大语言模型（LLM）在自然语言处理领域取得了显著进展，但如何利用其提升自动语音识别（ASR）性能仍不明确。本研究提出训练单一多语言语言模型（LM）以实现跨语言的浅层融合。通过采用专家混合架构的大语言模型——通用语言模型（GLaM），我们将多语言LM的覆盖能力突破至84种语言。当专家数量增加时，GLaM在每一步解码过程中动态选择仅两个专家参与计算，从而保持推理计算量基本恒定。基于当前最先进的端到端模型，我们将GLaM应用于多语言浅层融合任务。与计算量相近的稠密语言模型相比，GLaM在英语长尾测试集上相对降低了4.4%的词错误率（WER）。在多语言浅层融合任务中，GLaM在50种语言中的41种上实现性能提升，平均相对WER降低3.85%，最大降幅达10%。相较于基线模型，GLaM在43种语言上平均WER降低5.53%。


