# Explainability of Vision Transformers: A Comprehensive Review and New Perspectives

链接: http://arxiv.org/abs/2311.06786v1

原文摘要:
Transformers have had a significant impact on natural language processing and
have recently demonstrated their potential in computer vision. They have shown
promising results over convolution neural networks in fundamental computer
vision tasks. However, the scientific community has not fully grasped the inner
workings of vision transformers, nor the basis for their decision-making, which
underscores the importance of explainability methods. Understanding how these
models arrive at their decisions not only improves their performance but also
builds trust in AI systems. This study explores different explainability
methods proposed for visual transformers and presents a taxonomy for organizing
them according to their motivations, structures, and application scenarios. In
addition, it provides a comprehensive review of evaluation criteria that can be
used for comparing explanation results, as well as explainability tools and
frameworks. Finally, the paper highlights essential but unexplored aspects that
can enhance the explainability of visual transformers, and promising research
directions are suggested for future investment.

中文翻译:
Transformer模型对自然语言处理领域产生了深远影响，近期在计算机视觉领域也展现出巨大潜力。在基础计算机视觉任务中，其表现已超越卷积神经网络。然而科学界尚未完全理解视觉Transformer的内部工作机制及其决策依据，这凸显了可解释性研究方法的重要性。理解这些模型的决策过程不仅能提升其性能，更能增强对人工智能系统的信任。本研究系统探讨了针对视觉Transformer提出的各类可解释性方法，并根据方法动机、结构特征和应用场景提出了分类体系。同时全面综述了用于比较解释结果的评估标准，以及现有可解释性工具与框架。最后，本文指出了可提升视觉Transformer可解释性但尚未探索的关键维度，并为未来研究指明了值得投入的方向。  


