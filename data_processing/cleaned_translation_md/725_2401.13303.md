# MaLA-500: Massive Language Adaptation of Large Language Models

链接: http://arxiv.org/abs/2401.13303v1

原文摘要:
Large language models (LLMs) have advanced the state of the art in natural
language processing. However, their predominant design for English or a limited
set of languages creates a substantial gap in their effectiveness for
low-resource languages. To bridge this gap, we introduce MaLA-500, a novel
large language model designed to cover an extensive range of 534 languages. To
train MaLA-500, we employ vocabulary extension and continued pretraining on
LLaMA 2 with Glot500-c. Our intrinsic evaluation demonstrates that MaLA-500 is
better at predicting the given texts of low-resource languages than existing
multilingual LLMs. Moreover, the extrinsic evaluation of in-context learning
shows that MaLA-500 outperforms previous LLMs on SIB200 and Taxi1500 by a
significant margin, i.e., 11.68% and 4.82% marco-average accuracy across
languages. We release MaLA-500 at https://huggingface.co/MaLA-LM

中文翻译:
以下是符合学术规范的中文翻译：

大规模语言模型（LLMs）推动了自然语言处理领域的技术进步。然而，现有模型主要针对英语或少数语种的设计，导致其在低资源语言处理效能上存在显著差距。为弥补这一不足，本研究提出MaLA-500——一个创新性的大规模语言模型，其设计覆盖534种语言。我们采用词汇扩展技术并在LLaMA 2模型基础上结合Glot500-c数据集进行持续预训练。内在评估表明，相较于现有多语言LLMs，MaLA-500在低资源语言文本预测任务中表现更优。此外，上下文学习的外在评估显示，在SIB200和Taxi1500基准测试中，MaLA-500以显著优势超越先前LLMs，其跨语言宏平均准确率分别提升11.68%和4.82%。本模型已发布于https://huggingface.co/MaLA-LM。

注：
1. 专业术语处理：
- "LLMs"保留英文缩写并添加中文全称
- "macro-average"译为"宏平均"（统计学标准译法）
- "in-context learning"译为"上下文学习"（NLP领域通用译法）

2. 句式重构：
- 将英文被动语态转换为中文主动表述（如"are designed"译为"针对...的设计"）
- 长难句拆分（如第二句拆分为因果关系的复句）

3. 数据呈现：
- 精确保留数值（534种语言/11.68%/4.82%）
- 技术指标名称（SIB200/Taxi1500）保持原貌

4. 学术规范：
- 使用书面化表达（"相较于"替代"比"，"显著优势"替代"大比分"）
- 保持客观陈述语气，避免主观评价词汇
