# Skip-Layer Attention: Bridging Abstract and Detailed Dependencies in Transformers

链接: http://arxiv.org/abs/2406.11274v1

原文摘要:
The Transformer architecture has significantly advanced deep learning,
particularly in natural language processing, by effectively managing long-range
dependencies. However, as the demand for understanding complex relationships
grows, refining the Transformer's architecture becomes critical. This paper
introduces Skip-Layer Attention (SLA) to enhance Transformer models by enabling
direct attention between non-adjacent layers. This method improves the model's
ability to capture dependencies between high-level abstract features and
low-level details. By facilitating direct attention between these diverse
feature levels, our approach overcomes the limitations of current Transformers,
which often rely on suboptimal intra-layer attention. Our implementation
extends the Transformer's functionality by enabling queries in a given layer to
interact with keys and values from both the current layer and one preceding
layer, thus enhancing the diversity of multi-head attention without additional
computational burden. Extensive experiments demonstrate that our enhanced
Transformer model achieves superior performance in language modeling tasks,
highlighting the effectiveness of our skip-layer attention mechanism.

中文翻译:
Transformer架构通过有效处理长程依赖关系，显著推动了深度学习（尤其是自然语言处理领域）的发展。然而随着对复杂关系理解需求的增长，优化Transformer架构变得至关重要。本文提出跳跃层注意力机制（Skip-Layer Attention, SLA），通过允许非相邻层之间直接建立注意力连接来增强Transformer模型。该方法提升了模型捕获高层抽象特征与底层细节间依赖关系的能力，通过促进不同特征层级间的直接注意力交互，克服了现有Transformer仅依赖次优层内注意力的局限性。我们的实现方案使给定层的查询向量能够同时与当前层及前一层的键值对进行交互，从而在不增加计算负担的前提下增强多头注意力的多样性。大量实验表明，改进后的Transformer模型在语言建模任务中表现出更优性能，验证了跳跃层注意力机制的有效性。

（翻译说明：采用技术文献的严谨表述风格，保留"SLA"首字母缩写并在括号内标注全称；将"non-adjacent layers"译为"非相邻层"保持专业术语一致性；"high-level abstract features and low-level details"处理为"高层抽象特征与底层细节"形成中文常见的四字格对仗；通过"查询向量""键值对"等专业术语确保技术准确性；最后调整英文长句为符合中文表达习惯的短句结构。）
