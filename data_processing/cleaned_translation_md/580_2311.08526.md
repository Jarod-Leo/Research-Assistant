# GLiNER: Generalist Model for Named Entity Recognition using Bidirectional Transformer

链接: http://arxiv.org/abs/2311.08526v1

原文摘要:
Named Entity Recognition (NER) is essential in various Natural Language
Processing (NLP) applications. Traditional NER models are effective but limited
to a set of predefined entity types. In contrast, Large Language Models (LLMs)
can extract arbitrary entities through natural language instructions, offering
greater flexibility. However, their size and cost, particularly for those
accessed via APIs like ChatGPT, make them impractical in resource-limited
scenarios. In this paper, we introduce a compact NER model trained to identify
any type of entity. Leveraging a bidirectional transformer encoder, our model,
GLiNER, facilitates parallel entity extraction, an advantage over the slow
sequential token generation of LLMs. Through comprehensive testing, GLiNER
demonstrate strong performance, outperforming both ChatGPT and fine-tuned LLMs
in zero-shot evaluations on various NER benchmarks.

中文翻译:
命名实体识别（NER）是自然语言处理（NLP）众多应用中的关键技术。传统NER模型虽效果显著，但仅能识别预定义的有限实体类型。相比之下，大语言模型（LLMs）通过自然语言指令即可抽取任意类型的实体，具有更高的灵活性。然而，这类模型（尤其是ChatGPT等通过API调用的模型）的庞大体量和高昂成本，使其在资源受限场景中难以落地。本文提出一种经过训练可识别任意实体类型的轻量化NER模型。基于双向Transformer编码器架构的GLiNER模型支持并行实体抽取，相较LLMs缓慢的序列化token生成具有显著速度优势。大量实验表明，GLiNER在多个NER基准测试的零样本评估中表现优异，其性能不仅超越ChatGPT，也优于经过微调的LLMs。

（翻译说明：
1. 专业术语统一："Named Entity Recognition"规范译为"命名实体识别"，"zero-shot"译为"零样本"
2. 句式重构：将英文长句拆解为符合中文表达习惯的短句，如第一段通过"虽...但..."转折结构实现流畅转换
3. 被动语态转化："are limited to"译为主动态"仅能识别"
4. 技术概念显化：将"parallel entity extraction"具体化为"并行实体抽取"，与后文"序列化token生成"形成对比
5. 学术用语规范："comprehensive testing"译为"大量实验"符合中文论文表述习惯
6. 逻辑衔接强化：通过"相比之下""然而""本文"等连接词保持论证连贯性）
