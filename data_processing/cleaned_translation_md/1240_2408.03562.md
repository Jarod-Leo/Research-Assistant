# A Comparison of LLM Finetuning Methods & Evaluation Metrics with Travel Chatbot Use Case

链接: http://arxiv.org/abs/2408.03562v1

原文摘要:
This research compares large language model (LLM) fine-tuning methods,
including Quantized Low Rank Adapter (QLoRA), Retrieval Augmented fine-tuning
(RAFT), and Reinforcement Learning from Human Feedback (RLHF), and additionally
compared LLM evaluation methods including End to End (E2E) benchmark method of
"Golden Answers", traditional natural language processing (NLP) metrics, RAG
Assessment (Ragas), OpenAI GPT-4 evaluation metrics, and human evaluation,
using the travel chatbot use case. The travel dataset was sourced from the the
Reddit API by requesting posts from travel-related subreddits to get
travel-related conversation prompts and personalized travel experiences, and
augmented for each fine-tuning method. We used two pretrained LLMs utilized for
fine-tuning research: LLaMa 2 7B, and Mistral 7B. QLoRA and RAFT are applied to
the two pretrained models. The inferences from these models are extensively
evaluated against the aforementioned metrics. The best model according to human
evaluation and some GPT-4 metrics was Mistral RAFT, so this underwent a
Reinforcement Learning from Human Feedback (RLHF) training pipeline, and
ultimately was evaluated as the best model. Our main findings are that: 1)
quantitative and Ragas metrics do not align with human evaluation, 2) Open AI
GPT-4 evaluation most aligns with human evaluation, 3) it is essential to keep
humans in the loop for evaluation because, 4) traditional NLP metrics
insufficient, 5) Mistral generally outperformed LLaMa, 6) RAFT outperforms
QLoRA, but still needs postprocessing, 7) RLHF improves model performance
significantly. Next steps include improving data quality, increasing data
quantity, exploring RAG methods, and focusing data collection on a specific
city, which would improve data quality by narrowing the focus, while creating a
useful product.

中文翻译:
本研究对比了大型语言模型（LLM）的微调方法（包括量化低秩适配器QLoRA、检索增强微调RAFT和基于人类反馈的强化学习RLHF），并采用旅行聊天机器人用例评估了多种LLM评估方法（含端到端基准测试"黄金答案"、传统自然语言处理指标、RAG评估框架Ragas、OpenAI GPT-4评估指标及人工评估）。旅行数据集通过Reddit API从旅行相关板块获取，包含旅行对话提示和个性化旅行经历，并针对每种微调方法进行数据增强。研究选用LLaMa 2 7B和Mistral 7B两个预训练模型进行微调实验，应用QLoRA和RAFT方法后，使用前述指标对模型推理结果进行全面评估。根据人工评估和部分GPT-4指标，Mistral RAFT表现最佳，经RLHF训练流程后最终被评定为最优模型。主要发现包括：1）量化指标与Ragas评估与人工评价不一致；2）OpenAI GPT-4评估与人工评价契合度最高；3）必须保持人工参与评估环节；4）传统NLP指标不足；5）Mistral总体优于LLaMa；6）RAFT优于QLoRA但仍需后处理；7）RLHF显著提升模型性能。后续计划包括提升数据质量、增加数据量、探索RAG方法，以及聚焦特定城市的数据收集——通过缩小范围既可提升数据质量，又能打造实用产品。
