# DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging

链接: http://arxiv.org/abs/2402.02622v1

原文摘要:
The transformer architecture by Vaswani et al. (2017) is now ubiquitous
across application domains, from natural language processing to speech
processing and image understanding. We propose DenseFormer, a simple
modification to the standard architecture that improves the perplexity of the
model without increasing its size -- adding a few thousand parameters for
large-scale models in the 100B parameters range. Our approach relies on an
additional averaging step after each transformer block, which computes a
weighted average of current and past representations -- we refer to this
operation as Depth-Weighted-Average (DWA). The learned DWA weights exhibit
coherent patterns of information flow, revealing the strong and structured
reuse of activations from distant layers. Experiments demonstrate that
DenseFormer is more data efficient, reaching the same perplexity of much deeper
transformer models, and that for the same perplexity, these new models
outperform transformer baselines in terms of memory efficiency and inference
time.

中文翻译:
以下是符合学术规范的译文：

Vaswani等人（2017）提出的Transformer架构已在自然语言处理、语音处理和图像理解等多个应用领域得到广泛应用。我们提出DenseFormer——通过对标准架构的简单改进，在不增加模型规模的前提下提升模型困惑度（对于参数规模达1000亿级别的大型模型，仅需增加数千个参数）。该方法通过在每个Transformer模块后增加加权平均步骤实现，该操作对当前及历史表征进行深度加权平均（Depth-Weighted-Average, DWA）。学习得到的DWA权重呈现出清晰的信息流动模式，揭示了模型对远端层激活值的强结构化复用现象。实验表明：DenseFormer具有更高的数据效率，能以更浅的架构达到深层Transformer模型的困惑度水平；在相同困惑度条件下，新模型在内存效率和推理速度方面均优于基线Transformer模型。

注：
1. 专业术语处理：
- "perplexity"译为"困惑度"（NLP领域标准译法）
- "transformer block"译为"Transformer模块"（避免与"区块"等歧义译法混淆）
- "activations"译为"激活值"（机器学习标准术语）

2. 句式重构：
- 将原文复合长句拆分为符合中文表达习惯的短句
- 被动语态转为主动表述（如"is now ubiquitous"处理为"得到广泛应用"）
- 技术描述部分保持准确性的同时增强可读性

3. 学术规范：
- 保留原始文献引用格式（Vaswani等人（2017））
- 首字母缩略语DWA在首次出现时标注全称
- 数量级表述转换为中文习惯（"100B parameters"译为"1000亿级别"）
