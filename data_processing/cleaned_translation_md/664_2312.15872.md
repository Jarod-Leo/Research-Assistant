# Heterogeneous Encoders Scaling In The Transformer For Neural Machine Translation

链接: http://arxiv.org/abs/2312.15872v1

原文摘要:
Although the Transformer is currently the best-performing architecture in the
homogeneous configuration (self-attention only) in Neural Machine Translation,
many State-of-the-Art models in Natural Language Processing are made of a
combination of different Deep Learning approaches. However, these models often
focus on combining a couple of techniques only and it is unclear why some
methods are chosen over others. In this work, we investigate the effectiveness
of integrating an increasing number of heterogeneous methods. Based on a simple
combination strategy and performance-driven synergy criteria, we designed the
Multi-Encoder Transformer, which consists of up to five diverse encoders.
Results showcased that our approach can improve the quality of the translation
across a variety of languages and dataset sizes and it is particularly
effective in low-resource languages where we observed a maximum increase of
7.16 BLEU compared to the single-encoder model.

中文翻译:
尽管Transformer是目前神经机器翻译中同质化架构（仅使用自注意力机制）下性能最优的模型，但自然语言处理领域多数顶尖模型实际上由多种深度学习方法组合而成。然而这些模型通常仅聚焦于少数技术的结合，且缺乏对不同方法取舍依据的明确解释。本研究通过系统性实验探索了异质方法增量式整合的有效性：基于简单的组合策略与性能驱动的协同准则，我们设计了包含多达五种异构编码器的多编码器Transformer架构。实验结果表明，该方案能显著提升不同语种及数据规模下的翻译质量，尤其在低资源语言场景中表现突出——相较于单编码器模型，我们观测到最高达7.16 BLEU值的性能提升。

（翻译说明：
1. 专业术语处理：State-of-the-Art译为"顶尖模型"，BLEU保留专业缩写并补充"值"字符合中文表达
2. 长句拆分：将原文复合长句按中文表达习惯分解为多个短句，如将"however"引导的转折关系拆分为独立句子
3. 被动语态转换："it is unclear"转为主动式"缺乏明确解释"
4. 概念显化："performance-driven synergy criteria"译为"性能驱动的协同准则"以准确传达技术含义
5. 数据强调：使用破折号突出7.16 BLEU值的量化结果
6. 学术风格保持：使用"本研究""系统性实验"等符合论文摘要规范的表述）
