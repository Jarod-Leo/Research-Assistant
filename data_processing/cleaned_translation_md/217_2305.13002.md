# Rethinking Semi-supervised Learning with Language Models

链接: http://arxiv.org/abs/2305.13002v1

原文摘要:
Semi-supervised learning (SSL) is a popular setting aiming to effectively
utilize unlabelled data to improve model performance in downstream natural
language processing (NLP) tasks. Currently, there are two popular approaches to
make use of unlabelled data: Self-training (ST) and Task-adaptive pre-training
(TAPT). ST uses a teacher model to assign pseudo-labels to the unlabelled data,
while TAPT continues pre-training on the unlabelled data before fine-tuning. To
the best of our knowledge, the effectiveness of TAPT in SSL tasks has not been
systematically studied, and no previous work has directly compared TAPT and ST
in terms of their ability to utilize the pool of unlabelled data. In this
paper, we provide an extensive empirical study comparing five state-of-the-art
ST approaches and TAPT across various NLP tasks and data sizes, including in-
and out-of-domain settings. Surprisingly, we find that TAPT is a strong and
more robust SSL learner, even when using just a few hundred unlabelled samples
or in the presence of domain shifts, compared to more sophisticated ST
approaches, and tends to bring greater improvements in SSL than in
fully-supervised settings. Our further analysis demonstrates the risks of using
ST approaches when the size of labelled or unlabelled data is small or when
domain shifts exist. We offer a fresh perspective for future SSL research,
suggesting the use of unsupervised pre-training objectives over dependency on
pseudo labels.

中文翻译:
半监督学习（SSL）是一种旨在有效利用未标注数据以提升下游自然语言处理（NLP）任务模型性能的流行范式。当前主流方法主要包含两类：自训练（Self-training，ST）和任务自适应预训练（Task-adaptive pre-training，TAPT）。ST通过教师模型为未标注数据分配伪标签，而TAPT则先在未标注数据上继续预训练再进行微调。据我们所知，TAPT在SSL任务中的有效性尚未得到系统研究，此前也未有工作直接比较TAPT与ST在未标注数据利用能力上的差异。本文通过大量实验对比了五种前沿ST方法与TAPT在不同NLP任务、数据规模（含领域内与跨领域场景）下的表现。令人惊讶的是，我们发现相较于复杂的ST方法，TAPT展现出更强健的SSL学习能力——即使仅使用数百个未标注样本或存在领域偏移时依然稳定有效，且其在SSL场景带来的性能提升往往优于全监督场景。进一步分析揭示了ST方法在标注/未标注数据量不足或存在领域偏移时的潜在风险。本研究为未来SSL研究提供了新视角，建议更多关注无监督预训练目标而非依赖伪标签策略。

（翻译说明：采用学术论文摘要的规范表述，通过以下处理确保专业性：
1. 术语统一："pseudo-labels"译为"伪标签"，"domain shifts"译为"领域偏移"
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句（如首句的定语从句处理）
3. 逻辑显化：通过"——"引导补充说明，保持原文的递进关系
4. 被动语态转化："has not been systematically studied"译为主动式"尚未得到系统研究"
5. 概念准确："state-of-the-art"译为"前沿"而非字面直译，更符合学术语境）
