# A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks

链接: http://arxiv.org/abs/2306.07303v1

原文摘要:
Transformer is a deep neural network that employs a self-attention mechanism
to comprehend the contextual relationships within sequential data. Unlike
conventional neural networks or updated versions of Recurrent Neural Networks
(RNNs) such as Long Short-Term Memory (LSTM), transformer models excel in
handling long dependencies between input sequence elements and enable parallel
processing. As a result, transformer-based models have attracted substantial
interest among researchers in the field of artificial intelligence. This can be
attributed to their immense potential and remarkable achievements, not only in
Natural Language Processing (NLP) tasks but also in a wide range of domains,
including computer vision, audio and speech processing, healthcare, and the
Internet of Things (IoT). Although several survey papers have been published
highlighting the transformer's contributions in specific fields, architectural
differences, or performance evaluations, there is still a significant absence
of a comprehensive survey paper encompassing its major applications across
various domains. Therefore, we undertook the task of filling this gap by
conducting an extensive survey of proposed transformer models from 2017 to
2022. Our survey encompasses the identification of the top five application
domains for transformer-based models, namely: NLP, Computer Vision,
Multi-Modality, Audio and Speech Processing, and Signal Processing. We analyze
the impact of highly influential transformer-based models in these domains and
subsequently classify them based on their respective tasks using a proposed
taxonomy. Our aim is to shed light on the existing potential and future
possibilities of transformers for enthusiastic researchers, thus contributing
to the broader understanding of this groundbreaking technology.

中文翻译:
Transformer是一种采用自注意力机制的深度神经网络，能够理解序列数据中的上下文关联。与传统神经网络或循环神经网络（RNN）的改进版本（如长短期记忆网络LSTM）不同，Transformer模型擅长处理输入序列元素间的长程依赖关系，并支持并行计算。基于这些优势，Transformer模型在人工智能领域引发了研究者的广泛关注——这不仅源于其在自然语言处理（NLP）任务中的卓越表现，更因其在计算机视觉、音频语音处理、医疗健康和物联网（IoT）等多元领域展现的巨大潜力。尽管已有若干综述论文探讨了Transformer在特定领域的贡献、架构差异或性能评估，但目前仍缺乏全面涵盖该技术跨领域主要应用的系统性综述。为此，我们对2017至2022年间提出的Transformer模型进行了广泛调研，筛选出五大核心应用领域：自然语言处理、计算机视觉、多模态学习、音频语音处理以及信号处理。通过分析这些领域中具有重大影响力的Transformer模型，我们依据提出的分类框架对其进行了任务导向的系统归类。本研究旨在为热衷该领域的研究者揭示Transformer技术的现有潜力与未来前景，从而推动对这一突破性技术的更深入理解。
