# BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing

链接: http://arxiv.org/abs/2310.19975v1

原文摘要:
To enhance the performance of large language models (LLMs) in biomedical
natural language processing (BioNLP) by introducing a domain-specific
instruction dataset and examining its impact when combined with multi-task
learning principles. We created the BioInstruct, comprising 25,005 instructions
to instruction-tune LLMs(LLaMA 1 & 2, 7B & 13B version). The instructions were
created by prompting the GPT-4 language model with three-seed samples randomly
drawn from an 80 human curated instructions. We employed Low-Rank
Adaptation(LoRA) for parameter-efficient fine-tuning. We then evaluated these
instruction-tuned LLMs on several BioNLP tasks, which can be grouped into three
major categories: question answering(QA), information extraction(IE), and text
generation(GEN). We also examined whether categories(e.g., QA, IE, and
generation) of instructions impact model performance. Comparing with LLMs
without instruction-tuned, our instruction-tuned LLMs demonstrated marked
performance gains: 17.3% in QA, 5.7% in IE, and 96% in Generation tasks. Our
7B-parameter instruction-tuned LLaMA 1 model was competitive or even surpassed
other LLMs in the biomedical domain that were also fine-tuned from LLaMA 1 with
vast domain-specific data or a variety of tasks. Our results also show that the
performance gain is significantly higher when instruction fine-tuning is
conducted with closely related tasks. Our findings align with the observations
of multi-task learning, suggesting the synergies between two tasks. The
BioInstruct dataset serves as a valuable resource and instruction tuned LLMs
lead to the best performing BioNLP applications.

中文翻译:
为提高大语言模型（LLM）在生物医学自然语言处理（BioNLP）中的性能，本研究通过引入领域专用指令数据集，并结合多任务学习原理探究其影响。我们构建了包含25,005条指令的BioInstruct数据集，用于对LLaMA 1 & 2（7B和13B版本）进行指令微调。这些指令通过向GPT-4语言模型输入随机选取的三组种子样本生成（种子样本源自80条人工精选指令），并采用低秩自适应（LoRA）技术实现参数高效微调。经指令微调的模型在三大类BioNLP任务上接受评估：问答系统（QA）、信息抽取（IE）和文本生成（GEN），同时探究不同指令类别对模型性能的影响。与未微调模型相比，我们的模型表现出显著性能提升：QA任务提升17.3%，IE任务提升5.7%，生成任务提升96%。其中7B参数的指令微调版LLaMA 1模型表现优异，在生物医学领域甚至超越了其他基于LLaMA 1架构、使用海量领域数据或多任务微调的LLM。研究还发现，当指令微调涉及高度相关任务时，性能提升更为显著，这一发现与多任务学习的协同效应观察结果一致。BioInstruct数据集是宝贵的资源，基于其微调的LLM可打造最优性能的BioNLP应用。
