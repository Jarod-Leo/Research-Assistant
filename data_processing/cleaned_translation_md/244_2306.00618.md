# Effective Structured Prompting by Meta-Learning and Representative Verbalizer

链接: http://arxiv.org/abs/2306.00618v1

原文摘要:
Prompt tuning for pre-trained masked language models (MLM) has shown
promising performance in natural language processing tasks with few labeled
examples. It tunes a prompt for the downstream task, and a verbalizer is used
to bridge the predicted token and label prediction. Due to the limited training
data, prompt initialization is crucial for prompt tuning. Recently,
MetaPrompting (Hou et al., 2022) uses meta-learning to learn a shared
initialization for all task-specific prompts. However, a single initialization
is insufficient to obtain good prompts for all tasks and samples when the tasks
are complex. Moreover, MetaPrompting requires tuning the whole MLM, causing a
heavy burden on computation and memory as the MLM is usually large. To address
these issues, we use a prompt pool to extract more task knowledge and construct
instance-dependent prompts via attention. We further propose a novel soft
verbalizer (RepVerb) which constructs label embedding from feature embeddings
directly. Combining meta-learning the prompt pool and RepVerb, we propose
MetaPrompter for effective structured prompting. MetaPrompter is
parameter-efficient as only the pool is required to be tuned. Experimental
results demonstrate that MetaPrompter performs better than the recent
state-of-the-arts and RepVerb outperforms existing soft verbalizers.

中文翻译:
以下是符合学术规范的中文翻译：

针对预训练掩码语言模型（MLM）的提示调优技术，在少量标注样本的自然语言处理任务中展现出优异性能。该方法通过为下游任务优化提示模板，并借助标签映射器（verbalizer）将预测标记与标签预测相关联。由于训练数据有限，提示初始化对提示调优至关重要。近期提出的元提示学习（MetaPrompting，Hou等人2022）采用元学习技术为所有任务特定提示获取共享初始化参数。然而当任务复杂度较高时，单一初始化参数难以保证所有任务及样本都能获得优质提示。此外，该方法需对整个MLM进行调优，而MLM通常参数量庞大，会导致计算与内存的沉重负担。为解决这些问题，我们采用提示池（prompt pool）来提取更丰富的任务知识，并通过注意力机制构建样本自适应的提示。我们进一步提出新型软标签映射器RepVerb，可直接从特征嵌入构建标签嵌入。通过结合提示池的元学习与RepVerb，我们提出结构化提示框架MetaPrompter。该框架仅需调优提示池参数，具有显著参数效率优势。实验结果表明，MetaPrompter性能优于当前最先进方法，且RepVerb超越现有软标签映射器。

（翻译严格遵循以下原则：
1. 专业术语统一（如verbalizer固定译为"标签映射器"）
2. 被动语态转换为中文主动句式
3. 长难句合理切分，符合中文表达习惯
4. 保留技术细节的精确性
5. 学术用语规范（如"元学习"而非"meta学习"）
6. 重要方法名称首次出现标注原文）
