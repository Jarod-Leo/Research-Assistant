# Instruction Tuning for Story Understanding and Generation with Weak Supervision

链接: http://arxiv.org/abs/2501.15574v1

原文摘要:
Story understanding and generation have long been a challenging task in
natural language processing (NLP), especially when dealing with various levels
of instruction specificity. In this paper, we propose a novel approach called
"Weak to Strong Instruction Tuning" for improving story generation by tuning
models with instructions of varying clarity. We explore the potential of large
language models (LLMs) to adapt to different types of instructions, weak and
strong, and show that our method significantly enhances performance in story
comprehension and generation. By leveraging the strength of instruction tuning,
we train models to understand the nuances of story plots, characters, and
themes while generating coherent and engaging narratives. Through extensive
experiments on several benchmark datasets and comparison with state-of-the-art
baselines, we demonstrate that our method outperforms existing techniques,
yielding substantial improvements in both automatic evaluation metrics and
human evaluations. Our work shows that adaptive instruction tuning can be a
powerful tool in refining generative models for complex narrative tasks.

中文翻译:
故事理解与生成长期以来都是自然语言处理（NLP）领域的一项挑战性任务，尤其是在处理不同明确程度的指令时。本文提出了一种名为"从弱到强指令微调"的创新方法，通过使用清晰度各异的指令对模型进行微调，从而提升故事生成能力。我们探索了大语言模型（LLMs）适应强弱不同类型指令的潜力，并证明该方法能显著增强故事理解与生成的表现。借助指令微调的优势，我们训练模型在生成连贯且引人入胜的叙事时，能够理解故事情节、人物和主题的细微差别。通过在多个基准数据集上的大量实验以及与最先进基线的比较，我们证实该方法优于现有技术，在自动评估指标和人工评估中均实现了显著提升。本研究表明，自适应指令微调可以成为优化生成模型处理复杂叙事任务的有效工具。

（翻译说明：采用学术论文摘要的标准表述方式，通过以下处理确保专业性与可读性：
1. 专业术语准确对应："instruction specificity"译为"指令明确程度"，"nuances"译为"细微差别"
2. 长句拆分重组：将原文60词长句拆分为符合中文表达习惯的短句群
3. 被动语态转化："it is shown that"转为主动句式"我们证明"
4. 概念显化处理："weak/strong instruction"补充译为"强弱不同类型指令"以明确对比关系
5. 保持学术严谨性："state-of-the-art"规范译为"最先进的"，"benchmark datasets"译为"基准数据集"）
