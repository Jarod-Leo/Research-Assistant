# SPADE: Self-supervised Pretraining for Acoustic DisEntanglement

链接: http://arxiv.org/abs/2302.01483v1

原文摘要:
Self-supervised representation learning approaches have grown in popularity
due to the ability to train models on large amounts of unlabeled data and have
demonstrated success in diverse fields such as natural language processing,
computer vision, and speech. Previous self-supervised work in the speech domain
has disentangled multiple attributes of speech such as linguistic content,
speaker identity, and rhythm. In this work, we introduce a self-supervised
approach to disentangle room acoustics from speech and use the acoustic
representation on the downstream task of device arbitration. Our results
demonstrate that our proposed approach significantly improves performance over
a baseline when labeled training data is scarce, indicating that our
pretraining scheme learns to encode room acoustic information while remaining
invariant to other attributes of the speech signal.

中文翻译:
以下是符合要求的学术论文摘要中文翻译：

自监督表征学习方法因能利用大量无标注数据训练模型而日益流行，并在自然语言处理、计算机视觉和语音等多个领域展现出卓越成效。既往语音领域的自监督研究已成功解耦语音的多种属性，如语言内容、说话人身份和节奏韵律。本研究提出一种自监督方法，旨在从语音信号中分离房间混响特征，并将所得声学表征应用于设备仲裁的下游任务。实验结果表明，在标注训练数据稀缺的情况下，本方法相较基线模型具有显著性能提升，这证明我们的预训练方案能有效编码房间声学信息，同时保持对语音信号其他属性的不变性。

翻译说明：
1. 专业术语处理：
- "self-supervised representation learning"译为"自监督表征学习"，符合《人工智能术语》国标
- "device arbitration"译为"设备仲裁"，采用计算机领域通用译法
- "downstream task"译为"下游任务"，保留机器学习领域术语一致性

2. 句式结构调整：
- 将英文长句拆分为符合中文表达习惯的短句，如将"demonstrated success in..."独立成句
- 被动语态转换："has disentangled"译为主动式"已成功解耦"
- 逻辑关系显化：添加"这证明"来明确实验结果与结论的因果关系

3. 学术表达规范：
- 保持"预训练方案"、"基线模型"等标准学术用语
- 使用"显著性能提升"替代口语化的"大大提高"
- "invariant to"译为专业术语"不变性"而非字面翻译

4. 技术准确性：
- "room acoustics"准确译为"房间混响特征"而非字面"房间声学"
- "rhythm"根据语境译为"节奏韵律"以涵盖语音学含义
- "scarce"译为"稀缺"以准确表达数据量状态
