# Recent Advances of Foundation Language Models-based Continual Learning: A Survey

链接: http://arxiv.org/abs/2405.18653v1

原文摘要:
Recently, foundation language models (LMs) have marked significant
achievements in the domains of natural language processing (NLP) and computer
vision (CV). Unlike traditional neural network models, foundation LMs obtain a
great ability for transfer learning by acquiring rich commonsense knowledge
through pre-training on extensive unsupervised datasets with a vast number of
parameters. However, they still can not emulate human-like continuous learning
due to catastrophic forgetting. Consequently, various continual learning
(CL)-based methodologies have been developed to refine LMs, enabling them to
adapt to new tasks without forgetting previous knowledge. However, a systematic
taxonomy of existing approaches and a comparison of their performance are still
lacking, which is the gap that our survey aims to fill. We delve into a
comprehensive review, summarization, and classification of the existing
literature on CL-based approaches applied to foundation language models, such
as pre-trained language models (PLMs), large language models (LLMs) and
vision-language models (VLMs). We divide these studies into offline CL and
online CL, which consist of traditional methods, parameter-efficient-based
methods, instruction tuning-based methods and continual pre-training methods.
Offline CL encompasses domain-incremental learning, task-incremental learning,
and class-incremental learning, while online CL is subdivided into hard task
boundary and blurry task boundary settings. Additionally, we outline the
typical datasets and metrics employed in CL research and provide a detailed
analysis of the challenges and future work for LMs-based continual learning.

中文翻译:
近年来，基础语言模型（LMs）在自然语言处理（NLP）和计算机视觉（CV）领域取得了重大突破。与传统神经网络模型不同，基础语言模型通过在海量无监督数据集上进行参数规模庞大的预训练，获得了丰富的常识知识，从而展现出卓越的迁移学习能力。然而，由于存在灾难性遗忘问题，这类模型仍无法实现类人的持续学习。为此，研究者们开发了多种基于持续学习（CL）的方法来优化语言模型，使其在适应新任务时不遗忘已有知识。但目前仍缺乏对现有方法的系统性分类与性能比较，这正是本综述旨在填补的研究空白。

本文对基于基础语言模型（如预训练语言模型PLMs、大语言模型LLMs和视觉语言模型VLMs）的持续学习研究进行了全面梳理、总结与分类。我们将现有文献划分为离线持续学习与在线持续学习两大范式，其中包含传统方法、参数高效方法、指令微调方法和持续预训练方法。离线持续学习细分为领域增量学习、任务增量学习和类别增量学习；在线持续学习则划分为硬任务边界与模糊任务边界两种场景。此外，我们系统阐述了持续学习研究中常用的数据集与评估指标，并对语言模型持续学习面临的挑战与未来研究方向进行了深度剖析。

"）
5. 被动语态转换：将英文被动式转换为中文主动表述
6. 学术表达："We delve into..."译为"本文进行了全面梳理"，符合中文论文摘要惯例）
