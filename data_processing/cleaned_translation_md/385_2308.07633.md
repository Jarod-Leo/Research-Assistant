# A Survey on Model Compression for Large Language Models

链接: http://arxiv.org/abs/2308.07633v1

原文摘要:
Large Language Models (LLMs) have transformed natural language processing
tasks successfully. Yet, their large size and high computational needs pose
challenges for practical use, especially in resource-limited settings. Model
compression has emerged as a key research area to address these challenges.
This paper presents a survey of model compression techniques for LLMs. We cover
methods like quantization, pruning, and knowledge distillation, highlighting
recent advancements. We also discuss benchmarking strategies and evaluation
metrics crucial for assessing compressed LLMs. This survey offers valuable
insights for researchers and practitioners, aiming to enhance efficiency and
real-world applicability of LLMs while laying a foundation for future
advancements.

中文翻译:
以下是符合学术规范的中文翻译：

大型语言模型（LLMs）已成功革新了自然语言处理领域。然而，其庞大的参数量与高昂的计算需求为实际应用带来了挑战，尤其在资源受限的环境中。模型压缩技术由此成为解决这些问题的关键研究方向。本文系统综述了面向LLMs的模型压缩方法，涵盖量化、剪枝、知识蒸馏等技术，并重点阐释了最新研究进展。同时，我们探讨了评估压缩模型性能的基准测试策略与关键指标。本综述为研究者和实践者提供了有价值的参考，旨在提升LLMs的效能与现实适用性，并为未来技术突破奠定基础。

注：翻译过程中进行了以下专业处理：
1. 术语统一："quantization/pruning/knowledge distillation"分别规范译为"量化/剪枝/知识蒸馏"
2. 句式重构：将英文被动语态转换为中文主动表述（如"are crucial for"译为"探讨了"）
3. 学术表达："survey"译为"综述"而非简单直译"调查"
4. 逻辑显化：通过"由此"等连接词明确技术发展脉络
5. 专业表述："benchmarking strategies"译为"基准测试策略"符合计算机领域术语
