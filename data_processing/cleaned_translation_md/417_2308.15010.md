# TransPrompt v2: A Transferable Prompting Framework for Cross-task Text Classification

链接: http://arxiv.org/abs/2308.15010v1

原文摘要:
Text classification is one of the most imperative tasks in natural language
processing (NLP). Recent advances with pre-trained language models (PLMs) have
shown remarkable success on this task. However, the satisfying results obtained
by PLMs heavily depend on the large amounts of task-specific labeled data,
which may not be feasible in many application scenarios due to data access and
privacy constraints. The recently-proposed prompt-based fine-tuning paradigm
improves the performance of PLMs for few-shot text classification with
task-specific templates. Yet, it is unclear how the prompting knowledge can be
transferred across tasks, for the purpose of mutual reinforcement. We propose
TransPrompt v2, a novel transferable prompting framework for few-shot learning
across similar or distant text classification tasks. For learning across
similar tasks, we employ a multi-task meta-knowledge acquisition (MMA)
procedure to train a meta-learner that captures the cross-task transferable
knowledge. For learning across distant tasks, we further inject the task type
descriptions into the prompt, and capture the intra-type and inter-type prompt
embeddings among multiple distant tasks. Additionally, two de-biasing
techniques are further designed to make the trained meta-learner more
task-agnostic and unbiased towards any tasks. After that, the meta-learner can
be adapted to each specific task with better parameters initialization.
Extensive experiments show that TransPrompt v2 outperforms single-task and
cross-task strong baselines over multiple NLP tasks and datasets. We further
show that the meta-learner can effectively improve the performance of PLMs on
previously unseen tasks. In addition, TransPrompt v2 also outperforms strong
fine-tuning baselines when learning with full training sets.

中文翻译:
文本分类是自然语言处理（NLP）中最关键的任务之一。预训练语言模型（PLMs）的最新进展在该任务上取得了显著成功。然而，PLMs所获得的理想效果高度依赖于大量任务特定的标注数据，由于数据获取和隐私限制，这在许多应用场景中并不可行。近期提出的基于提示的微调范式通过任务特定模板提升了PLMs在小样本文本分类中的性能。但如何实现提示知识在任务间的迁移以实现相互增强，目前尚不明确。我们提出TransPrompt v2——一个面向相似或异构文本分类任务的小样本学习可迁移提示框架。针对相似任务间的学习，我们采用多任务元知识获取（MMA）流程训练能捕捉跨任务可迁移知识的元学习器；对于异构任务学习，我们进一步将任务类型描述注入提示模板，并捕获多异构任务间的类型内与类型间提示嵌入。此外，设计两种去偏技术使训练后的元学习器更具任务无关性，避免对特定任务的偏向性。经此优化后，元学习器可通过更优的参数初始化适配具体任务。大量实验表明，TransPrompt v2在多个NLP任务和数据集上优于单任务及跨任务基线模型。我们进一步证明该元学习器能有效提升PLMs在未见任务上的表现。当使用完整训练集时，TransPrompt v2也显著优于强微调基线方法。
