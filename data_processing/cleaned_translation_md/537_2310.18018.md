# NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark

链接: http://arxiv.org/abs/2310.18018v1

原文摘要:
In this position paper, we argue that the classical evaluation on Natural
Language Processing (NLP) tasks using annotated benchmarks is in trouble. The
worst kind of data contamination happens when a Large Language Model (LLM) is
trained on the test split of a benchmark, and then evaluated in the same
benchmark. The extent of the problem is unknown, as it is not straightforward
to measure. Contamination causes an overestimation of the performance of a
contaminated model in a target benchmark and associated task with respect to
their non-contaminated counterparts. The consequences can be very harmful, with
wrong scientific conclusions being published while other correct ones are
discarded. This position paper defines different levels of data contamination
and argues for a community effort, including the development of automatic and
semi-automatic measures to detect when data from a benchmark was exposed to a
model, and suggestions for flagging papers with conclusions that are
compromised by data contamination.

中文翻译:
在这份立场文件中，我们指出基于标注基准测试的自然语言处理（NLP）任务传统评估方式正面临严峻挑战。当大语言模型（LLM）在基准测试的测试集上进行训练后又于同一基准进行评估时，就会发生最严重的数据污染问题。由于难以直接测量，该问题的实际影响范围尚不明确。数据污染会导致受污染模型在目标基准及相关任务中的表现被高估，从而扭曲其与未受污染模型的真实性能对比。这可能引发严重后果：错误的科学结论被公开发表，而正确的结论反而遭到忽视。本文界定了不同级别的数据污染，并呼吁学界协同应对——包括开发自动/半自动检测工具来识别模型是否接触过基准测试数据，同时建议对因数据污染导致结论失实的研究论文进行标记警示。
