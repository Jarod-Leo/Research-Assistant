# An Exploration of In-Context Learning for Speech Language Model

链接: http://arxiv.org/abs/2310.12477v1

原文摘要:
Ever since the development of GPT-3 in the natural language processing (NLP)
field, in-context learning (ICL) has played an essential role in utilizing
large language models (LLMs). By presenting the LM utterance-label
demonstrations at the input, the LM can accomplish few-shot learning without
relying on gradient descent or requiring explicit modification of its
parameters. This enables the LM to perform various downstream tasks in a
black-box manner. Despite the success of ICL in NLP, little work is exploring
the possibility of ICL in speech processing. This study is the first work
exploring ICL for speech classification tasks with textless speech LM. We first
show that the current speech LM lacks the ICL capability. We then perform
warmup training on the speech LM, equipping the LM with demonstration learning
capability. This paper explores and proposes the first speech LM capable of
performing unseen classification tasks in an ICL manner.

中文翻译:
自自然语言处理（NLP）领域开发出GPT-3以来，情境学习（ICL）在大型语言模型（LLM）的应用中始终发挥着关键作用。通过在输入中提供语言模型的语句-标签示例，该模型无需依赖梯度下降或显式参数调整即可实现小样本学习，从而以黑箱方式执行各类下游任务。尽管ICL在NLP领域成效显著，但关于其在语音处理中应用可能性的探索仍属空白。本研究首次针对无文本语音语言模型（textless speech LM）探索语音分类任务的ICL应用。我们首先证实当前语音语言模型缺乏ICL能力，随后通过预热训练使模型获得示例学习能力。本文开创性地提出首个能以ICL方式执行未见分类任务的语音语言模型。
