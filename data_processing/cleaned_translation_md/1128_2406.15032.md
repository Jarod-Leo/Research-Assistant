# GiusBERTo: A Legal Language Model for Personal Data De-identification in Italian Court of Auditors Decisions

链接: http://arxiv.org/abs/2406.15032v1

原文摘要:
Recent advances in Natural Language Processing have demonstrated the
effectiveness of pretrained language models like BERT for a variety of
downstream tasks. We present GiusBERTo, the first BERT-based model specialized
for anonymizing personal data in Italian legal documents. GiusBERTo is trained
on a large dataset of Court of Auditors decisions to recognize entities to
anonymize, including names, dates, locations, while retaining contextual
relevance. We evaluate GiusBERTo on a held-out test set and achieve 97%
token-level accuracy. GiusBERTo provides the Italian legal community with an
accurate and tailored BERT model for de-identification, balancing privacy and
data protection.

中文翻译:
自然语言处理领域的最新进展已证明，BERT等预训练语言模型在各种下游任务中具有显著成效。本研究推出GiusBERTo——首个专用于意大利法律文书个人数据匿名化的BERT模型。该模型基于意大利审计法院判决书构建的大规模数据集进行训练，能够准确识别需匿名处理的实体（包括人名、日期、地点等），同时保持上下文关联性。经预留测试集验证，GiusBERTo在字符级准确率达到97%。该模型为意大利法律界提供了一个兼顾隐私保护与数据安全的精准定制化去标识解决方案。

（翻译说明：
1. 专业术语处理："Court of Auditors"采用中国法律体系对应译法"审计法院"，"de-identification"译为专业术语"去标识"
2. 句式重构：将原文复合句拆分为符合中文表达习惯的短句，如将"trained on..."长定语转换为独立分句
3. 被动语态转换："is trained"等被动结构转为中文主动式表达
4. 概念显化："retaining contextual relevance"补充译为"保持上下文关联性"以明确技术含义
5. 数据呈现：保留精确数值"97%"的原始表达方式
6. 术语统一：全篇保持"匿名化/去标识"的术语一致性
7. 文化适配："Italian legal community"译为"意大利法律界"符合中文社会语境）
