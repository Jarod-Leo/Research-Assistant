# Development of Pre-Trained Transformer-based Models for the Nepali Language

链接: http://arxiv.org/abs/2411.15734v1

原文摘要:
Transformer-based pre-trained language models have dominated the field of
Natural Language Processing (NLP) for quite some time now. However, the Nepali
language, spoken by approximately 32 million people worldwide, remains
significantly underrepresented in this domain. This underrepresentation is
primarily attributed to the scarcity of monolingual data corpora and limited
available resources for the Nepali language. While existing efforts have
predominantly concentrated on basic encoder-based models, there is a notable
gap in the exploration of decoder-based architectures. To address this gap, we
have collected 27.5 GB of Nepali text data, approximately 2.4x larger than any
previously available Nepali language corpus. Leveraging this data, we
pre-trained three different models i.e., BERT, RoBERTa, and GPT-2, exclusively
for the Nepali Language. Furthermore, we performed instruction tuning and
explored its potential for monolingual Nepali data, providing a foundation for
future research. Our models outperformed the existing best model by 2 points on
Nep-gLUE benchmark, scoring 95.60 and also outperformed existing models on text
generation tasks, demonstrating improvements in both understanding and
generating Nepali text.

中文翻译:
基于Transformer架构的预训练语言模型在自然语言处理（NLP）领域长期占据主导地位。然而，全球约3200万人使用的尼泊尔语在该领域仍存在显著的代表性不足问题，这主要归因于尼泊尔语单语语料库的稀缺性和可用资源的局限性。现有研究主要集中于基于编码器的基础模型，而在基于解码器的架构探索方面存在明显空白。为填补这一空白，我们收集了27.5GB的尼泊尔语文本数据（规模约为现有最大尼泊尔语语料库的2.4倍），并基于该数据预训练了三个专用于尼泊尔语的模型：BERT、RoBERTa和GPT-2。此外，我们还进行了指令微调，探索了其在尼泊尔语单语数据中的应用潜力，为后续研究奠定了基础。我们的模型在Nep-gLUE基准测试中以95.60分的成绩超越现有最佳模型2个百分点，同时在文本生成任务中也优于现有模型，在尼泊尔语文本理解和生成方面均展现出性能提升。
