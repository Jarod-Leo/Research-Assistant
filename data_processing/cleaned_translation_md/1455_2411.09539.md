# A Practical Guide to Fine-tuning Language Models with Limited Data

链接: http://arxiv.org/abs/2411.09539v1

原文摘要:
Employing pre-trained Large Language Models (LLMs) has become the de facto
standard in Natural Language Processing (NLP) despite their extensive data
requirements. Motivated by the recent surge in research focused on training
LLMs with limited data, particularly in low-resource domains and languages,
this paper surveys recent transfer learning approaches to optimize model
performance in downstream tasks where data is scarce. We first address initial
and continued pre-training strategies to better leverage prior knowledge in
unseen domains and languages. We then examine how to maximize the utility of
limited data during fine-tuning and few-shot learning. The final section takes
a task-specific perspective, reviewing models and methods suited for different
levels of data scarcity. Our goal is to provide practitioners with practical
guidelines for overcoming the challenges posed by constrained data while also
highlighting promising directions for future research.

中文翻译:
尽管预训练大语言模型（LLMs）对数据需求量极大，但其已成为自然语言处理（NLP）领域事实上的标准技术。受近期有限数据条件下训练LLMs研究热潮的启发——尤其是在低资源领域和语言场景中——本文系统综述了针对下游任务数据稀缺时的迁移学习优化方法。我们首先探讨如何通过初始预训练与持续预训练策略，更高效地利用未知领域和语言中的先验知识；继而研究在微调与小样本学习阶段最大化有限数据效用的技术；最后从任务特异性视角出发，梳理适用于不同数据稀缺程度的模型与方法。本研究旨在为从业者提供应对数据约束挑战的实用指南，同时指明未来研究的潜在方向。  

（翻译说明：  
1. 专业术语统一处理："pre-trained"译为"预训练"，"few-shot learning"译为"小样本学习"  
2. 长句拆分重构：将原文复合句分解为符合中文表达习惯的短句，如第一句通过破折号处理插入语  
3. 被动语态转化："is surveyed"转为主动式"系统综述"  
4. 学术风格保留：使用"探讨""梳理""旨在"等学术用语  
5. 逻辑显化：通过分号与冒号明确原文隐含的递进关系  
6. 文化适配："de facto standard"采用"事实上的标准技术"既准确又符合中文科技文献表述）
