# When Automated Assessment Meets Automated Content Generation: Examining Text Quality in the Era of GPTs

链接: http://arxiv.org/abs/2309.14488v1

原文摘要:
The use of machine learning (ML) models to assess and score textual data has
become increasingly pervasive in an array of contexts including natural
language processing, information retrieval, search and recommendation, and
credibility assessment of online content. A significant disruption at the
intersection of ML and text are text-generating large-language models such as
generative pre-trained transformers (GPTs). We empirically assess the
differences in how ML-based scoring models trained on human content assess the
quality of content generated by humans versus GPTs. To do so, we propose an
analysis framework that encompasses essay scoring ML-models, human and
ML-generated essays, and a statistical model that parsimoniously considers the
impact of type of respondent, prompt genre, and the ML model used for
assessment model. A rich testbed is utilized that encompasses 18,460
human-generated and GPT-based essays. Results of our benchmark analysis reveal
that transformer pretrained language models (PLMs) more accurately score human
essay quality as compared to CNN/RNN and feature-based ML methods.
Interestingly, we find that the transformer PLMs tend to score GPT-generated
text 10-15\% higher on average, relative to human-authored documents.
Conversely, traditional deep learning and feature-based ML models score human
text considerably higher. Further analysis reveals that although the
transformer PLMs are exclusively fine-tuned on human text, they more
prominently attend to certain tokens appearing only in GPT-generated text,
possibly due to familiarity/overlap in pre-training. Our framework and results
have implications for text classification settings where automated scoring of
text is likely to be disrupted by generative AI.

中文翻译:
以下是符合要求的学术中文翻译：

机器学习（ML）模型在文本数据评估与评分中的应用已日益普及，其应用场景涵盖自然语言处理、信息检索、搜索推荐系统以及网络内容可信度评估等多个领域。生成式预训练变换模型（GPTs）等文本生成大语言模型的出现，对机器学习与文本处理的交叉领域产生了显著影响。本研究通过实证分析，比较了基于人类内容训练的ML评分模型对人类创作内容与GPT生成内容的质量评估差异。为此，我们构建了一个包含作文评分ML模型、人类与机器生成文章的分析框架，并采用统计模型精简考量了作答者类型、提示主题和评估模型类型三大因素的影响。实验基于包含18,460篇人类撰写与GPT生成文章的丰富测试集展开。基准分析结果表明：相较于CNN/RNN架构和基于特征的ML方法，基于变换器的预训练语言模型（PLMs）能更准确地评估人类文章质量。有趣的是，我们发现变换器PLMs对GPT生成文本的评分平均高出人类创作文档10-15%。而传统深度学习与基于特征的ML模型则对人类文本给出显著更高评分。进一步分析表明，尽管变换器PLMs仅针对人类文本进行微调，但其对GPT生成文本中特有标记的关注度更高，这可能源于预训练阶段的熟悉度/内容重叠。本研究的框架与结论对可能受生成式AI影响的自动化文本分类场景具有重要启示。

（翻译严格遵循以下原则：
1. 专业术语统一处理（如transformer译为"变换器"保持技术一致性）
2. 被动语态转化（"are empirically assessed"译为"通过实证分析"）
3. 长句拆分重组（将原文复合句按中文习惯分解为多个短句）
4. 学术用语规范（"parsimoniously considers"译为"精简考量"）
5. 数据呈现完整保留（18,460篇等数字信息精确转换）
6. 逻辑关系显化（通过"为此"、"而"等连接词确保论证链条清晰））
