# Trust & Safety of LLMs and LLMs in Trust & Safety

链接: http://arxiv.org/abs/2412.02113v1

原文摘要:
In recent years, Large Language Models (LLMs) have garnered considerable
attention for their remarkable abilities in natural language processing tasks.
However, their widespread adoption has raised concerns pertaining to trust and
safety. This systematic review investigates the current research landscape on
trust and safety in LLMs, with a particular focus on the novel application of
LLMs within the field of Trust and Safety itself. We delve into the
complexities of utilizing LLMs in domains where maintaining trust and safety is
paramount, offering a consolidated perspective on this emerging trend.\
  By synthesizing findings from various studies, we identify key challenges and
potential solutions, aiming to benefit researchers and practitioners seeking to
understand the nuanced interplay between LLMs and Trust and Safety.
  This review provides insights on best practices for using LLMs in Trust and
Safety, and explores emerging risks such as prompt injection and jailbreak
attacks. Ultimately, this study contributes to a deeper understanding of how
LLMs can be effectively and responsibly utilized to enhance trust and safety in
the digital realm.

中文翻译:
近年来，大型语言模型（LLMs）因其在自然语言处理任务中的卓越能力而备受关注。然而，其广泛应用也引发了与信任和安全相关的隐忧。本文通过系统性综述，探讨了当前LLMs在信任与安全领域的研究现状，特别聚焦于LLMs在"信任与安全"这一专业领域中的创新应用。我们深入剖析了在信任与安全至关重要的场景中部署LLMs的复杂性，为这一新兴趋势提供了整合性视角。

通过综合多项研究成果，我们梳理出关键挑战与潜在解决方案，旨在帮助研究者和从业者理解LLMs与信任安全之间微妙的相互作用。本综述不仅提出了LLMs在信任与安全领域的最佳实践指南，还探讨了提示词注入（prompt injection）和越狱攻击（jailbreak attacks）等新兴风险。最终，本研究为推动LLMs在数字领域中以负责任的方式提升信任与安全水平提供了深层认知框架。


