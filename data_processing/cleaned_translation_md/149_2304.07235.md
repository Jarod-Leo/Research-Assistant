# Optimal inference of a generalised Potts model by single-layer transformers with factored attention

链接: http://arxiv.org/abs/2304.07235v1

原文摘要:
Transformers are neural networks that revolutionized natural language
processing and machine learning. They process sequences of inputs, like words,
using a mechanism called self-attention, which is trained via masked language
modeling (MLM). In MLM, a word is randomly masked in an input sequence, and the
network is trained to predict the missing word. Despite the practical success
of transformers, it remains unclear what type of data distribution
self-attention can learn efficiently. Here, we show analytically that if one
decouples the treatment of word positions and embeddings, a single layer of
self-attention learns the conditionals of a generalized Potts model with
interactions between sites and Potts colors. Moreover, we show that training
this neural network is exactly equivalent to solving the inverse Potts problem
by the so-called pseudo-likelihood method, well known in statistical physics.
Using this mapping, we compute the generalization error of self-attention in a
model scenario analytically using the replica method.

中文翻译:
Transformer是一种彻底改变了自然语言处理和机器学习领域的神经网络架构。其通过名为自注意力（self-attention）的机制处理输入序列（如单词序列），该机制通过掩码语言建模（MLM）进行训练。在MLM中，输入序列中的单词会被随机掩码，网络通过预测被遮蔽的单词进行训练。尽管Transformer已取得实际成功，但自注意力机制能高效学习何种数据分布仍不明确。本文通过解析证明：若将词位置与词嵌入的处理解耦，单层自注意力学习到的实际上是位点与Potts颜色之间存在相互作用的广义Potts模型的条件概率分布。更重要的是，我们发现训练该神经网络完全等价于统计物理学中著名的伪似然方法求解逆Potts问题。基于这种对应关系，我们运用统计物理中的复本方法，在模型场景下解析计算了自注意力机制的泛化误差。
