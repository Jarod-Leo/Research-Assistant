# Executing Arithmetic: Fine-Tuning Large Language Models as Turing Machines

链接: http://arxiv.org/abs/2410.07896v1

原文摘要:
Large Language Models (LLMs) have demonstrated remarkable capabilities across
a wide range of natural language processing and reasoning tasks. However, their
performance in the foundational domain of arithmetic remains unsatisfactory.
When dealing with arithmetic tasks, LLMs often memorize specific examples
rather than learning the underlying computational logic, limiting their ability
to generalize to new problems. In this paper, we propose a Composable
Arithmetic Execution Framework (CAEF) that enables LLMs to learn to execute
step-by-step computations by emulating Turing Machines, thereby gaining a
genuine understanding of computational logic. Moreover, the proposed framework
is highly scalable, allowing composing learned operators to significantly
reduce the difficulty of learning complex operators. In our evaluation, CAEF
achieves nearly 100% accuracy across seven common mathematical operations on
the LLaMA 3.1-8B model, effectively supporting computations involving operands
with up to 100 digits, a level where GPT-4o falls short noticeably in some
settings.

中文翻译:
以下是符合您要求的中文翻译：

大型语言模型（LLMs）在自然语言处理和推理任务中展现出卓越能力，但其在基础算术领域的表现仍不尽如人意。面对算术任务时，LLMs往往通过记忆特定示例而非理解底层计算逻辑来应对，这限制了其解决新问题的泛化能力。本文提出可组合算术执行框架（CAEF），通过模拟图灵机使LLMs逐步学习分步计算，从而真正掌握计算逻辑。该框架具备高度可扩展性，可通过组合已学习运算符显著降低复杂运算的学习难度。实验表明，在LLaMA 3.1-8B模型上，CAEF对七种常见数学运算实现近100%准确率，有效支持高达100位操作数的运算——该场景下GPT-4o在某些设定中表现明显不足。

翻译说明：
1. 专业术语处理：LLMs/Turing Machines等专业名词保留英文缩写并添加中文全称（大型语言模型/图灵机）
2. 句式重构：将原文复合句拆分为符合中文表达习惯的短句，如将"memorize...rather than..."处理为"通过...而非..."的对比结构
3. 被动语态转换："are composed"译为主动态"通过组合"
4. 数据呈现优化：精确保持"100 digits/100% accuracy"等关键数据的表述
5. 技术概念显化：将"generalize to new problems"意译为"解决新问题的泛化能力"以突出计算机领域特性
6. 比较级处理："falls short noticeably"译为"表现明显不足"以符合中文比较级表达习惯
