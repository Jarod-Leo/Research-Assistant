# Do LLMs Really Adapt to Domains? An Ontology Learning Perspective

链接: http://arxiv.org/abs/2407.19998v1

原文摘要:
Large Language Models (LLMs) have demonstrated unprecedented prowess across
various natural language processing tasks in various application domains.
Recent studies show that LLMs can be leveraged to perform lexical semantic
tasks, such as Knowledge Base Completion (KBC) or Ontology Learning (OL).
However, it has not effectively been verified whether their success is due to
their ability to reason over unstructured or semi-structured data, or their
effective learning of linguistic patterns and senses alone. This unresolved
question is particularly crucial when dealing with domain-specific data, where
the lexical senses and their meaning can completely differ from what a LLM has
learned during its training stage. This paper investigates the following
question: Do LLMs really adapt to domains and remain consistent in the
extraction of structured knowledge, or do they only learn lexical senses
instead of reasoning? To answer this question and, we devise a controlled
experiment setup that uses WordNet to synthesize parallel corpora, with English
and gibberish terms. We examine the differences in the outputs of LLMs for each
corpus in two OL tasks: relation extraction and taxonomy discovery. Empirical
results show that, while adapting to the gibberish corpora, off-the-shelf LLMs
do not consistently reason over semantic relationships between concepts, and
instead leverage senses and their frame. However, fine-tuning improves the
performance of LLMs on lexical semantic tasks even when the domain-specific
terms are arbitrary and unseen during pre-training, hinting at the
applicability of pre-trained LLMs for OL.

中文翻译:
大型语言模型（LLMs）在多个应用领域的自然语言处理任务中展现出前所未有的卓越能力。近期研究表明，LLMs可被用于执行词汇语义任务，如知识库补全（KBC）或本体学习（OL）。然而，其成功究竟源于对非结构化/半结构化数据的推理能力，还是仅依赖于对语言模式及语义的有效学习，这一问题尚未得到有效验证。当处理领域特定数据时——这些数据的词汇语义及其含义可能完全不同于LLMs训练阶段所学内容——这一悬而未决的问题显得尤为关键。本文探究以下核心问题：LLMs是真正实现了领域适应并保持结构化知识提取的一致性，还是仅习得了词汇语义而非推理能力？为解答该问题，我们设计了一个受控实验框架，利用WordNet构建包含英语词汇与无意义术语的平行语料库，通过关系抽取和分类体系发现两项OL任务，对比分析LLMs对不同语料库的输出差异。实证结果表明：现成的LLMs在适应无意义语料时，并未持续保持对概念间语义关系的推理能力，而是依赖于语义框架的感知；但经过微调后，即便面对预训练阶段未见的任意领域术语，LLMs在词汇语义任务上的表现仍能提升，这暗示了预训练LLMs在本体学习中的适用潜力。
