# Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models

链接: http://arxiv.org/abs/2402.04614v1

原文摘要:
Large Language Models (LLMs) are deployed as powerful tools for several
natural language processing (NLP) applications. Recent works show that modern
LLMs can generate self-explanations (SEs), which elicit their intermediate
reasoning steps for explaining their behavior. Self-explanations have seen
widespread adoption owing to their conversational and plausible nature.
However, there is little to no understanding of their faithfulness. In this
work, we discuss the dichotomy between faithfulness and plausibility in SEs
generated by LLMs. We argue that while LLMs are adept at generating plausible
explanations -- seemingly logical and coherent to human users -- these
explanations do not necessarily align with the reasoning processes of the LLMs,
raising concerns about their faithfulness. We highlight that the current trend
towards increasing the plausibility of explanations, primarily driven by the
demand for user-friendly interfaces, may come at the cost of diminishing their
faithfulness. We assert that the faithfulness of explanations is critical in
LLMs employed for high-stakes decision-making. Moreover, we emphasize the need
for a systematic characterization of faithfulness-plausibility requirements of
different real-world applications and ensure explanations meet those needs.
While there are several approaches to improving plausibility, improving
faithfulness is an open challenge. We call upon the community to develop novel
methods to enhance the faithfulness of self explanations thereby enabling
transparent deployment of LLMs in diverse high-stakes settings.

中文翻译:
大型语言模型（LLMs）作为强大工具被应用于多种自然语言处理（NLP）任务。最新研究表明，现代LLMs能够生成自我解释（SEs），通过展示其推理过程来解释自身行为。这类自我解释因其对话式的表达和看似合理的特性而广受采纳。然而，学界对其可信性（faithfulness）的认知却近乎空白。本文探讨了LLMs生成自我解释中可信性与合理性（plausibility）的二元对立：虽然LLMs擅长生成符合人类逻辑、具有表面合理性的解释，但这些解释未必真实反映模型的推理过程，从而引发对解释可信性的质疑。我们指出，当前为满足用户友好需求而片面追求解释合理性的趋势，可能正在以牺牲可信性为代价。在涉及高风险决策的LLMs应用中，解释的可信性至关重要。我们强调需要系统性地界定不同现实应用对可信性与合理性的需求标准，并确保解释符合相应要求。尽管提升合理性的方法众多，如何增强可信性仍是待解难题。我们呼吁学界开发创新方法以提升自我解释的可信性，从而推动LLMs在高风险场景中的透明化部署。

（翻译说明：采用学术论文摘要的规范表达，通过术语统一（如"faithfulness"固定译为"可信性"）、逻辑显化（如"dichotomy"译为"二元对立"）、句式重组（将英语长句拆分为符合中文表达习惯的短句）等策略，在保持专业性的同时确保行文流畅。关键概念首次出现时标注英文原词，重要论点通过冒号引导的阐释结构进行强调，符合中文科技论文的写作范式。）
