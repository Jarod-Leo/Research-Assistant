# Achieving Peak Performance for Large Language Models: A Systematic Review

链接: http://arxiv.org/abs/2409.04833v1

原文摘要:
In recent years, large language models (LLMs) have achieved remarkable
success in natural language processing (NLP). LLMs require an extreme amount of
parameters to attain high performance. As models grow into the
trillion-parameter range, computational and memory costs increase
significantly. This makes it difficult for many researchers to access the
resources needed to train or apply these models. Optimizing LLM performance
involves two main approaches: fine-tuning pre-trained models for specific tasks
to achieve state-of-the-art performance, and reducing costs or improving
training time while maintaining similar performance. This paper presents a
systematic literature review (SLR) following the Preferred Reporting Items for
Systematic Reviews and Meta-Analyses (PRISMA) statement. We reviewed 65
publications out of 983 from 2017 to December 2023, retrieved from 5 databases.
The study presents methods to optimize and accelerate LLMs while achieving
cutting-edge results without sacrificing accuracy. We begin with an overview of
the development of language modeling, followed by a detailed explanation of
commonly used frameworks and libraries, and a taxonomy for improving and
speeding up LLMs based on three classes: LLM training, LLM inference, and
system serving. We then delve into recent optimization and acceleration
strategies such as training optimization, hardware optimization, scalability
and reliability, accompanied by the taxonomy and categorization of these
strategies. Finally, we provide an in-depth comparison of each class and
strategy, with two case studies on optimizing model training and enhancing
inference efficiency. These case studies showcase practical approaches to
address LLM resource limitations while maintaining performance.

中文翻译:
近年来，大型语言模型（LLM）在自然语言处理（NLP）领域取得了显著成就。这类模型需要海量参数以实现高性能，但随着参数规模突破万亿量级，计算与内存成本急剧攀升，导致许多研究者难以获取训练或应用这些模型所需的资源。优化LLM性能主要涉及两大方向：针对特定任务微调预训练模型以获得最优性能；在保持相近性能的同时降低成本或提升训练效率。本文遵循系统综述与荟萃分析优先报告条目（PRISMA）标准，对2017年至2023年12月期间从5个数据库检索的983篇文献中的65篇进行了系统文献综述（SLR）。研究提出了在不牺牲准确性的前提下优化加速LLM并实现尖端成果的方法体系：首先概述语言建模的发展历程，继而详解常用框架与工具库，提出基于LLM训练、推理及系统服务三大维度的优化加速分类法；随后深入探讨训练优化、硬件优化、可扩展性与可靠性等最新策略，并建立相应的分类体系；最后通过模型训练优化与推理效率提升两个案例研究，对各优化策略进行深度对比分析，为突破LLM资源限制同时保持性能提供了实践范例。
