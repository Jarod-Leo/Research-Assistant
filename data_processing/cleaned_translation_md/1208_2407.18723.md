# LLASP: Fine-tuning Large Language Models for Answer Set Programming

链接: http://arxiv.org/abs/2407.18723v1

原文摘要:
Recently, Large Language Models (LLMs) have showcased their potential in
various natural language processing tasks, including code generation. However,
while significant progress has been made in adapting LLMs to generate code for
several imperative programming languages and tasks, there remains a notable gap
in their application to declarative formalisms, such as Answer Set Programming
(ASP). In this paper, we move a step towards exploring the capabilities of LLMs
for ASP code generation. First, we perform a systematic evaluation of several
state-of-the-art LLMs. Despite their power in terms of number of parameters,
training data and computational resources, empirical results demonstrate
inadequate performances in generating correct ASP programs. Therefore, we
propose LLASP, a fine-tuned lightweight model specifically trained to encode
fundamental ASP program patterns. To this aim, we create an ad-hoc dataset
covering a wide variety of fundamental problem specifications that can be
encoded in ASP. Our experiments demonstrate that the quality of ASP programs
generated by LLASP is remarkable. This holds true not only when compared to the
non-fine-tuned counterpart but also when compared to the majority of eager LLM
candidates, particularly from a semantic perspective. All the code and data
used to perform the experiments are publicly available at
https://anonymous.4open.science/r/LLASP-D86C/.

中文翻译:
近年来，大型语言模型（LLMs）在各类自然语言处理任务中展现出巨大潜力，包括代码生成领域。然而，尽管LLMs在生成多种命令式编程语言的代码方面已取得显著进展，但其在声明式范式（如回答集编程ASP）中的应用仍存在明显不足。本文迈出了探索LLMs在ASP代码生成能力的重要一步：首先对多个最先进的LLMs进行系统评估，结果表明即便这些模型具备海量参数、训练数据和计算资源，其生成的ASP程序正确率仍不理想。为此，我们提出LLASP——一个经过微调的轻量级模型，专门训练用于编码基础ASP程序模式。为此我们构建了专用数据集，涵盖可被ASP编码的各类基础问题规范。实验证明LLASP生成的ASP程序质量显著提升，不仅优于未微调的原始模型，在语义层面更超越了多数主流LLM候选方案。所有实验代码与数据已公开于https://anonymous.4open.science/r/LLASP-D86C/。

（翻译说明：
1. 专业术语处理："Answer Set Programming"采用学界通用译名"回答集编程"并保留ASP缩写
2. 句式重构：将原文复合长句拆分为符合中文表达习惯的短句，如将"despite..."让步状语从句转化为"即便..."的转折句式
3. 概念显化："eager LLM candidates"意译为"主流LLM候选方案"以保持技术准确性
4. 语态转换：将被动语态"a dataset covering..."主动化为"我们构建了..."
5. 逻辑衔接：通过"为此"、"结果表明"等连接词保持学术文本的严谨性
6. 文化适配：保留技术缩写ASP的同时在首次出现时标注全称，符合中文技术文献规范）
