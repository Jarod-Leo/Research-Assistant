# A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

链接: http://arxiv.org/abs/2311.05232v1

原文摘要:
The emergence of large language models (LLMs) has marked a significant
breakthrough in natural language processing (NLP), fueling a paradigm shift in
information acquisition. Nevertheless, LLMs are prone to hallucination,
generating plausible yet nonfactual content. This phenomenon raises significant
concerns over the reliability of LLMs in real-world information retrieval (IR)
systems and has attracted intensive research to detect and mitigate such
hallucinations. Given the open-ended general-purpose attributes inherent to
LLMs, LLM hallucinations present distinct challenges that diverge from prior
task-specific models. This divergence highlights the urgency for a nuanced
understanding and comprehensive overview of recent advances in LLM
hallucinations. In this survey, we begin with an innovative taxonomy of
hallucination in the era of LLM and then delve into the factors contributing to
hallucinations. Subsequently, we present a thorough overview of hallucination
detection methods and benchmarks. Our discussion then transfers to
representative methodologies for mitigating LLM hallucinations. Additionally,
we delve into the current limitations faced by retrieval-augmented LLMs in
combating hallucinations, offering insights for developing more robust IR
systems. Finally, we highlight the promising research directions on LLM
hallucinations, including hallucination in large vision-language models and
understanding of knowledge boundaries in LLM hallucinations.

中文翻译:
大型语言模型（LLM）的出现标志着自然语言处理（NLP）领域的重大突破，推动了信息获取方式的范式变革。然而，这类模型容易产生幻觉现象——生成看似合理实则违背事实的内容。这种现象引发了人们对LLM在实际信息检索（IR）系统中可靠性的深切担忧，也促使学界投入大量研究来检测和缓解此类幻觉。由于LLM本身具有开放域通用型特性，其幻觉现象呈现出与以往任务专用模型截然不同的挑战，这种差异凸显了对LLM幻觉进行细致理解和系统梳理的紧迫性。

本综述首先提出LLM时代幻觉现象的创新分类体系，继而深入剖析导致幻觉的成因。随后系统梳理当前主流的幻觉检测方法与评估基准，并重点讨论缓解LLM幻觉的代表性方法。此外，我们深入探讨检索增强型LLM在对抗幻觉时面临的技术瓶颈，为构建更健壮的IR系统提供洞见。最后，我们指明该领域具有前景的研究方向，包括大型视觉语言模型中的幻觉现象，以及对LLM幻觉知识边界的理解探索。


