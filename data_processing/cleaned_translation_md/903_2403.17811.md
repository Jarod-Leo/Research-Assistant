# Are Compressed Language Models Less Subgroup Robust?

链接: http://arxiv.org/abs/2403.17811v1

原文摘要:
To reduce the inference cost of large language models, model compression is
increasingly used to create smaller scalable models. However, little is known
about their robustness to minority subgroups defined by the labels and
attributes of a dataset. In this paper, we investigate the effects of 18
different compression methods and settings on the subgroup robustness of BERT
language models. We show that worst-group performance does not depend on model
size alone, but also on the compression method used. Additionally, we find that
model compression does not always worsen the performance on minority subgroups.
Altogether, our analysis serves to further research into the subgroup
robustness of model compression.

中文翻译:
为降低大语言模型的推理成本，模型压缩技术正被越来越多地用于创建可扩展的小型模型。然而，目前对于这些压缩模型在数据集中由标签和属性定义的少数子群体上的鲁棒性仍知之甚少。本文研究了18种不同压缩方法及参数设置对BERT语言模型子群体鲁棒性的影响。研究表明：最差子群体性能不仅取决于模型规模，更与所采用的压缩方法密切相关；同时发现模型压缩并不总是会恶化少数子群体的性能表现。本研究为模型压缩的子群体鲁棒性领域提供了新的探索方向。

（翻译说明：
1. 专业术语处理："inference cost"译为"推理成本"，"model compression"统一译为"模型压缩"，"subgroup robustness"译为"子群体鲁棒性"
2. 句式重构：将原文三个段落整合为符合中文摘要习惯的连贯段落，通过分号连接研究发现
3. 被动语态转换："little is known"译为主动式"仍知之甚少"
4. 学术表达规范："we show"译为客观表述"研究表明"，"we find"转化为"发现"
5. 文化适配："Altogether"不直译为"总之"，而处理为"本研究为...提供了新的探索方向"的学术收尾句式）
