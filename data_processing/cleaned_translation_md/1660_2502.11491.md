# Ontology-Guided Reverse Thinking Makes Large Language Models Stronger on Knowledge Graph Question Answering

链接: http://arxiv.org/abs/2502.11491v1

原文摘要:
Large language models (LLMs) have shown remarkable capabilities in natural
language processing. However, in knowledge graph question answering tasks
(KGQA), there remains the issue of answering questions that require multi-hop
reasoning. Existing methods rely on entity vector matching, but the purpose of
the question is abstract and difficult to match with specific entities. As a
result, it is difficult to establish reasoning paths to the purpose, which
leads to information loss and redundancy. To address this issue, inspired by
human reverse thinking, we propose Ontology-Guided Reverse Thinking (ORT), a
novel framework that constructs reasoning paths from purposes back to
conditions. ORT operates in three key phases: (1) using LLM to extract purpose
labels and condition labels, (2) constructing label reasoning paths based on
the KG ontology, and (3) using the label reasoning paths to guide knowledge
retrieval. Experiments on the WebQSP and CWQ datasets show that ORT achieves
state-of-the-art performance and significantly enhances the capability of LLMs
for KGQA.

中文翻译:
大语言模型（LLM）在自然语言处理领域展现出卓越能力。然而在知识图谱问答任务（KGQA）中，针对需要多跳推理的问题仍存在解答困难。现有方法依赖实体向量匹配，但问题意图往往具有抽象性，难以与具体实体直接匹配，导致无法建立通向意图的推理路径，进而引发信息缺失与冗余。受人类逆向思维启发，本研究提出本体引导的逆向推理框架（ORT），通过从意图回溯到条件的路径构建来解决该问题。ORT框架包含三个核心阶段：（1）利用LLM提取意图标签与条件标签；（2）基于知识图谱本体构建标签推理路径；（3）利用标签路径指导知识检索。在WebQSP和CWQ数据集上的实验表明，ORT实现了最先进的性能表现，显著提升了LLM在KGQA任务中的推理能力。

（翻译说明：
1. 专业术语处理：LLM/KGQA等专业缩写首次出现时保留英文全称，后续使用中文简称
2. 学术概念转换："multi-hop reasoning"译为"多跳推理"，"ontology"译为"本体"符合计算机领域术语规范
3. 句式重构：将英文长句拆分为符合中文表达习惯的短句，如将"difficult to establish..."处理为因果句式
4. 被动语态转换：将"are extracted"等被动结构转为主动式"利用LLM提取"
5. 学术风格保持：使用"显著提升""最先进的"等符合学术论文表述的词汇
6. 逻辑显化：通过"进而""导致"等连接词明确原文隐含的逻辑关系）
