# Tensor Train Low-rank Approximation (TT-LoRA): Democratizing AI with Accelerated LLMs

链接: http://arxiv.org/abs/2408.01008v1

原文摘要:
In recent years, Large Language Models (LLMs) have demonstrated remarkable
capabilities across a wide range of natural language processing (NLP) tasks,
such as question-answering, sentiment analysis, text summarization, and machine
translation. However, the ever-growing complexity of LLMs demands immense
computational resources, hindering the broader research and application of
these models. To address this, various parameter-efficient fine-tuning
strategies, such as Low-Rank Approximation (LoRA) and Adapters, have been
developed. Despite their potential, these methods often face limitations in
compressibility. Specifically, LoRA struggles to scale effectively with the
increasing number of trainable parameters in modern large scale LLMs.
Additionally, Low-Rank Economic Tensor-Train Adaptation (LoRETTA), which
utilizes tensor train decomposition, has not yet achieved the level of
compression necessary for fine-tuning very large scale models with limited
resources. This paper introduces Tensor Train Low-Rank Approximation (TT-LoRA),
a novel parameter-efficient fine-tuning (PEFT) approach that extends LoRETTA
with optimized tensor train (TT) decomposition integration. By eliminating
Adapters and traditional LoRA-based structures, TT-LoRA achieves greater model
compression without compromising downstream task performance, along with
reduced inference latency and computational overhead. We conduct an exhaustive
parameter search to establish benchmarks that highlight the trade-off between
model compression and performance. Our results demonstrate significant
compression of LLMs while maintaining comparable performance to larger models,
facilitating their deployment on resource-constraint platforms.

中文翻译:
近年来，大型语言模型（LLMs）在问答系统、情感分析、文本摘要和机器翻译等自然语言处理（NLP）任务中展现出卓越能力。然而，随着模型复杂度持续攀升，其庞大的计算资源需求严重制约了相关研究与应用推广。为此，学界提出了多种参数高效微调策略，如低秩近似（LoRA）和适配器（Adapters）等方法。尽管这些技术具有潜力，但其压缩能力仍存在明显局限：LoRA难以适应现代大规模LLMs中可训练参数数量的指数级增长；而基于张量火车分解的低秩经济张量适配方法（LoRETTA）尚未实现资源受限环境下超大规模模型微调所需的压缩水平。本文提出张量火车低秩近似（TT-LoRA）——一种创新的参数高效微调（PEFT）方法，通过优化张量火车（TT）分解集成扩展了LoRETTA框架。该方法摒弃适配器和传统LoRA结构，在保持下游任务性能的同时实现了更高程度的模型压缩，并显著降低推理延迟与计算开销。我们通过详尽的参数搜索建立基准测试，量化模型压缩与性能之间的权衡关系。实验结果表明，TT-LoRA能在保持与大型模型相当性能的前提下显著压缩LLMs规模，为资源受限平台的部署提供了可行性。
