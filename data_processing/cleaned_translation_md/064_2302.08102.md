# Prompt Tuning of Deep Neural Networks for Speaker-adaptive Visual Speech Recognition

链接: http://arxiv.org/abs/2302.08102v1

原文摘要:
Visual Speech Recognition (VSR) aims to infer speech into text depending on
lip movements alone. As it focuses on visual information to model the speech,
its performance is inherently sensitive to personal lip appearances and
movements, and this makes the VSR models show degraded performance when they
are applied to unseen speakers. In this paper, to remedy the performance
degradation of the VSR model on unseen speakers, we propose prompt tuning
methods of Deep Neural Networks (DNNs) for speaker-adaptive VSR. Specifically,
motivated by recent advances in Natural Language Processing (NLP), we finetune
prompts on adaptation data of target speakers instead of modifying the
pre-trained model parameters. Different from the previous prompt tuning methods
mainly limited to Transformer variant architecture, we explore different types
of prompts, the addition, the padding, and the concatenation form prompts that
can be applied to the VSR model which is composed of CNN and Transformer in
general. With the proposed prompt tuning, we show that the performance of the
pre-trained VSR model on unseen speakers can be largely improved by using a
small amount of adaptation data (e.g., less than 5 minutes), even if the
pre-trained model is already developed with large speaker variations. Moreover,
by analyzing the performance and parameters of different types of prompts, we
investigate when the prompt tuning is preferred over the finetuning methods.
The effectiveness of the proposed method is evaluated on both word- and
sentence-level VSR databases, LRW-ID and GRID.

中文翻译:
视觉语音识别（VSR）旨在仅通过唇部动作推断语音内容。由于该技术依赖视觉信息建立语音模型，其性能本质上对个体唇部外观与运动特征具有敏感性，导致预训练模型在面对陌生说话者时表现显著下降。为缓解这一问题，本文提出基于深度神经网络（DNN）的提示调优方法，实现说话者自适应的VSR模型优化。受自然语言处理（NLP）领域最新进展启发，我们通过对目标说话者的适应数据微调提示参数（而非调整预训练模型参数）来实现优化。不同于以往主要局限于Transformer变体架构的提示调优方法，我们探索了适用于CNN-Transformer混合架构VSR模型的多种提示类型：加法式提示、填充式提示和拼接式提示。实验表明，即使预训练模型已涵盖大量说话者变体，采用少量适应数据（如不足5分钟）进行提示调优仍能大幅提升模型对陌生说话者的识别性能。通过对比分析不同提示类型的参数效率与性能表现，我们进一步揭示了提示调优相较于传统微调方法的优势场景。本方法在单词级（LRW-ID）和句子级（GRID）VSR数据库上的实验验证了其有效性。
