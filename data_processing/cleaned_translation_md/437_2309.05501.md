# Black-Box Analysis: GPTs Across Time in Legal Textual Entailment Task

链接: http://arxiv.org/abs/2309.05501v1

原文摘要:
The evolution of Generative Pre-trained Transformer (GPT) models has led to
significant advancements in various natural language processing applications,
particularly in legal textual entailment. We present an analysis of GPT-3.5
(ChatGPT) and GPT-4 performances on COLIEE Task 4 dataset, a prominent
benchmark in this domain. The study encompasses data from Heisei 18 (2006) to
Reiwa 3 (2021), exploring the models' abilities to discern entailment
relationships within Japanese statute law across different periods. Our
preliminary experimental results unveil intriguing insights into the models'
strengths and weaknesses in handling legal textual entailment tasks, as well as
the patterns observed in model performance. In the context of proprietary
models with undisclosed architectures and weights, black-box analysis becomes
crucial for evaluating their capabilities. We discuss the influence of training
data distribution and the implications on the models' generalizability. This
analysis serves as a foundation for future research, aiming to optimize
GPT-based models and enable their successful adoption in legal information
extraction and entailment applications.

中文翻译:
以下是符合学术规范的译文：

生成式预训练变换模型（GPT）的演进为各类自然语言处理应用带来了显著进步，尤其在法律文本蕴含任务领域表现突出。本研究基于COLIEE任务4数据集（该领域的权威基准），对GPT-3.5（ChatGPT）与GPT-4的性能表现展开分析。研究涵盖平成18年（2006）至令和3年（2021）期间数据，探究模型在不同时期日本成文法中识别蕴含关系的能力。初步实验结果揭示了模型处理法律文本蕴含任务时的优势与局限，以及性能表现中呈现的规律模式。针对架构参数未公开的专有模型，黑箱分析方法对其能力评估至关重要。我们探讨了训练数据分布的影响及其对模型泛化能力的启示。本分析为后续研究奠定基础，旨在优化基于GPT的模型，推动其在法律信息抽取与蕴含应用中的成功落地。

注：
1. 专业术语处理：
- "textual entailment"译为"文本蕴含"（自然语言处理领域标准译法）
- "statute law"译为"成文法"（法律专业术语）
- "black-box analysis"译为"黑箱分析"（计算机科学常用译法）

2. 文化适配：
- 保留"平成/令和"年号并标注公元年份，符合中文法学论文惯例
- "COLIEE Task 4"保留英文原名并添加括号说明，符合学术规范

3. 句式重构：
- 将英文长句拆分为符合中文表达习惯的短句
- 被动语态转换为主动表述（如"data from...was explored"→"探究...数据"）

4. 学术风格：
- 使用"本研究""探究""揭示"等学术用语
- 保持客观严谨的叙述风格
