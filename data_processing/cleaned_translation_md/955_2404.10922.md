# Teaching a Multilingual Large Language Model to Understand Multilingual Speech via Multi-Instructional Training

链接: http://arxiv.org/abs/2404.10922v1

原文摘要:
Recent advancements in language modeling have led to the emergence of Large
Language Models (LLMs) capable of various natural language processing tasks.
Despite their success in text-based tasks, applying LLMs to the speech domain
remains limited and challenging. This paper presents BLOOMZMMS, a novel model
that integrates a multilingual LLM with a multilingual speech encoder, aiming
to harness the capabilities of LLMs for speech recognition and beyond.
Utilizing a multi-instructional training approach, we demonstrate the
transferability of linguistic knowledge from the text to the speech modality.
Our experiments, conducted on 1900 hours of transcribed data from 139
languages, establish that a multilingual speech representation can be
effectively learned and aligned with a multilingual LLM. While this learned
representation initially shows limitations in task generalization, we address
this issue by generating synthetic targets in a multi-instructional style. Our
zero-shot evaluation results confirm the robustness of our approach across
multiple tasks, including speech translation and multilingual spoken language
understanding, thereby opening new avenues for applying LLMs in the speech
domain.

中文翻译:
近年来，语言建模技术的进步催生了能够执行多种自然语言处理任务的大规模语言模型（LLMs）。尽管这些模型在文本任务中表现优异，但其在语音领域的应用仍存在局限与挑战。本文提出BLOOMZMMS模型，通过将多语言LLM与多语言语音编码器相结合，旨在释放LLMs在语音识别等领域的潜力。采用多指令训练方法，我们验证了语言知识从文本模态到语音模态的可迁移性。基于139种语言、1900小时转录数据的实验表明，多语言语音表征能够被有效学习并与多语言LLM对齐。虽然初始学习到的表征在任务泛化方面存在局限，但我们通过生成多指令风格的合成目标解决了这一问题。零样本评估结果证实，该方法在语音翻译、多语言口语理解等任务中具有鲁棒性，从而为LLMs在语音领域的应用开辟了新途径。

（翻译说明：采用学术论文摘要的规范表达，处理了专业术语如"zero-shot evaluation"译为"零样本评估"；将长句合理切分为符合中文表达习惯的短句；保持"multilingual"统一译为"多语言"；通过"催生""释放""开辟"等动词增强学术文本的严谨性与表现力；准确转换被动语态为中文主动表达，如"conducted on"译为"基于"）
