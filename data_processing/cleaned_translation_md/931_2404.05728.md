# A Large-Scale Exploration of $μ$-Transfer

链接: http://arxiv.org/abs/2404.05728v1

原文摘要:
Deep learning models have become a cornerstone of modern AI research, yet
their initializations and learning rates may at times be set in an opaque or
ad-hoc fashion due to the high cost of hyperparameter sweeps. The
$\mu$-Parameterization ($\mu$P) offers a possible solution to this challenge,
yielding scaling rules for model initialization and learning rates while
reportedly enabling zero-shot hyperparameter transfer from small to large
models. Despite its evident promise, the $\mu$P method is not yet widely
adopted, perhaps due to higher implementation complexity, many variations, or
complex theoretical background. This work considers $\mu$P empirically,
focusing on the popular transformer architecture, and aims to answer a simple
question: does $\mu$-Transfer yield near-optimal learning rates in practice?
Studying over a dozen ablations with up to 1.2B parameters and 33B tokens and a
large-scale experiment with up to 10B parameters and 190B tokens, we observe a
positive answer for most settings, and discuss improvements otherwise.

中文翻译:
深度学习模型已成为现代人工智能研究的基石，然而由于超参数扫描的高昂成本，其初始参数和学习率的设置往往存在不透明或临时性的问题。μ参数化（μP）为这一挑战提供了可能的解决方案——该方法不仅能推导出模型初始化与学习率的缩放规则，据称还可实现超参数从小模型到大模型的零样本迁移。尽管前景显著，μP方法目前仍未得到广泛采用，原因可能在于较高的实现复杂度、诸多变体或复杂的理论背景。本文通过实证研究探讨μP在主流Transformer架构中的应用，旨在回答一个核心问题：μ迁移在实践中能否产生接近最优的学习率？通过对超过12项消融实验（最大12亿参数/330亿token）和一项大规模实验（最大100亿参数/1900亿token）的观察，我们发现该方法在多数情况下确实有效，并对未达理想效果的场景提出了改进方案。
