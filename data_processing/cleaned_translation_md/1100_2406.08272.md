# The Importance of Positional Encoding Initialization in Transformers for Relational Reasoning

链接: http://arxiv.org/abs/2406.08272v1

原文摘要:
The attention mechanism is central to the transformer's ability to capture
complex dependencies between tokens of an input sequence. Key to the successful
application of the attention mechanism in transformers is its choice of
positional encoding (PE). The PE provides essential information that
distinguishes the position and order amongst tokens in a sequence. Most prior
investigations of PE effects on generalization were tailored to 1D input
sequences, such as those presented in natural language, where adjacent tokens
(e.g., words) are highly related. In contrast, many real world tasks involve
datasets with highly non-trivial positional arrangements, such as datasets
organized in multiple spatial dimensions, or datasets for which ground truth
positions are not known, such as in biological data. Here we study the
importance of learning accurate PE for problems which rely on a non-trivial
arrangement of input tokens. Critically, we find that the choice of
initialization of a learnable PE greatly influences its ability to learn
accurate PEs that lead to enhanced generalization. We empirically demonstrate
our findings in three experiments: 1) A 2D relational reasoning task; 2) A
nonlinear stochastic network simulation; 3) A real world 3D neuroscience
dataset, applying interpretability analyses to verify the learning of accurate
PEs. Overall, we find that a learned PE initialized from a small-norm
distribution can 1) uncover interpretable PEs that mirror ground truth
positions in multiple dimensions, and 2) lead to improved downstream
generalization in empirical evaluations. Importantly, choosing an ill-suited PE
can be detrimental to both model interpretability and generalization. Together,
our results illustrate the feasibility of learning identifiable and
interpretable PEs for enhanced generalization.

中文翻译:
注意力机制是Transformer模型捕捉输入序列各单元间复杂依赖关系的核心。该机制成功应用的关键在于位置编码（PE）的选择——这种编码为序列中的单元提供区分位置与顺序的关键信息。先前关于位置编码对泛化能力影响的研究主要针对一维输入序列（如自然语言中相邻词汇高度关联的序列），而现实任务常涉及具有复杂空间排列的数据（如多维空间组织的数据集）或真实位置未知的数据（如生物数据）。本文重点研究了在输入单元呈非平凡排列的问题中，学习精确位置编码的重要性。关键发现表明：可学习位置编码的初始化方式会显著影响其学习准确编码的能力，进而决定模型的泛化性能提升效果。我们通过三项实验验证了这一结论：1）二维关系推理任务；2）非线性随机网络模拟；3）真实三维神经科学数据集（辅以可解释性分析验证位置编码的准确性）。研究表明：从小范数分布初始化的可学习位置编码能够：1）在多维空间中还原与真实位置对应的可解释编码；2）在实证评估中提升下游任务泛化能力。值得注意的是，选择不当的位置编码会同时损害模型的可解释性与泛化性能。本研究证实了通过学习可识别、可解释的位置编码来增强泛化能力的可行性。
