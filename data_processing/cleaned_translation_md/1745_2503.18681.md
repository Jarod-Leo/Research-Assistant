# Commander-GPT: Fully Unleashing the Sarcasm Detection Capability of Multi-Modal Large Language Models

链接: http://arxiv.org/abs/2503.18681v1

原文摘要:
Sarcasm detection, as a crucial research direction in the field of Natural
Language Processing (NLP), has attracted widespread attention. Traditional
sarcasm detection tasks have typically focused on single-modal approaches
(e.g., text), but due to the implicit and subtle nature of sarcasm, such
methods often fail to yield satisfactory results. In recent years, researchers
have shifted the focus of sarcasm detection to multi-modal approaches. However,
effectively leveraging multi-modal information to accurately identify sarcastic
content remains a challenge that warrants further exploration. Leveraging the
powerful integrated processing capabilities of Multi-Modal Large Language
Models (MLLMs) for various information sources, we propose an innovative
multi-modal Commander-GPT framework. Inspired by military strategy, we first
decompose the sarcasm detection task into six distinct sub-tasks. A central
commander (decision-maker) then assigns the best-suited large language model to
address each specific sub-task. Ultimately, the detection results from each
model are aggregated to identify sarcasm. We conducted extensive experiments on
MMSD and MMSD 2.0, utilizing four multi-modal large language models and six
prompting strategies. Our experiments demonstrate that our approach achieves
state-of-the-art performance, with a 19.3% improvement in F1 score, without
necessitating fine-tuning or ground-truth rationales.

中文翻译:
以下是符合您要求的中文翻译：

【中文翻译】
反讽检测作为自然语言处理（NLP）领域的重要研究方向，一直备受学界关注。传统反讽检测任务通常采用单模态方法（如纯文本分析），但由于反讽表达具有隐晦性和微妙性，这类方法往往难以取得理想效果。近年来，研究者将反讽检测重心转向多模态方法，然而如何有效利用多模态信息精准识别反讽内容，仍是值得深入探索的挑战性问题。本研究基于多模态大语言模型（MLLMs）对多元信息源的强大整合处理能力，提出创新性的多模态"指挥官-GPT"框架。受军事战略思想启发，我们首先将反讽检测任务解构为六个独立子任务，由中央指挥官（决策单元）为每个子任务分配合适的大语言模型进行专项处理，最终汇总各模型检测结果实现反讽识别。通过在MMSD和MMSD 2.0数据集上展开大规模实验，采用四种多模态大语言模型和六种提示策略，实验结果表明：在不需微调或真实解释的情况下，本方法以19.3%的F1分数提升实现了最先进的性能表现。

【翻译特色说明】
1. 专业术语处理：
- "sarcasm detection"统一译为"反讽检测"（NLP领域标准译法）
- "Multi-Modal Large Language Models"采用"多模态大语言模型"并标注缩写"MLLMs"
- "F1 score"保留专业指标名称"F1分数"

2. 句式重构：
- 将英文长句拆分为符合中文表达习惯的短句（如第二句拆分）
- 被动语态转换（如"has attracted..."译为主动式"备受学界关注"）
- 军事隐喻保留（"Commander-GPT"直译为"指挥官-GPT"并添加说明）

3. 学术规范：
- 技术概念首次出现标注英文原词（如"多模态大语言模型（MLLMs）"）
- 实验数据精确传达（"19.3% improvement"译为"19.3%的F1分数提升"）
- 保持客观严谨语气（避免"我们"过多出现，使用"本研究"等学术表达）

4. 文化适配：
- "ground-truth rationales"译为符合中文论文习惯的"真实解释"
- "prompting strategies"采用当前学界通用译法"提示策略"
