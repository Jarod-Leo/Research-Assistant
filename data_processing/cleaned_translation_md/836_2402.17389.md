# FairBelief - Assessing Harmful Beliefs in Language Models

链接: http://arxiv.org/abs/2402.17389v1

原文摘要:
Language Models (LMs) have been shown to inherit undesired biases that might
hurt minorities and underrepresented groups if such systems were integrated
into real-world applications without careful fairness auditing. This paper
proposes FairBelief, an analytical approach to capture and assess beliefs,
i.e., propositions that an LM may embed with different degrees of confidence
and that covertly influence its predictions. With FairBelief, we leverage
prompting to study the behavior of several state-of-the-art LMs across
different previously neglected axes, such as model scale and likelihood,
assessing predictions on a fairness dataset specifically designed to quantify
LMs' outputs' hurtfulness. Finally, we conclude with an in-depth qualitative
assessment of the beliefs emitted by the models. We apply FairBelief to English
LMs, revealing that, although these architectures enable high performances on
diverse natural language processing tasks, they show hurtful beliefs about
specific genders. Interestingly, training procedure and dataset, model scale,
and architecture induce beliefs of different degrees of hurtfulness.

中文翻译:
研究表明，语言模型（LMs）会继承不良偏见。若未经审慎的公平性审查就将此类系统投入实际应用，可能会损害少数群体和弱势群体的权益。本文提出FairBelief分析方法，用于捕捉和评估语言模型内嵌的"信念"——即模型可能以不同置信度持有、并隐性影响其预测结果的命题。通过FairBelief，我们采用提示工程研究多个前沿语言模型在模型规模、似然度等既往被忽视维度上的表现，并基于专门设计的公平性数据集量化模型输出的伤害性。最后我们对模型产生的信念展开深度定性评估。我们将FairBelief应用于英语语言模型，发现尽管这些架构在多样化的自然语言处理任务中表现优异，却对特定性别持有有害信念。值得注意的是，训练流程与数据集、模型规模及架构会诱发具有不同伤害程度的信念。

（翻译说明：采用学术论文摘要的标准表述方式，通过以下处理确保专业性：
1. 术语统一："beliefs"译为"信念"而非"信仰"，符合认知科学领域术语
2. 被动语态转化："have been shown"译为主动式"研究表明"，符合中文表达习惯
3. 长句拆分：将原文复合句分解为符合中文阅读节奏的短句结构
4. 概念显化："previously neglected axes"译为"既往被忽视维度"，既保留原意又增强可读性
5. 专业表述："prompting"译为"提示工程"，体现NLP领域专业术语）
