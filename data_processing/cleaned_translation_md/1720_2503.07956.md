# EFPC: Towards Efficient and Flexible Prompt Compression

链接: http://arxiv.org/abs/2503.07956v1

原文摘要:
The emergence of large language models (LLMs) like GPT-4 has revolutionized
natural language processing (NLP), enabling diverse, complex tasks. However,
extensive token counts lead to high computational and financial burdens. To
address this, we propose Efficient and Flexible Prompt Compression (EFPC), a
novel method unifying task-aware and task-agnostic compression for a favorable
accuracy-efficiency trade-off. EFPC uses GPT-4 to generate compressed prompts
and integrates them with original prompts for training. During training and
inference, we selectively prepend user instructions and compress prompts based
on predicted probabilities. EFPC is highly data-efficient, achieving
significant performance with minimal data. Compared to the state-of-the-art
method LLMLingua-2, EFPC achieves a 4.8% relative improvement in F1-score with
1% additional data at a 4x compression rate, and an 11.4% gain with 10%
additional data on the LongBench single-doc QA benchmark. EFPC's unified
framework supports broad applicability and enhances performance across various
models, tasks, and domains, offering a practical advancement in NLP.

中文翻译:
以下是符合学术规范的中文翻译：

以GPT-4为代表的大语言模型（LLMs）的出现革新了自然语言处理（NLP）领域，使其能够执行多样化复杂任务。然而，过长的token序列会导致高昂的计算成本和财务负担。为此，我们提出高效灵活提示压缩方法（EFPC），这种创新框架通过统一任务感知与任务无关的压缩模式，实现了精度与效率的优化平衡。EFPC利用GPT-4生成压缩提示，并将其与原始提示融合进行训练。在训练和推理阶段，我们基于预测概率动态选择前置用户指令并执行提示压缩。该方法具有显著的数据效率优势，仅需少量数据即可实现优异性能。相比当前最先进的LLMLingua-2方法，在4倍压缩率下，EFPC仅增加1%数据量就使LongBench单文档问答基准的F1值相对提升4.8%；当数据量增加10%时，性能增益可达11.4%。EFPC的统一框架展现出广泛的适用性，能有效提升不同模型、任务及领域的性能表现，为NLP研究提供了实用化的技术推进。

（翻译说明：采用学术论文的正式语体，保留专业术语首字母缩写；将英语长句拆分为符合中文表达习惯的短句；"task-aware/task-agnostic"译为专业术语"任务感知/任务无关"；"prepend"准确译为"前置"；性能指标F1-score规范译为"F1值"；通过"动态选择"等表述准确传达原文的算法逻辑）
