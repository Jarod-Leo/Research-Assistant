# HeTraX: Energy Efficient 3D Heterogeneous Manycore Architecture for Transformer Acceleration

链接: http://arxiv.org/abs/2408.03397v1

原文摘要:
Transformers have revolutionized deep learning and generative modeling to
enable unprecedented advancements in natural language processing tasks and
beyond. However, designing hardware accelerators for executing transformer
models is challenging due to the wide variety of computing kernels involved in
the transformer architecture. Existing accelerators are either inadequate to
accelerate end-to-end transformer models or suffer notable thermal limitations.
In this paper, we propose the design of a three-dimensional heterogeneous
architecture referred to as HeTraX specifically optimized to accelerate
end-to-end transformer models. HeTraX employs hardware resources aligned with
the computational kernels of transformers and optimizes both performance and
energy. Experimental results show that HeTraX outperforms existing
state-of-the-art by up to 5.6x in speedup and improves EDP by 14.5x while
ensuring thermally feasibility.

中文翻译:
Transformer模型已经彻底改变了深度学习和生成式建模，为自然语言处理等任务带来了前所未有的进步。然而，由于Transformer架构涉及多样化的计算核心，为其设计硬件加速器面临巨大挑战。现有加速器要么无法完整加速端到端Transformer模型，要么存在显著的热效率限制。本文提出了一种名为HeTraX的三维异构架构设计方案，专门针对端到端Transformer模型进行优化。该架构通过配置与Transformer计算核心相匹配的硬件资源，在性能和能效方面实现双重优化。实验结果表明，HeTraX相比现有最优方案最高可实现5.6倍的速度提升，能量延迟积（EDP）改善达14.5倍，同时确保热设计可行性。
