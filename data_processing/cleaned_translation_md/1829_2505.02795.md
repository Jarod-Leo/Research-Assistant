# HSplitLoRA: A Heterogeneous Split Parameter-Efficient Fine-Tuning Framework for Large Language Models

链接: http://arxiv.org/abs/2505.02795v1

原文摘要:
Recently, large language models (LLMs) have achieved remarkable
breakthroughs, revolutionizing the natural language processing domain and
beyond. Due to immense parameter sizes, fine-tuning these models with private
data for diverse downstream tasks has become mainstream. Though federated
learning (FL) offers a promising solution for fine-tuning LLMs without sharing
raw data, substantial computing costs hinder its democratization. Moreover, in
real-world scenarios, private client devices often possess heterogeneous
computing resources, further complicating LLM fine-tuning. To combat these
challenges, we propose HSplitLoRA, a heterogeneous parameter-efficient
fine-tuning (PEFT) framework built on split learning (SL) and low-rank
adaptation (LoRA) fine-tuning, for efficiently fine-tuning LLMs on
heterogeneous client devices. HSplitLoRA first identifies important weights
based on their contributions to LLM training. It then dynamically configures
the decomposition ranks of LoRA adapters for selected weights and determines
the model split point according to varying computing budgets of client devices.
Finally, a noise-free adapter aggregation mechanism is devised to support
heterogeneous adapter aggregation without introducing noise. Extensive
experiments demonstrate that HSplitLoRA outperforms state-of-the-art benchmarks
in training accuracy and convergence speed.

中文翻译:
近年来，大型语言模型（LLMs）取得了突破性进展，为自然语言处理及其他领域带来了革命性变革。由于参数量庞大，利用私有数据对这些模型进行微调以适应多样化下游任务已成为主流方法。虽然联邦学习（FL）为不共享原始数据的LLM微调提供了可行方案，但高昂的计算成本阻碍了其普及应用。此外在实际场景中，私有客户端设备往往具有异构计算资源，这进一步增加了LLM微调的复杂性。为应对这些挑战，我们提出HSplitLoRA——一个基于拆分学习（SL）和低秩自适应（LoRA）微调的异构参数高效微调（PEFT）框架，可在异构客户端设备上高效微调LLMs。该框架首先根据权重对模型训练的贡献度识别重要参数，随后针对选定权重动态配置LoRA适配器的分解秩，并根据客户端设备不同的计算预算确定模型拆分点。最后设计了一种无噪声适配器聚合机制，支持异构适配器的高效聚合而不引入噪声。大量实验表明，HSplitLoRA在训练准确率和收敛速度方面均优于当前最先进的基准方法。

（翻译说明：采用学术论文的规范表述方式，通过以下处理确保专业性与可读性：
1. 技术术语统一："fine-tuning"译为"微调"，"federated learning"保留专业译名"联邦学习"
2. 长句拆分：将原文复合句分解为符合中文表达习惯的短句，如通过分号处理因果逻辑关系
3. 被动语态转化："are often possessed"转为主动式"往往具有"
4. 概念显化："democratization"译为"普及应用"以准确传达技术民主化内涵
5. 保持技术准确性：严格区分"adapter/适配器"与"aggregation/聚合"等专业表述）
