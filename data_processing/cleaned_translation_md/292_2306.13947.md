# Comparison of Pre-trained Language Models for Turkish Address Parsing

链接: http://arxiv.org/abs/2306.13947v1

原文摘要:
Transformer based pre-trained models such as BERT and its variants, which are
trained on large corpora, have demonstrated tremendous success for natural
language processing (NLP) tasks. Most of academic works are based on the
English language; however, the number of multilingual and language specific
studies increase steadily. Furthermore, several studies claimed that language
specific models outperform multilingual models in various tasks. Therefore, the
community tends to train or fine-tune the models for the language of their case
study, specifically. In this paper, we focus on Turkish maps data and
thoroughly evaluate both multilingual and Turkish based BERT, DistilBERT,
ELECTRA and RoBERTa. Besides, we also propose a MultiLayer Perceptron (MLP) for
fine-tuning BERT in addition to the standard approach of one-layer fine-tuning.
For the dataset, a mid-sized Address Parsing corpus taken with a relatively
high quality is constructed. Conducted experiments on this dataset indicate
that Turkish language specific models with MLP fine-tuning yields slightly
better results when compared to the multilingual fine-tuned models. Moreover,
visualization of address tokens' representations further indicates the
effectiveness of BERT variants for classifying a variety of addresses.

中文翻译:
基于Transformer的预训练模型（如BERT及其变体）通过在大规模语料库上的训练，已在自然语言处理（NLP）任务中展现出卓越成效。当前学术研究主要围绕英语展开，但针对多语言及特定语言的模型研究正持续增长。值得注意的是，多项研究表明特定语言模型在各类任务中的表现优于多语言模型。因此，学术界更倾向于针对研究目标语言专门训练或微调模型。本文以土耳其语地图数据为研究对象，系统评估了多语言模型与土耳其语专用模型（包括BERT、DistilBERT、ELECTRA和RoBERTa）的性能差异。此外，我们提出在标准单层微调方法基础上，引入多层感知机（MLP）对BERT进行增强微调。实验采用自建的中等规模地址解析语料库，该数据集具有较高质量。实验结果表明：经过MLP微调的土耳其语专用模型相较多语言微调模型能获得轻微性能提升。同时，地址标记表征的可视化分析进一步验证了BERT系列模型在多样化地址分类任务中的有效性。
