# ProxyLM: Predicting Language Model Performance on Multilingual Tasks via Proxy Models

链接: http://arxiv.org/abs/2406.09334v2

原文摘要:
Performance prediction is a method to estimate the performance of Language
Models (LMs) on various Natural Language Processing (NLP) tasks, mitigating
computational costs associated with model capacity and data for fine-tuning.
Our paper presents ProxyLM, a scalable task- and language-agnostic framework
designed to predict the performance of LMs using proxy models. These proxy
models act as surrogates, approximating the performance of the LM of interest.
By leveraging these proxy models, ProxyLM significantly reduces computational
overhead in task evaluations, achieving up to a 37.08x speedup over traditional
methods, even with our smallest proxy models. Our results across multiple
multilingual NLP tasks and various robustness tests demonstrate that ProxyLM
not only adapts well to previously unseen languages in pre-trained LMs, but
also generalizes effectively across different datasets, outperforming the
state-of-the-art by at least 1.78x in terms of root-mean-square error (RMSE).

中文翻译:
性能预测是一种用于评估语言模型（LMs）在各类自然语言处理（NLP）任务中表现的方法，可显著降低因模型容量和微调数据产生的计算成本。本文提出ProxyLM——一个可扩展的、与任务及语言无关的框架，通过代理模型预测目标语言模型的性能表现。这些代理模型作为替代品，能够近似模拟目标语言模型的性能。通过运用代理模型，ProxyLM在任务评估中大幅降低计算开销，即使使用最小规模的代理模型，也能实现相比传统方法最高37.08倍的加速效果。我们在多语种NLP任务和多种鲁棒性测试中的结果表明，ProxyLM不仅能良好适配预训练语言模型中未曾见过的语言，还能有效泛化至不同数据集，其均方根误差（RMSE）指标较现有最优方法至少提升1.78倍。

（翻译说明：采用技术文档的简洁风格，保持术语一致性；将英文长句拆分为符合中文表达习惯的短句；"task- and language-agnostic"译为"与任务及语言无关"准确传达其技术含义；"37.08x speedup"采用中文技术文献惯用的"37.08倍加速"表述；通过"可显著降低"等措辞保持学术文本的严谨性；最后一句通过"较现有最优方法"的对比结构清晰呈现性能优势。）
