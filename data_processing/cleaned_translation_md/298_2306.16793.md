# Benchmarking Large Language Model Capabilities for Conditional Generation

链接: http://arxiv.org/abs/2306.16793v1

原文摘要:
Pre-trained large language models (PLMs) underlie most new developments in
natural language processing. They have shifted the field from
application-specific model pipelines to a single model that is adapted to a
wide range of tasks. Autoregressive PLMs like GPT-3 or PaLM, alongside
techniques like few-shot learning, have additionally shifted the output
modality to generation instead of classification or regression. Despite their
ubiquitous use, the generation quality of language models is rarely evaluated
when these models are introduced. Additionally, it is unclear how existing
generation tasks--while they can be used to compare systems at a high
level--relate to the real world use cases for which people have been adopting
them. In this work, we discuss how to adapt existing application-specific
generation benchmarks to PLMs and provide an in-depth, empirical study of the
limitations and capabilities of PLMs in natural language generation tasks along
dimensions such as scale, architecture, input and output language. Our results
show that PLMs differ in their applicability to different data regimes and
their generalization to multiple languages and inform which PLMs to use for a
given generation task setup. We share best practices to be taken into
consideration when benchmarking generation capabilities during the development
of upcoming PLMs.

中文翻译:
以下是符合您要求的学术化中文翻译：

预训练大语言模型（PLMs）已成为自然语言处理领域大多数新进展的核心技术。它们推动该领域从针对特定应用的模型流水线转向可适配多种任务的单一模型架构。以GPT-3或PaLM为代表的自回归预训练模型，结合小样本学习等技术，进一步将输出范式从分类/回归转变为生成式输出。尽管这类模型已被广泛采用，但学界在提出新模型时鲜少系统评估其生成质量。此外，现有生成任务虽能用于高层次系统比较，但其与用户实际应用场景的关联性仍不明确。本研究探讨了如何将面向特定应用的生成基准适配到预训练模型，并围绕模型规模、架构、输入输出语言等维度，对PLMs在自然语言生成任务中的能力与局限展开深度实证分析。实验结果表明：不同PLMs在数据适应范围、多语言泛化能力方面存在显著差异，这些发现为特定生成任务场景下的模型选型提供了依据。我们同时提出了下一代PLMs开发过程中评估生成能力时需考虑的最佳实践方案。

翻译说明：
1. 专业术语处理：
- "autoregressive PLMs"译为"自回归预训练模型"符合NLP领域术语规范
- "few-shot learning"采用通用译法"小样本学习"
- "data regimes"译为"数据适应范围"以准确传达原文指代不同数据规模场景的含义

2. 句式重构：
- 将原文复合长句拆分为符合中文表达习惯的短句（如第一句拆分为两个逻辑单元）
- 被动语态转换为主动表述（如"are rarely evaluated"译为"鲜少系统评估"）
- 增译"本研究"等主语使学术表述更完整

3. 学术规范：
- 保留"PLMs"等专业缩写首次出现时的全称
- "benchmark"统一译为"基准"而非"标杆"等非学术表述
- "empirical study"译为"实证分析"符合社科研究方法术语

4. 概念对应：
- "output modality"译为"输出范式"准确反映技术特征
- "generation quality"译为"生成质量"而非直译"产生质量"
- "real world use cases"译为"实际应用场景"保持专业性与可读性平衡
