# Transformers4NewsRec: A Transformer-based News Recommendation Framework

链接: http://arxiv.org/abs/2410.13125v1

原文摘要:
Pre-trained transformer models have shown great promise in various natural
language processing tasks, including personalized news recommendations. To
harness the power of these models, we introduce Transformers4NewsRec, a new
Python framework built on the \textbf{Transformers} library. This framework is
designed to unify and compare the performance of various news recommendation
models, including deep neural networks and graph-based models.
Transformers4NewsRec offers flexibility in terms of model selection, data
preprocessing, and evaluation, allowing both quantitative and qualitative
analysis.

中文翻译:
预训练的Transformer模型在各类自然语言处理任务中展现出巨大潜力，个性化新闻推荐领域亦不例外。为充分发挥此类模型的效能，我们推出了Transformers4NewsRec——一个基于\textbf{Transformers}库构建的新型Python框架。该框架旨在统一评估各类新闻推荐模型的性能表现，涵盖深度神经网络与图结构模型等多种架构。Transformers4NewsRec在模型选择、数据预处理及评估环节均具有高度灵活性，支持定量与定性双重分析维度。

（翻译说明：采用技术文本的简洁风格，通过以下处理实现专业性与可读性平衡：
1. 术语统一："pre-trained"译为行业通用表述"预训练"，"graph-based models"译为专业术语"图结构模型"
2. 句式重构：将原文复合句拆分为符合中文表达习惯的短句，如使用破折号衔接框架定义
3. 动态对等："harness the power"意译为"发挥效能"而非字面直译
4. 专业表述："quantitative and qualitative analysis"采用学术规范译法"定量与定性分析"
5. 文化适配：保留"Python"等技术名词原称，确保专业读者准确理解）
