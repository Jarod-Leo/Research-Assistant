# Big Little Transformer Decoder

链接: http://arxiv.org/abs/2302.07863v1

原文摘要:
The recent emergence of Large Language Models based on the Transformer
architecture has enabled dramatic advancements in the field of Natural Language
Processing. However, these models have long inference latency, which limits
their deployment and makes them prohibitively expensive for various real-time
applications. The inference latency is further exacerbated by autoregressive
generative tasks, as models need to run iteratively to generate tokens
sequentially without leveraging token-level parallelization. To address this,
we propose Big Little Decoder (BiLD), a framework that can improve inference
efficiency and latency for a wide range of text generation applications. The
BiLD framework contains two models with different sizes that collaboratively
generate text. The small model runs autoregressively to generate text with a
low inference cost, and the large model is only invoked occasionally to refine
the small model's inaccurate predictions in a non-autoregressive manner. To
coordinate the small and large models, BiLD introduces two simple yet effective
policies: (1) the fallback policy that determines when to hand control over to
the large model; and (2) the rollback policy that determines when the large
model needs to correct the small model's inaccurate predictions. To evaluate
our framework across different tasks and models, we apply BiLD to various text
generation scenarios encompassing machine translation on IWSLT 2017 De-En and
WMT 2014 De-En, and summarization on XSUM and CNN/DailyMail. On an NVIDIA T4
GPU, our framework achieves a speedup of up to 2.12x speedup with minimal
generation quality degradation. Furthermore, our framework is fully
plug-and-play and can be applied without any modifications in the training
process or model architecture. Our code is open-sourced

中文翻译:
近年来，基于Transformer架构的大语言模型的出现推动了自然语言处理领域的显著进步。然而，这些模型存在推理延迟高的问题，不仅限制了其部署范围，更使得各类实时应用场景面临难以承受的计算成本。自回归生成任务进一步加剧了这一问题——由于需要迭代运行模型来逐个生成词元，无法充分利用词元级并行计算能力。为此，我们提出"大小解码器协同框架"（BiLD），该框架可显著提升多种文本生成应用的推理效率和延迟表现。BiLD框架包含两个不同规模的协作模型：小型模型以自回归方式低成本生成文本，大型模型则仅在必要时以非自回归方式介入，修正小型模型的错误预测。为实现双模型协同，BiLD引入两项简单高效的控制策略：（1）回退策略——决定何时移交控制权至大型模型；（2）回滚策略——判定大型模型需修正小型模型错误预测的时机。为验证框架的普适性，我们在IWSLT 2017德英翻译、WMT 2014德英翻译、XSUM摘要生成和CNN/DailyMail摘要生成等多个文本生成任务上测试BiLD。在NVIDIA T4 GPU上的实验表明，该框架在保证生成质量基本不变的前提下，最高可实现2.12倍的加速效果。此外，本框架具备完全即插即用的特性，无需修改训练流程或模型架构即可直接应用。相关代码已开源。
