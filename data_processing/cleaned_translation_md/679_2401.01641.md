# Towards a Foundation Purchasing Model: Pretrained Generative Autoregression on Transaction Sequences

链接: http://arxiv.org/abs/2401.01641v2

原文摘要:
Machine learning models underpin many modern financial systems for use cases
such as fraud detection and churn prediction. Most are based on supervised
learning with hand-engineered features, which relies heavily on the
availability of labelled data. Large self-supervised generative models have
shown tremendous success in natural language processing and computer vision,
yet so far they haven't been adapted to multivariate time series of financial
transactions. In this paper, we present a generative pretraining method that
can be used to obtain contextualised embeddings of financial transactions.
Benchmarks on public datasets demonstrate that it outperforms state-of-the-art
self-supervised methods on a range of downstream tasks. We additionally perform
large-scale pretraining of an embedding model using a corpus of data from 180
issuing banks containing 5.1 billion transactions and apply it to the card
fraud detection problem on hold-out datasets. The embedding model significantly
improves value detection rate at high precision thresholds and transfers well
to out-of-domain distributions.

中文翻译:
以下是符合要求的学术性中文翻译：

机器学习模型支撑着现代金融系统中诸多应用场景，如欺诈检测与客户流失预测。当前主流方法依赖于基于人工特征工程的监督学习，这类方法对标注数据的可获得性有极高要求。尽管大规模自监督生成模型已在自然语言处理与计算机视觉领域取得显著成功，但迄今为止尚未被成功应用于金融交易的多变量时间序列分析。本文提出一种生成式预训练方法，可用于获取金融交易的上下文嵌入表征。公开数据集上的基准测试表明，该方法在一系列下游任务中优于当前最先进的自监督方法。我们进一步利用来自180家发卡行的51亿笔交易数据语料库，对嵌入模型进行大规模预训练，并将其应用于保留数据集上的信用卡欺诈检测问题。实验证明该嵌入模型在高精度阈值下显著提升了价值检测率，并展现出优异的跨领域分布迁移能力。

（译文严格遵循以下规范：
1. 专业术语准确统一："self-supervised"译为"自监督"，"contextualised embeddings"译为"上下文嵌入表征"
2. 被动语态转化："are based on"处理为"依赖于"的主动句式
3. 长句拆分：将原文复合句按中文习惯分解为多个短句
4. 学术用语规范："outperforms"译为"优于"，"hold-out datasets"译为"保留数据集"
5. 数量单位转换："5.1 billion"按中文习惯译为"51亿"
6. 逻辑关系显化：通过"尽管...但"等连接词明确原文隐含的转折关系）
