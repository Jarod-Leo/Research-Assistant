# Iterative Mask Filling: An Effective Text Augmentation Method Using Masked Language Modeling

链接: http://arxiv.org/abs/2401.01830v1

原文摘要:
Data augmentation is an effective technique for improving the performance of
machine learning models. However, it has not been explored as extensively in
natural language processing (NLP) as it has in computer vision. In this paper,
we propose a novel text augmentation method that leverages the Fill-Mask
feature of the transformer-based BERT model. Our method involves iteratively
masking words in a sentence and replacing them with language model predictions.
We have tested our proposed method on various NLP tasks and found it to be
effective in many cases. Our results are presented along with a comparison to
existing augmentation methods. Experimental results show that our proposed
method significantly improves performance, especially on topic classification
datasets.

中文翻译:
数据增强是提升机器学习模型性能的有效技术。然而在自然语言处理（NLP）领域，这项技术的探索程度远不及计算机视觉领域。本文提出了一种基于Transformer架构BERT模型"填充掩码"特性的新型文本增强方法。该方法通过迭代式地遮蔽句子中的词语，并利用语言模型的预测结果进行替换填充。我们在多个NLP任务上测试了该方法的有效性，实验结果表明其在多数场景下均表现优异。研究不仅展示了与现有增强方法的对比分析，更通过实验数据证实：所提出的方法能显著提升模型性能，尤其在主题分类数据集上效果尤为突出。
