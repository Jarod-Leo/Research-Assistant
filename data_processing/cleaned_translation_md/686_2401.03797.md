# Anatomy of Neural Language Models

链接: http://arxiv.org/abs/2401.03797v1

原文摘要:
The fields of generative AI and transfer learning have experienced remarkable
advancements in recent years especially in the domain of Natural Language
Processing (NLP). Transformers have been at the heart of these advancements
where the cutting-edge transformer-based Language Models (LMs) have led to new
state-of-the-art results in a wide spectrum of applications. While the number
of research works involving neural LMs is exponentially increasing, their vast
majority are high-level and far from self-contained. Consequently, a deep
understanding of the literature in this area is a tough task especially in the
absence of a unified mathematical framework explaining the main types of neural
LMs. We address the aforementioned problem in this tutorial where the objective
is to explain neural LMs in a detailed, simplified and unambiguous mathematical
framework accompanied by clear graphical illustrations. Concrete examples on
widely used models like BERT and GPT2 are explored. Finally, since transformers
pretrained on language-modeling-like tasks have been widely adopted in computer
vision and time series applications, we briefly explore some examples of such
solutions in order to enable readers to understand how transformers work in the
aforementioned domains and compare this use with the original one in NLP.

中文翻译:
近年来，生成式人工智能与迁移学习领域——尤其是自然语言处理（NLP）方向——取得了显著进展。Transformer模型成为这些突破的核心技术，基于Transformer的前沿语言模型（LM）在广泛的应用场景中创造了最新性能标杆。尽管涉及神经语言模型的研究数量呈指数级增长，但绝大多数研究停留在高阶层面且缺乏自洽性。因此，在缺乏统一数学框架来解释主要神经语言模型类型的情况下，要深入理解该领域文献具有相当难度。本教程针对上述问题，旨在通过清晰简明的数学框架配以直观图示，对神经语言模型进行详尽而通俗的阐释。我们以BERT、GPT2等广泛使用的模型作为具体案例展开分析。最后，鉴于基于类语言建模任务预训练的Transformer模型已广泛应用于计算机视觉和时间序列领域，我们简要探讨了若干跨领域应用案例，以帮助读者理解Transformer在这些场景中的运作机制，并与NLP领域的原始应用进行对比分析。

（翻译说明：
1. 专业术语处理：采用"Transformer/神经语言模型"等学界通用译法，保留"BERT/GPT2"等专有名词原称
2. 长句拆分：将原文复合长句分解为符合中文表达习惯的短句，如将"where"引导的定语从句转为独立分句
3. 被动语态转换："have been widely adopted"译为主动式"已广泛应用"
4. 概念显化："high-level"具体化为"停留在高阶层面"，"self-contained"译为"缺乏自洽性"
5. 逻辑衔接：通过"鉴于/最后"等连接词保持论证脉络清晰
6. 学术风格保持：使用"旨在/阐释/运作机制"等符合学术文本特征的表述）
