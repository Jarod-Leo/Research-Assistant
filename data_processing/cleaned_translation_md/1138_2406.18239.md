# Zero-shot prompt-based classification: topic labeling in times of foundation models in German Tweets

链接: http://arxiv.org/abs/2406.18239v1

原文摘要:
Filtering and annotating textual data are routine tasks in many areas, like
social media or news analytics. Automating these tasks allows to scale the
analyses wrt. speed and breadth of content covered and decreases the manual
effort required. Due to technical advancements in Natural Language Processing,
specifically the success of large foundation models, a new tool for automating
such annotation processes by using a text-to-text interface given written
guidelines without providing training samples has become available.
  In this work, we assess these advancements in-the-wild by empirically testing
them in an annotation task on German Twitter data about social and political
European crises. We compare the prompt-based results with our human annotation
and preceding classification approaches, including Naive Bayes and a BERT-based
fine-tuning/domain adaptation pipeline. Our results show that the prompt-based
approach - despite being limited by local computation resources during the
model selection - is comparable with the fine-tuned BERT but without any
annotated training data. Our findings emphasize the ongoing paradigm shift in
the NLP landscape, i.e., the unification of downstream tasks and elimination of
the need for pre-labeled training data.

中文翻译:
在许多领域（如社交媒体或新闻分析）中，文本数据的过滤与标注是常规工作。自动化这些任务能提升分析效率，扩展内容覆盖广度，同时减少人工投入。随着自然语言处理技术的进步——特别是大型基础模型取得的成功——如今可通过文本到文本交互界面，仅依据书面指导（无需提供训练样本）来实现标注流程的自动化。

本研究通过实证测试评估了这项技术在现实场景中的应用效果：我们对涉及欧洲社会政治危机的德语推特数据进行了标注任务实验。我们将基于提示词的方法与人工标注结果及传统分类方法（包括朴素贝叶斯和基于BERT的微调/领域适应流程）进行对比。结果显示：尽管模型选择过程受限于本地计算资源，基于提示词的方法在无需任何标注训练数据的情况下，其表现仍可与微调后的BERT模型相媲美。这一发现印证了自然语言处理领域正在发生的范式转变——即下游任务的统一化，以及对预标注训练数据需求的消除。
