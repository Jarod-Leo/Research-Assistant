# Research on Key Technologies for Cross-Cloud Federated Training of Large Language Models

链接: http://arxiv.org/abs/2410.19130v1

原文摘要:
With the rapid development of natural language processing technology, large
language models have demonstrated exceptional performance in various
application scenarios. However, training these models requires significant
computational resources and data processing capabilities. Cross-cloud federated
training offers a new approach to addressing the resource bottlenecks of a
single cloud platform, allowing the computational resources of multiple clouds
to collaboratively complete the training tasks of large models. This study
analyzes the key technologies of cross-cloud federated training, including data
partitioning and distribution, communication optimization, model aggregation
algorithms, and the compatibility of heterogeneous cloud platforms.
Additionally, the study examines data security and privacy protection
strategies in cross-cloud training, particularly the application of data
encryption and differential privacy techniques. Through experimental
validation, the proposed technical framework demonstrates enhanced training
efficiency, ensured data security, and reduced training costs, highlighting the
broad application prospects of cross-cloud federated training.

中文翻译:
随着自然语言处理技术的快速发展，大型语言模型在多种应用场景中展现出卓越性能。然而训练这类模型需要消耗大量计算资源和数据处理能力。跨云联邦训练为突破单一云平台的资源瓶颈提供了新思路，通过整合多个云平台的计算资源协同完成大模型训练任务。本研究系统分析了跨云联邦训练中的关键技术，包括数据分片与分发、通信优化、模型聚合算法以及异构云平台兼容性等问题，同时探讨了跨云训练中的数据安全与隐私保护策略，特别是数据加密和差分隐私技术的应用。实验验证表明，所提出的技术框架能有效提升训练效率、保障数据安全并降低训练成本，展现了跨云联邦训练广阔的应用前景。

（译文说明：采用学术论文摘要的规范表达方式，通过以下处理实现专业性与可读性的平衡：
1. 技术术语统一："federated training"译为"联邦训练"符合机器学习领域规范
2. 长句拆分：将原文复合句重组为符合中文表达习惯的短句结构
3. 被动语态转化："have demonstrated"等英文被动结构转换为中文主动表述
4. 概念显化处理："resource bottlenecks"意译为"资源瓶颈"而非直译
5. 逻辑连接强化：添加"特别是"等衔接词保持论证连贯性
6. 专业表述保留："差分隐私"等术语直接采用学界通用译法）
