# A distributional simplicity bias in the learning dynamics of transformers

链接: http://arxiv.org/abs/2410.19637v1

原文摘要:
The remarkable capability of over-parameterised neural networks to generalise
effectively has been explained by invoking a ``simplicity bias'': neural
networks prevent overfitting by initially learning simple classifiers before
progressing to more complex, non-linear functions. While simplicity biases have
been described theoretically and experimentally in feed-forward networks for
supervised learning, the extent to which they also explain the remarkable
success of transformers trained with self-supervised techniques remains
unclear. In our study, we demonstrate that transformers, trained on natural
language data, also display a simplicity bias. Specifically, they sequentially
learn many-body interactions among input tokens, reaching a saturation point in
the prediction error for low-degree interactions while continuing to learn
high-degree interactions. To conduct this analysis, we develop a procedure to
generate \textit{clones} of a given natural language data set, which rigorously
capture the interactions between tokens up to a specified order. This approach
opens up the possibilities of studying how interactions of different orders in
the data affect learning, in natural language processing and beyond.

中文翻译:
以下为英文论文摘要的中文翻译：

过参数化神经网络之所以展现出卓越的泛化能力，学界常以"简洁性偏好"来解释：神经网络通过先学习简单分类器，再逐步掌握更复杂的非线性函数，从而避免过拟合。虽然前馈网络在监督学习中的简洁性偏好已得到理论与实验验证，但这种现象是否能同样解释基于自监督技术训练的Transformer模型的巨大成功仍不明确。我们的研究表明，在自然语言数据上训练的Transformer同样表现出简洁性偏好——它们会依次学习输入标记之间的多体交互作用：当低阶交互的预测误差达到饱和时，模型仍在持续学习高阶交互。为开展分析，我们开发了一种生成自然语言数据集克隆样本的方法，该方法能严格捕获指定阶数内的标记交互关系。这一技术路径为研究自然语言处理及其他领域中，不同阶数的数据交互如何影响学习过程开辟了新途径。

（翻译说明：
1. 专业术语处理："over-parameterised"译为"过参数化"，"simplicity bias"译为"简洁性偏好"，"transformers"保留英文原名
2. 长句拆分：将原文复合句按中文表达习惯分解为多个短句，如使用破折号衔接解释性内容
3. 被动语态转化："has been explained"转为主动句式"学界常以...来解释"
4. 概念显化："many-body interactions"译为"多体交互作用"，通过注释性翻译明确其数学含义
5. 学术风格保持：使用"学界"、"研究表明"等符合学术论文表达的措辞
6. 技术术语统一："tokens"始终译为"标记"，"non-linear functions"译为"非线性函数"）
