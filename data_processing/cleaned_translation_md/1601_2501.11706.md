# Trustformer: A Trusted Federated Transformer

链接: http://arxiv.org/abs/2501.11706v1

原文摘要:
Transformers, a cornerstone of deep-learning architectures for sequential
data, have achieved state-of-the-art results in tasks like Natural Language
Processing (NLP). Models such as BERT and GPT-3 exemplify their success and
have driven the rise of large language models (LLMs). However, a critical
challenge persists: safeguarding the privacy of data used in LLM training.
Privacy-preserving techniques like Federated Learning (FL) offer potential
solutions, but practical limitations hinder their effectiveness for Transformer
training. Two primary issues are (I) the risk of sensitive information leakage
due to aggregation methods like FedAvg or FedSGD, and (II) the high
communication overhead caused by the large size of Transformer models.
  This paper introduces a novel FL method that reduces communication overhead
while maintaining competitive utility. Our approach avoids sharing full model
weights by simulating a global model locally. We apply k-means clustering to
each Transformer layer, compute centroids locally, and transmit only these
centroids to the server instead of full weights or gradients. To enhance
security, we leverage Intel SGX for secure transmission of centroids. Evaluated
on a translation task, our method achieves utility comparable to
state-of-the-art baselines while significantly reducing communication costs.
This provides a more efficient and privacy-preserving FL solution for
Transformer models.

中文翻译:
以下是符合学术规范的中文翻译：

Transformer作为序列数据深度学习架构的基石，已在自然语言处理（NLP）等任务中取得最先进的性能表现。BERT和GPT-3等模型不仅验证了其成功，更推动了大语言模型（LLM）的兴起。然而，一个关键挑战始终存在：如何保障LLM训练数据的隐私安全。联邦学习（FL）等隐私保护技术虽能提供潜在解决方案，但其在Transformer训练中的实际应用仍存在局限，主要体现在两大核心问题：（I）FedAvg或FedSGD等聚合方法可能导致敏感信息泄露；（II）Transformer模型的庞大规模会引发高昂的通信开销。

本文提出一种新型联邦学习方法，在保持模型性能竞争力的同时显著降低通信开销。我们的方法通过本地模拟全局模型，避免共享完整模型参数：首先对每个Transformer层进行k-means聚类，本地计算聚类中心后，仅向服务器传输这些中心点而非完整权重或梯度。为增强安全性，我们采用Intel SGX技术实现中心点的安全传输。在翻译任务上的实验表明，该方法在通信成本显著降低的同时，仍能达到与最先进基线相当的模型性能，从而为Transformer模型提供了更高效且隐私保护的联邦学习解决方案。


2. 被动语态转换为中文主动表述（如"have driven"译为"推动"）
3. 长难句拆分重组（如原文最后两句合并为符合中文表达习惯的复合句）
4. 技术概念准确传达（如"centroids"译为"聚类中心"而非字面直译）
5. 保持学术文本的客观性与简洁性）
