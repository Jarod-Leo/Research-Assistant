# How Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities of Large Language Models

链接: http://arxiv.org/abs/2408.16756v1

原文摘要:
The rapid evolution of large language models (LLMs) has transformed the
competitive landscape in natural language processing (NLP), particularly for
English and other data-rich languages. However, underrepresented languages like
Cantonese, spoken by over 85 million people, face significant development gaps,
which is particularly concerning given the economic significance of the
Guangdong-Hong Kong-Macau Greater Bay Area, and in substantial
Cantonese-speaking populations in places like Singapore and North America.
Despite its wide use, Cantonese has scant representation in NLP research,
especially compared to other languages from similarly developed regions. To
bridge these gaps, we outline current Cantonese NLP methods and introduce new
benchmarks designed to evaluate LLM performance in factual generation,
mathematical logic, complex reasoning, and general knowledge in Cantonese,
which aim to advance open-source Cantonese LLM technology. We also propose
future research directions and recommended models to enhance Cantonese LLM
development.

中文翻译:
大型语言模型（LLMs）的快速发展重塑了自然语言处理（NLP）领域的竞争格局，尤其对英语等数据资源丰富的语言影响显著。然而，以粤语为代表的弱势语言（使用人口超8500万）却面临显著的发展断层——这一现象在粤港澳大湾区等经济枢纽区域，以及新加坡、北美等粤语社区集中的地区尤为值得关注。尽管使用广泛，粤语在NLP研究中的代表性严重不足，其发展水平与同等经济发达地区的语言相比存在明显差距。为弥合这些鸿沟，本文系统梳理了当前粤语NLP技术现状，并创新性地提出了一套评估LLMs粤语能力的基准测试体系，涵盖事实陈述、数理逻辑、复杂推理和常识认知等维度，旨在推动开源粤语大模型技术的进步。此外，我们提出了未来重点研究方向及推荐模型架构，为粤语大模型的可持续发展提供建设性路径。

（翻译说明：采用学术论文摘要的规范表述，通过以下处理实现专业性与可读性的平衡：
1. 术语统一："underrepresented languages"译为"弱势语言"符合语言学领域表述
2. 数据呈现：将"over 85 million"转换为"超8500万"符合中文计量习惯
3. 长句拆分：将原文复合句分解为多个短句，保留逻辑关系的同时符合中文表达习惯
4. 概念显化："Greater Bay Area"补充译为"粤港澳大湾区"确保读者认知准确
5. 动态对等："benchmarks designed to evaluate"转译为"评估...的基准测试体系"实现功能对等）
