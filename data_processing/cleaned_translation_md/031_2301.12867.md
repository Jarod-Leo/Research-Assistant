# Exploring AI Ethics of ChatGPT: A Diagnostic Analysis

链接: http://arxiv.org/abs/2301.12867v1

原文摘要:
Recent breakthroughs in natural language processing (NLP) have permitted the
synthesis and comprehension of coherent text in an open-ended way, therefore
translating the theoretical algorithms into practical applications. The large
language models (LLMs) have significantly impacted businesses such as report
summarization software and copywriters. Observations indicate, however, that
LLMs may exhibit social prejudice and toxicity, posing ethical and societal
dangers of consequences resulting from irresponsibility. Large-scale benchmarks
for accountable LLMs should consequently be developed. Although several
empirical investigations reveal the existence of a few ethical difficulties in
advanced LLMs, there is little systematic examination and user study of the
risks and harmful behaviors of current LLM usage. To further educate future
efforts on constructing ethical LLMs responsibly, we perform a qualitative
research method called ``red teaming'' on OpenAI's ChatGPT\footnote{In this
paper, ChatGPT refers to the version released on Dec 15th.} to better
understand the practical features of ethical dangers in recent LLMs. We analyze
ChatGPT comprehensively from four perspectives: 1) \textit{Bias} 2)
\textit{Reliability} 3) \textit{Robustness} 4) \textit{Toxicity}. In accordance
with our stated viewpoints, we empirically benchmark ChatGPT on multiple sample
datasets. We find that a significant number of ethical risks cannot be
addressed by existing benchmarks, and hence illustrate them via additional case
studies. In addition, we examine the implications of our findings on AI ethics
and harmal behaviors of ChatGPT, as well as future problems and practical
design considerations for responsible LLMs. We believe that our findings may
give light on future efforts to determine and mitigate the ethical hazards
posed by machines in LLM applications.

中文翻译:
自然语言处理（NLP）领域的最新突破使得开放式连贯文本的生成与理解成为可能，从而将理论算法转化为实际应用。大型语言模型（LLM）已显著影响了报告摘要软件、文案撰写等商业领域。然而观察表明，LLM可能表现出社会偏见与毒性，因不负责任的应用而引发伦理与社会风险。为此，亟需建立针对可信LLM的大规模评估基准。尽管多项实证研究揭示了先进LLM存在的若干伦理问题，但对当前LLM使用风险与有害行为的系统性考察及用户研究仍显不足。

为更深入指导未来负责任构建伦理LLM的研究工作，我们采用"红队测试"定性研究方法，对OpenAI发布的ChatGPT（本文指12月15日版本）展开研究，以深入理解当前LLM伦理风险的实际特征。我们从四个维度对ChatGPT进行全面分析：1）偏见性 2）可靠性 3）鲁棒性 4）毒性。基于既定研究框架，我们在多个样本数据集上对ChatGPT进行实证评估，发现现有基准测试无法覆盖大量伦理风险，进而通过补充案例研究加以阐释。此外，我们探讨了研究发现对AI伦理的启示、ChatGPT的有害行为表现，以及未来构建负责任LLM面临的问题与实践设计考量。我们相信，本研究可为未来识别和缓解LLM应用中机器伦理风险的相关工作提供重要启示。


