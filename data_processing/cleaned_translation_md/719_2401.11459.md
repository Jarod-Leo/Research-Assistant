# AttentionLego: An Open-Source Building Block For Spatially-Scalable Large Language Model Accelerator With Processing-In-Memory Technology

链接: http://arxiv.org/abs/2401.11459v1

原文摘要:
Large language models (LLMs) with Transformer architectures have become
phenomenal in natural language processing, multimodal generative artificial
intelligence, and agent-oriented artificial intelligence. The self-attention
module is the most dominating sub-structure inside Transformer-based LLMs.
Computation using general-purpose graphics processing units (GPUs) inflicts
reckless demand for I/O bandwidth for transferring intermediate calculation
results between memories and processing units. To tackle this challenge, this
work develops a fully customized vanilla self-attention accelerator,
AttentionLego, as the basic building block for constructing spatially
expandable LLM processors. AttentionLego provides basic implementation with
fully-customized digital logic incorporating Processing-In-Memory (PIM)
technology. It is based on PIM-based matrix-vector multiplication and look-up
table-based Softmax design. The open-source code is available online:
https://bonany.cc/attentionleg.

中文翻译:
基于Transformer架构的大语言模型（LLMs）已在自然语言处理、多模态生成式人工智能以及面向智能体的人工智能领域取得突破性进展。其中自注意力模块是Transformer类大语言模型中占据主导地位的子结构。使用通用图形处理器（GPU）进行计算时，需要在存储单元与处理单元之间频繁传输中间计算结果，这对I/O带宽提出了极高要求。为应对这一挑战，本研究开发了一款全定制化基础自注意力加速器AttentionLego，作为构建可空间扩展大语言处理器的基本单元。该加速器采用存内计算（PIM）技术实现全定制数字逻辑设计，核心架构包含基于PIM技术的矩阵-向量乘法器以及基于查找表的Softmax计算模块。项目开源代码已发布：https://bonany.cc/attentionleg。

（翻译说明：
1. 专业术语处理："Transformer architectures"保留技术特征译为"Transformer架构"，"self-attention module"译为专业术语"自注意力模块"
2. 长句拆分：将原文复合长句拆分为符合中文表达习惯的短句，如将GPU计算部分拆分为因果关系的两个分句
3. 被动语态转换："are employed"等英文被动结构转换为中文主动表述
4. 概念显化："phenomenal"根据语境译为"突破性进展"而非字面义"现象级的"
5. 技术表述准确："Processing-In-Memory"采用学界通用译名"存内计算"，"look-up table"译为"查找表"
6. 机构名称规范："general-purpose graphics processing units"采用行业标准译名"通用图形处理器"）
