# LTNER: Large Language Model Tagging for Named Entity Recognition with Contextualized Entity Marking

链接: http://arxiv.org/abs/2404.05624v1

原文摘要:
The use of LLMs for natural language processing has become a popular trend in
the past two years, driven by their formidable capacity for context
comprehension and learning, which has inspired a wave of research from
academics and industry professionals. However, for certain NLP tasks, such as
NER, the performance of LLMs still falls short when compared to supervised
learning methods. In our research, we developed a NER processing framework
called LTNER that incorporates a revolutionary Contextualized Entity Marking
Gen Method. By leveraging the cost-effective GPT-3.5 coupled with context
learning that does not require additional training, we significantly improved
the accuracy of LLMs in handling NER tasks. The F1 score on the CoNLL03 dataset
increased from the initial 85.9% to 91.9%, approaching the performance of
supervised fine-tuning. This outcome has led to a deeper understanding of the
potential of LLMs.

中文翻译:
过去两年间，大语言模型（LLM）在自然语言处理领域的应用已成为显著趋势。凭借其强大的上下文理解与学习能力，LLM激发了学术界与产业界的研究热潮。然而对于命名实体识别（NER）等特定任务，LLM的表现仍逊色于监督学习方法。本研究提出名为LTNER的创新处理框架，其核心是突破性的"语境化实体标记生成方法"。通过采用高性价比的GPT-3.5模型，结合无需额外训练的上下文学习策略，我们显著提升了LLM处理NER任务的准确率——在CoNLL03数据集上的F1值从初始的85.9%提升至91.9%，已接近监督微调的性能水平。这一成果使我们对LLM的潜力有了更深刻的认识。

（翻译说明：
1. 专业术语采用中文领域通用译法，如"LLMs"译作"大语言模型"，"NER"保留英文缩写并补充中文全称
2. 被动语态转换为中文主动句式，如"has been driven by"处理为"凭借"
3. 长难句拆分重组，如原文第二句拆分为两个中文短句，符合汉语表达习惯
4. 关键方法名称采用引号标注突出，如"Contextualized Entity Marking Gen Method"
5. 数据呈现保留原始格式，百分比与小数点与原文严格对应
6. 学术用语保持严谨性，如"supervised fine-tuning"译为专业术语"监督微调"）
