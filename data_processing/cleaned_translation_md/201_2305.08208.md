# Learning to Generalize for Cross-domain QA

链接: http://arxiv.org/abs/2305.08208v1

原文摘要:
There have been growing concerns regarding the out-of-domain generalization
ability of natural language processing (NLP) models, particularly in
question-answering (QA) tasks. Current synthesized data augmentation methods
for QA are hampered by increased training costs. To address this issue, we
propose a novel approach that combines prompting methods and linear probing
then fine-tuning strategy, which does not entail additional cost. Our method
has been theoretically and empirically shown to be effective in enhancing the
generalization ability of both generative and discriminative models. Our
approach outperforms state-of-the-art baselines, with an average increase in F1
score of 4.5%-7.9%. Furthermore, our method can be easily integrated into any
pre-trained models and offers a promising solution to the under-explored
cross-domain QA task. We release our source code at GitHub*.

中文翻译:
近年来，自然语言处理（NLP）模型在领域外泛化能力上的表现日益引发关注，尤其在问答（QA）任务中更为突出。当前针对问答任务的合成数据增强方法往往受限于高昂的训练成本。为此，我们提出了一种创新性解决方案：通过结合提示学习（prompting）方法与线性探测-微调（linear probing then fine-tuning）策略，在不增加额外成本的前提下提升模型性能。理论与实验证明，该方法能有效增强生成式与判别式模型的泛化能力，其F1分数平均提升幅度达4.5%-7.9%，显著优于现有最优基线模型。此外，本方法可便捷地集成至各类预训练模型，为尚未充分探索的跨领域问答任务提供了可行方案。相关源代码已发布于GitHub*平台。


