# Design Proteins Using Large Language Models: Enhancements and Comparative Analyses

链接: http://arxiv.org/abs/2408.06396v1

原文摘要:
Pre-trained LLMs have demonstrated substantial capabilities across a range of
conventional natural language processing (NLP) tasks, such as summarization and
entity recognition. In this paper, we explore the application of LLMs in the
generation of high-quality protein sequences. Specifically, we adopt a suite of
pre-trained LLMs, including Mistral-7B1, Llama-2-7B2, Llama-3-8B3, and
gemma-7B4, to produce valid protein sequences. All of these models are publicly
available.5 Unlike previous work in this field, our approach utilizes a
relatively small dataset comprising 42,000 distinct human protein sequences. We
retrain these models to process protein-related data, ensuring the generation
of biologically feasible protein structures. Our findings demonstrate that even
with limited data, the adapted models exhibit efficiency comparable to
established protein-focused models such as ProGen varieties, ProtGPT2, and
ProLLaMA, which were trained on millions of protein sequences. To validate and
quantify the performance of our models, we conduct comparative analyses
employing standard metrics such as pLDDT, RMSD, TM-score, and REU. Furthermore,
we commit to making the trained versions of all four models publicly available,
fostering greater transparency and collaboration in the field of computational
biology.

中文翻译:
以下是符合您要求的中文翻译：

预训练大语言模型（LLMs）已在摘要生成、实体识别等传统自然语言处理（NLP）任务中展现出强大能力。本文探索了LLMs在高质量蛋白质序列生成中的应用：我们采用包括Mistral-7B、Llama-2-7B、Llama-3-8B和gemma-7B在内的一系列开源预训练模型（均可公开获取）来生成有效蛋白质序列。与既往研究不同，我们的方法仅使用包含42,000条人类蛋白质序列的相对小型数据集，通过模型重训练使其能够处理蛋白质数据并确保生成具有生物可行性的蛋白质结构。实验表明，即使在有限数据条件下，这些适配后的模型仍能表现出与ProGen系列、ProtGPT2和ProLLaMA等专业蛋白质模型相当的效能——后者均需数百万条序列进行训练。为量化评估模型性能，我们采用pLDDT、RMSD、TM-score和REU等标准指标进行对比分析。所有四个训练完成的模型都将公开共享，以促进计算生物学领域的透明化研究与合作。

（翻译严格遵循以下技术规范：
1. 专业术语统一处理（如LLMs不译、pLDDT等指标保留原名）
2. 被动语态转化（"are publicly available"→"可公开获取"）
3. 长句拆分重组（将原文复合句按中文表达习惯分解）
4. 补充逻辑连接词（"通过"、"使其"等）
5. 学术风格保持（使用"既往研究"、"实验表明"等规范表述）
6. 括号注释处理（模型版本号保留原文标注格式）
7. 数值精确转换（42,000→42,000条））
