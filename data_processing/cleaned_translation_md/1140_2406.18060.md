# AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning

链接: http://arxiv.org/abs/2406.18060v1

原文摘要:
Fine-tuning large language models (LLMs) has achieved remarkable performance
across various natural language processing tasks, yet it demands more and more
memory as model sizes keep growing. To address this issue, the recently
proposed Memory-efficient Zeroth-order (MeZO) methods attempt to fine-tune LLMs
using only forward passes, thereby avoiding the need for a backpropagation
graph. However, significant performance drops and a high risk of divergence
have limited their widespread adoption. In this paper, we propose the Adaptive
Zeroth-order Tensor-Train Adaption (AdaZeta) framework, specifically designed
to improve the performance and convergence of the ZO methods. To enhance
dimension-dependent ZO estimation accuracy, we introduce a fast-forward,
low-parameter tensorized adapter. To tackle the frequently observed divergence
issue in large-scale ZO fine-tuning tasks, we propose an adaptive query number
schedule that guarantees convergence. Detailed theoretical analysis and
extensive experimental results on Roberta-Large and Llama-2-7B models
substantiate the efficacy of our AdaZeta framework in terms of accuracy, memory
efficiency, and convergence speed.

中文翻译:
以下是符合要求的学术中文翻译：

微调大型语言模型（LLMs）在各类自然语言处理任务中展现出卓越性能，但随着模型规模持续扩大，其内存需求也不断攀升。针对该问题，近期提出的内存高效零阶优化方法（MeZO）尝试仅通过前向传播实现LLMs微调，从而避免反向传播计算图的构建。然而，该方法存在显著的性能下降与高发散风险，限制了其广泛应用。本文提出自适应零阶张量链微调框架（AdaZeta），专门用于提升零阶方法的性能与收敛性：首先通过快速前向传播的低参数量张量化适配器提升维度相关的零阶估计精度；其次针对大规模零阶微调中常见的发散问题，设计能保证收敛的自适应查询次数调度机制。基于Roberta-Large和Llama-2-7B模型的理论分析与大量实验证明，AdaZeta框架在准确率、内存效率及收敛速度方面均具有显著优势。

（翻译说明：
1. 专业术语统一处理：LLMs统一译为"大型语言模型"，ZO对应"零阶"，tensor-train译为专业术语"张量链"
2. 长句拆分重构：将原文复合长句按中文表达习惯拆分为多个短句，如理论分析部分重组为因果逻辑链
3. 被动语态转化："has been limited"等被动式转为主动式"限制了"
4. 学术规范：保留Roberta-Large/Llama-2-7B等模型名称原貌，技术概念首次出现标注英文缩写
5. 逻辑显化：通过"首先/其次"等连接词明确论文创新点的递进关系）
