# Anonymization by Design of Language Modeling

链接: http://arxiv.org/abs/2501.02407v1

原文摘要:
Rapid advances in Natural Language Processing (NLP) have revolutionized many
fields, including healthcare. However, these advances raise significant privacy
concerns, especially when pre-trained models fine-tuned and specialized on
sensitive data can memorize and then expose and regurgitate personal
information. This paper presents a privacy-preserving language modeling
approach to address the problem of language models anonymization, and thus
promote their sharing. Specifically, we propose both a Masking Language
Modeling (MLM) methodology to specialize a BERT-like language model, and a
Causal Language Modeling (CLM) methodology to specialize a GPT-like model that
avoids the model from memorizing direct and indirect identifying information
present in the training data. We have comprehensively evaluated our approaches
using a medical dataset and compared them against different baselines. Our
results indicate that by avoiding memorizing both direct and indirect
identifiers during model specialization, our masking and causal language
modeling schemes offer a good tradeoff for maintaining high privacy while
retaining high utility.

中文翻译:
自然语言处理（NLP）技术的快速发展为医疗健康等诸多领域带来了革命性变革。然而这些进步也引发了重大隐私隐忧——当基于敏感数据微调定制的预训练模型可能记忆并泄露个人身份信息时尤为突出。本文提出一种隐私保护语言建模方法，通过解决语言模型匿名化问题来促进模型共享。具体而言，我们既设计了用于定制BERT类模型的掩码语言建模（MLM）方案，又开发了适用于GPT类模型的因果语言建模（CLM）方案，二者均能防止模型记忆训练数据中的直接与间接身份标识信息。我们采用医疗数据集对方法进行全面评估，并与多种基线模型对比。实验结果表明：通过在模型定制过程中规避对直接/间接标识符的记忆，我们的掩码与因果语言建模方案在保持高实用性的同时，实现了隐私保护与模型效用的最佳平衡。

（翻译说明：采用学术论文摘要的标准表述方式，通过以下处理确保专业性：
1. 技术术语准确对应（如"fine-tuned"译为"微调"、"memorize"译为"记忆"）
2. 被动语态转换为中文主动句式（如"have been evaluated"译为"我们...评估"）
3. 长难句拆分重组（如将原文复合从句拆分为多个短句）
4. 概念显化处理（如"regurgitate"译为"泄露"而非字面直译）
5. 保持学术文本的客观严谨性，同时符合中文科技论文摘要的简洁特征）
