# $γ-$MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models

链接: http://arxiv.org/abs/2410.13859v1

原文摘要:
Despite the significant progress in multimodal large language models (MLLMs),
their high computational cost remains a barrier to real-world deployment.
Inspired by the mixture of depths (MoDs) in natural language processing, we aim
to address this limitation from the perspective of ``activated tokens''. Our
key insight is that if most tokens are redundant for the layer computation,
then can be skipped directly via the MoD layer. However, directly converting
the dense layers of MLLMs to MoD layers leads to substantial performance
degradation. To address this issue, we propose an innovative MoD adaptation
strategy for existing MLLMs called $\gamma$-MoD. In $\gamma$-MoD, a novel
metric is proposed to guide the deployment of MoDs in the MLLM, namely rank of
attention maps (ARank). Through ARank, we can effectively identify which layer
is redundant and should be replaced with the MoD layer. Based on ARank, we
further propose two novel designs to maximize the computational sparsity of
MLLM while maintaining its performance, namely shared vision-language router
and masked routing learning. With these designs, more than 90% dense layers of
the MLLM can be effectively converted to the MoD ones. To validate our method,
we apply it to three popular MLLMs, and conduct extensive experiments on 9
benchmark datasets. Experimental results not only validate the significant
efficiency benefit of $\gamma$-MoD to existing MLLMs but also confirm its
generalization ability on various MLLMs. For example, with a minor performance
drop, i.e., -1.5%, $\gamma$-MoD can reduce the training and inference time of
LLaVA-HR by 31.0% and 53.2%, respectively.

中文翻译:
尽管多模态大语言模型（MLLMs）已取得显著进展，但其高昂的计算成本仍是实际部署的主要障碍。受自然语言处理中混合深度（MoD）方法的启发，我们尝试从"激活令牌"的视角突破这一限制。核心思路是：若多数令牌对层级计算具有冗余性，则可通过MoD层直接跳过。然而直接将MLLMs的稠密层转换为MoD层会导致性能显著下降。

为解决该问题，我们提出创新性的$\gamma$-MoD适配策略，包含两项关键设计：首先提出注意力图秩次（ARank）作为指导指标，有效识别应替换为MoD层的冗余层级；其次开发了共享视觉-语言路由器和掩码路由学习机制，在保持模型性能的同时最大化计算稀疏度。经此改造，可成功将90%以上的稠密层转换为MoD层。

我们在三种主流MLLMs和9个基准数据集上进行验证实验。结果表明：$\gamma$-MoD不仅能显著提升效率（如LLaVA-HR模型的训练和推理时间分别降低31.0%和53.2%），仅产生1.5%的性能损失；还展现出优异的跨模型泛化能力。
