# Offline Energy-Optimal LLM Serving: Workload-Based Energy Models for LLM Inference on Heterogeneous Systems

链接: http://arxiv.org/abs/2407.04014v1

原文摘要:
The rapid adoption of large language models (LLMs) has led to significant
advances in natural language processing and text generation. However, the
energy consumed through LLM model inference remains a major challenge for
sustainable AI deployment. To address this problem, we model the
workload-dependent energy consumption and runtime of LLM inference tasks on
heterogeneous GPU-CPU systems. By conducting an extensive characterization
study of several state-of-the-art LLMs and analyzing their energy and runtime
behavior across different magnitudes of input prompts and output text, we
develop accurate (R^2>0.96) energy and runtime models for each LLM. We employ
these models to explore an offline, energy-optimal LLM workload scheduling
framework. Through a case study, we demonstrate the advantages of energy and
accuracy aware scheduling compared to existing best practices.

中文翻译:
大型语言模型（LLMs）的快速普及推动了自然语言处理和文本生成领域的重大进展。然而，LLM模型推理所消耗的能源仍是实现可持续人工智能部署的主要挑战。为解决这一问题，我们在异构GPU-CPU系统上对LLM推理任务的工作负载依赖性能耗与运行时进行建模。通过对多个前沿LLM开展广泛的特性研究，并分析其在不同规模输入提示和输出文本下的能耗与运行时表现，我们为每个LLM构建了预测精度（R²>0.96）的能耗与运行时模型。基于这些模型，我们探索了一种离线的、能耗最优的LLM工作负载调度框架。通过案例研究，我们证明了这种兼顾能耗与精度的调度方案相较于现行最佳实践的优势。

（翻译说明：
1. 专业术语处理："state-of-the-art"译为"前沿"，"characterization study"译为"特性研究"符合计算机领域表述
2. 被动语态转换：将英文被动结构转换为中文主动式，如"we demonstrate"译为"我们证明了"
3. 长句拆分：将原文复合长句拆分为符合中文表达习惯的短句，如第二句拆分为两个逻辑单元
4. 技术概念保留：保留"GPU-CPU""R²"等技术指标原表述确保专业性
5. 动态对等："workload-dependent"译为"工作负载依赖性"准确传达技术内涵
6. 文化适配："best practices"译为"最佳实践"符合中文技术文献惯例）
