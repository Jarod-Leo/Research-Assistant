# COBRA: Algorithm-Architecture Co-optimized Binary Transformer Accelerator for Edge Inference

链接: http://arxiv.org/abs/2504.16269v1

原文摘要:
Transformer-based models have demonstrated superior performance in various
fields, including natural language processing and computer vision. However,
their enormous model size and high demands in computation, memory, and
communication limit their deployment to edge platforms for local, secure
inference. Binary transformers offer a compact, low-complexity solution for
edge deployment with reduced bandwidth needs and acceptable accuracy. However,
existing binary transformers perform inefficiently on current hardware due to
the lack of binary specific optimizations. To address this, we introduce COBRA,
an algorithm-architecture co-optimized binary Transformer accelerator for edge
computing. COBRA features a real 1-bit binary multiplication unit, enabling
matrix operations with -1, 0, and +1 values, surpassing ternary methods. With
further hardware-friendly optimizations in the attention block, COBRA achieves
up to 3,894.7 GOPS throughput and 448.7 GOPS/Watt energy efficiency on edge
FPGAs, delivering a 311x energy efficiency improvement over GPUs and a 3.5x
throughput improvement over the state-of-the-art binary accelerator, with only
negligible inference accuracy degradation.

中文翻译:
基于Transformer的模型在自然语言处理和计算机视觉等多个领域展现出卓越性能。然而，其庞大的模型规模以及对计算、存储和通信的高要求，限制了其在边缘平台进行本地安全推理的部署。二值化Transformer通过降低带宽需求并保持可接受的精度，为边缘部署提供了紧凑、低复杂度的解决方案。但现有二值化Transformer由于缺乏针对二值计算的专门优化，在当前硬件上运行效率低下。为此，我们提出COBRA——一种面向边缘计算的算法-架构协同优化二值化Transformer加速器。COBRA创新性地采用真1比特二值乘法单元，支持包含-1、0和+1值的矩阵运算，性能超越三值化方法。通过对注意力模块进行硬件友好优化，COBRA在边缘FPGA上实现3,894.7 GOPS吞吐量和448.7 GOPS/瓦能效，较GPU提升311倍能效，较现有最优二值加速器提升3.5倍吞吐量，且推理精度损失可忽略不计。

（翻译说明：
1. 专业术语处理："binary transformers"译为"二值化Transformer"，"FPGAs"保留英文缩写形式
2. 技术概念转化："real 1-bit binary multiplication unit"译为"真1比特二值乘法单元"以体现技术特性
3. 长句拆分：将原文复合句拆分为符合中文表达习惯的短句结构
4. 数据呈现：精确保留所有性能指标数值及单位
5. 逻辑衔接：通过"为此"、"通过对...进行"等连接词保持论证逻辑清晰
6. 术语一致性：全篇统一"throughput"译为"吞吐量"，"energy efficiency"译为"能效"）
