# Revolutionizing Cyber Threat Detection with Large Language Models

链接: http://arxiv.org/abs/2306.14263v1

原文摘要:
The field of Natural Language Processing (NLP) is currently undergoing a
revolutionary transformation driven by the power of pre-trained Large Language
Models (LLMs) based on groundbreaking Transformer architectures. As the
frequency and diversity of cybersecurity attacks continue to rise, the
importance of incident detection has significantly increased. IoT devices are
expanding rapidly, resulting in a growing need for efficient techniques to
autonomously identify network-based attacks in IoT networks with both high
precision and minimal computational requirements. This paper presents
SecurityBERT, a novel architecture that leverages the Bidirectional Encoder
Representations from Transformers (BERT) model for cyber threat detection in
IoT networks. During the training of SecurityBERT, we incorporated a novel
privacy-preserving encoding technique called Privacy-Preserving Fixed-Length
Encoding (PPFLE). We effectively represented network traffic data in a
structured format by combining PPFLE with the Byte-level Byte-Pair Encoder
(BBPE) Tokenizer. Our research demonstrates that SecurityBERT outperforms
traditional Machine Learning (ML) and Deep Learning (DL) methods, such as
Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs), in
cyber threat detection. Employing the Edge-IIoTset cybersecurity dataset, our
experimental analysis shows that SecurityBERT achieved an impressive 98.2%
overall accuracy in identifying fourteen distinct attack types, surpassing
previous records set by hybrid solutions such as GAN-Transformer-based
architectures and CNN-LSTM models. With an inference time of less than 0.15
seconds on an average CPU and a compact model size of just 16.7MB, SecurityBERT
is ideally suited for real-life traffic analysis and a suitable choice for
deployment on resource-constrained IoT devices.

中文翻译:
当前，自然语言处理（NLP）领域正经历着由基于Transformer架构的预训练大语言模型（LLM）驱动的革命性变革。随着网络攻击频率和多样性的持续攀升，安全事件检测的重要性显著提升。物联网（IoT）设备的快速普及，使得对高效技术的需求日益增长——这类技术需以高精度和低计算成本自主识别物联网网络中的基于网络的攻击。本文提出SecurityBERT，这是一种利用Transformer双向编码器表征（BERT）模型进行物联网网络威胁检测的创新架构。在训练过程中，我们采用了一种名为"隐私保护定长编码"（PPFLE）的新型隐私保护编码技术，通过将PPFLE与字节级字节对编码器（BBPE）分词器相结合，实现了网络流量数据的结构化表征。研究表明，在网络安全威胁检测任务中，SecurityBERT的表现优于传统机器学习（ML）和深度学习方法（如卷积神经网络CNN或循环神经网络RNN）。基于Edge-IIoTset网络安全数据集的实验分析表明，SecurityBERT在识别14种不同攻击类型时实现了98.2%的整体准确率，超越了基于GAN-Transformer混合架构和CNN-LSTM模型等现有方案的性能记录。该模型在普通CPU上推理时间不足0.15秒，且模型体积仅16.7MB，非常适合实时流量分析，是资源受限物联网设备部署的理想选择。
