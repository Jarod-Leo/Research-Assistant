# All in One and One for All: A Simple yet Effective Method towards Cross-domain Graph Pretraining

链接: http://arxiv.org/abs/2402.09834v1

原文摘要:
Large Language Models (LLMs) have revolutionized the fields of computer
vision (CV) and natural language processing (NLP). One of the most notable
advancements of LLMs is that a single model is trained on vast and diverse
datasets spanning multiple domains -- a paradigm we term `All in One'. This
methodology empowers LLMs with super generalization capabilities, facilitating
an encompassing comprehension of varied data distributions. Leveraging these
capabilities, a single LLM demonstrates remarkable versatility across a variety
of domains -- a paradigm we term `One for All'. However, applying this idea to
the graph field remains a formidable challenge, with cross-domain pretraining
often resulting in negative transfer. This issue is particularly important in
few-shot learning scenarios, where the paucity of training data necessitates
the incorporation of external knowledge sources. In response to this challenge,
we propose a novel approach called Graph COordinators for PrEtraining (GCOPE),
that harnesses the underlying commonalities across diverse graph datasets to
enhance few-shot learning. Our novel methodology involves a unification
framework that amalgamates disparate graph datasets during the pretraining
phase to distill and transfer meaningful knowledge to target tasks. Extensive
experiments across multiple graph datasets demonstrate the superior efficacy of
our approach. By successfully leveraging the synergistic potential of multiple
graph datasets for pretraining, our work stands as a pioneering contribution to
the realm of graph foundational model.

中文翻译:
以下是符合学术规范的中文翻译：

大语言模型（LLMs）为计算机视觉（CV）和自然语言处理（NLP）领域带来了革命性变革。LLMs最显著的突破在于：单个模型能够基于跨多领域的海量异构数据集进行训练——这种范式我们称之为"All in One（万源归一）"。该方法赋予LLMs超强的泛化能力，使其能够全面理解多样化的数据分布。基于这种能力，单个LLM在多个领域展现出卓越的通用性——这种范式我们称为"One for All（一模型通）"。然而，将该理念应用于图学习领域仍面临严峻挑战，跨领域预训练常导致负迁移现象。这一问题在少样本学习场景中尤为突出，因为训练数据的匮乏亟需引入外部知识源。针对此挑战，我们提出创新性解决方案GCOPE（图协调预训练框架），通过挖掘异构图数据集间的潜在共性来增强少样本学习能力。我们的新方法包含统一化框架，在预训练阶段整合异构图数据集以提炼可迁移知识。跨多个图数据集的广泛实验验证了本方法的卓越效能。通过成功利用多图数据集的协同效应进行预训练，本研究为图基础模型领域做出了开创性贡献。

（翻译说明：
1. 专业术语处理：LLMs/CV/NLP等缩写保留英文，关键概念"All in One/One for All"采用"英文+中文注释"的学术规范译法
2. 句式重构：将英语长句拆分为符合中文表达习惯的短句，如将"facilitating..."独立成句译为"使其能够..."
3. 学术表达："negative transfer"规范译为"负迁移"，"few-shot learning"译为"少样本学习"
4. 概念统一："amalgamates disparate graph datasets"译为"整合异构图数据集"，前后保持术语一致性
5. 被动语态转化："our approach is demonstrated..."主动化为"实验验证了..."
6. 文化适配："pioneering contribution"译为"开创性贡献"符合中文学术评价用语）
