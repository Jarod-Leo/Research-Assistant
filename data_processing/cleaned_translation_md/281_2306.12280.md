# SIFTER: A Task-specific Alignment Strategy for Enhancing Sentence Embeddings

链接: http://arxiv.org/abs/2306.12280v1

原文摘要:
The paradigm of pre-training followed by fine-tuning on downstream tasks has
become the mainstream method in natural language processing tasks. Although
pre-trained models have the advantage of generalization, their performance may
still vary significantly across different domain tasks. This is because the
data distribution in different domains varies. For example, the different parts
of the sentence 'He married Smt. Dipali Ghosh in 1947 and led a very happy
married life' may have different impact for downstream tasks. For similarity
calculations, words such as 'led' and 'life' are more important. On the other
hand, for sentiment analysis, the word 'happy' is crucial. This indicates that
different downstream tasks have different levels of sensitivity to sentence
components. Our starting point is to scale information of the model and data
according to the specifics of downstream tasks, enhancing domain information of
relevant parts for these tasks and reducing irrelevant elements for different
domain tasks, called SIFTER. In the experimental part, we use the SIFTER to
improve SimCSE by constructing positive sample pairs based on enhancing the
sentence stem and reducing the unimportant components in the sentence, and
maximize the similarity between three sentences. Similarly, SIFTER can improve
the gate mechanism of the LSTM model by short-circuiting the input gate of
important words so that the LSTM model remembers the important parts of the
sentence. Our experiments demonstrate that SIFTER outperforms the SimCSE and
LSTM baselines.

中文翻译:
以下是符合学术规范的中文翻译：

预训练-下游任务微调范式已成为自然语言处理任务的主流方法。尽管预训练模型具有泛化优势，但其在不同领域任务中的性能仍可能存在显著差异。这是由于不同领域的数据分布存在差异。例如，在句子"He married Smt. Dipali Ghosh in 1947 and led a very happy married life"中，不同成分对下游任务的影响各异：对于相似度计算，"led"和"life"等词汇更为重要；而对于情感分析，"happy"一词则至关重要。这表明不同下游任务对句子成分具有差异化的敏感度。

本研究提出SIFTER方法，其核心思想是根据下游任务特性对模型和数据信息进行动态缩放：增强任务相关部分的领域信息，同时弱化不同领域任务中的无关要素。在实验部分，我们应用SIFTER改进SimCSE模型——通过强化句子主干和弱化非重要成分来构建正样本对，并最大化三个句子间的相似度。同样地，SIFTER可通过短路重要词汇的输入门来改进LSTM模型的门控机制，使模型更好地记忆句子关键部分。实验结果表明，SIFTER在SimCSE和LSTM基准模型上均取得了性能提升。

（翻译说明：
1. 专业术语处理："pre-training/fine-tuning"译为"预训练/微调"，"domain"译为"领域"，"gate mechanism"译为"门控机制"
2. 长句拆分：将原文复合句按中文表达习惯分解为多个短句
3. 被动语态转换："our starting point is..."译为主动句式"本研究提出..."
4. 概念显化："SIFTER"首次出现时补充"方法"作为范畴词
5. 学术表达："demonstrate"译为"结果表明"符合论文摘要惯例
6. 逻辑连接：添加"其核心思想是"等过渡词保持论证连贯性）
