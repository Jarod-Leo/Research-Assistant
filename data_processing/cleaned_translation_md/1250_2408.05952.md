# Optimizing Vision Transformers with Data-Free Knowledge Transfer

链接: http://arxiv.org/abs/2408.05952v1

原文摘要:
The groundbreaking performance of transformers in Natural Language Processing
(NLP) tasks has led to their replacement of traditional Convolutional Neural
Networks (CNNs), owing to the efficiency and accuracy achieved through the
self-attention mechanism. This success has inspired researchers to explore the
use of transformers in computer vision tasks to attain enhanced long-term
semantic awareness. Vision transformers (ViTs) have excelled in various
computer vision tasks due to their superior ability to capture long-distance
dependencies using the self-attention mechanism. Contemporary ViTs like Data
Efficient Transformers (DeiT) can effectively learn both global semantic
information and local texture information from images, achieving performance
comparable to traditional CNNs. However, their impressive performance comes
with a high computational cost due to very large number of parameters,
hindering their deployment on devices with limited resources like smartphones,
cameras, drones etc. Additionally, ViTs require a large amount of data for
training to achieve performance comparable to benchmark CNN models. Therefore,
we identified two key challenges in deploying ViTs on smaller form factor
devices: the high computational requirements of large models and the need for
extensive training data. As a solution to these challenges, we propose
compressing large ViT models using Knowledge Distillation (KD), which is
implemented data-free to circumvent limitations related to data availability.
Additionally, we conducted experiments on object detection within the same
environment in addition to classification tasks. Based on our analysis, we
found that datafree knowledge distillation is an effective method to overcome
both issues, enabling the deployment of ViTs on less resourceconstrained
devices.

中文翻译:
以下是符合学术规范的中文翻译：

自然语言处理（NLP）领域中，基于自注意力机制的Transformer模型凭借其卓越的效率和准确性，已取代传统卷积神经网络（CNN）成为主流架构。这一突破性进展促使研究者探索将Transformer应用于计算机视觉任务，以获取更强的长程语义理解能力。视觉Transformer（ViT）凭借自注意力机制在远距离依赖建模方面的优势，已在多种视觉任务中展现出卓越性能。当代ViT模型（如数据高效Transformer/DeiT）能够同时学习图像的全局语义信息和局部纹理特征，其性能已可比肩传统CNN。然而，这类模型参数量庞大导致计算成本高昂，阻碍了其在智能手机、摄像设备、无人机等资源受限终端上的部署。此外，ViT需要海量训练数据才能达到基准CNN模型的性能水平。因此，我们总结出ViT在小型化设备部署面临的两大核心挑战：大模型的高计算需求与大量训练数据的依赖性。针对这些问题，我们提出采用无数据知识蒸馏（KD）技术对大型ViT模型进行压缩，以规避数据可获性限制。除分类任务外，我们还在相同环境下进行了目标检测实验。分析结果表明，无数据知识蒸馏能有效解决上述双重挑战，为ViT在资源受限设备上的部署提供了可行方案。

注：本译文严格遵循以下学术翻译原则：
1. 专业术语统一（如self-attention mechanism统一译为"自注意力机制"）
2. 被动语态转化（英文被动句转换为中文主动句式）
3. 长句拆分重组（符合中文多用短句的表达习惯）
4. 概念准确传达（如"data-free"译为"无数据"而非字面直译）
5. 逻辑连接显化（增补"因此""针对"等连接词确保行文连贯）
6. 学术用语规范（如"benchmark models"译为"基准模型"）
