# Approximation and Estimation Ability of Transformers for Sequence-to-Sequence Functions with Infinite Dimensional Input

链接: http://arxiv.org/abs/2305.18699v1

原文摘要:
Despite the great success of Transformer networks in various applications
such as natural language processing and computer vision, their theoretical
aspects are not well understood. In this paper, we study the approximation and
estimation ability of Transformers as sequence-to-sequence functions with
infinite dimensional inputs. Although inputs and outputs are both infinite
dimensional, we show that when the target function has anisotropic smoothness,
Transformers can avoid the curse of dimensionality due to their feature
extraction ability and parameter sharing property. In addition, we show that
even if the smoothness changes depending on each input, Transformers can
estimate the importance of features for each input and extract important
features dynamically. Then, we proved that Transformers achieve similar
convergence rate as in the case of the fixed smoothness. Our theoretical
results support the practical success of Transformers for high dimensional
data.

中文翻译:
尽管Transformer网络在自然语言处理和计算机视觉等众多应用中取得了巨大成功，但其理论机制尚未得到充分阐释。本文研究了Transformer作为处理无限维输入的序列到序列函数时的逼近与估计能力。研究表明：当目标函数具有各向异性平滑特性时，凭借其特征提取能力和参数共享特性，Transformer能够有效规避维度灾难问题。进一步发现，即使平滑度随输入动态变化，Transformer仍能自适应评估各输入的特征重要性并进行动态特征提取。我们通过理论证明，在此情况下Transformer仍可获得与固定平滑度情形相当的收敛速率。这些理论发现为Transformer在高维数据实践中的卓越表现提供了学理支撑。

（译文特点说明：
1. 专业术语准确统一："anisotropic smoothness"译为"各向异性平滑特性"，"curse of dimensionality"采用计算机领域通用译法"维度灾难"
2. 长句拆分重构：将原文复合句按中文表达习惯分解为多个短句，如"we show that..."处理为"研究表明..."的独立句式
3. 被动语态转化："it is proved that..."转为主动句式"我们通过理论证明..."
4. 学术风格保持：使用"阐释""学理支撑"等符合论文摘要语体的词汇
5. 逻辑连接显化：通过"进一步发现""在此情况下"等衔接词明确原文隐含的逻辑关系）
