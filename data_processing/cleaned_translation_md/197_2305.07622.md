# PALR: Personalization Aware LLMs for Recommendation

链接: http://arxiv.org/abs/2305.07622v1

原文摘要:
Large language models (LLMs) have recently received significant attention for
their exceptional capabilities. Despite extensive efforts in developing
general-purpose LLMs that can be utilized in various natural language
processing (NLP) tasks, there has been less research exploring their potential
in recommender systems. In this paper, we propose a novel framework, named
PALR, which aiming to combine user history behaviors (such as clicks,
purchases, ratings, etc.) with LLMs to generate user preferred items.
Specifically, we first use user/item interactions as guidance for candidate
retrieval. Then we adopt a LLM-based ranking model to generate recommended
items. Unlike existing approaches that typically adopt general-purpose LLMs for
zero/few-shot recommendation testing or training on small-sized language models
(with less than 1 billion parameters), which cannot fully elicit LLMs'
reasoning abilities and leverage rich item side parametric knowledge, we
fine-tune a 7 billion parameters LLM for the ranking purpose. This model takes
retrieval candidates in natural language format as input, with instruction
which explicitly asking to select results from input candidates during
inference. Our experimental results demonstrate that our solution outperforms
state-of-the-art models on various sequential recommendation tasks.

中文翻译:
以下是符合要求的学术中文翻译：

大型语言模型（LLMs）近期因其卓越能力受到广泛关注。尽管学界已投入大量精力开发适用于多种自然语言处理（NLP）任务的通用LLMs，但探索其在推荐系统中应用潜力的研究仍相对匮乏。本文提出创新框架PALR，旨在将用户历史行为（如点击、购买、评分等）与LLMs相结合以生成用户偏好项目。具体而言，我们首先以用户-项目交互作为候选检索指引，继而采用基于LLM的排序模型生成推荐项目。现有方法通常采用通用LLMs进行零样本/小样本推荐测试，或训练参数量小于10亿的小型语言模型，这些方法既无法充分激发LLMs的推理能力，也难以利用丰富的项目侧参数知识。与之不同，我们针对排序任务微调了70亿参数的LLM。该模型以自然语言格式的检索候选集作为输入，并通过显式指令要求推理时从输入候选中选择结果。实验结果表明，我们的解决方案在多种序列推荐任务上优于当前最先进模型。

翻译说明：
1. 专业术语处理：
- "LLMs"保留英文缩写并添加中文全称
- "zero/few-shot"译为"零样本/小样本"
- "7 billion parameters"规范化为"70亿参数"

2. 句式重构：
- 将英文长句拆分为符合中文表达习惯的短句（如第一段重组为两个逻辑层次）
- 被动语态转为主动表述（如"can be utilized"译为"适用于"）
- 复杂从句转换为分句结构（如"which aiming to..."处理为独立分句）

3. 学术规范：
- 保持"本文""我们"等学术论文主体指称的一致性
- 技术表述准确（如"fine-tune"译为专业术语"微调"）
- 重要概念首次出现时保持中英对照（LLMs）

4. 逻辑显化：
- 增译"与之不同"等连接词强化对比关系
- "instruction which explicitly..."译为"通过显式指令"突出方法创新点

5. 数据呈现：
- 统一数字单位（billion统一换算为"亿"）
- 参数规模描述符合中文计量习惯
