# Linguistically Grounded Analysis of Language Models using Shapley Head Values

链接: http://arxiv.org/abs/2410.13396v1

原文摘要:
Understanding how linguistic knowledge is encoded in language models is
crucial for improving their generalisation capabilities. In this paper, we
investigate the processing of morphosyntactic phenomena, by leveraging a
recently proposed method for probing language models via Shapley Head Values
(SHVs). Using the English language BLiMP dataset, we test our approach on two
widely used models, BERT and RoBERTa, and compare how linguistic constructions
such as anaphor agreement and filler-gap dependencies are handled. Through
quantitative pruning and qualitative clustering analysis, we demonstrate that
attention heads responsible for processing related linguistic phenomena cluster
together. Our results show that SHV-based attributions reveal distinct patterns
across both models, providing insights into how language models organize and
process linguistic information. These findings support the hypothesis that
language models learn subnetworks corresponding to linguistic theory, with
potential implications for cross-linguistic model analysis and interpretability
in Natural Language Processing (NLP).

中文翻译:
理解语言知识如何在语言模型中编码，对于提升其泛化能力至关重要。本文通过最新提出的Shapley头值（SHV）探测方法，研究了语言模型对形态句法现象的处理机制。基于英语BLiMP数据集，我们在BERT和RoBERTa两大主流模型上进行实验，对比分析了回指一致性和填充语-空缺依存等语言结构的处理方式。通过定量剪枝与定性聚类分析，我们发现负责处理相关语言现象的注意力头会形成聚类。研究结果表明，基于SHV的归因分析揭示了两类模型间的差异化模式，为理解语言模型如何组织加工语言信息提供了新视角。这些发现支持了"语言模型会形成与语言学理论对应的子网络"的假设，对跨语言模型分析和自然语言处理（NLP）可解释性研究具有潜在启示意义。

（翻译说明：采用学术论文摘要的规范表述，处理了专业术语如"morphosyntactic phenomena"译为"形态句法现象"、"anaphor agreement"译为"回指一致性"等；将长句合理切分为符合中文表达习惯的短句；保留"SHV"等专业缩写首次出现时的全称；通过"机制"、"视角"等词实现学术文本的语体适配；最后一句采用转译手法突出研究价值）
