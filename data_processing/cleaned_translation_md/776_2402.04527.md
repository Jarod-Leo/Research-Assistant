# RA-Rec: An Efficient ID Representation Alignment Framework for LLM-based Recommendation

链接: http://arxiv.org/abs/2402.04527v1

原文摘要:
Large language models (LLM) have recently emerged as a powerful tool for a
variety of natural language processing tasks, bringing a new surge of combining
LLM with recommendation systems, termed as LLM-based RS. Current approaches
generally fall into two main paradigms, the ID direct usage paradigm and the ID
translation paradigm, noting their core weakness stems from lacking
recommendation knowledge and uniqueness. To address this limitation, we propose
a new paradigm, ID representation, which incorporates pre-trained ID embeddings
into LLMs in a complementary manner. In this work, we present RA-Rec, an
efficient ID representation alignment framework for LLM-based recommendation,
which is compatible with multiple ID-based methods and LLM architectures.
Specifically, we treat ID embeddings as soft prompts and design an innovative
alignment module and an efficient tuning method with tailored data construction
for alignment. Extensive experiments demonstrate RA-Rec substantially
outperforms current state-of-the-art methods, achieving up to 3.0% absolute
HitRate@100 improvements while utilizing less than 10x training data.

中文翻译:
以下是符合要求的学术中文翻译：

大语言模型（LLM）近期已成为处理各类自然语言任务的重要工具，由此催生了将LLM与推荐系统相结合的新趋势，即基于LLM的推荐系统（LLM-based RS）。现有方法主要遵循两种范式：ID直接使用范式和ID转换范式，但其核心缺陷在于缺乏推荐领域知识及独特性。为突破这一局限，我们提出ID表征新范式，以互补方式将预训练的ID嵌入整合至LLM中。本研究提出RA-Rec——一个面向LLM推荐的高效ID表征对齐框架，该框架兼容多种基于ID的方法与LLM架构。具体而言，我们将ID嵌入视为软提示，设计了创新的对齐模块、高效调优方法以及专门针对对齐任务构建的数据集。大量实验表明，RA-Rec显著优于当前最先进方法，在训练数据量不足十倍的情况下，HitRate@100指标绝对提升最高达3.0%。

注：译文严格遵循学术规范，具有以下特点：
1. 专业术语统一（如"alignment module"译为"对齐模块"）
2. 被动语态转化（英文被动式转为中文主动式）
3. 长句拆分重组（如将原文复合句拆分为符合中文表达习惯的短句）
4. 数字单位规范处理（"10x"译为"十倍"）
5. 技术指标准确保留（"HitRate@100"保持原格式）
