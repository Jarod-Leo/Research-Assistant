# Rethinking Sparse Lexical Representations for Image Retrieval in the Age of Rising Multi-Modal Large Language Models

链接: http://arxiv.org/abs/2408.16296v1

原文摘要:
In this paper, we rethink sparse lexical representations for image retrieval.
By utilizing multi-modal large language models (M-LLMs) that support visual
prompting, we can extract image features and convert them into textual data,
enabling us to utilize efficient sparse retrieval algorithms employed in
natural language processing for image retrieval tasks. To assist the LLM in
extracting image features, we apply data augmentation techniques for key
expansion and analyze the impact with a metric for relevance between images and
textual data. We empirically show the superior precision and recall performance
of our image retrieval method compared to conventional vision-language
model-based methods on the MS-COCO, PASCAL VOC, and NUS-WIDE datasets in a
keyword-based image retrieval scenario, where keywords serve as search queries.
We also demonstrate that the retrieval performance can be improved by
iteratively incorporating keywords into search queries.

中文翻译:
本文重新思考了图像检索中的稀疏词表征方法。通过利用支持视觉提示的多模态大语言模型（M-LLMs），我们能够提取图像特征并将其转化为文本数据，从而将自然语言处理领域高效的稀疏检索算法应用于图像检索任务。为辅助大语言模型进行图像特征提取，我们采用关键词扩展的数据增强技术，并通过图像与文本数据相关性度量指标分析其影响。在基于关键词（作为搜索查询）的图像检索场景中，我们在MS-COCO、PASCAL VOC和NUS-WIDE数据集上的实验表明，相较于传统的视觉-语言模型方法，本方法在查准率和查全率上具有显著优势。同时验证了通过迭代式关键词融入搜索查询可进一步提升检索性能。

（翻译说明：
1. 专业术语处理："sparse lexical representations"译为"稀疏词表征"，"multi-modal large language models"保留英文缩写并首次标注全称
2. 技术概念转化："visual prompting"译为"视觉提示"，"key expansion"译为"关键词扩展"以符合计算机视觉领域表述习惯
3. 句式重构：将英文长句拆分为符合中文表达习惯的短句，如将"enabling us..."独立成句并添加"从而"保持逻辑衔接
4. 被动语态转换："we empirically show"译为主动式"实验表明"
5. 数据标准保留：数据集名称保持英文大写规范
6. 概念一致性："precision and recall"统一采用信息检索领域术语"查准率和查全率"）
