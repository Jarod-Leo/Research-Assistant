# Enhanced Transformer Architecture for Natural Language Processing

链接: http://arxiv.org/abs/2310.10930v1

原文摘要:
Transformer is a state-of-the-art model in the field of natural language
processing (NLP). Current NLP models primarily increase the number of
transformers to improve processing performance. However, this technique
requires a lot of training resources such as computing capacity. In this paper,
a novel structure of Transformer is proposed. It is featured by full layer
normalization, weighted residual connection, positional encoding exploiting
reinforcement learning, and zero masked self-attention. The proposed
Transformer model, which is called Enhanced Transformer, is validated by the
bilingual evaluation understudy (BLEU) score obtained with the Multi30k
translation dataset. As a result, the Enhanced Transformer achieves 202.96%
higher BLEU score as compared to the original transformer with the translation
dataset.

中文翻译:
以下是符合要求的学术中文翻译：

Transformer是自然语言处理（NLP）领域的尖端模型。当前NLP模型主要通过增加Transformer数量来提升处理性能，但这种方法需要消耗大量计算能力等训练资源。本文提出一种新型Transformer结构，其创新点包括：全层归一化、加权残差连接、基于强化学习的位置编码以及零掩码自注意力机制。我们通过Multi30k翻译数据集的双语评估替补（BLEU）分数验证了该模型（称为增强型Transformer）的性能。实验结果表明，在使用相同翻译数据集时，增强型Transformer相较于原始Transformer模型实现了202.96%的BLEU分数提升。

注：翻译严格遵循以下学术规范：
1. 专业术语统一（如BLEU保持首字母大写并标注中文全称）
2. 被动语态转换为主动句式（如"is proposed"译为"提出"）
3. 长句拆分符合中文表达习惯（如将原文特征列举式结构转换为冒号分项）
4. 数字格式标准化（百分比统一使用"%"符号）
5. 技术概念准确传达（如"zero masked self-attention"译为专业术语"零掩码自注意力机制"）
6. 保持客观严谨的学术语气
