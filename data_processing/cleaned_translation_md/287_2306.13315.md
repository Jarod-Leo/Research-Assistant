# Abstractive Text Summarization for Resumes With Cutting Edge NLP Transformers and LSTM

链接: http://arxiv.org/abs/2306.13315v1

原文摘要:
Text summarization is a fundamental task in natural language processing that
aims to condense large amounts of textual information into concise and coherent
summaries. With the exponential growth of content and the need to extract key
information efficiently, text summarization has gained significant attention in
recent years. In this study, LSTM and pre-trained T5, Pegasus, BART and
BART-Large model performances were evaluated on the open source dataset (Xsum,
CNN/Daily Mail, Amazon Fine Food Review and News Summary) and the prepared
resume dataset. This resume dataset consists of many information such as
language, education, experience, personal information, skills, and this data
includes 75 resumes. The primary objective of this research was to classify
resume text. Various techniques such as LSTM, pre-trained models, and
fine-tuned models were assessed using a dataset of resumes. The BART-Large
model fine-tuned with the resume dataset gave the best performance.

中文翻译:
文本摘要作为自然语言处理领域的基础任务，旨在将海量文本信息压缩为简洁连贯的概要。随着内容数据的指数级增长以及对高效提取关键信息的需求，文本摘要技术近年来受到广泛关注。本研究在开源数据集（Xsum、CNN/Daily Mail、亚马逊美食评论和新闻摘要）及自建简历数据集上评估了LSTM与预训练模型T5、Pegasus、BART及BART-Large的性能表现。该简历数据集包含语言能力、教育背景、工作经历、个人信息、技能等多元字段，共收录75份简历样本。本研究核心目标在于实现简历文本分类，通过采用LSTM、预训练模型及微调模型等多种技术方案进行系统评估。实验结果表明，基于简历数据集微调的BART-Large模型展现出最优性能。
