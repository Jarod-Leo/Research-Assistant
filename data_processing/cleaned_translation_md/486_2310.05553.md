# Regulation and NLP (RegNLP): Taming Large Language Models

链接: http://arxiv.org/abs/2310.05553v1

原文摘要:
The scientific innovation in Natural Language Processing (NLP) and more
broadly in artificial intelligence (AI) is at its fastest pace to date. As
large language models (LLMs) unleash a new era of automation, important debates
emerge regarding the benefits and risks of their development, deployment and
use. Currently, these debates have been dominated by often polarized narratives
mainly led by the AI Safety and AI Ethics movements. This polarization, often
amplified by social media, is swaying political agendas on AI regulation and
governance and posing issues of regulatory capture. Capture occurs when the
regulator advances the interests of the industry it is supposed to regulate, or
of special interest groups rather than pursuing the general public interest.
Meanwhile in NLP research, attention has been increasingly paid to the
discussion of regulating risks and harms. This often happens without systematic
methodologies or sufficient rooting in the disciplines that inspire an extended
scope of NLP research, jeopardizing the scientific integrity of these
endeavors. Regulation studies are a rich source of knowledge on how to
systematically deal with risk and uncertainty, as well as with scientific
evidence, to evaluate and compare regulatory options. This resource has largely
remained untapped so far. In this paper, we argue how NLP research on these
topics can benefit from proximity to regulatory studies and adjacent fields. We
do so by discussing basic tenets of regulation, and risk and uncertainty, and
by highlighting the shortcomings of current NLP discussions dealing with risk
assessment. Finally, we advocate for the development of a new multidisciplinary
research space on regulation and NLP (RegNLP), focused on connecting scientific
knowledge to regulatory processes based on systematic methodologies.

中文翻译:
自然语言处理（NLP）及更广泛的人工智能（AI）领域的科学创新正以前所未有的速度发展。随着大语言模型（LLMs）开启自动化新时代，关于其开发、部署和使用的效益与风险的重大讨论日益凸显。当前，这些讨论往往被AI安全与AI伦理运动主导的两极化叙事所裹挟。这种常被社交媒体放大的对立态势，正影响着人工智能监管与治理的政治议程，并引发"监管俘获"问题——即监管机构为应受监管的行业或特殊利益集团谋利，而非维护公共利益。与此同时，NLP研究领域对风险与危害监管的讨论日渐增多，但往往缺乏系统方法论支撑，也未能充分扎根于那些启发NLP研究扩展的学科领域，这危及了相关研究的科学严谨性。规制研究作为知识宝库，本可为系统应对风险与不确定性、运用科学证据评估比较监管方案提供参考，但这一资源至今尚未得到充分发掘。本文论证了NLP相关研究如何通过贴近规制研究及相邻领域获得裨益：通过阐释规制基础理论、风险与不确定性的本质，指出现有NLP风险评估讨论的缺陷。最后我们主张建立一个规制与NLP（RegNLP）的新型跨学科研究空间，致力于基于系统方法论将科学知识与规制流程相衔接。

（翻译说明：采用学术文本的严谨句式结构，通过以下处理实现专业性与可读性平衡：
1. 关键术语统一："regulatory capture"译为"监管俘获"并添加解释性括号
2. 长句拆分重组：将原文复合句按中文表达习惯分解为多个短句
3. 概念显化处理："polarized narratives"译为"两极化叙事"并补充"所裹挟"的动词
4. 专业表述转换："systematic methodologies"译为"系统方法论"符合中文社科表述
5. 逻辑连接强化：使用"本可...但..."等转折结构保持论证脉络清晰
6. 机构名称保留：RegNLP采用首字母缩写+中文全称的规范译法）
