# Generative Pre-trained Transformer for Vietnamese Community-based COVID-19 Question Answering

链接: http://arxiv.org/abs/2310.14602v1

原文摘要:
Recent studies have provided empirical evidence of the wide-ranging potential
of Generative Pre-trained Transformer (GPT), a pretrained language model, in
the field of natural language processing. GPT has been effectively employed as
a decoder within state-of-the-art (SOTA) question answering systems, yielding
exceptional performance across various tasks. However, the current research
landscape concerning GPT's application in Vietnamese remains limited. This
paper aims to address this gap by presenting an implementation of GPT-2 for
community-based question answering specifically focused on COVID-19 related
queries in Vietnamese. We introduce a novel approach by conducting a
comparative analysis of different Transformers vs SOTA models in the
community-based COVID-19 question answering dataset. The experimental findings
demonstrate that the GPT-2 models exhibit highly promising outcomes,
outperforming other SOTA models as well as previous community-based COVID-19
question answering models developed for Vietnamese.

中文翻译:
近期研究为生成式预训练转换器（GPT）这一预训练语言模型在自然语言处理领域的广泛应用潜力提供了实证依据。作为前沿问答系统（SOTA）的解码器，GPT已在多项任务中展现出卓越性能。然而目前关于GPT在越南语应用的研究仍较为有限。本文通过实现基于GPT-2的社区问答系统（专门针对越南语COVID-19相关查询），旨在填补这一研究空白。我们创新性地采用对比分析方法，在社区COVID-19问答数据集中对不同Transformer模型与SOTA模型进行性能比较。实验结果表明，GPT-2模型展现出极具前景的效果，其表现不仅优于其他SOTA模型，也超越了先前针对越南语开发的社区COVID-19问答模型。
