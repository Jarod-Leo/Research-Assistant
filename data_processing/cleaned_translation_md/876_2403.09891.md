# Fisher Mask Nodes for Language Model Merging

链接: http://arxiv.org/abs/2403.09891v1

原文摘要:
Fine-tuning pre-trained models provides significant advantages in downstream
performance. The ubiquitous nature of pre-trained models such as BERT and its
derivatives in natural language processing has also led to a proliferation of
task-specific fine-tuned models. As these models typically only perform one
task well, additional training or ensembling is required in multi-task
scenarios. The growing field of model merging provides a solution, dealing with
the challenge of combining multiple task-specific models into a single
multi-task model. In this study, we introduce a novel model merging method for
Transformers, combining insights from previous work in Fisher-weighted
averaging and the use of Fisher information in model pruning. Utilizing the
Fisher information of mask nodes within the Transformer architecture, we devise
a computationally efficient weighted-averaging scheme. Our method exhibits a
regular and significant performance increase across various models in the BERT
family, outperforming full-scale Fisher-weighted averaging in a fraction of the
computational cost, with baseline performance improvements of up to +6.5 and a
speedup between 57.4x and 321.7x across models. Our results prove the potential
of our method in current multi-task learning environments and suggest its
scalability and adaptability to new model architectures and learning scenarios.

中文翻译:
以下是符合您要求的学术中文翻译：

微调预训练模型能显著提升下游任务性能。在自然语言处理领域，诸如BERT及其衍生模型等预训练模型的普及应用，也导致了针对特定任务微调模型的激增。由于这些模型通常仅擅长单一任务，在多任务场景中需要额外训练或集成学习。快速发展的模型融合领域为此提供了解决方案，致力于将多个专用模型整合为统一的多任务模型。本研究提出了一种创新的Transformer模型融合方法，有机融合了Fisher加权平均与模型剪枝中Fisher信息应用的前沿成果。通过利用Transformer架构中掩码节点的Fisher信息，我们设计出计算高效的加权平均方案。该方法在BERT系列各类模型中均展现出稳定且显著的性能提升：仅需极小部分计算成本即超越全量Fisher加权平均的效果，基线性能最高提升+6.5，计算加速比达到57.4至321.7倍。实验结果表明，本方法在当前多任务学习环境中具有重要应用价值，同时展现出对新模型架构和学习场景的强扩展性与适应能力。

（翻译严格遵循以下原则：
1. 专业术语准确统一（如fine-tuning=微调，downstream performance=下游任务性能）
2. 长句合理切分，符合中文表达习惯
3. 被动语态转化（如"is required"译为"需要"）
4. 关键数据完整保留
5. 学术用语规范（如"proliferation"译为"激增"而非简单直译）
6. 逻辑关系显化（如"as"译为"由于"明确因果关系）
