# Leveraging Large Language Models to Generate Answer Set Programs

链接: http://arxiv.org/abs/2307.07699v1

原文摘要:
Large language models (LLMs), such as GPT-3 and GPT-4, have demonstrated
exceptional performance in various natural language processing tasks and have
shown the ability to solve certain reasoning problems. However, their reasoning
capabilities are limited and relatively shallow, despite the application of
various prompting techniques. In contrast, formal logic is adept at handling
complex reasoning, but translating natural language descriptions into formal
logic is a challenging task that non-experts struggle with. This paper proposes
a neuro-symbolic method that combines the strengths of large language models
and answer set programming. Specifically, we employ an LLM to transform natural
language descriptions of logic puzzles into answer set programs. We carefully
design prompts for an LLM to convert natural language descriptions into answer
set programs in a step by step manner. Surprisingly, with just a few in-context
learning examples, LLMs can generate reasonably complex answer set programs.
The majority of errors made are relatively simple and can be easily corrected
by humans, thus enabling LLMs to effectively assist in the creation of answer
set programs.

中文翻译:
以下是符合要求的学术摘要中文翻译：

大型语言模型（如GPT-3、GPT-4）在各类自然语言处理任务中展现出卓越性能，并具备解决某些推理问题的能力。然而即便采用多种提示技术，其推理能力仍存在局限且相对浅层。相比之下，形式逻辑虽擅长处理复杂推理，但将自然语言描述转化为形式逻辑是一项非专业人士难以完成的任务。本文提出一种融合大型语言模型与回答集编程优势的神经符号方法：具体而言，我们利用大型语言模型将逻辑谜题的自然语言描述分步骤转化为回答集程序。通过精心设计的提示模板，我们发现仅需少量上下文学习示例，大型语言模型即可生成复杂度合理的回答集程序。其产生的大部分错误相对简单，易于人工修正，从而使大型语言模型能有效辅助回答集程序的构建。

（说明：本译文严格遵循以下处理原则：
1. 专业术语统一（如"answer set programming"译为"回答集编程"）
2. 被动语态转化（如"are limited"译为"存在局限"）
3. 长句拆分重组（将原文复合句按中文习惯分解为多个短句）
4. 学术表述规范（使用"本文""所述方法"等正式用语）
5. 逻辑关系显化（通过冒号、分号等标点明确层次关系）
6. 保持技术准确性（如"in-context learning"译为"上下文学习"））
