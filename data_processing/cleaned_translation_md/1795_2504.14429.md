# ResNetVLLM-2: Addressing ResNetVLLM's Multi-Modal Hallucinations

链接: http://arxiv.org/abs/2504.14429v1

原文摘要:
Large Language Models (LLMs) have transformed natural language processing
(NLP) tasks, but they suffer from hallucination, generating plausible yet
factually incorrect content. This issue extends to Video-Language Models
(VideoLLMs), where textual descriptions may inaccurately represent visual
content, resulting in multi-modal hallucinations. In this paper, we address
hallucination in ResNetVLLM, a video-language model combining ResNet visual
encoders with LLMs. We introduce a two-step protocol: (1) a faithfulness
detection strategy that uses a modified Lynx model to assess semantic alignment
between generated captions and ground-truth video references, and (2) a
hallucination mitigation strategy using Retrieval-Augmented Generation (RAG)
with an ad-hoc knowledge base dynamically constructed during inference. Our
enhanced model, ResNetVLLM-2, reduces multi-modal hallucinations by
cross-verifying generated content against external knowledge, improving factual
consistency. Evaluation on the ActivityNet-QA benchmark demonstrates a
substantial accuracy increase from 54.8% to 65.3%, highlighting the
effectiveness of our hallucination detection and mitigation strategies in
enhancing video-language model reliability.

中文翻译:
以下是符合要求的学术论文摘要中文翻译：

大型语言模型（LLMs）虽已革新自然语言处理任务，但其存在的幻觉问题会生成看似合理实则违背事实的内容。这种现象同样存在于视频语言模型（VideoLLMs）中，其生成的文本描述可能错误呈现视觉内容，导致多模态幻觉。本文针对结合ResNet视觉编码器与LLMs的视频语言模型ResNetVLLM，提出两步解决方案：（1）采用改进版Lynx模型的忠实度检测策略，评估生成字幕与真实视频参考间的语义对齐；（2）通过检索增强生成（RAG）技术，在推理过程中动态构建特定知识库以实现幻觉缓解。优化后的ResNetVLLM-2模型通过外部知识交叉验证生成内容，减少多模态幻觉并提升事实一致性。在ActivityNet-QA基准测试中，模型准确率从54.8%显著提升至65.3%，验证了所提幻觉检测与缓解策略对增强视频语言模型可靠性的有效性。


