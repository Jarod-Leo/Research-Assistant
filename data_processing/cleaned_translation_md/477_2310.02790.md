# Low Resource Summarization using Pre-trained Language Models

链接: http://arxiv.org/abs/2310.02790v1

原文摘要:
With the advent of Deep Learning based Artificial Neural Networks models,
Natural Language Processing (NLP) has witnessed significant improvements in
textual data processing in terms of its efficiency and accuracy. However, the
research is mostly restricted to high-resource languages such as English and
low-resource languages still suffer from a lack of available resources in terms
of training datasets as well as models with even baseline evaluation results.
Considering the limited availability of resources for low-resource languages,
we propose a methodology for adapting self-attentive transformer-based
architecture models (mBERT, mT5) for low-resource summarization, supplemented
by the construction of a new baseline dataset (76.5k article, summary pairs) in
a low-resource language Urdu. Choosing news (a publicly available source) as
the application domain has the potential to make the proposed methodology
useful for reproducing in other languages with limited resources. Our adapted
summarization model \textit{urT5} with up to 44.78\% reduction in size as
compared to \textit{mT5} can capture contextual information of low resource
language effectively with evaluation score (up to 46.35 ROUGE-1, 77 BERTScore)
at par with state-of-the-art models in high resource language English
\textit{(PEGASUS: 47.21, BART: 45.14 on XSUM Dataset)}. The proposed method
provided a baseline approach towards extractive as well as abstractive
summarization with competitive evaluation results in a limited resource setup.

中文翻译:
随着基于深度学习的人工神经网络模型的出现，自然语言处理（NLP）在文本数据处理效率和准确性方面取得了显著进步。然而现有研究主要集中于英语等高资源语言，低资源语言仍面临训练数据集匮乏、基准评估模型短缺等问题。针对低资源语言可用资源有限的现状，本文提出一种基于自注意力Transformer架构模型（mBERT、mT5）的适配方法，用于低资源文本摘要任务，并构建了乌尔都语（低资源语言）的新基准数据集（含7.65万篇新闻文章与摘要对）。选择新闻领域（公开可用资源）作为应用场景，使得该方法具备推广至其他低资源语言的潜力。我们改进的摘要模型\textit{urT5}相较\textit{mT5}体积最大缩减44.78\%，在低资源语言语境信息捕捉方面表现优异（评估指标达46.35 ROUGE-1和77 BERTScore），与英语高资源语言顶级模型\textit{（PEGASUS:47.21，BART:45.14 on XSUM数据集）}性能相当。该方法为抽取式和生成式摘要提供了基准解决方案，在有限资源条件下取得了具有竞争力的评估结果。
