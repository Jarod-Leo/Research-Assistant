# A Transformer-based Framework For Multi-variate Time Series: A Remaining Useful Life Prediction Use Case

链接: http://arxiv.org/abs/2308.09884v1

原文摘要:
In recent times, Large Language Models (LLMs) have captured a global
spotlight and revolutionized the field of Natural Language Processing. One of
the factors attributed to the effectiveness of LLMs is the model architecture
used for training, transformers. Transformer models excel at capturing
contextual features in sequential data since time series data are sequential,
transformer models can be leveraged for more efficient time series data
prediction. The field of prognostics is vital to system health management and
proper maintenance planning. A reliable estimation of the remaining useful life
(RUL) of machines holds the potential for substantial cost savings. This
includes avoiding abrupt machine failures, maximizing equipment usage, and
serving as a decision support system (DSS). This work proposed an
encoder-transformer architecture-based framework for multivariate time series
prediction for a prognostics use case. We validated the effectiveness of the
proposed framework on all four sets of the C-MAPPS benchmark dataset for the
remaining useful life prediction task. To effectively transfer the knowledge
and application of transformers from the natural language domain to time
series, three model-specific experiments were conducted. Also, to enable the
model awareness of the initial stages of the machine life and its degradation
path, a novel expanding window method was proposed for the first time in this
work, it was compared with the sliding window method, and it led to a large
improvement in the performance of the encoder transformer model. Finally, the
performance of the proposed encoder-transformer model was evaluated on the test
dataset and compared with the results from 13 other state-of-the-art (SOTA)
models in the literature and it outperformed them all with an average
performance increase of 137.65% over the next best model across all the
datasets.

中文翻译:
近年来，大型语言模型（LLMs）在全球范围内引发广泛关注，并彻底改变了自然语言处理领域的发展格局。其卓越性能的关键因素之一在于采用了基于Transformer的模型架构进行训练。由于时间序列数据具有顺序特性，而Transformer模型擅长捕捉序列数据中的上下文特征，因此该架构可显著提升时间序列数据预测效率。在系统健康管理领域，预测技术对制定科学维护计划至关重要。准确预测机器的剩余使用寿命（RUL）能带来巨大的经济效益，包括避免突发性设备故障、最大化设备利用率，并为决策支持系统（DSS）提供依据。

本研究提出了一种基于编码器-Transformer架构的多变量时间序列预测框架，专门面向设备寿命预测场景。我们在C-MAPPS基准数据集的所有四个子集上验证了该框架在剩余使用寿命预测任务中的有效性。为有效实现Transformer模型从自然语言领域到时序预测领域的知识迁移，我们设计了三组针对性实验。此外，为使模型能感知设备生命周期初始阶段及其退化轨迹，本研究首次提出创新的扩展窗口方法，通过与滑动窗口法的对比实验证明，该方法使编码器-Transformer模型的性能获得显著提升。最终测试结果表明，相较于文献中13种最先进的（SOTA）模型，我们提出的编码器-Transformer模型在所有数据集上平均性能提升达137.65%，全面超越现有最佳模型。
