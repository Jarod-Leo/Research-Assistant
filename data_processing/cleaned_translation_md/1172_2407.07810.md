# Transformer Alignment in Large Language Models

链接: http://arxiv.org/abs/2407.07810v1

原文摘要:
Large Language Models (LLMs) have made significant strides in natural
language processing, and a precise understanding of the internal mechanisms
driving their success is essential. In this work, we analyze the trajectories
of token embeddings as they pass through transformer blocks, linearizing the
system along these trajectories through their Jacobian matrices. By examining
the relationships between these block Jacobians, we uncover the phenomenon of
\textbf{transformer block coupling} in a multitude of LLMs, characterized by
the coupling of their top singular vectors across tokens and depth. Our
findings reveal that coupling \textit{positively correlates} with model
performance, and that this relationship is stronger than with other
hyperparameters such as parameter count, model depth, and embedding dimension.
We further investigate how these properties emerge during training, observing a
progressive development of coupling, increased linearity, and layer-wise
exponential growth in token trajectories. Additionally, experiments with Vision
Transformers (ViTs) corroborate the emergence of coupling and its relationship
with generalization, reinforcing our findings in LLMs. Collectively, these
insights offer a novel perspective on token interactions in transformers,
opening new directions for studying their mechanisms as well as improving
training and generalization.

中文翻译:
大型语言模型（LLMs）在自然语言处理领域取得了重大进展，而准确理解其成功背后的内部机制至关重要。本研究通过分析词元嵌入在Transformer块间传递的轨迹，利用雅可比矩阵沿这些轨迹对系统进行线性化处理。通过考察这些块雅可比矩阵之间的关系，我们在多种LLMs中发现了**Transformer块耦合**现象——其特征表现为跨词元与深度的顶部奇异向量耦合。研究结果表明：耦合强度与模型性能呈**正相关**，且这种关联性显著强于参数量、模型深度和嵌入维度等其他超参数。我们进一步探究了这些特性在训练过程中的涌现规律，观察到耦合强度渐进发展、线性程度持续提升，以及词元轨迹呈现层间指数级增长的现象。在视觉Transformer（ViTs）上的实验也验证了耦合现象的涌现及其与泛化能力的关联，强化了我们在LLMs中的发现。这些发现为理解Transformer中的词元交互提供了全新视角，为研究其工作机制以及改进训练与泛化性能开辟了新方向。  

，并运用"其"等代词保持指代清晰；专业表述如"singular vectors"译为"奇异向量"、"generalization"译为"泛化"符合计算机领域惯例；主动语态转换（如"we uncover"译为"发现"）增强可读性；关键结论"positively correlates"通过加粗与斜体对应处理突出强调。）
