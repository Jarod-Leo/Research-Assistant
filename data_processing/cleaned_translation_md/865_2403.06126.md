# In-context Prompt Learning for Test-time Vision Recognition with Frozen Vision-language Model

链接: http://arxiv.org/abs/2403.06126v1

原文摘要:
Current pre-trained vision-language models, such as CLIP, have demonstrated
remarkable zero-shot generalization capabilities across various downstream
tasks. However, their performance significantly degrades when test inputs
exhibit different distributions. In this paper, we explore the concept of
test-time prompt tuning (TTPT), which facilitates the adaptation of the CLIP
model to novel downstream tasks through a one-step unsupervised optimization
that involves only test samples. Inspired by in-context learning in natural
language processing (NLP), we propose In-Context Prompt Learning (InCPL) for
test-time visual recognition tasks, which empowers a pre-trained
vision-language model with labeled examples as context information on
downstream task. Specifically, InCPL associates a new test sample with very few
labeled examples (sometimes just one) as context information, enabling reliable
label estimation for the test sample and facilitating model adaptation. To
achieve this, InCPL employs an efficient language-to-vision translator to
explore the textual prior information for visual prompt learning. Further, we
introduce a context-aware unsupervised loss to optimize visual prompts tailored
to test samples. Finally, we design a cyclic learning strategy for visual and
textual prompts to ensure mutual synergy across different modalities. This
enables a pre-trained, frozen CLIP model to adapt to any task using its learned
adaptive prompt. Our method demonstrates superior performance and achieves
state-of-the-art results across various downstream datasets.

中文翻译:
当前预训练的视觉语言模型（如CLIP）在各类下游任务中展现出卓越的零样本泛化能力。然而当测试输入数据呈现不同分布时，其性能会显著下降。本文探索了测试时提示调优（TTPT）的概念，该方法通过仅使用测试样本的一步无监督优化，实现CLIP模型对新下游任务的快速适配。受自然语言处理（NLP）中上下文学习启发，我们提出面向测试时视觉识别任务的上下文提示学习框架（InCPL），该框架通过将标注样本作为下游任务的上下文信息，增强预训练视觉语言模型的适应能力。具体而言，InCPL将新测试样本与极少量标注样本（有时仅需一个）关联为上下文信息，从而实现测试样本的可靠标签预测并促进模型适配。为实现这一目标，InCPL采用高效的语言-视觉转换器挖掘文本先验信息以指导视觉提示学习。进一步地，我们提出上下文感知的无监督损失函数来优化针对测试样本的视觉提示。最后设计跨模态的视觉-文本提示循环学习策略，确保不同模态间的协同增效。这使得参数冻结的预训练CLIP模型能够通过自适应提示适配任意任务。我们的方法在多个下游数据集上展现出卓越性能，取得了最先进的实验结果。

（翻译说明：采用学术论文的规范表达，保留"CLIP"等专业术语不译；将"zero-shot"译为专业术语"零样本"；"in-context learning"译为"上下文学习"以保持NLP领域一致性；处理长句时进行合理切分，如将"one-step unsupervised optimization"译为"一步无监督优化"；运用"适配""协同增效"等符合中文科技文本特点的表述；确保专业概念如"模态""预训练模型"等表述准确统一）
