# Efficient Multimodal Fusion via Interactive Prompting

链接: http://arxiv.org/abs/2304.06306v1

原文摘要:
Large-scale pre-training has brought unimodal fields such as computer vision
and natural language processing to a new era. Following this trend, the size of
multi-modal learning models constantly increases, leading to an urgent need to
reduce the massive computational cost of finetuning these models for downstream
tasks. In this paper, we propose an efficient and flexible multimodal fusion
method, namely PMF, tailored for fusing unimodally pre-trained transformers.
Specifically, we first present a modular multimodal fusion framework that
exhibits high flexibility and facilitates mutual interactions among different
modalities. In addition, we disentangle vanilla prompts into three types in
order to learn different optimizing objectives for multimodal learning. It is
also worth noting that we propose to add prompt vectors only on the deep layers
of the unimodal transformers, thus significantly reducing the training memory
usage. Experiment results show that our proposed method achieves comparable
performance to several other multimodal finetuning methods with less than 3%
trainable parameters and up to 66% saving of training memory usage.

中文翻译:
大规模预训练技术已将计算机视觉和自然语言处理等单模态领域推向新时代。在此趋势下，多模态学习模型的规模持续扩大，如何降低这些模型在下游任务微调过程中的巨大计算成本成为迫切需求。本文提出一种高效灵活的多模态融合方法PMF，专为融合单模态预训练Transformer而设计。具体而言，我们首先提出模块化多模态融合框架，该框架具有高度灵活性，并能促进不同模态间的交互作用。此外，我们将传统提示向量解耦为三种类型，以学习多模态任务中不同的优化目标。值得注意的是，我们提出仅在单模态Transformer的深层添加提示向量，从而显著降低训练内存消耗。实验结果表明，在可训练参数不足3%、训练内存最高节省66%的情况下，本方法性能与多种多模态微调方法相当。

（翻译说明：采用学术论文摘要的规范表达，处理了长句拆分与专业术语统一性问题。将"unimodally pre-trained transformers"译为"单模态预训练Transformer"符合领域术语；"disentangle"译为"解耦"准确体现技术操作；"vanilla prompts"译为"传统提示向量"既保留原文比喻又确保专业性；通过"显著降低"等措辞强化方法优势，同时保持客观表述风格。）
