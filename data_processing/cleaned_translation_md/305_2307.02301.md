# Sumformer: Universal Approximation for Efficient Transformers

链接: http://arxiv.org/abs/2307.02301v1

原文摘要:
Natural language processing (NLP) made an impressive jump with the
introduction of Transformers. ChatGPT is one of the most famous examples,
changing the perception of the possibilities of AI even outside the research
community. However, besides the impressive performance, the quadratic time and
space complexity of Transformers with respect to sequence length pose
significant limitations for handling long sequences. While efficient
Transformer architectures like Linformer and Performer with linear complexity
have emerged as promising solutions, their theoretical understanding remains
limited. In this paper, we introduce Sumformer, a novel and simple architecture
capable of universally approximating equivariant sequence-to-sequence
functions. We use Sumformer to give the first universal approximation results
for Linformer and Performer. Moreover, we derive a new proof for Transformers,
showing that just one attention layer is sufficient for universal
approximation.

中文翻译:
随着Transformer模型的引入，自然语言处理（NLP）实现了跨越式发展。ChatGPT作为最具代表性的案例之一，不仅改变了学术界对人工智能潜力的认知，更在社会各界引发广泛反响。然而，尽管Transformer表现出卓越性能，其随序列长度呈二次方增长的时间和空间复杂度，严重制约了长序列处理能力。虽然Linformer、Performer等具有线性复杂度的改进架构展现出应用前景，但相关理论研究仍存在不足。本文提出Sumformer——一种新颖而简洁的通用架构，能够均匀逼近等变序列到序列函数。基于Sumformer，我们首次为Linformer和Performer提供了普适性逼近的理论证明。此外，我们通过新方法论证了Transformer的逼近能力，揭示仅需单层注意力机制即可实现通用逼近。
