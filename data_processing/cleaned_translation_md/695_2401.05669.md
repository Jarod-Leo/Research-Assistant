# ConcEPT: Concept-Enhanced Pre-Training for Language Models

链接: http://arxiv.org/abs/2401.05669v1

原文摘要:
Pre-trained language models (PLMs) have been prevailing in state-of-the-art
methods for natural language processing, and knowledge-enhanced PLMs are
further proposed to promote model performance in knowledge-intensive tasks.
However, conceptual knowledge, one essential kind of knowledge for human
cognition, still remains understudied in this line of research. This limits
PLMs' performance in scenarios requiring human-like cognition, such as
understanding long-tail entities with concepts. In this paper, we propose
ConcEPT, which stands for Concept-Enhanced Pre-Training for language models, to
infuse conceptual knowledge into PLMs. ConcEPT exploits external taxonomies
with entity concept prediction, a novel pre-training objective to predict the
concepts of entities mentioned in the pre-training contexts. Unlike previous
concept-enhanced methods, ConcEPT can be readily adapted to various downstream
applications without entity linking or concept mapping. Results of extensive
experiments show the effectiveness of ConcEPT in four tasks such as entity
typing, which validates that our model gains improved conceptual knowledge with
concept-enhanced pre-training.

中文翻译:
以下是符合学术规范的中文翻译：

预训练语言模型（PLMs）已成为自然语言处理领域的主流方法，而知识增强型PLMs的提出进一步提升了模型在知识密集型任务中的表现。然而在这类研究中，作为人类认知关键要素的概念性知识仍未得到充分探索。这限制了PLMs在需要类人认知场景（例如通过概念理解长尾实体）中的表现。本文提出概念增强预训练模型ConcEPT（Concept-Enhanced Pre-Training），通过新型预训练目标——实体概念预测，将外部分类体系中的概念知识注入PLMs。与现有概念增强方法不同，ConcEPT无需实体链接或概念映射即可直接适配各类下游应用。在实体类型标注等四项任务上的实验结果表明，ConcEPT能有效通过概念增强预训练获取更丰富的概念知识。

（翻译说明：
1. 专业术语统一处理："pre-trained language models"统一译为"预训练语言模型"并首次出现标注英文缩写PLMs
2. 被动语态转化：将"have been prevailing"等被动结构转换为中文主动表达
3. 长句拆分：将原文复合长句按中文习惯分解为多个短句
4. 概念显化："this line of research"译为"这类研究"而非字面直译
5. 技术表述规范："entity linking"等专业术语采用学界通用译法
6. 保持学术严谨性：准确传达原文方法论特征（如"无需实体链接或概念映射"的对比强调））
