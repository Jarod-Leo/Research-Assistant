# LLMCad: Fast and Scalable On-device Large Language Model Inference

链接: http://arxiv.org/abs/2309.04255v1

原文摘要:
Generative tasks, such as text generation and question answering, hold a
crucial position in the realm of mobile applications. Due to their sensitivity
to privacy concerns, there is a growing demand for their execution directly on
mobile devices. Currently, the execution of these generative tasks heavily
depends on Large Language Models (LLMs). Nevertheless, the limited memory
capacity of these devices presents a formidable challenge to the scalability of
such models.
  In our research, we introduce LLMCad, an innovative on-device inference
engine specifically designed for efficient generative Natural Language
Processing (NLP) tasks. The core idea behind LLMCad revolves around model
collaboration: a compact LLM, residing in memory, takes charge of generating
the most straightforward tokens, while a high-precision LLM steps in to
validate these tokens and rectify any identified errors. LLMCad incorporates
three novel techniques: (1) Instead of generating candidate tokens in a
sequential manner, LLMCad employs the smaller LLM to construct a token tree,
encompassing a wider range of plausible token pathways. Subsequently, the
larger LLM can efficiently validate all of these pathways simultaneously. (2)
It employs a self-adjusting fallback strategy, swiftly initiating the
verification process whenever the smaller LLM generates an erroneous token. (3)
To ensure a continuous flow of token generation, LLMCad speculatively generates
tokens during the verification process by implementing a compute-IO pipeline.
Through an extensive series of experiments, LLMCad showcases an impressive
token generation speed, achieving rates up to 9.3x faster than existing
inference engines.

中文翻译:
文本生成与问答等生成式任务在移动应用领域占据关键地位。由于这类任务对隐私问题高度敏感，业界对其直接在移动设备上运行的需求日益增长。当前，这类生成式任务的执行主要依赖于大语言模型（LLMs）。然而，移动设备有限的内存容量对这些模型的扩展性构成了严峻挑战。

本研究提出LLMCad——一种专为高效生成式自然语言处理（NLP）任务设计的创新性设备端推理引擎。其核心思想在于模型协同机制：一个常驻内存的轻量级LLM负责生成最基础的词元，而高精度LLM则对这些词元进行验证并修正错误。LLMCad包含三项创新技术：（1）摒弃传统顺序生成候选词元的方式，采用小型LLM构建词元树以覆盖更广的可行路径，使大型LLM能同步高效验证所有路径；（2）采用自适应回退策略，当小型LLM生成错误词元时可立即触发验证流程；（3）通过计算-IO流水线设计，在验证过程中预生成词元以保障持续输出流。

经过大量实验验证，LLMCad展现出卓越的词元生成速度，较现有推理引擎最高可提升9.3倍。
