# Bidirectional Awareness Induction in Autoregressive Seq2Seq Models

链接: http://arxiv.org/abs/2408.13959v1

原文摘要:
Autoregressive Sequence-To-Sequence models are the foundation of many Deep
Learning achievements in major research fields such as Vision and Natural
Language Processing. Despite that, they still present significant limitations.
For instance, when errors occur in the early steps of the prediction, the whole
output is severely affected. Such reliance on previously predicted tokens and
the inherent computational unfriendliness of sequential algorithms, motivated
researchers to explore different architectures and methods in the search for
bidirectional approaches. In this work, we introduce the Bidirectional
Awareness Induction (BAI), a training method that leverages a subset of
elements in the network, the Pivots, to perform bidirectional learning without
breaking the autoregressive constraints. To showcase its flexibility, we apply
the method to three architectures, the Transformer, ExpansionNet v2 and GPT,
then perform experiments over three tasks. Experimental results showcase BAI's
effectiveness on all selected tasks and architectures. In particular, we
observed an increase of up to 2.4 CIDEr in Image-Captioning, 4.96 BLEU in
Neural Machine Translation, and 1.16 ROUGE in Text Summarization compared to
the respective baselines. Notably, BAI not only has a positive impact on models
trained from scratch but on pre-trained models as well. Such an aspect,
combined with the absence of architectural requirements synergizes well with
the current trend of LLMs.

中文翻译:
自回归序列到序列模型是计算机视觉和自然语言处理等主要研究领域中许多深度学习成果的基础。尽管如此，这类模型仍存在显著缺陷：当预测早期步骤出现错误时，整个输出会受到严重影响。这种对先前预测标记的依赖性，以及序列算法固有的计算效率问题，促使研究者们探索不同架构与方法以寻求双向建模途径。本研究提出双向感知诱导（BAI）训练方法，该方法利用网络中的特定元素子集——枢轴（Pivots）——在不破坏自回归约束的前提下实现双向学习。为验证其灵活性，我们将该方法应用于Transformer、ExpansionNet v2和GPT三种架构，并在三项任务上进行实验。结果表明BAI在所有选定任务和架构中均表现优异：在图像描述生成任务中CIDEr指标提升2.4分，神经机器翻译任务BLEU值提高4.96分，文本摘要任务ROUGE分数增长1.16分。值得注意的是，BAI不仅对从头训练的模型有效，对预训练模型同样具有提升作用。这一特性加之无需修改模型架构的优势，与当前大语言模型的发展趋势形成了良好协同。
