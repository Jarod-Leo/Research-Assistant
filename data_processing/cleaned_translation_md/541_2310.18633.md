# Setting the Trap: Capturing and Defeating Backdoors in Pretrained Language Models through Honeypots

链接: http://arxiv.org/abs/2310.18633v1

原文摘要:
In the field of natural language processing, the prevalent approach involves
fine-tuning pretrained language models (PLMs) using local samples. Recent
research has exposed the susceptibility of PLMs to backdoor attacks, wherein
the adversaries can embed malicious prediction behaviors by manipulating a few
training samples. In this study, our objective is to develop a
backdoor-resistant tuning procedure that yields a backdoor-free model, no
matter whether the fine-tuning dataset contains poisoned samples. To this end,
we propose and integrate a honeypot module into the original PLM, specifically
designed to absorb backdoor information exclusively. Our design is motivated by
the observation that lower-layer representations in PLMs carry sufficient
backdoor features while carrying minimal information about the original tasks.
Consequently, we can impose penalties on the information acquired by the
honeypot module to inhibit backdoor creation during the fine-tuning process of
the stem network. Comprehensive experiments conducted on benchmark datasets
substantiate the effectiveness and robustness of our defensive strategy.
Notably, these results indicate a substantial reduction in the attack success
rate ranging from 10\% to 40\% when compared to prior state-of-the-art methods.

中文翻译:
在自然语言处理领域，当前主流方法是通过本地样本对预训练语言模型（PLMs）进行微调。最新研究揭示，这类模型易受后门攻击威胁——攻击者仅需操纵少量训练样本即可植入恶意预测行为。本研究旨在开发一种抗后门干扰的微调方案，确保无论微调数据集是否包含污染样本，最终都能获得无后门的纯净模型。为此，我们在原始预训练模型中创新性地植入"蜜罐模块"，该模块专为吸附后门信息而设计。我们的方案基于关键发现：预训练模型底层表征不仅携带充足的后门特征，且几乎不包含原始任务信息。通过惩罚蜜罐模块获取的信息，可有效抑制主干网络在微调过程中后门特征的生成。基准数据集上的全面实验证实了该防御策略的有效性与鲁棒性。尤为显著的是，相较于现有最优防御方案，本方法能将攻击成功率降低10%至40%。
