# Equipping Pretrained Unconditional Music Transformers with Instrument and Genre Controls

链接: http://arxiv.org/abs/2311.12257v1

原文摘要:
The ''pretraining-and-finetuning'' paradigm has become a norm for training
domain-specific models in natural language processing and computer vision. In
this work, we aim to examine this paradigm for symbolic music generation
through leveraging the largest ever symbolic music dataset sourced from the
MuseScore forum. We first pretrain a large unconditional transformer model
using 1.5 million songs. We then propose a simple technique to equip this
pretrained unconditional music transformer model with instrument and genre
controls by finetuning the model with additional control tokens. Our proposed
representation offers improved high-level controllability and expressiveness
against two existing representations. The experimental results show that the
proposed model can successfully generate music with user-specified instruments
and genre. In a subjective listening test, the proposed model outperforms the
pretrained baseline model in terms of coherence, harmony, arrangement and
overall quality.

中文翻译:
以下是您提供的英文论文摘要的中文翻译：

"预训练-微调"范式已成为自然语言处理和计算机视觉领域训练专用模型的常规方法。本研究旨在通过利用从MuseScore论坛获取的有史以来最大规模的符号音乐数据集，探索这一范式在符号音乐生成领域的应用。我们首先使用150万首乐曲预训练了一个大型无条件Transformer模型。随后提出一种简单技术：通过添加控制标记进行微调，使这个预训练的无条件音乐生成模型具备乐器类型和音乐风格的控制能力。相较于现有的两种音乐表征方法，我们提出的表征方案在高层级可控性和表现力方面均有提升。实验结果表明，该模型能成功生成符合用户指定乐器和风格的音乐。在主观听觉测试中，本模型在音乐连贯性、和声效果、编排质量及整体表现上均优于预训练的基线模型。

（翻译说明：
1. 专业术语处理："symbolic music"译为"符号音乐"，"transformer model"保留英文原名
2. 长句拆分：将原文复合句按中文习惯分解为多个短句
3. 被动语态转换："is shown"等结构转为主动表述
4. 概念一致性：保持"pretraining/finetuning"译为"预训练/微调"的学术惯例
5. 文化适配："MuseScore forum"补充说明为获取数据的来源平台）
