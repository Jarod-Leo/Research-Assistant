# A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial Sentiment Analysis

链接: http://arxiv.org/abs/2312.08725v1

原文摘要:
Financial sentiment analysis plays a crucial role in uncovering latent
patterns and detecting emerging trends, enabling individuals to make
well-informed decisions that may yield substantial advantages within the
constantly changing realm of finance. Recently, Large Language Models (LLMs)
have demonstrated their effectiveness in diverse domains, showcasing remarkable
capabilities even in zero-shot and few-shot in-context learning for various
Natural Language Processing (NLP) tasks. Nevertheless, their potential and
applicability in the context of financial sentiment analysis have not been
thoroughly explored yet. To bridge this gap, we employ two approaches:
in-context learning (with a focus on gpt-3.5-turbo model) and fine-tuning LLMs
on a finance-domain dataset. Given the computational costs associated with
fine-tuning LLMs with large parameter sizes, our focus lies on smaller LLMs,
spanning from 250M to 3B parameters for fine-tuning. We then compare the
performances with state-of-the-art results to evaluate their effectiveness in
the finance-domain. Our results demonstrate that fine-tuned smaller LLMs can
achieve comparable performance to state-of-the-art fine-tuned LLMs, even with
models having fewer parameters and a smaller training dataset. Additionally,
the zero-shot and one-shot performance of LLMs produces comparable results with
fine-tuned smaller LLMs and state-of-the-art outcomes. Furthermore, our
analysis demonstrates that there is no observed enhancement in performance for
finance-domain sentiment analysis when the number of shots for in-context
learning is increased.

中文翻译:
金融情感分析在揭示潜在规律和探测新兴趋势方面发挥着关键作用，能帮助个体在不断变化的金融领域做出明智决策，从而获得显著优势。近年来，大型语言模型（LLMs）已在多个领域展现出卓越性能，即便在零样本和小样本上下文学习场景下，也能为各类自然语言处理（NLP）任务提供出色表现。然而，其在金融情感分析领域的潜力与应用价值尚未得到充分探索。为填补这一空白，我们采用两种方法：上下文学习（以gpt-3.5-turbo模型为核心）和在金融领域数据集上对LLMs进行微调。鉴于大参数量LLMs微调所需的高计算成本，我们重点研究了参数量在2.5亿至30亿之间的小型LLMs的微调效果，并将其性能与最先进成果进行对比评估。研究结果表明：经过微调的小型LLMs即使参数更少、训练数据集较小，仍能达到与最先进微调LLMs相媲美的性能；同时，LLMs的零样本和单样本表现与微调小型LLMs及最优成果相比具有可比性。此外，我们的分析显示：增加上下文学习的样本数量时，金融领域情感分析的性能并未出现提升。
