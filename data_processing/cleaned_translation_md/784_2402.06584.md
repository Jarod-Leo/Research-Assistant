# G-SciEdBERT: A Contextualized LLM for Science Assessment Tasks in German

链接: http://arxiv.org/abs/2402.06584v1

原文摘要:
The advancement of natural language processing has paved the way for
automated scoring systems in various languages, such as German (e.g., German
BERT [G-BERT]). Automatically scoring written responses to science questions in
German is a complex task and challenging for standard G-BERT as they lack
contextual knowledge in the science domain and may be unaligned with student
writing styles. This paper presents a contextualized German Science Education
BERT (G-SciEdBERT), an innovative large language model tailored for scoring
German-written responses to science tasks and beyond. Using G-BERT, we
pre-trained G-SciEdBERT on a corpus of 30K German written science responses
with 3M tokens on the Programme for International Student Assessment (PISA)
2018. We fine-tuned G-SciEdBERT on an additional 20K student-written responses
with 2M tokens and examined the scoring accuracy. We then compared its scoring
performance with G-BERT. Our findings revealed a substantial improvement in
scoring accuracy with G-SciEdBERT, demonstrating a 10.2% increase of quadratic
weighted Kappa compared to G-BERT (mean difference = 0.1026, SD = 0.069). These
insights underline the significance of specialized language models like
G-SciEdBERT, which is trained to enhance the accuracy of contextualized
automated scoring, offering a substantial contribution to the field of AI in
education.

中文翻译:
自然语言处理技术的进步为德语等语言的自动评分系统（如德语BERT[G-BERT]）铺平了道路。然而，对德语科学问答题的书面回答进行自动评分是一项复杂任务，标准G-BERT模型因缺乏科学领域的语境知识且可能与学生写作风格不匹配而面临挑战。本文提出一种情境化的德语科学教育BERT模型（G-SciEdBERT），这种创新的大型语言模型专为德语科学问答题及其他场景的书面回答评分而设计。基于G-BERT框架，我们使用2018年国际学生评估计划（PISA）中3万个标记化德语科学回答（含300万词元）对G-SciEdBERT进行预训练，并在额外2万个学生书面回答（含200万词元）上微调后检验其评分准确性。通过与G-BERT的对比实验发现，G-SciEdBERT的评分准确率显著提升，二次加权Kappa系数较G-BERT提高10.2%（均值差=0.1026，标准差=0.069）。这些发现凸显了G-SciEdBERT等专业语言模型的价值——通过针对性训练提升情境化自动评分的准确性，为教育领域的人工智能应用做出重要贡献。  

（翻译说明：  
1. 专业术语处理："quadratic weighted Kappa"译为统计学通用译名"二次加权Kappa系数"  
2. 长句拆分：将原文复合句按中文表达习惯分解为多个短句  
3. 被动语态转换："is pre-trained"等结构转为主动式"进行预训练"  
4. 数据呈现：保留精确数值及统计学术语（均值差/标准差）  
5. 概念显化："contextual knowledge"译为"语境知识"以突出语言学特征  
6. 逻辑衔接：通过"然而""基于""通过"等连词保持论证连贯性）
