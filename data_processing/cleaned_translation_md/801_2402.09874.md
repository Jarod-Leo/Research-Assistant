# Camouflage is all you need: Evaluating and Enhancing Language Model Robustness Against Camouflage Adversarial Attacks

链接: http://arxiv.org/abs/2402.09874v1

原文摘要:
Adversarial attacks represent a substantial challenge in Natural Language
Processing (NLP). This study undertakes a systematic exploration of this
challenge in two distinct phases: vulnerability evaluation and resilience
enhancement of Transformer-based models under adversarial attacks.
  In the evaluation phase, we assess the susceptibility of three Transformer
configurations, encoder-decoder, encoder-only, and decoder-only setups, to
adversarial attacks of escalating complexity across datasets containing
offensive language and misinformation. Encoder-only models manifest a 14% and
21% performance drop in offensive language detection and misinformation
detection tasks, respectively. Decoder-only models register a 16% decrease in
both tasks, while encoder-decoder models exhibit a maximum performance drop of
14% and 26% in the respective tasks.
  The resilience-enhancement phase employs adversarial training, integrating
pre-camouflaged and dynamically altered data. This approach effectively reduces
the performance drop in encoder-only models to an average of 5% in offensive
language detection and 2% in misinformation detection tasks. Decoder-only
models, occasionally exceeding original performance, limit the performance drop
to 7% and 2% in the respective tasks. Although not surpassing the original
performance, Encoder-decoder models can reduce the drop to an average of 6% and
2% respectively.
  Results suggest a trade-off between performance and robustness, with some
models maintaining similar performance while gaining robustness. Our study and
adversarial training techniques have been incorporated into an open-source tool
for generating camouflaged datasets. However, methodology effectiveness depends
on the specific camouflage technique and data encountered, emphasizing the need
for continued exploration.

中文翻译:
对抗性攻击对自然语言处理（NLP）领域构成重大挑战。本研究通过两个递进阶段系统性地探索这一挑战：基于Transformer架构的模型在对抗攻击下的脆弱性评估与鲁棒性增强。

在评估阶段，我们测试了三种Transformer架构（编码器-解码器、纯编码器、纯解码器）在包含攻击性言论和虚假信息的数据集上，面对复杂度递增的对抗攻击时的表现。结果显示：纯编码器模型在攻击性语言检测和虚假信息检测任务中的性能分别下降14%和21%；纯解码器模型在两项任务中均出现16%的性能衰减；编码器-解码器模型则在两项任务中分别出现最高14%和26%的性能降幅。

鲁棒性增强阶段采用对抗训练策略，整合预 camouflaged（预伪装）和动态篡改数据。该方法显著降低了性能衰减：纯编码器模型在攻击性语言检测任务中的平均性能降幅降至5%，虚假信息检测任务中仅2%；纯解码器模型在部分情况下甚至超越基线性能，将两项任务的性能衰减分别控制在7%和2%；编码器-解码器模型虽未突破原始性能，但能将衰减幅度平均降至6%和2%。

研究表明性能与鲁棒性之间存在权衡关系，部分模型在保持性能的同时提升了抗干扰能力。本研究成果及对抗训练技术已集成至开源工具中，可用于生成伪装数据集。但需注意，方法有效性取决于具体采用的伪装技术和数据特征，这凸显了持续探索的必要性。
