# Simple is Better and Large is Not Enough: Towards Ensembling of Foundational Language Models

链接: http://arxiv.org/abs/2308.12272v1

原文摘要:
Foundational Language Models (FLMs) have advanced natural language processing
(NLP) research. Current researchers are developing larger FLMs (e.g., XLNet,
T5) to enable contextualized language representation, classification, and
generation. While developing larger FLMs has been of significant advantage, it
is also a liability concerning hallucination and predictive uncertainty.
Fundamentally, larger FLMs are built on the same foundations as smaller FLMs
(e.g., BERT); hence, one must recognize the potential of smaller FLMs which can
be realized through an ensemble. In the current research, we perform a reality
check on FLMs and their ensemble on benchmark and real-world datasets. We
hypothesize that the ensembling of FLMs can influence the individualistic
attention of FLMs and unravel the strength of coordination and cooperation of
different FLMs. We utilize BERT and define three other ensemble techniques:
{Shallow, Semi, and Deep}, wherein the Deep-Ensemble introduces a
knowledge-guided reinforcement learning approach. We discovered that the
suggested Deep-Ensemble BERT outperforms its large variation i.e. BERTlarge, by
a factor of many times using datasets that show the usefulness of NLP in
sensitive fields, such as mental health.

中文翻译:
以下是符合学术规范的中文翻译：

基础语言模型（FLMs）推动了自然语言处理（NLP）研究的发展。当前研究者正在开发更大型的FLMs（如XLNet、T5）以实现语境化语言表征、分类与生成。虽然开发更大规模的FLMs具有显著优势，但其在幻觉效应和预测不确定性方面也存在缺陷。本质上，大型FLMs与小型FLMs（如BERT）建立在相同的基础架构上，因此我们必须认识到通过集成方法可实现小型FLMs的潜在价值。本研究对基准数据集和真实场景数据集中的FLMs及其集成效果进行了现实检验。我们提出假设：FLMs的集成可以影响其个体注意力机制，并揭示不同FLMs协同合作的优势。本研究以BERT为基础，定义了三种集成技术：{浅层、半深层、深层}，其中深层集成采用了知识引导的强化学习方法。实验发现，在心理健康等敏感领域NLP应用数据集上，提出的深层集成BERT模型性能超越其大型变体BERT-large达数倍之多。

（翻译说明：
1. 专业术语统一处理："hallucination"译为"幻觉效应"，"predictive uncertainty"译为"预测不确定性"
2. 被动语态转换："are built on"译为主动态"建立在"
3. 长句拆分：将原文复合句拆分为符合中文表达习惯的短句
4. 概念显化处理："individualistic attention"译为"个体注意力机制"
5. 保留技术术语原称：BERT-large等专有名词不做翻译
6. 符合学术论文摘要的客观表述风格）
