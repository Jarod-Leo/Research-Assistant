# Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method

链接: http://arxiv.org/abs/2310.17918v1

原文摘要:
Large Language Models (LLMs) have shown great potential in Natural Language
Processing (NLP) tasks. However, recent literature reveals that LLMs generate
nonfactual responses intermittently, which impedes the LLMs' reliability for
further utilization. In this paper, we propose a novel self-detection method to
detect which questions that a LLM does not know that are prone to generate
nonfactual results. Specifically, we first diversify the textual expressions
for a given question and collect the corresponding answers. Then we examine the
divergencies between the generated answers to identify the questions that the
model may generate falsehoods. All of the above steps can be accomplished by
prompting the LLMs themselves without referring to any other external
resources. We conduct comprehensive experiments and demonstrate the
effectiveness of our method on recently released LLMs, e.g., Vicuna, ChatGPT,
and GPT-4.

中文翻译:
大语言模型（LLMs）在自然语言处理（NLP）任务中展现出巨大潜力。然而最新研究表明，LLMs会间歇性生成与事实不符的响应，这影响了模型进一步应用的可靠性。本文提出了一种创新的自我检测方法，用于识别大语言模型自身意识不到、但容易产生虚假回答的问题。具体而言，我们首先对给定问题进行多样化文本表达并收集对应答案，继而通过分析生成答案之间的差异性来定位可能产生谬误的问题。上述所有步骤均可通过提示大语言模型自主完成，无需依赖任何外部资源。我们在Vicuna、ChatGPT和GPT-4等最新发布的模型上进行了全面实验，结果验证了该方法的有效性。

（翻译说明：采用学术论文摘要的标准表述方式，通过以下技巧实现专业性与可读性的平衡：
1. 术语统一处理："nonfactual responses"译为"与事实不符的响应"，"falsehoods"根据语境译为"虚假回答/谬误"
2. 长句拆分：将原文复合句分解为符合中文表达习惯的短句结构
3. 被动语态转化："can be accomplished"译为主动式"可自主完成"
4. 概念显化处理："diversify the textual expressions"具体化为"多样化文本表达"
5. 保持技术严谨性：保留"Vicuna"等模型专有名词不翻译
6. 逻辑连接优化：使用"继而"等连接词保持论证链条清晰）
