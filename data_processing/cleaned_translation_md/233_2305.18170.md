# Leveraging Training Data in Few-Shot Prompting for Numerical Reasoning

链接: http://arxiv.org/abs/2305.18170v1

原文摘要:
Chain-of-thought (CoT) prompting with large language models has proven
effective in numerous natural language processing tasks, but designing prompts
that generalize well to diverse problem types can be challenging, especially in
the context of math word problem (MWP) solving. Additionally, it is common to
have a large amount of training data that have a better diversity coverage but
CoT annotations are not available, which limits the use of supervised learning
techniques. To address these issues, we investigate two approaches to leverage
the training data in a few-shot prompting scenario: dynamic program prompting
and program distillation. Our approach is largely inspired by Gao et al.,
(2022), where they proposed to replace the CoT with the programs as the
intermediate reasoning step. Such a prompting strategy allows us to accurately
verify the answer correctness through program execution in MWP solving. Our
dynamic program prompting involves annotating the training data by sampling
correct programs from a large language model, while program distillation
involves adapting a smaller model to the program-annotated training data. Our
experiments on three standard MWP datasets demonstrate the effectiveness of
these approaches, yielding significant improvements over previous baselines for
prompting and fine-tuning. Our results suggest that leveraging a large amount
of training data can improve the generalization ability of prompts and boost
the performance of fine-tuned small models in MWP solving.

中文翻译:
以下是符合您要求的中文翻译：

【译文】
基于大语言模型的思维链（CoT）提示方法已在众多自然语言处理任务中被证明有效，但设计能泛化至多种问题类型的提示仍具挑战性，尤其在数学应用题（MWP）求解场景中。此外，现实中常存在大量覆盖性良好的训练数据，却缺乏思维链标注，这限制了监督学习技术的应用。为解决这些问题，我们探索了在少样本提示场景下利用训练数据的两种方法：动态程序提示与程序蒸馏。该方法主要受Gao等人（2022）研究的启发，他们提出用程序替代思维链作为中间推理步骤。这种提示策略使我们能通过程序执行准确验证数学应用题求解的正确性。动态程序提示通过从大语言模型采样正确程序来标注训练数据，而程序蒸馏则使小模型适配经程序标注的训练数据。在三个标准数学应用题数据集上的实验表明，这些方法显著超越了现有提示与微调基线模型。结果表明，利用大量训练数据既可提升提示的泛化能力，也能增强微调后小模型在数学应用题求解中的表现。

【翻译要点说明】
1. 术语处理：
- "Chain-of-thought (CoT)" 译为"思维链"并保留英文缩写
- "math word problem (MWP)" 译为"数学应用题"并标注英文缩写
- "dynamic program prompting" 和 "program distillation" 采用意译+专业术语组合

2. 句式重构：
- 将英语长句拆分为符合中文表达习惯的短句（如第一句拆分为两个逻辑单元）
- 被动语态转为主动表述（如"can be challenging"译为"仍具挑战性"）
- 学术用语规范化处理（如"demonstrate"译为"表明"而非"展示"）

3. 概念显化：
- "generalize well" 译为"泛化"而非字面直译
- "supervised learning techniques" 译为"监督学习技术"保持专业度
- "intermediate reasoning step" 译为"中间推理步骤"准确传达概念

4. 衔接处理：
- 添加"此外""而""则"等连接词保持逻辑连贯
- 通过"该方法""这种提示策略"等指代保持上下文衔接

5. 学术风格：
- 使用"被证明""显著超越""结果表明"等学术用语
- 保持客观陈述语气，避免口语化表达

译文在保持学术严谨性的同时，通过合理的句式重组和术语处理，确保了中文表达的自然流畅，符合计算机领域论文摘要的文体特征。
