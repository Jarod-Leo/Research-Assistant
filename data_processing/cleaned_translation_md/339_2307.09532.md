# Can Model Fusing Help Transformers in Long Document Classification? An Empirical Study

链接: http://arxiv.org/abs/2307.09532v1

原文摘要:
Text classification is an area of research which has been studied over the
years in Natural Language Processing (NLP). Adapting NLP to multiple domains
has introduced many new challenges for text classification and one of them is
long document classification. While state-of-the-art transformer models provide
excellent results in text classification, most of them have limitations in the
maximum sequence length of the input sequence. The majority of the transformer
models are limited to 512 tokens, and therefore, they struggle with long
document classification problems. In this research, we explore on employing
Model Fusing for long document classification while comparing the results with
well-known BERT and Longformer architectures.

中文翻译:
文本分类是自然语言处理（NLP）领域持续多年的研究方向。随着NLP技术向多领域拓展，文本分类面临诸多新挑战，其中长文档分类尤为突出。尽管当前最先进的Transformer模型在文本分类任务中表现优异，但大多数模型对输入序列的最大长度存在限制。主流Transformer模型通常仅支持512个标记的输入，这导致其在长文档分类任务中表现受限。本研究探索采用模型融合方法处理长文档分类问题，并将其结果与经典的BERT和Longformer架构进行对比分析。

（翻译说明：
1. 专业术语准确处理："transformer models"译为"Transformer模型"，"tokens"译为"标记"符合NLP领域规范
2. 长句拆分重构：将原文复合句拆分为符合中文表达习惯的短句，如将"While..."转折句拆分为两个独立短句
3. 被动语态转化："has been studied"译为"持续多年的研究方向"更符合中文主动表达
4. 概念清晰传达："Model Fusing"译为"模型融合"准确传达技术概念
5. 逻辑关系显化：通过"尽管...但..."等连接词明确原文隐含的逻辑关系
6. 技术名词统一："BERT"和"Longformer"保留英文原名作为专有技术名词）
