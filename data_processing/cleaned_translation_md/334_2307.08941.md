# NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning

链接: http://arxiv.org/abs/2307.08941v1

原文摘要:
Fine-tuning a pre-trained language model (PLM) emerges as the predominant
strategy in many natural language processing applications. However, this
process is known to be expensive, especially on edge devices with low computing
power. While general approaches (e.g. quantization and distillation) have been
widely studied to reduce the compute/memory of PLM fine-tuning, one-shot
compression techniques specifically designed for fine-tuning remain largely
unexplored. In this paper, we investigate the neural tangent kernel
(NTK)--which reveals the gradient descent dynamics of neural networks--of the
multilayer perceptrons (MLP) modules in a PLM and propose to coin a lightweight
PLM through NTK-approximating MLP fusion. By incorporating NTK into the
compression process, MLP Fusion not only preserves the original model's output
but also maintains its training dynamics. To achieve this, we reconsider the
MLP as a bundle of sub-MLPs and cluster them into a given number of centroids,
which can then be restored as a compressed MLP and surprisingly well
approximate the NTK of the original PLM. Our approach is applicable to both
standard MLP modules and Mixture-of-Experts (MoE) modules in PLMs,
demonstrating its scalability and versatility. Additionally, we provide
theoretical derivations to demonstrate how the proposed compression preserves
the NTK. Extensive experiments of PLM fine-tuning on both natural language
understanding and generation tasks are provided to verify the effectiveness of
MLP fusion. Our code is available at 