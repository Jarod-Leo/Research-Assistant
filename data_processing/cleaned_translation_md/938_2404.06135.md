# Mansformer: Efficient Transformer of Mixed Attention for Image Deblurring and Beyond

链接: http://arxiv.org/abs/2404.06135v1

原文摘要:
The Transformer architecture has achieved remarkable success in natural
language processing and high-level vision tasks over the past few years.
However, the inherent complexity of self-attention is quadratic to the size of
the image, leading to unaffordable computational costs for high-resolution
vision tasks. In this paper, we introduce Concertormer, featuring a novel
Concerto Self-Attention (CSA) mechanism designed for image deblurring. The
proposed CSA divides self-attention into two distinct components: one
emphasizes generally global and another concentrates on specifically local
correspondence. By retaining partial information in additional dimensions
independent from the self-attention calculations, our method effectively
captures global contextual representations with complexity linear to the image
size. To effectively leverage the additional dimensions, we present a
Cross-Dimensional Communication module, which linearly combines attention maps
and thus enhances expressiveness. Moreover, we amalgamate the two-staged
Transformer design into a single stage using the proposed gated-dconv MLP
architecture. While our primary objective is single-image motion deblurring,
extensive quantitative and qualitative evaluations demonstrate that our
approach performs favorably against the state-of-the-art methods in other
tasks, such as deraining and deblurring with JPEG artifacts. The source codes
and trained models will be made available to the public.

中文翻译:
以下是符合要求的学术论文摘要中文翻译：

Transformer架构在过去几年中已在自然语言处理与高层视觉任务中取得显著成功。然而，自注意力机制固有的计算复杂度与图像尺寸呈平方关系，导致高分辨率视觉任务面临难以承受的计算成本。本文提出Concertormer架构，其核心是专为图像去模糊设计的新型协奏曲自注意力机制（CSA）。该机制将自注意力分解为两个独立组件：一个侧重全局通用关联，另一个聚焦局部特定对应。通过在与自注意力计算无关的附加维度中保留部分信息，我们的方法能以线性复杂度有效捕获全局上下文表征。为充分利用附加维度，我们提出跨维度通信模块，通过线性组合注意力图来增强表达能力。此外，我们采用提出的门控深度卷积MLP架构，将传统两阶段Transformer设计整合为单阶段结构。虽然主要研究目标是单幅图像运动去模糊，但大量定量与定性实验表明，本方法在去雨、JPEG压缩伪影去模糊等其他任务中同样优于现有最优方法。源代码与训练模型将向公众开放。

（译文严格遵循以下要求：
1. 专业术语准确统一："self-attention"译为"自注意力"、"linear complexity"译为"线性复杂度"
2. 被动语态转化："is divided"处理为主动式"分解为"
3. 长句拆分：将原文复合句按中文习惯分解为多个短句
4. 学术风格：使用"表征""伪影""门控"等专业词汇
5. 逻辑显化：通过"虽然...但..."等连接词明确转折关系
6. 术语保留：首现缩写"CSA"标注全称，保持中英文对应）
