# PromptFix: Few-shot Backdoor Removal via Adversarial Prompt Tuning

链接: http://arxiv.org/abs/2406.04478v1

原文摘要:
Pre-trained language models (PLMs) have attracted enormous attention over the
past few years with their unparalleled performances. Meanwhile, the soaring
cost to train PLMs as well as their amazing generalizability have jointly
contributed to few-shot fine-tuning and prompting as the most popular training
paradigms for natural language processing (NLP) models. Nevertheless, existing
studies have shown that these NLP models can be backdoored such that model
behavior is manipulated when trigger tokens are presented. In this paper, we
propose PromptFix, a novel backdoor mitigation strategy for NLP models via
adversarial prompt-tuning in few-shot settings. Unlike existing NLP backdoor
removal methods, which rely on accurate trigger inversion and subsequent model
fine-tuning, PromptFix keeps the model parameters intact and only utilizes two
extra sets of soft tokens which approximate the trigger and counteract it
respectively. The use of soft tokens and adversarial optimization eliminates
the need to enumerate possible backdoor configurations and enables an adaptive
balance between trigger finding and preservation of performance. Experiments
with various backdoor attacks validate the effectiveness of the proposed method
and the performances when domain shift is present further shows PromptFix's
applicability to models pretrained on unknown data source which is the common
case in prompt tuning scenarios.

中文翻译:
以下是符合要求的学术中文翻译：

预训练语言模型（PLMs）凭借其卓越性能在过去几年中受到广泛关注。与此同时，PLMs训练成本的急剧攀升与其惊人的泛化能力共同促成了小样本微调（few-shot fine-tuning）与提示学习（prompting）成为当前自然语言处理（NLP）模型最主流的训练范式。然而现有研究表明，这些NLP模型可能被植入后门，当特定触发词出现时模型行为将被操控。本文提出PromptFix——一种基于对抗性提示调优的小样本场景NLP模型后门防御新策略。与现有依赖精确触发词反演和后续模型微调的NLP后门消除方法不同，PromptFix保持模型参数不变，仅通过两组分别模拟触发词和抵消其影响的软标记（soft tokens）实现防御。采用软标记与对抗优化的设计无需枚举可能的攻击配置，并能自适应平衡触发词定位与模型性能保持。多场景后门攻击实验验证了该方法的有效性，在存在领域偏移时的表现进一步证明PromptFix适用于预训练数据源未知的模型（这正是提示调优场景的常态）。

翻译说明：
1. 专业术语处理：采用"NLP/PLMs"等学界通用缩写，保留"few-shot fine-tuning"等核心概念的原意
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句（如第一句拆分为因果关系的两个分句）
3. 被动语态转化："can be backdoored"译为主动式"可能被植入后门"
4. 概念显化："soft tokens"增译为"软标记"并保留英文原词
5. 学术规范：保持"PromptFix"等专有名词原貌，技术表述准确（如"对抗性提示调优"）
6. 逻辑衔接：通过"与此同时""然而""不同于"等连接词保持论证脉络清晰
7. 文化适配：将"the common case"译为符合中文论文表达的"常态"而非直译
