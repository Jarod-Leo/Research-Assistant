# Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the Ugly

链接: http://arxiv.org/abs/2310.03150v1

原文摘要:
Large Language Models (LLM) and foundation models are popular as they offer
new opportunities for individuals and businesses to improve natural language
processing, interact with data, and retrieve information faster. However,
training or fine-tuning LLMs requires a vast amount of data, which can be
challenging to access due to legal or technical restrictions and may require
private computing resources. Federated Learning (FL) is a solution designed to
overcome these challenges and expand data access for deep learning
applications.
  This paper takes a hardware-centric approach to explore how LLMs can be
brought to modern edge computing systems. Our study fine-tunes the FLAN-T5
model family, ranging from 80M to 3B parameters, using FL for a text
summarization task. We provide a micro-level hardware benchmark, compare the
model FLOP utilization to a state-of-the-art data center GPU, and study the
network utilization in realistic conditions. Our contribution is twofold:
First, we evaluate the current capabilities of edge computing systems and their
potential for LLM FL workloads. Second, by comparing these systems with a
data-center GPU, we demonstrate the potential for improvement and the next
steps toward achieving greater computational efficiency at the edge.

中文翻译:
以下是符合要求的学术中文翻译：

大型语言模型（LLM）与基础模型因其为个人和企业提升自然语言处理能力、增强数据交互效率以及加速信息检索提供了新机遇而广受欢迎。然而，LLM的训练或微调需要海量数据，这些数据可能因法律或技术限制难以获取，且往往需要私有计算资源支持。联邦学习（FL）正是为解决这些挑战、扩展深度学习应用的数据访问而设计的解决方案。

本文采用以硬件为核心的研究方法，探讨如何将LLM部署至现代边缘计算系统。我们通过联邦学习对FLAN-T5模型系列（参数量从8000万到30亿不等）进行文本摘要任务的微调，提供微观层面的硬件基准测试，将模型浮点运算利用率与尖端数据中心GPU进行对比，并研究实际场景中的网络利用率。本研究具有双重贡献：首先，我们系统评估了边缘计算体系当前支持LLM联邦学习工作负载的能力与潜力；其次，通过与数据中心GPU的对比研究，揭示了提升边缘计算效率的改进空间与发展路径。

（翻译严格遵循以下原则：
1. 专业术语准确统一（如FL/GPU等缩写保留英文）
2. 被动语态转换为中文主动句式（如"are popular"译为"广受欢迎"）
3. 长难句合理切分（如将原文最后复合句拆分为两个层次）
4. 学术用语规范化（如"benchmark"译为"基准测试"）
5. 保留技术细节精确性（如参数量单位"M/B"转换为中文"万/亿"））
