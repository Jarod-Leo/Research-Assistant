# Bridging the Bosphorus: Advancing Turkish Large Language Models through Strategies for Low-Resource Language Adaptation and Benchmarking

链接: http://arxiv.org/abs/2405.04685v1

原文摘要:
Large Language Models (LLMs) are becoming crucial across various fields,
emphasizing the urgency for high-quality models in underrepresented languages.
This study explores the unique challenges faced by low-resource languages, such
as data scarcity, model selection, evaluation, and computational limitations,
with a special focus on Turkish. We conduct an in-depth analysis to evaluate
the impact of training strategies, model choices, and data availability on the
performance of LLMs designed for underrepresented languages. Our approach
includes two methodologies: (i) adapting existing LLMs originally pretrained in
English to understand Turkish, and (ii) developing a model from the ground up
using Turkish pretraining data, both supplemented with supervised fine-tuning
on a novel Turkish instruction-tuning dataset aimed at enhancing reasoning
capabilities. The relative performance of these methods is evaluated through
the creation of a new leaderboard for Turkish LLMs, featuring benchmarks that
assess different reasoning and knowledge skills. Furthermore, we conducted
experiments on data and model scaling, both during pretraining and fine-tuning,
simultaneously emphasizing the capacity for knowledge transfer across languages
and addressing the challenges of catastrophic forgetting encountered during
fine-tuning on a different language. Our goal is to offer a detailed guide for
advancing the LLM framework in low-resource linguistic contexts, thereby making
natural language processing (NLP) benefits more globally accessible.

中文翻译:
大型语言模型（LLMs）正日益成为各领域的关键技术，这凸显了在资源匮乏语言中开发高质量模型的紧迫性。本研究以土耳其语为焦点，深入探讨低资源语言面临的独特挑战——包括数据稀缺、模型选择、评估体系及算力限制等问题。我们通过系统分析，评估了训练策略、模型架构选择和数据可用性对面向弱势语言的大型语言模型性能的影响。

研究采用双轨方法论：(i) 对以英语预训练的现有LLMs进行土耳其语适应性改造；(ii) 基于土耳其语预训练数据从头构建模型，两种方法均辅以监督微调——使用我们新开发的土耳其语指令调优数据集来增强推理能力。通过建立土耳其语LLMs评估排行榜（包含多种推理与知识技能的基准测试），我们对这两种方法的相对性能进行了量化评估。

此外，我们在预训练和微调阶段同步开展了数据规模与模型规模的扩展实验，既探索跨语言知识迁移的潜力，也着力解决不同语言微调过程中出现的灾难性遗忘问题。本研究旨在为低资源语言环境下的LLM框架发展提供详细指南，从而推动自然语言处理（NLP）技术在全球范围内更平等地惠及各类语言群体。
