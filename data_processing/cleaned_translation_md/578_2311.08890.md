# Large Language Models are legal but they are not: Making the case for a powerful LegalLLM

链接: http://arxiv.org/abs/2311.08890v1

原文摘要:
Realizing the recent advances in Natural Language Processing (NLP) to the
legal sector poses challenging problems such as extremely long sequence
lengths, specialized vocabulary that is usually only understood by legal
professionals, and high amounts of data imbalance. The recent surge of Large
Language Models (LLMs) has begun to provide new opportunities to apply NLP in
the legal domain due to their ability to handle lengthy, complex sequences.
Moreover, the emergence of domain-specific LLMs has displayed extremely
promising results on various tasks. In this study, we aim to quantify how
general LLMs perform in comparison to legal-domain models (be it an LLM or
otherwise). Specifically, we compare the zero-shot performance of three
general-purpose LLMs (ChatGPT-20b, LLaMA-2-70b, and Falcon-180b) on the LEDGAR
subset of the LexGLUE benchmark for contract provision classification. Although
the LLMs were not explicitly trained on legal data, we observe that they are
still able to classify the theme correctly in most cases. However, we find that
their mic-F1/mac-F1 performance is up to 19.2/26.8\% lesser than smaller models
fine-tuned on the legal domain, thus underscoring the need for more powerful
legal-domain LLMs.

中文翻译:
将自然语言处理（NLP）领域的最新进展应用于法律行业面临着诸多挑战，包括超长文本序列、仅限法律专业人士理解的专有术语以及严重的数据不平衡问题。近期兴起的大语言模型（LLM）因其处理长复杂序列的能力，为法律领域的NLP应用开辟了新机遇。而特定领域大语言模型的出现，更是在多项任务中展现出极具前景的表现。本研究旨在量化通用大语言模型与法律领域模型（无论是否属于LLM）的性能差异，具体通过对比三种通用大语言模型（ChatGPT-20b、LLaMA-2-70b和Falcon-180b）在LexGLUE基准测试LEDGAR子集（合同条款分类任务）上的零样本表现。研究发现，尽管这些大语言模型未经专门法律数据训练，但在多数情况下仍能正确分类条款主题。然而其微观F1/宏观F1分数比经过法律领域微调的小型模型最高低19.2%/26.8%，这一结果凸显了开发更强大法律领域大语言模型的必要性。

（翻译说明：采用学术论文摘要的标准句式结构，通过拆分英文长句为中文短句增强可读性。专业术语如"zero-shot"译为"零样本"符合NLP领域规范，计量单位保留国际通用符号%。关键概念"mic-F1/mac-F1"采用"微观F1/宏观F1"的译法既准确又便于理解。通过"开辟新机遇""极具前景"等措辞保持原文的学术严谨性同时提升中文表达流畅度。）
