# A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT

链接: http://arxiv.org/abs/2302.09419v1

原文摘要:
Pretrained Foundation Models (PFMs) are regarded as the foundation for
various downstream tasks with different data modalities. A PFM (e.g., BERT,
ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable
parameter initialization for a wide range of downstream applications. BERT
learns bidirectional encoder representations from Transformers, which are
trained on large datasets as contextual language models. Similarly, the
generative pretrained transformer (GPT) method employs Transformers as the
feature extractor and is trained using an autoregressive paradigm on large
datasets. Recently, ChatGPT shows promising success on large language models,
which applies an autoregressive language model with zero shot or few shot
prompting. The remarkable achievements of PFM have brought significant
breakthroughs to various fields of AI. Numerous studies have proposed different
methods, raising the demand for an updated survey. This study provides a
comprehensive review of recent research advancements, challenges, and
opportunities for PFMs in text, image, graph, as well as other data modalities.
The review covers the basic components and existing pretraining methods used in
natural language processing, computer vision, and graph learning. Additionally,
it explores advanced PFMs used for different data modalities and unified PFMs
that consider data quality and quantity. The review also discusses research
related to the fundamentals of PFMs, such as model efficiency and compression,
security, and privacy. Finally, the study provides key implications, future
research directions, challenges, and open problems in the field of PFMs.
Overall, this survey aims to shed light on the research of the PFMs on
scalability, security, logical reasoning ability, cross-domain learning
ability, and the user-friendly interactive ability for artificial general
intelligence.

中文翻译:
以下是符合要求的学术中文翻译：

预训练基础模型（PFMs）被视为支撑多模态下游任务的通用基础架构。此类模型（如BERT、ChatGPT和GPT-4）通过大规模数据训练，为广泛的下游应用提供了优质的参数初始化方案。其中，BERT基于Transformer架构学习双向编码表征，通过海量数据集训练实现上下文感知的语言建模；而生成式预训练Transformer（GPT）方法同样采用Transformer作为特征提取器，通过自回归范式在大规模数据上进行训练。最新推出的ChatGPT通过零样本/少样本提示的自动回归语言模型，在大型语言模型领域取得突破性进展。PFMs的革命性成就为人工智能各领域带来重大突破，大量创新方法的涌现亟需系统性综述研究。本文从文本、图像、图结构及其他模态维度，全面梳理了PFMs的最新研究进展、核心挑战与发展机遇：首先阐释了自然语言处理、计算机视觉和图学习领域的基础模块与现有预训练方法；进而分析了跨模态专用PFMs与考虑数据质量/数量的统一化PFMs；同时探讨了模型效率与压缩、安全隐私等基础性问题；最后提出关键启示、未来方向及开放性问题。本综述旨在为PFMs的可扩展性、安全性、逻辑推理能力、跨域学习能力及面向通用人工智能的人机交互能力等研究方向提供理论指引。


