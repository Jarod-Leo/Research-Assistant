# UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache Pruning for Efficient Long-Context LLM Inference

链接: http://arxiv.org/abs/2504.07479v1

原文摘要:
Transformer-based large language models (LLMs) have achieved impressive
performance in various natural language processing (NLP) applications. However,
the high memory and computation cost induced by the KV cache limits the
inference efficiency, especially for long input sequences. Compute-in-memory
(CIM)-based accelerators have been proposed for LLM acceleration with KV cache
pruning. However, as existing accelerators only support static pruning with a
fixed pattern or dynamic pruning with primitive implementations, they suffer
from either high accuracy degradation or low efficiency. In this paper, we
propose a ferroelectric FET (FeFET)-based unified content addressable memory
(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous
support for static and dynamic pruning with 3 computation modes: 1) in the CAM
mode, UniCAIM enables approximate similarity measurement in O(1) time for
dynamic KV cache pruning with high energy efficiency; 2) in the charge-domain
CIM mode, static pruning can be supported based on accumulative similarity
score, which is much more flexible compared to fixed patterns; 3) in the
current-domain mode, exact attention computation can be conducted with a subset
of selected KV cache. We further propose a novel CAM/CIM cell design that
leverages the multi-level characteristics of FeFETs for signed multibit storage
of the KV cache and in-place attention computation. With extensive experimental
results, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)
by 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit
level, along with high accuracy comparable with dense attention at the
application level, showing its great potential for efficient long-context LLM
inference.

中文翻译:
基于Transformer架构的大语言模型（LLMs）在各种自然语言处理（NLP）任务中展现出卓越性能。然而，键值缓存（KV cache）导致的高内存与计算开销严重制约了推理效率，尤其对于长输入序列而言。现有基于存内计算（CIM）的加速器虽提出通过KV缓存剪枝来优化LLM推理，但由于仅支持固定模式的静态剪枝或低效实现的动态剪枝，往往面临精度大幅下降或效率低下的问题。本文提出一种基于铁电场效应晶体管（FeFET）的统一内容可寻址存储器（CAM）与存内计算架构UniCAIM，其创新性体现在：1）CAM模式下通过O(1)时间复杂度的近似相似度测量实现高能效动态KV缓存剪枝；2）电荷域CIM模式下基于累积相似度得分支持灵活度远超固定模式的静态剪枝；3）电流域模式下可对筛选后的KV缓存子集执行精确注意力计算。我们进一步设计新型CAM/CIM单元，利用FeFET的多级特性实现KV缓存的有符号多比特存储及原位注意力计算。实验表明，UniCAIM在电路层面将面积-能耗-时延乘积（AEDP）较现有CIM加速器降低8.2-831倍，在应用层面保持与稠密注意力相当的精度，为长上下文LLM高效推理提供了突破性解决方案。
