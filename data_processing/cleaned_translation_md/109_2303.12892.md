# A Small-Scale Switch Transformer and NLP-based Model for Clinical Narratives Classification

链接: http://arxiv.org/abs/2303.12892v1

原文摘要:
Transformer-based models have shown outstanding results in natural language
processing but face challenges in applications like classifying small-scale
clinical texts, especially with constrained computational resources. This study
presents a customized Mixture of Expert (MoE) Transformer models for
classifying small-scale French clinical texts at CHU Sainte-Justine Hospital.
The MoE-Transformer addresses the dual challenges of effective training with
limited data and low-resource computation suitable for in-house hospital use.
Despite the success of biomedical pre-trained models such as CamemBERT-bio,
DrBERT, and AliBERT, their high computational demands make them impractical for
many clinical settings. Our MoE-Transformer model not only outperforms
DistillBERT, CamemBERT, FlauBERT, and Transformer models on the same dataset
but also achieves impressive results: an accuracy of 87\%, precision of 87\%,
recall of 85\%, and F1-score of 86\%. While the MoE-Transformer does not
surpass the performance of biomedical pre-trained BERT models, it can be
trained at least 190 times faster, offering a viable alternative for settings
with limited data and computational resources. Although the MoE-Transformer
addresses challenges of generalization gaps and sharp minima, demonstrating
some limitations for efficient and accurate clinical text classification, this
model still represents a significant advancement in the field. It is
particularly valuable for classifying small French clinical narratives within
the privacy and constraints of hospital-based computational resources.

中文翻译:
基于Transformer的模型在自然语言处理领域表现卓越，但在小规模临床文本分类等应用中面临挑战——尤其在计算资源受限的情况下。本研究为CHU Sainte-Justine医院的小规模法语临床文本分类任务定制了混合专家（MoE）Transformer模型。该MoE-Transformer模型通过创新架构设计，有效解决了有限数据下的高效训练与适合医院内部使用的低资源计算双重挑战。尽管CamemBERT-bio、DrBERT和AliBERT等生物医学预训练模型成效显著，但其高昂的计算成本使得多数临床场景难以实际部署。我们的MoE-Transformer模型不仅在相同数据集上超越了DistillBERT、CamemBERT、FlauBERT及标准Transformer模型，更取得了87%准确率、87%精确率、85%召回率和86% F1分数的优异表现。虽然性能尚未超越生物医学预训练BERT模型，但其训练速度至少提升190倍，为数据与计算资源受限的环境提供了可行方案。尽管该模型在泛化差距和尖锐最小值问题上展现出临床文本分类效率与准确性的某些局限，但其仍标志着该领域的重要突破，尤其对医院隐私保护框架下、基于内部计算资源的小型法语临床叙事分类具有特殊价值。
