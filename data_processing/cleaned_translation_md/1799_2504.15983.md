# W-PCA Based Gradient-Free Proxy for Efficient Search of Lightweight Language Models

链接: http://arxiv.org/abs/2504.15983v1

原文摘要:
The demand for efficient natural language processing (NLP) systems has led to
the development of lightweight language models. Previous work in this area has
primarily focused on manual design or training-based neural architecture search
(NAS) methods. Recently, zero-shot NAS methods have been proposed for
evaluating language models without the need for training. However, prevailing
approaches to zero-shot NAS often face challenges such as biased evaluation
metrics and computational inefficiencies. In this paper, we introduce
weight-weighted PCA (W-PCA), a novel zero-shot NAS method specifically tailored
for lightweight language models. Our approach utilizes two evaluation proxies:
the parameter count and the number of principal components with cumulative
contribution exceeding $\eta$ in the feed-forward neural (FFN) layer.
Additionally, by eliminating the need for gradient computations, we optimize
the evaluation time, thus enhancing the efficiency of designing and evaluating
lightweight language models. We conduct a comparative analysis on the GLUE and
SQuAD datasets to evaluate our approach. The results demonstrate that our
method significantly reduces training time compared to one-shot NAS methods and
achieves higher scores in the testing phase compared to previous
state-of-the-art training-based methods. Furthermore, we perform ranking
evaluations on a dataset sampled from the FlexiBERT search space. Our approach
exhibits superior ranking correlation and further reduces solving time compared
to other zero-shot NAS methods that require gradient computation.

中文翻译:
以下是符合要求的学术摘要中文翻译：

对高效自然语言处理（NLP）系统的需求推动了轻量级语言模型的发展。该领域先前研究主要集中于手动设计或基于训练的神经架构搜索（NAS）方法。近期提出的零样本NAS方法可在无需训练的情况下评估语言模型，但现有方案常面临评估指标偏差和计算效率低下等挑战。本文提出权重加权主成分分析（W-PCA）——一种专为轻量级语言模型设计的新型零样本NAS方法。我们的方法采用双重评估代理：参数总量与前馈神经网络（FFN）层中累计贡献超过η阈值的主成分数量。通过消除梯度计算需求，我们优化了评估时间，从而提升轻量级语言模型设计与评估效率。我们在GLUE和SQuAD数据集上进行了对比实验，结果表明：相比单次NAS方法，本方法显著减少训练时间；相较于现有基于训练的最优方法，在测试阶段获得更高评分。此外，我们在FlexiBERT搜索空间采样数据集上进行排序评估，相较于其他需要梯度计算的零样本NAS方法，本方法展现出更优的排序相关性，并进一步缩短求解时间。


2. 数学符号η保留原文形式
3. 被动语态转换为中文主动表述（如"has led to"→"推动了"）
4. 长难句拆分重组（如原文最后两句的复合结构处理）
5. 学术指标名称（GLUE/SQuAD）保留英文大写形式
6. 技术概念首次出现时标注英文缩写（W-PCA））
