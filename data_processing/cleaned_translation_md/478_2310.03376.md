# Procedural Text Mining with Large Language Models

链接: http://arxiv.org/abs/2310.03376v1

原文摘要:
Recent advancements in the field of Natural Language Processing, particularly
the development of large-scale language models that are pretrained on vast
amounts of knowledge, are creating novel opportunities within the realm of
Knowledge Engineering. In this paper, we investigate the usage of large
language models (LLMs) in both zero-shot and in-context learning settings to
tackle the problem of extracting procedures from unstructured PDF text in an
incremental question-answering fashion. In particular, we leverage the current
state-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model,
accompanied by two variations of in-context learning that involve an ontology
with definitions of procedures and steps and a limited number of samples of
few-shot learning. The findings highlight both the promise of this approach and
the value of the in-context learning customisations. These modifications have
the potential to significantly address the challenge of obtaining sufficient
training data, a hurdle often encountered in deep learning-based Natural
Language Processing techniques for procedure extraction.

中文翻译:
自然语言处理领域的最新进展，尤其是基于海量知识预训练的大规模语言模型的发展，正在为知识工程领域创造新的机遇。本文研究了大型语言模型（LLMs）在零样本学习和上下文学习两种设置中的应用，通过渐进式问答的方式从非结构化PDF文本中提取流程信息。我们特别采用了当前最先进的GPT-4（生成式预训练变换模型4），并结合两种上下文学习改进方案：其一是引入包含流程步骤定义的领域本体，其二是采用少量样本的小样本学习。研究结果既验证了该方法的可行性，也凸显了上下文学习定制化方案的价值。这些改进方案有望显著缓解流程提取任务中训练数据不足的挑战——这一难题在基于深度学习的自然语言处理技术中屡见不鲜。

（翻译说明：
1. 专业术语处理："zero-shot"译为"零样本"、"few-shot"译为"小样本"保持学术规范性
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句，如将"in an incremental question-answering fashion"转化为"通过渐进式问答的方式"作为独立状语
3. 概念显化："in-context learning customisations"译为"上下文学习定制化方案"以准确传达技术内涵
4. 被动语态转换：将"are creating"等英文被动结构转化为中文主动表述"正在创造"
5. 文化适配："hurdle often encountered"译为"屡见不鲜"既保持专业又符合中文表达
6. 术语统一性：全篇保持"流程提取"与"procedure extraction"的严格对应）
