# Training language models for deeper understanding improves brain alignment

链接: http://arxiv.org/abs/2212.10898v1

原文摘要:
Building systems that achieve a deeper understanding of language is one of
the central goals of natural language processing (NLP). Towards this goal,
recent works have begun to train language models on narrative datasets which
require extracting the most critical information by integrating across long
contexts. However, it is still an open question whether these models are
learning a deeper understanding of the text, or if the models are simply
learning a heuristic to complete the task. This work investigates this further
by turning to the one language processing system that truly understands complex
language: the human brain. We show that training language models for deeper
narrative understanding results in richer representations that have improved
alignment to human brain activity. We further find that the improvements in
brain alignment are larger for character names than for other discourse
features, which indicates that these models are learning important narrative
elements. Taken together, these results suggest that this type of training can
indeed lead to deeper language understanding. These findings have consequences
both for cognitive neuroscience by revealing some of the significant factors
behind brain-NLP alignment, and for NLP by highlighting that understanding of
long-range context can be improved beyond language modeling.

中文翻译:
构建能够深入理解语言的系统是自然语言处理（NLP）的核心目标之一。为实现这一目标，近期研究开始尝试在叙事数据集上训练语言模型——这类任务要求通过整合长上下文来提取最关键的信息。然而这些模型究竟是在学习对文本的深层理解，还是仅仅掌握了完成任务的经验法则，目前仍无定论。本研究通过转向真正理解复杂语言的语言处理系统——人类大脑——对此展开深入探究。我们发现，针对深层叙事理解训练的语言模型能产生更丰富的表征，这些表征与人类大脑活动具有更高的对齐性。进一步研究发现，这种大脑对齐性提升在角色名称方面比其他话语特征更为显著，表明模型确实掌握了重要的叙事元素。综合来看，这些结果表明此类训练确实能够促进更深层的语言理解。这些发现既为认知神经科学揭示了影响大脑-NLP对齐的关键因素，也为NLP领域指明：通过超越语言建模的方法，对长距离上下文的理解能力还能获得进一步提升。
