# A Hybrid Transformer Architecture with a Quantized Self-Attention Mechanism Applied to Molecular Generation

链接: http://arxiv.org/abs/2502.19214v1

原文摘要:
The success of the self-attention mechanism in classical machine learning
models has inspired the development of quantum analogs aimed at reducing
computational overhead. Self-attention integrates learnable query and key
matrices to calculate attention scores between all pairs of tokens in a
sequence. These scores are then multiplied by a learnable value matrix to
obtain the output self-attention matrix, enabling the model to effectively
capture long-range dependencies within the input sequence. Here, we propose a
hybrid quantum-classical self-attention mechanism as part of a transformer
decoder, the architecture underlying large language models (LLMs). To
demonstrate its utility in chemistry, we train this model on the QM9 dataset
for conditional generation, using SMILES strings as input, each labeled with a
set of physicochemical properties that serve as conditions during inference.
Our theoretical analysis shows that the time complexity of the query-key dot
product is reduced from $\mathcal{O}(n^2 d)$ in a classical model to
$\mathcal{O}(n^2\log d)$ in our quantum model, where $n$ and $d$ represent the
sequence length and embedding dimension, respectively. We perform simulations
using NVIDIA's CUDA-Q platform, which is designed for efficient GPU
scalability. This work provides a promising avenue for quantum-enhanced natural
language processing (NLP).

中文翻译:
以下是符合学术规范的中文翻译：

经典机器学习模型中自注意力机制的成功，启发了旨在降低计算开销的量子类比方法开发。自注意力机制通过整合可学习的查询矩阵和键矩阵，计算序列中所有标记对之间的注意力分数。这些分数随后与可学习的值矩阵相乘，最终输出自注意力矩阵，使模型能够有效捕捉输入序列中的长程依赖关系。本文提出一种混合量子-经典自注意力机制，将其作为支撑大语言模型（LLMs）的Transformer解码器架构组成部分。为验证其在化学领域的实用性，我们在QM9数据集上以SMILES字符串作为输入进行条件生成训练，每个字符串标注的物理化学性质集合将作为推理时的生成条件。理论分析表明，查询-键点积运算的时间复杂度从经典模型的$\mathcal{O}(n^2 d)$降至量子模型的$\mathcal{O}(n^2\log d)$，其中$n$和$d$分别表示序列长度和嵌入维度。我们使用专为高效GPU扩展设计的NVIDIA CUDA-Q平台进行模拟实验。该研究为量子增强的自然语言处理（NLP）提供了可行路径。

（翻译说明：1. 专业术语如"self-attention"统一译为"自注意力"；2. 保持数学表达式原貌；3. 机构名"NVIDIA"保留原名；4. 技术术语"SMILES"作为专有名词未翻译；5. 被动语态转换为中文主动表述；6. 长句按中文习惯拆分，确保符合科技论文摘要的简洁性要求）
