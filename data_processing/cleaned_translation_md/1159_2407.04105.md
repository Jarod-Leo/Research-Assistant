# Can Pre-trained Language Models Understand Chinese Humor?

链接: http://arxiv.org/abs/2407.04105v1

原文摘要:
Humor understanding is an important and challenging research in natural
language processing. As the popularity of pre-trained language models (PLMs),
some recent work makes preliminary attempts to adopt PLMs for humor recognition
and generation. However, these simple attempts do not substantially answer the
question: {\em whether PLMs are capable of humor understanding?} This paper is
the first work that systematically investigates the humor understanding ability
of PLMs. For this purpose, a comprehensive framework with three evaluation
steps and four evaluation tasks is designed. We also construct a comprehensive
Chinese humor dataset, which can fully meet all the data requirements of the
proposed evaluation framework. Our empirical study on the Chinese humor dataset
yields some valuable observations, which are of great guiding value for future
optimization of PLMs in humor understanding and generation.

中文翻译:
幽默理解是自然语言处理领域一项重要且富有挑战性的研究课题。随着预训练语言模型（PLMs）的普及，近期研究开始初步尝试将PLMs应用于幽默识别与生成任务。然而这些简单尝试并未从根本上回答关键问题："预训练语言模型是否真正具备幽默理解能力？"本文首次系统性地探究了PLMs的幽默理解能力，为此设计出包含三个评估阶段、四项测评任务的综合框架，并构建了能够完全满足该评估框架所有数据需求的中文幽默数据集。基于该数据集的实证研究得出了若干重要发现，这些发现对未来优化PLMs的幽默理解与生成能力具有重要指导价值。


