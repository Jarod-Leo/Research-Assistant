# Robustness of LLMs to Perturbations in Text

链接: http://arxiv.org/abs/2407.08989v1

原文摘要:
Having a clean dataset has been the foundational assumption of most natural
language processing (NLP) systems. However, properly written text is rarely
found in real-world scenarios and hence, oftentimes invalidates the
aforementioned foundational assumption. Recently, Large language models (LLMs)
have shown impressive performance, but can they handle the inevitable noise in
real-world data? This work tackles this critical question by investigating
LLMs' resilience against morphological variations in text. To that end, we
artificially introduce varying levels of noise into a diverse set of datasets
and systematically evaluate LLMs' robustness against the corrupt variations of
the original text. Our findings show that contrary to popular beliefs,
generative LLMs are quiet robust to noisy perturbations in text. This is a
departure from pre-trained models like BERT or RoBERTa whose performance has
been shown to be sensitive to deteriorating noisy text. Additionally, we test
LLMs' resilience on multiple real-world benchmarks that closely mimic commonly
found errors in the wild. With minimal prompting, LLMs achieve a new
state-of-the-art on the benchmark tasks of Grammar Error Correction (GEC) and
Lexical Semantic Change (LSC). To empower future research, we also release a
dataset annotated by humans stating their preference for LLM vs.
human-corrected outputs along with the code to reproduce our results.

中文翻译:
拥有干净的数据集一直是大多数自然语言处理（NLP）系统的基本假设。然而在现实场景中，规范书写的文本极为罕见，这使得上述基础假设往往难以成立。近期，大语言模型（LLMs）展现出令人瞩目的性能表现，但它们能否应对现实数据中不可避免的噪声？本研究通过探究LLMs对文本形态变化的适应能力，对这一关键问题展开探讨。为此，我们在多样化数据集上人为引入不同强度的噪声，系统评估LLMs对原始文本污染变体的鲁棒性。研究发现与普遍认知相反，生成式LLMs对文本噪声扰动表现出显著稳健性——这与BERT或RoBERTa等预训练模型形成鲜明对比，后者的性能已被证实对文本质量退化十分敏感。此外，我们在多个模拟现实常见错误的基准测试中验证了LLMs的适应能力。通过极简提示，LLMs在语法纠错（GEC）和词汇语义演变（LSC）基准任务上创造了最新性能记录。为助力未来研究，我们同步发布了包含人类标注偏好的数据集（对比LLM与人工修正输出的质量评估）以及可复现实验结果的代码库。

（翻译说明：采用学术论文的严谨表述风格，通过以下处理实现专业性与可读性平衡：
1. 术语统一："morphological variations"译为"形态变化"符合计算语言学规范
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句（如第一段处理）
3. 概念显化："deteriorating noisy text"译为"文本质量退化"准确传达原文指涉现象
4. 被动语态转化："has been shown"转为主动式"已被证实"
5. 技术名词保留：LLMs/BERT/RoBERTa等专业缩写首次出现时保留英文并标注中文全称
6. 文化适配："in the wild"译为"现实场景"比直译"野外"更符合中文学术语境）
