# Mitigating Social Bias in Large Language Models: A Multi-Objective Approach within a Multi-Agent Framework

链接: http://arxiv.org/abs/2412.15504v1

原文摘要:
Natural language processing (NLP) has seen remarkable advancements with the
development of large language models (LLMs). Despite these advancements, LLMs
often produce socially biased outputs. Recent studies have mainly addressed
this problem by prompting LLMs to behave ethically, but this approach results
in unacceptable performance degradation. In this paper, we propose a
multi-objective approach within a multi-agent framework (MOMA) to mitigate
social bias in LLMs without significantly compromising their performance. The
key idea of MOMA involves deploying multiple agents to perform causal
interventions on bias-related contents of the input questions, breaking the
shortcut connection between these contents and the corresponding answers.
Unlike traditional debiasing techniques leading to performance degradation,
MOMA substantially reduces bias while maintaining accuracy in downstream tasks.
Our experiments conducted on two datasets and two models demonstrate that MOMA
reduces bias scores by up to 87.7%, with only a marginal performance
degradation of up to 6.8% in the BBQ dataset. Additionally, it significantly
enhances the multi-objective metric icat in the StereoSet dataset by up to
58.1%. Code will be made available at 