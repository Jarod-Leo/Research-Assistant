# A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look

链接: http://arxiv.org/abs/2411.08275v1

原文摘要:
The application of large language models to provide relevance assessments
presents exciting opportunities to advance information retrieval, natural
language processing, and beyond, but to date many unknowns remain. This paper
reports on the results of a large-scale evaluation (the TREC 2024 RAG Track)
where four different relevance assessment approaches were deployed in situ: the
"standard" fully manual process that NIST has implemented for decades and three
different alternatives that take advantage of LLMs to different extents using
the open-source UMBRELA tool. This setup allows us to correlate system rankings
induced by the different approaches to characterize tradeoffs between cost and
quality. We find that in terms of nDCG@20, nDCG@100, and Recall@100, system
rankings induced by automatically generated relevance assessments from UMBRELA
correlate highly with those induced by fully manual assessments across a
diverse set of 77 runs from 19 teams. Our results suggest that automatically
generated UMBRELA judgments can replace fully manual judgments to accurately
capture run-level effectiveness. Surprisingly, we find that LLM assistance does
not appear to increase correlation with fully manual assessments, suggesting
that costs associated with human-in-the-loop processes do not bring obvious
tangible benefits. Overall, human assessors appear to be stricter than UMBRELA
in applying relevance criteria. Our work validates the use of LLMs in academic
TREC-style evaluations and provides the foundation for future studies.

中文翻译:
以下是您提供的英文论文摘要的中文翻译：

【大规模语言模型在相关性评估中的应用为推进信息检索、自然语言处理等领域带来了令人振奋的机遇，但迄今为止仍存在诸多未知。本文报告了TREC 2024 RAG Track大规模评估结果，该研究在真实场景中部署了四种不同的相关性评估方法：美国国家标准与技术研究院（NIST）沿用数十年的"标准"全人工评估流程，以及三种不同程度利用开源UMBRELA工具集成LLM技术的替代方案。这种实验设计使我们能够通过不同方法产生的系统排名相关性来量化成本与质量之间的权衡。研究发现，在nDCG@20、nDCG@100和Recall@100指标上，UMBRELA自动生成的相关性评估所导出的系统排名，与来自19个团队77组实验的全人工评估结果呈现高度相关性。结果表明自动生成的UMBRELA判断可替代全人工判断来准确捕捉系统级别的有效性。出乎意料的是，LLM的辅助并未显著提升与全人工评估的相关性，这表明人机协同流程的相关成本并未带来明显的实质性收益。总体而言，人类评估员在应用相关性标准时比UMBRELA更为严格。本研究验证了LLM在学术性TREC风格评估中的适用性，为未来研究奠定了基础。】

翻译说明：
1. 专业术语处理：保留"TREC"、"nDCG"等技术缩写，采用"信息检索"等标准译法
2. 机构名称处理：NIST译为全称"美国国家标准与技术研究院"并首次标注缩写
3. 被动语态转换：将英文被动结构转换为中文主动句式（如"were deployed"译为"部署了"）
4. 长句拆分：将原文复合长句按中文表达习惯分解为多个短句
5. 概念对应："human-in-the-loop"译为"人机协同流程"以准确传达技术内涵
6. 数据呈现：精确保持"77 runs from 19 teams"等量化信息的完整性
7. 学术风格：使用"呈现高度相关性"等符合学术论文表达的措辞
