# Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations

链接: http://arxiv.org/abs/2310.11207v1

原文摘要:
Large language models (LLMs) such as ChatGPT have demonstrated superior
performance on a variety of natural language processing (NLP) tasks including
sentiment analysis, mathematical reasoning and summarization. Furthermore,
since these models are instruction-tuned on human conversations to produce
"helpful" responses, they can and often will produce explanations along with
the response, which we call self-explanations. For example, when analyzing the
sentiment of a movie review, the model may output not only the positivity of
the sentiment, but also an explanation (e.g., by listing the sentiment-laden
words such as "fantastic" and "memorable" in the review). How good are these
automatically generated self-explanations? In this paper, we investigate this
question on the task of sentiment analysis and for feature attribution
explanation, one of the most commonly studied settings in the interpretability
literature (for pre-ChatGPT models). Specifically, we study different ways to
elicit the self-explanations, evaluate their faithfulness on a set of
evaluation metrics, and compare them to traditional explanation methods such as
occlusion or LIME saliency maps. Through an extensive set of experiments, we
find that ChatGPT's self-explanations perform on par with traditional ones, but
are quite different from them according to various agreement metrics, meanwhile
being much cheaper to produce (as they are generated along with the
prediction). In addition, we identified several interesting characteristics of
them, which prompt us to rethink many current model interpretability practices
in the era of ChatGPT(-like) LLMs.

中文翻译:
以下是符合您要求的中文翻译：

以ChatGPT为代表的大语言模型（LLM）在情感分析、数学推理和文本摘要等多种自然语言处理（NLP）任务中展现出卓越性能。这些模型经过人类对话指令微调后能够生成"有帮助"的响应，因此经常会在输出答案时同步产生解释性内容，我们称之为"自我解释"。例如，在分析电影评论情感倾向时，模型不仅会输出情感极性（如正面评价），还会附带解释（例如列举评论中的情感关键词，如"精彩绝伦"、"令人难忘"等）。这类自动生成的自我解释质量如何？本文针对情感分析任务，聚焦可解释性研究中最常探讨的特征归因解释场景（针对ChatGPT之前的传统模型），系统探究了以下问题：我们通过多种方式激发模型的自我解释，采用多维度评估指标检验其忠实度，并与遮挡法、LIME显著性图等传统解释方法进行对比。大量实验表明，ChatGPT的自我解释在效果上与传统方法相当，但根据各类一致性指标显示其解释机制存在本质差异，且生成成本显著更低（因其与预测结果同步产生）。此外，我们还发现这些自我解释具有若干独特性质，这些发现促使我们重新思考当前ChatGPT类大语言模型时代的可解释性研究范式。

（翻译严格遵循以下要点：
1. 专业术语准确统一（如LLM译作"大语言模型"，sentiment analysis译作"情感分析"）
2. 长句合理切分，如将原文复合从句拆解为符合中文表达习惯的短句结构
3. 被动语态转化（如"are instruction-tuned"译为主动式"经过...微调后"）
4. 文化适配处理（如"fantastic"译为"精彩绝伦"而非直译）
5. 学术风格保持，使用"归因"、"极性"等规范术语
6. 逻辑连接词优化，如"furthermore"转为"因此"实现自然衔接）
