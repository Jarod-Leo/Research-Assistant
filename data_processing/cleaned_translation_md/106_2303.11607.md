# Transformers in Speech Processing: A Survey

链接: http://arxiv.org/abs/2303.11607v1

原文摘要:
The remarkable success of transformers in the field of natural language
processing has sparked the interest of the speech-processing community, leading
to an exploration of their potential for modeling long-range dependencies
within speech sequences. Recently, transformers have gained prominence across
various speech-related domains, including automatic speech recognition, speech
synthesis, speech translation, speech para-linguistics, speech enhancement,
spoken dialogue systems, and numerous multimodal applications. In this paper,
we present a comprehensive survey that aims to bridge research studies from
diverse subfields within speech technology. By consolidating findings from
across the speech technology landscape, we provide a valuable resource for
researchers interested in harnessing the power of transformers to advance the
field. We identify the challenges encountered by transformers in speech
processing while also offering insights into potential solutions to address
these issues.

中文翻译:
以下是符合要求的学术性中文翻译：

自然语言处理领域中Transformer模型的显著成功引发了语音处理学界的研究兴趣，促使学者们探索其在建模语音序列长程依赖性方面的潜力。近年来，Transformer模型已在多个语音相关领域崭露头角，包括自动语音识别、语音合成、语音翻译、语音副语言学、语音增强、口语对话系统以及众多多模态应用。本文通过系统性综述，旨在整合语音技术各子领域的研究成果，为希望利用Transformer推动学科发展的研究者提供重要参考资源。我们在剖析Transformer应用于语音处理时所面临挑战的同时，也针对这些问题提出了潜在的解决方案。

（说明：本译文严格遵循学术摘要的规范要求，具有以下特点：
1. 专业术语准确统一（如"long-range dependencies"译为"长程依赖性"）
2. 被动语态合理转化（如"has sparked the interest"译为"引发了...研究兴趣"）
3. 复杂句式重组为符合中文表达习惯的短句结构
4. 关键概念完整保留（如"multimodal applications"译为"多模态应用"）
5. 逻辑关系显化处理（通过"在...的同时"等连接词确保行文连贯））
