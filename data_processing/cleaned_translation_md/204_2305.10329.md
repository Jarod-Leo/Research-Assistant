# G-Adapter: Towards Structure-Aware Parameter-Efficient Transfer Learning for Graph Transformer Networks

链接: http://arxiv.org/abs/2305.10329v1

原文摘要:
It has become a popular paradigm to transfer the knowledge of large-scale
pre-trained models to various downstream tasks via fine-tuning the entire model
parameters. However, with the growth of model scale and the rising number of
downstream tasks, this paradigm inevitably meets the challenges in terms of
computation consumption and memory footprint issues. Recently,
Parameter-Efficient Fine-Tuning (PEFT) (e.g., Adapter, LoRA, BitFit) shows a
promising paradigm to alleviate these concerns by updating only a portion of
parameters. Despite these PEFTs having demonstrated satisfactory performance in
natural language processing, it remains under-explored for the question of
whether these techniques could be transferred to graph-based tasks with Graph
Transformer Networks (GTNs). Therefore, in this paper, we fill this gap by
providing extensive benchmarks with traditional PEFTs on a range of graph-based
downstream tasks. Our empirical study shows that it is sub-optimal to directly
transfer existing PEFTs to graph-based tasks due to the issue of feature
distribution shift. To address this issue, we propose a novel structure-aware
PEFT approach, named G-Adapter, which leverages graph convolution operation to
introduce graph structure (e.g., graph adjacent matrix) as an inductive bias to
guide the updating process. Besides, we propose Bregman proximal point
optimization to further alleviate feature distribution shift by preventing the
model from aggressive update. Extensive experiments demonstrate that G-Adapter
obtains the state-of-the-art performance compared to the counterparts on nine
graph benchmark datasets based on two pre-trained GTNs, and delivers tremendous
memory footprint efficiency compared to the conventional paradigm.

中文翻译:
以下为符合学术规范的中文翻译：

通过微调整个预训练模型的参数来将其知识迁移至各类下游任务已成为当前主流范式。然而随着模型规模扩大与下游任务数量激增，这种范式不可避免地面临计算消耗和内存占用方面的挑战。近期，参数高效微调方法（PEFT，如Adapter、LoRA、BitFit）通过仅更新部分参数展现出解决这些问题的潜力。尽管这些方法在自然语言处理领域已取得显著成效，但其在基于图Transformer网络（GTNs）的图任务中的适用性仍缺乏系统探索。为此，本文通过在一系列图下游任务上对传统PEFT方法进行广泛基准测试填补了这一空白。实证研究表明，由于特征分布偏移问题，直接将现有PEFT迁移至图任务存在次优性。针对该问题，我们提出一种新型结构感知PEFT方法——G-Adapter，该方法利用图卷积运算将图结构（如邻接矩阵）作为归纳偏置来指导参数更新过程。此外，我们引入Bregman近端点优化算法，通过抑制模型的激进更新进一步缓解特征分布偏移。基于两个预训练GTNs在九个图基准数据集上的实验表明，G-Adapter相比现有方法取得最先进的性能，同时较传统范式显著提升了内存使用效率。

（翻译严格遵循以下原则：
1. 专业术语准确统一（如PEFT/参数高效微调、inductive bias/归纳偏置）
2. 长句合理切分，符合中文表达习惯
3. 被动语态转换为主动句式（如"it remains under-explored"→"仍缺乏系统探索"）
4. 关键概念首次出现标注英文原名
5. 保留学术文本的严谨性，避免口语化表达）
