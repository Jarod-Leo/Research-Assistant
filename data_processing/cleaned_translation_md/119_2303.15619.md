# Typhoon: Towards an Effective Task-Specific Masking Strategy for Pre-trained Language Models

链接: http://arxiv.org/abs/2303.15619v1

原文摘要:
Through exploiting a high level of parallelism enabled by graphics processing
units, transformer architectures have enabled tremendous strides forward in the
field of natural language processing. In a traditional masked language model,
special MASK tokens are used to prompt our model to gather contextual
information from surrounding words to restore originally hidden information. In
this paper, we explore a task-specific masking framework for pre-trained large
language models that enables superior performance on particular downstream
tasks on the datasets in the GLUE benchmark. We develop our own masking
algorithm, Typhoon, based on token input gradients, and compare this with other
standard baselines. We find that Typhoon offers performance competitive with
whole-word masking on the MRPC dataset. Our implementation can be found in a
public Github Repository.

中文翻译:
通过充分利用图形处理器（GPU）所提供的高度并行计算能力，Transformer架构在自然语言处理领域实现了重大突破。传统掩码语言模型中，特殊的[MASK]标记被用于提示模型从上下文词汇中收集信息，以还原被遮蔽的原始内容。本文研究了一种面向预训练大语言模型的"任务特定掩码框架"，该框架在GLUE基准测试数据集的下游任务中展现出卓越性能。我们基于词元输入梯度开发了新型掩码算法Typhoon，并与多种标准基线方法进行对比实验。研究发现，在MRPC数据集上，Typhoon算法与全词掩码技术（whole-word masking）具有相当的竞争力。相关实现代码已公开在Github仓库中。

（翻译说明：
1. 专业术语处理："graphics processing units"译为行业通用简称"图形处理器（GPU）"，"Transformer"保留原名
2. 技术概念转化："masked language model"译为"掩码语言模型"，符合NLP领域术语规范
3. 长句拆分：将原文复合长句拆分为符合中文表达习惯的短句结构
4. 被动语态转换："are used to"译为主动式"被用于"
5. 专有名词保留：GLUE、MRPC等基准名称保留英文原名
6. 算法名称处理：Typhoon作为专有算法名保留不译
7. 补充说明：在首次出现GPU时添加括号注释，符合中文技术文献惯例）
