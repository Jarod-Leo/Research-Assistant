# Is LLM the Silver Bullet to Low-Resource Languages Machine Translation?

链接: http://arxiv.org/abs/2503.24102v1

原文摘要:
Low-Resource Languages (LRLs) present significant challenges in natural
language processing due to their limited linguistic resources and
underrepresentation in standard datasets. While recent advancements in Large
Language Models (LLMs) and Neural Machine Translation (NMT) have substantially
improved translation capabilities for high-resource languages, performance
disparities persist for LRLs, particularly impacting privacy-sensitive and
resource-constrained scenarios. This paper systematically evaluates the
limitations of current LLMs across 200 languages using benchmarks such as
FLORES-200. We also explore alternative data sources, including news articles
and bilingual dictionaries, and demonstrate how knowledge distillation from
large pre-trained models can significantly improve smaller LRL translations.
Additionally, we investigate various fine-tuning strategies, revealing that
incremental enhancements markedly reduce performance gaps on smaller LLMs.

中文翻译:
低资源语言（LRLs）由于语言资源匮乏且在标准数据集中代表性不足，给自然语言处理带来了重大挑战。尽管大语言模型（LLMs）和神经机器翻译（NMT）的最新进展显著提升了高资源语言的翻译能力，但低资源语言仍存在性能差距，这对隐私敏感型和资源受限场景影响尤为突出。本文通过FLORES-200等基准测试，系统评估了当前大语言模型在200种语言中的局限性。我们探索了新闻文本、双语词典等替代数据源，并证明从大型预训练模型进行知识蒸馏可显著提升小型低资源翻译模型的性能。此外，通过研究不同微调策略发现，增量式改进能明显缩小小型大语言模型的性能差距。

（翻译说明：
1. 专业术语处理：采用"低资源语言（LRLs）"、"大语言模型（LLMs）"等括号标注英文原词，符合学术翻译规范
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句，如将"performance disparities persist..."独立成句
3. 被动语态转换："are underrepresented"译为主动式"代表性不足"
4. 概念显化处理："privacy-sensitive and resource-constrained scenarios"具体化为"隐私敏感型和资源受限场景"
5. 技术术语统一："knowledge distillation"始终译为"知识蒸馏"，"fine-tuning"译为"微调"
6. 逻辑连接显化：添加"此外"等连接词保持行文连贯性）
