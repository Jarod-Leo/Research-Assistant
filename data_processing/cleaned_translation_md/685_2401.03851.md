# Aligned with LLM: a new multi-modal training paradigm for encoding fMRI activity in visual cortex

链接: http://arxiv.org/abs/2401.03851v1

原文摘要:
Recently, there has been a surge in the popularity of pre trained large
language models (LLMs) (such as GPT-4), sweeping across the entire Natural
Language Processing (NLP) and Computer Vision (CV) communities. These LLMs have
demonstrated advanced multi-modal understanding capabilities and showcased
strong performance across various benchmarks. The LLM has started to embody
traits of artificial general intelligence, which holds vital guidance for
enhancing brain-like characteristics within visual encoding models. Hence, This
paper proposes a new multi-modal training paradigm, aligning with LLM, for
encoding fMRI activity in visual cortex. Based on this paradigm, we trained an
encoding model in fMRI data named the LLM-Visual Encoding Model (LLM-VEM).
Specifically, we utilize LLM (miniGPT4) to generate descriptive text for all
stimulus images, forming a high-quality textual description set. Moreover, we
use the pre-trained text encoder (CLIP) to process these detailed descriptions,
obtaining the text embedding features. Next, we use the contrast loss function
to minimize the distance between the image embedding features and the text
embedding features to complete the alignment operation of the stimulus image
and text information. With the assistance of the pre-trained LLM, this
alignment process facilitates better learning of the visual encoding model,
resulting in higher precision. The final experimental results indicate that our
training paradigm has significantly aided in enhancing the performance of the
visual encoding model.

中文翻译:
近年来，预训练大语言模型（LLMs）（如GPT-4）的兴起席卷了整个自然语言处理（NLP）与计算机视觉（CV）领域。这些大语言模型不仅展现出先进的多模态理解能力，更在各种基准测试中表现出卓越性能。大语言模型已开始呈现通用人工智能的特征，这对提升视觉编码模型的类脑特性具有重要指导意义。为此，本文提出一种与大语言模型对齐的新型多模态训练范式，用于编码视觉皮层的功能磁共振成像（fMRI）活动。基于该范式，我们在fMRI数据上训练了一个名为LLM-视觉编码模型（LLM-VEM）的编码模型。

具体而言，我们利用大语言模型（miniGPT4）为所有刺激图像生成描述性文本，构建高质量文本描述集；继而通过预训练文本编码器（CLIP）处理这些详细描述，获取文本嵌入特征；随后采用对比损失函数最小化图像嵌入特征与文本嵌入特征之间的距离，完成刺激图像与文本信息的对齐操作。在预训练大语言模型的辅助下，这一对齐过程能帮助视觉编码模型更高效地学习，从而获得更高精度。最终实验结果表明，我们的训练范式对提升视觉编码模型性能具有显著促进作用。


