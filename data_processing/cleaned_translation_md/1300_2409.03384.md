# Hardware Acceleration of LLMs: A comprehensive survey and comparison

链接: http://arxiv.org/abs/2409.03384v1

原文摘要:
Large Language Models (LLMs) have emerged as powerful tools for natural
language processing tasks, revolutionizing the field with their ability to
understand and generate human-like text. In this paper, we present a
comprehensive survey of the several research efforts that have been presented
for the acceleration of transformer networks for Large Language Models using
hardware accelerators.
  The survey presents the frameworks that have been proposed and then performs
a qualitative and quantitative comparison regarding the technology, the
processing platform (FPGA, ASIC, In-Memory, GPU), the speedup, the energy
efficiency, the performance (GOPs), and the energy efficiency (GOPs/W) of each
framework. The main challenge in comparison is that every proposed scheme is
implemented on a different process technology making hard a fair comparison.
The main contribution of this paper is that we extrapolate the results of the
performance and the energy efficiency on the same technology to make a fair
comparison; one theoretical and one more practical. We implement part of the
LLMs on several FPGA chips to extrapolate the results to the same process
technology and then we make a fair comparison of the performance.

中文翻译:
以下是符合您要求的中文翻译：

大型语言模型（LLMs）已成为自然语言处理领域的强大工具，其理解和生成类人文本的能力为这一领域带来了革命性变化。本文针对使用硬件加速器优化大型语言模型中变压器网络的相关研究进行了全面综述。

本调查首先系统梳理了现有加速框架，随后从技术路线、处理平台（FPGA、ASIC、存内计算、GPU）、加速比、能效比、运算性能（GOPs）及能效指标（GOPs/W）等维度进行了定性与定量对比。研究面临的主要挑战在于：由于各方案采用不同制程工艺实现，导致直接对比存在困难。

本文的核心贡献在于：通过将不同方案的性能与能效数据归一化至相同制程工艺下，建立了两种公平比较基准（理论基准与实践基准）。我们通过在多种FPGA芯片上实现部分LLM功能模块，将实验结果折算至相同制程工艺，最终实现了性能指标的公正对比。


3. 被动语态转换为主动表述（如"are implemented"译为"采用"）
4. 关键量化指标保留国际通用单位（GOPs/W）
5. 通过"归一化"等专业词汇准确传达"extrapolate"的技术内涵）
