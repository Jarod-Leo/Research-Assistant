# CoLT5: Faster Long-Range Transformers with Conditional Computation

链接: http://arxiv.org/abs/2303.09752v1

原文摘要:
Many natural language processing tasks benefit from long inputs, but
processing long documents with Transformers is expensive -- not only due to
quadratic attention complexity but also from applying feedforward and
projection layers to every token. However, not all tokens are equally
important, especially for longer documents. We propose CoLT5, a long-input
Transformer model that builds on this intuition by employing conditional
computation, devoting more resources to important tokens in both feedforward
and attention layers. We show that CoLT5 achieves stronger performance than
LongT5 with much faster training and inference, achieving SOTA on the
long-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractably
make use of extremely long inputs, showing strong gains up to 64k input length.

中文翻译:
许多自然语言处理任务受益于长文本输入，但使用Transformer模型处理长文档的成本高昂——这不仅源于注意力机制二次方的计算复杂度，还因为需要将前馈网络和投影层应用于每个token。然而并非所有token都同等重要，尤其对于长文档而言。我们提出CoLT5模型，这一长文本Transformer基于条件计算思想，在前馈层和注意力层中为重要token分配更多计算资源。实验表明，CoLT5在训练和推理速度显著提升的同时，性能优于LongT5模型，在长文本基准SCROLLS上达到当前最优水平。该模型还能高效处理超长输入，在64k token长度的输入上仍能保持显著的性能提升。
