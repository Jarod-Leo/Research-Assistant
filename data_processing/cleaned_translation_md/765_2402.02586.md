# ClipFormer: Key-Value Clipping of Transformers on Memristive Crossbars for Write Noise Mitigation

链接: http://arxiv.org/abs/2402.02586v1

原文摘要:
Transformers have revolutionized various real-world applications from natural
language processing to computer vision. However, traditional von-Neumann
computing paradigm faces memory and bandwidth limitations in accelerating
transformers owing to their massive model sizes. To this end, In-memory
Computing (IMC) crossbars based on Non-volatile Memories (NVMs), due to their
ability to perform highly parallelized Matrix-Vector-Multiplications (MVMs)
with high energy-efficiencies, have emerged as a promising solution for
accelerating transformers. However, analog MVM operations in crossbars
introduce non-idealities, such as stochastic read & write noise, which affect
the inference accuracy of the deployed transformers. Specifically, we find
pre-trained Vision Transformers (ViTs) to be vulnerable on crossbars due to the
impact of write noise on the dynamically-generated Key (K) and Value (V)
matrices in the attention layers, an effect not accounted for in prior studies.
We, thus, propose ClipFormer, a transformation on the K and V matrices during
inference, to boost the non-ideal accuracies of pre-trained ViT models.
ClipFormer requires no additional hardware and training overhead and is
amenable to transformers deployed on any memristive crossbar platform. Our
experiments on Imagenet-1k dataset using pre-trained DeiT-S transformers,
subjected to standard training and variation-aware-training, show >10-40%
higher non-ideal accuracies at the high write noise regime by applying
ClipFormer.

中文翻译:
Transformer模型已彻底革新了从自然语言处理到计算机视觉的各类现实应用。然而，由于模型规模庞大，传统冯·诺依曼计算架构在加速Transformer时面临内存与带宽限制。基于非易失性存储器的存内计算（IMC）交叉阵列因其能高效执行高并行化矩阵向量乘法（MVM）运算，成为加速Transformer的理想解决方案。但交叉阵列中的模拟MVM操作会引入非理想因素（如随机读写噪声），影响部署模型的推理精度。研究发现，预训练视觉Transformer（ViT）在交叉阵列上表现脆弱，主要原因在于写入噪声会动态影响注意力层中生成的键（K）矩阵和值（V）矩阵——这一关键影响在先前研究中未被充分考虑。为此，我们提出ClipFormer方案：在推理阶段对K、V矩阵进行变换处理，无需额外硬件投入和训练开销，即可提升预训练ViT模型在非理想条件下的准确率。该方案适用于所有忆阻器交叉阵列平台部署的Transformer模型。在ImageNet-1k数据集上对预训练DeiT-S模型的实验表明，无论是标准训练还是考虑器件变异的训练方式，ClipFormer在高写入噪声环境下可使非理想准确率提升10-40%以上。
