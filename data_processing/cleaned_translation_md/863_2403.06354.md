# Amharic LLaMA and LLaVA: Multimodal LLMs for Low Resource Languages

链接: http://arxiv.org/abs/2403.06354v1

原文摘要:
Large Language Models (LLMs) like GPT-4 and LLaMA have shown incredible
proficiency at natural language processing tasks and have even begun to excel
at tasks across other modalities such as vision and audio. Despite their
success, LLMs often struggle to perform well on low-resource languages because
there is so little training data available. This shortcoming is especially
prevalent with open source models. In this work, we explore training LLaMA-2 to
speak Amharic, a language which is spoken by over 50 million people world wide,
but has orders of magnitude less data available than languages like English. We
employ methods previously used for training LLMs on other languages with data
scarcity, and use open source translation models to perform data augmentation
and grow our dataset from millions of tokens to billions. We further enhance
the capabilities of our model by connecting an image encoder and training on a
translated visual instruction tuning dataset in the same manner as LLaVA,
resulting in a multimodal Amharic LLM that can understand images along with
text. We introduce an Amharic version of a popular benchmarking dataset to
evaluate our work. Our models and dataset are open sourced and available on
GitHub.

中文翻译:
以下是符合您要求的中文翻译：

【中文译文】
以GPT-4和LLaMA为代表的大语言模型（LLMs）在自然语言处理任务中展现出卓越能力，甚至开始在视觉、听觉等多模态任务中取得突破。然而这些模型在资源匮乏语言上的表现仍不尽如人意，主要由于训练数据严重不足，这一现象在开源模型中尤为突出。本研究探索了LLaMA-2模型的阿姆哈拉语（全球使用人口超5000万，但可用数据量较英语等语言相差数个数量级）训练方案：首先采用其他低资源语言训练中已验证的方法，通过开源翻译模型进行数据增强，将原始百万级token数据集扩展至十亿规模；随后仿照LLaVA方法连接图像编码器，使用翻译版视觉指令调优数据集进行训练，最终获得能同时理解图像与文本的多模态阿姆哈拉语大模型。我们特别构建了阿姆哈拉语版基准测试数据集用于评估，所有模型及数据集均已开源发布。

【关键术语处理】
1. LLMs = 大语言模型（学术通用译法）
2. tokens = token（保留英文）/词元（根据语境）
3. data augmentation = 数据增强（计算机领域标准译法）
4. multimodal = 多模态（AI领域标准译法）
5. benchmarking dataset = 基准测试数据集（符合技术文档规范）

【技术细节说明】
1. 将"orders of magnitude"译为"相差数个数量级"更符合中文科技文献表达习惯
2. "visual instruction tuning"译为"视觉指令调优"准确传达模型微调技术内涵
3. 采用"仿照...方法"的表述明确技术路线继承关系
4. 对阿姆哈拉语使用人口数据采用"超5000万"的简洁表述

【学术风格保持】
1. 使用被动语态"采用...方法"保持客观性
2. 通过分号结构保持长句逻辑清晰
3. 专业术语首次出现标注英文原名
4. 结果陈述使用"最终获得"等学术论文常用表述

【可读性优化】
1. 将原文三个段落整合为符合中文摘要习惯的单一段落
2. 调整英文长句为中文短句结构（如处理"we employ..."复合句）
3. 使用"首先/随后"等连接词替代部分英文衔接词
4. 技术方法描述采用动词引导结构（如"通过...进行"）增强动态感

需要调整任何技术细节或表达风格，请随时告知。
