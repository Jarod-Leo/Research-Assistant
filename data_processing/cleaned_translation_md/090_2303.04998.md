# Rethinking Visual Prompt Learning as Masked Visual Token Modeling

链接: http://arxiv.org/abs/2303.04998v1

原文摘要:
Prompt learning has achieved great success in efficiently exploiting
large-scale pre-trained models in natural language processing (NLP). It
reformulates the downstream tasks as the generative pre-training ones to
achieve consistency, thus improving the performance stably. However, when
transferring it to the vision area, current visual prompt learning methods are
almost designed on discriminative pre-trained models, and there is also a lack
of careful design to unify the forms of pre-training and downstream tasks. To
explore prompt learning on the generative pre-trained visual model, as well as
keeping the task consistency, we propose Visual Prompt learning as masked
visual Token Modeling (VPTM) to transform the downstream visual classification
into the pre-trained masked visual token prediction. In addition, we develop
the prototypical verbalizer for mapping the predicted visual token with
implicit semantics to explicit downstream labels. To our best knowledge, VPTM
is the first visual prompt method on the generative pre-trained visual model,
which achieves consistency between pre-training and downstream visual
classification by task reformulation. Experiments show that VPTM outperforms
other visual prompt methods and achieves excellent efficiency. Moreover, the
task consistency of VPTM contributes to the robustness against prompt location,
prompt length and prototype dimension, and could be deployed uniformly.

中文翻译:
以下是符合学术规范的中文翻译：

提示学习在自然语言处理（NLP）领域的高效利用大规模预训练模型方面取得了显著成功。该方法通过将下游任务重新构建为生成式预训练任务来实现任务形式的一致性，从而稳定提升模型性能。然而，当将其迁移至视觉领域时，现有视觉提示学习方法几乎都基于判别式预训练模型设计，且缺乏统一预训练与下游任务形式的精心设计。为探索生成式预训练视觉模型上的提示学习并保持任务一致性，我们提出将视觉提示学习构建为掩码视觉标记建模（VPTM），将下游视觉分类任务转化为预训练的掩码视觉标记预测任务。此外，我们开发了原型化语义映射器，用于将具有隐式语义的预测视觉标记映射至显式的下游任务标签。据我们所知，VPTM是首个基于生成式预训练视觉模型的提示学习方法，通过任务重构实现了预训练与下游视觉分类的任务一致性。实验表明，VPTM在性能上优于其他视觉提示方法且具有优异效率。此外，VPTM的任务一致性使其对提示位置、提示长度和原型维度具有鲁棒性，可实现统一部署。

（翻译说明：
1. 专业术语处理："verbalizer"译为"语义映射器"符合计算机视觉领域术语规范
2. 被动语态转换：将英文被动式转换为中文主动式（如"are designed"→"基于...设计"）
3. 长句拆分：将原文复合长句按中文习惯分解为多个短句
4. 概念一致性：保持"prompt learning"统一译为"提示学习"，"downstream tasks"统一译为"下游任务"
5. 学术表达："To our best knowledge"采用学术规范译法"据我们所知"
6. 技术准确性：准确翻译"masked visual token prediction"等核心技术概念）
