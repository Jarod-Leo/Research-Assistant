# Can we obtain significant success in RST discourse parsing by using Large Language Models?

链接: http://arxiv.org/abs/2403.05065v1

原文摘要:
Recently, decoder-only pre-trained large language models (LLMs), with several
tens of billion parameters, have significantly impacted a wide range of natural
language processing (NLP) tasks. While encoder-only or encoder-decoder
pre-trained language models have already proved to be effective in discourse
parsing, the extent to which LLMs can perform this task remains an open
research question. Therefore, this paper explores how beneficial such LLMs are
for Rhetorical Structure Theory (RST) discourse parsing. Here, the parsing
process for both fundamental top-down and bottom-up strategies is converted
into prompts, which LLMs can work with. We employ Llama 2 and fine-tune it with
QLoRA, which has fewer parameters that can be tuned. Experimental results on
three benchmark datasets, RST-DT, Instr-DT, and the GUM corpus, demonstrate
that Llama 2 with 70 billion parameters in the bottom-up strategy obtained
state-of-the-art (SOTA) results with significant differences. Furthermore, our
parsers demonstrated generalizability when evaluated on RST-DT, showing that,
in spite of being trained with the GUM corpus, it obtained similar performances
to those of existing parsers trained with RST-DT.

中文翻译:
近年来，仅解码器架构的预训练大语言模型（LLMs）凭借其数百亿参数量，已对自然语言处理（NLP）各类任务产生深远影响。尽管仅编码器或编码器-解码器架构的预训练语言模型在篇章解析任务中已被证实有效，但LLMs在此任务中的表现仍是一个待解的研究问题。为此，本文探究了此类LLMs在修辞结构理论（RST）篇章解析中的适用性。研究将自顶向下和自底向上两种基础解析策略转化为LLMs可处理的提示模板，并采用参数量可调部分较少的QLoRA方法对Llama 2模型进行微调。在RST-DT、Instr-DT和GUM语料库三个基准数据集上的实验表明：采用自底向上策略的700亿参数Llama 2模型以显著优势取得了最先进（SOTA）性能。值得注意的是，尽管模型仅在GUM语料库上训练，其在RST-DT上的评估结果与现有基于RST-DT训练的解析器性能相当，这验证了我们提出的解析器具有优异的泛化能力。
