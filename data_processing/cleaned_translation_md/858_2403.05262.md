# Debiasing Large Visual Language Models

链接: http://arxiv.org/abs/2403.05262v1

原文摘要:
In the realms of computer vision and natural language processing, Large
Vision-Language Models (LVLMs) have become indispensable tools, proficient in
generating textual descriptions based on visual inputs. Despite their
advancements, our investigation reveals a noteworthy bias in the generated
content, where the output is primarily influenced by the underlying Large
Language Models (LLMs) prior rather than the input image. Our empirical
experiments underscore the persistence of this bias, as LVLMs often provide
confident answers even in the absence of relevant images or given incongruent
visual input. To rectify these biases and redirect the model's focus toward
vision information, we introduce two simple, training-free strategies. Firstly,
for tasks such as classification or multi-choice question-answering (QA), we
propose a ``calibration'' step through affine transformation to adjust the
output distribution. This ``Post-Hoc debias'' approach ensures uniform scores
for each answer when the image is absent, serving as an effective
regularization technique to alleviate the influence of LLM priors. For more
intricate open-ended generation tasks, we extend this method to ``Debias
sampling'', drawing inspirations from contrastive decoding methods.
Furthermore, our investigation sheds light on the instability of LVLMs across
various decoding configurations. Through systematic exploration of different
settings, we significantly enhance performance, surpassing reported results and
raising concerns about the fairness of existing evaluations. Comprehensive
experiments substantiate the effectiveness of our proposed strategies in
mitigating biases. These strategies not only prove beneficial in minimizing
hallucinations but also contribute to the generation of more helpful and
precise illustrations.

中文翻译:
在计算机视觉与自然语言处理领域，大型视觉语言模型（LVLMs）已成为不可或缺的工具，能够基于视觉输入生成文本描述。然而研究发现，这些模型存在显著偏差：生成内容主要受底层大语言模型（LLMs）先验知识主导，而非输入图像。实证实验表明，即便在没有相关图像或视觉输入矛盾的情况下，LVLMs仍会给出高置信度答案，这种偏差具有持续性。

为纠正偏差并引导模型关注视觉信息，我们提出两种无需重新训练的简易策略：首先针对分类或多选题（QA）任务，通过仿射变换实现"校准"步骤来调整输出分布。这种"事后去偏"方法确保当图像缺失时各答案得分均匀分布，成为缓解LLM先验影响的有效正则化技术；对于更复杂的开放式生成任务，受对比解码方法启发，我们将该方法扩展为"去偏采样"。

研究还揭示了LVLMs在不同解码配置下的不稳定性。通过系统探索不同参数设置，我们显著提升了模型性能，不仅超越了现有报告结果，更引发了对当前评估公平性的质疑。综合实验证实了所提策略在缓解偏差方面的有效性：这些策略不仅能有效减少幻觉现象，还可生成更具帮助性且精确的图文描述。
