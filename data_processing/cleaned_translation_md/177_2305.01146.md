# RadAdapt: Radiology Report Summarization via Lightweight Domain Adaptation of Large Language Models

链接: http://arxiv.org/abs/2305.01146v1

原文摘要:
We systematically investigate lightweight strategies to adapt large language
models (LLMs) for the task of radiology report summarization (RRS).
Specifically, we focus on domain adaptation via pretraining (on natural
language, biomedical text, or clinical text) and via discrete prompting or
parameter-efficient fine-tuning. Our results consistently achieve best
performance by maximally adapting to the task via pretraining on clinical text
and fine-tuning on RRS examples. Importantly, this method fine-tunes a mere
0.32% of parameters throughout the model, in contrast to end-to-end fine-tuning
(100% of parameters). Additionally, we study the effect of in-context examples
and out-of-distribution (OOD) training before concluding with a radiologist
reader study and qualitative analysis. Our findings highlight the importance of
domain adaptation in RRS and provide valuable insights toward developing
effective natural language processing solutions for clinical tasks.

中文翻译:
我们系统研究了将大语言模型（LLMs）适配于放射学报告摘要（RRS）任务的轻量化策略。具体而言，我们聚焦于通过预训练（基于自然语言、生物医学文本或临床文本）以及离散提示或参数高效微调来实现领域适应。研究结果表明：通过在临床文本上进行预训练并结合RRS样本微调来实现任务最大化适配时，模型始终能获得最佳性能。值得注意的是，该方法仅需微调模型总量的0.32%参数，而传统端到端微调则需要调整100%参数。此外，我们还研究了上下文示例和分布外（OOD）训练的影响，最终通过放射科医师的阅读评估和定性分析完成研究。我们的发现凸显了领域适应在RRS任务中的重要性，并为开发适用于临床任务的高效自然语言处理解决方案提供了宝贵见解。
