# All in One: Multi-task Prompting for Graph Neural Networks

链接: http://arxiv.org/abs/2307.01504v1

原文摘要:
Recently, ''pre-training and fine-tuning'' has been adopted as a standard
workflow for many graph tasks since it can take general graph knowledge to
relieve the lack of graph annotations from each application. However, graph
tasks with node level, edge level, and graph level are far diversified, making
the pre-training pretext often incompatible with these multiple tasks. This gap
may even cause a ''negative transfer'' to the specific application, leading to
poor results. Inspired by the prompt learning in natural language processing
(NLP), which has presented significant effectiveness in leveraging prior
knowledge for various NLP tasks, we study the prompting topic for graphs with
the motivation of filling the gap between pre-trained models and various graph
tasks. In this paper, we propose a novel multi-task prompting method for graph
models. Specifically, we first unify the format of graph prompts and language
prompts with the prompt token, token structure, and inserting pattern. In this
way, the prompting idea from NLP can be seamlessly introduced to the graph
area. Then, to further narrow the gap between various graph tasks and
state-of-the-art pre-training strategies, we further study the task space of
various graph applications and reformulate downstream problems to the
graph-level task. Afterward, we introduce meta-learning to efficiently learn a
better initialization for the multi-task prompt of graphs so that our prompting
framework can be more reliable and general for different tasks. We conduct
extensive experiments, results from which demonstrate the superiority of our
method.

中文翻译:
近年来，"预训练-微调"模式已成为图学习领域的标准流程，其通过迁移通用图知识有效缓解了各应用场景中图标注数据不足的问题。然而，节点级、边级和图级任务的巨大差异性，使得预训练任务往往难以与多样化的下游任务兼容，这种隔阂甚至可能导致"负迁移"现象，致使具体应用效果劣化。受自然语言处理（NLP）中提示学习技术的启发——该方法在利用先验知识适应不同NLP任务方面展现出显著优势——我们致力于研究图提示学习，以弥合预训练模型与多样化图任务之间的鸿沟。本文提出一种创新的多任务图提示学习方法：首先通过提示标记、标记结构和插入模式的统一设计，实现图提示与语言提示的格式对齐，从而将NLP领域的提示思想无缝迁移至图领域；其次，为缩小各类图任务与前沿预训练策略的差距，我们系统分析图应用的任务空间，将下游问题重新表述为图级任务；进而引入元学习机制，高效获取多任务图提示的优化初始化参数，使提示框架具备跨任务的可靠性与泛化性。大量实验结果表明，该方法具有显著优越性。
