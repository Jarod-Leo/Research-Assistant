# KazMMLU: Evaluating Language Models on Kazakh, Russian, and Regional Knowledge of Kazakhstan

链接: http://arxiv.org/abs/2502.12829v1

原文摘要:
Despite having a population of twenty million, Kazakhstan's culture and
language remain underrepresented in the field of natural language processing.
Although large language models (LLMs) continue to advance worldwide, progress
in Kazakh language has been limited, as seen in the scarcity of dedicated
models and benchmark evaluations. To address this gap, we introduce KazMMLU,
the first MMLU-style dataset specifically designed for Kazakh language. KazMMLU
comprises 23,000 questions that cover various educational levels, including
STEM, humanities, and social sciences, sourced from authentic educational
materials and manually validated by native speakers and educators. The dataset
includes 10,969 Kazakh questions and 12,031 Russian questions, reflecting
Kazakhstan's bilingual education system and rich local context. Our evaluation
of several state-of-the-art multilingual models (Llama-3.1, Qwen-2.5, GPT-4,
and DeepSeek V3) demonstrates substantial room for improvement, as even the
best-performing models struggle to achieve competitive performance in Kazakh
and Russian. These findings underscore significant performance gaps compared to
high-resource languages. We hope that our dataset will enable further research
and development of Kazakh-centric LLMs. Data and code will be made available
upon acceptance.

中文翻译:
尽管拥有两千万人口，哈萨克斯坦的文化与语言在自然语言处理领域仍处于代表性不足的状态。尽管全球范围内大语言模型（LLMs）持续发展，但哈萨克语的进展却十分有限——专用模型与基准评估的稀缺便是有力佐证。为填补这一空白，我们推出了首个专为哈萨克语设计的MMLU风格数据集KazMMLU。该数据集包含23,000道题目，涵盖STEM、人文社科等多个学科领域，题目源自真实教育资料并经母语人士与教育工作者人工校验。其中包含10,969道哈萨克语题目与12,031道俄语题目，充分体现了哈萨克斯坦双语教育体系与本土特色。我们对多款前沿多语言模型（Llama-3.1、Qwen-2.5、GPT-4及DeepSeek V3）的评估表明：即便表现最优的模型在哈萨克语和俄语任务中也难以达到竞争力水平，这凸显出与高资源语言之间显著的性能差距。我们期待该数据集能推动以哈萨克语为核心的LLMs研究发展。数据与代码将在论文录用后公开。
