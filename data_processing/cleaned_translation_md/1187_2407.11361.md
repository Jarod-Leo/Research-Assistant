# Graph Structure Prompt Learning: A Novel Methodology to Improve Performance of Graph Neural Networks

链接: http://arxiv.org/abs/2407.11361v1

原文摘要:
Graph neural networks (GNNs) are widely applied in graph data modeling.
However, existing GNNs are often trained in a task-driven manner that fails to
fully capture the intrinsic nature of the graph structure, resulting in
sub-optimal node and graph representations. To address this limitation, we
propose a novel Graph structure Prompt Learning method (GPL) to enhance the
training of GNNs, which is inspired by prompt mechanisms in natural language
processing. GPL employs task-independent graph structure losses to encourage
GNNs to learn intrinsic graph characteristics while simultaneously solving
downstream tasks, producing higher-quality node and graph representations. In
extensive experiments on eleven real-world datasets, after being trained by
GPL, GNNs significantly outperform their original performance on node
classification, graph classification, and edge prediction tasks (up to 10.28%,
16.5%, and 24.15%, respectively). By allowing GNNs to capture the inherent
structural prompts of graphs in GPL, they can alleviate the issue of
over-smooth and achieve new state-of-the-art performances, which introduces a
novel and effective direction for GNN research with potential applications in
various domains.

中文翻译:
以下是符合要求的学术中文翻译：

图神经网络（GNNs）在图数据建模领域得到广泛应用。然而，现有GNNs通常采用任务驱动型训练方式，难以充分捕捉图结构的本质特征，导致生成的节点与图表示未能达到最优。为突破这一局限，受自然语言处理中提示机制的启发，我们提出一种新型图结构提示学习方法（GPL）来增强GNNs的训练。GPL通过引入与任务无关的图结构损失函数，促使GNNs在解决下游任务的同时学习图的本质特性，从而生成更高质量的节点与图表示。在11个真实数据集上的大量实验表明，经GPL训练后的GNNs在节点分类、图分类和边预测任务中性能显著提升（最高分别达到10.28%、16.5%和24.15%）。通过让GNNs在GPL框架中捕捉图的固有结构提示，该方法能有效缓解过平滑问题并实现新的最先进性能，为GNN研究开辟了具有多领域应用潜力的创新方向。

翻译说明：
1. 专业术语处理：保留GNNs/GPL等专业缩写，首次出现时标注全称
2. 被动语态转换："are often trained"译为"通常采用...训练方式"符合中文表达
3. 长句拆分：将原文复合句拆分为符合中文阅读习惯的短句结构
4. 学术表达："state-of-the-art"规范译为"最先进性能"
5. 数据呈现：精确保持百分比数值格式，使用中文标点规范
6. 逻辑衔接：通过"经...表明"等学术转承词保持论证连贯性
7. 概念一致性："intrinsic nature"统一译为"本质特征"，"representations"统一译为"表示"
