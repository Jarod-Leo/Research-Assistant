# CourseGPT-zh: an Educational Large Language Model Based on Knowledge Distillation Incorporating Prompt Optimization

链接: http://arxiv.org/abs/2405.04781v1

原文摘要:
Large language models (LLMs) have demonstrated astonishing capabilities in
natural language processing (NLP) tasks, sparking interest in their application
to professional domains with higher specialized requirements. However,
restricted access to closed-source LLMs via APIs and the difficulty in
collecting massive high-quality datasets pose obstacles to the development of
large language models in education fields of various courses. Given these
challenges, we propose CourseGPT-zh, a course-oriented education LLM that
supports customization and low-cost deployment. To address the
comprehensiveness and diversity requirements of course-specific corpora, we
design a high-quality question-answering corpus distillation framework
incorporating prompt optimization, which effectively mines textbook knowledge
and enhances its diversity. Moreover, considering the alignment of LLM
responses with user needs, a novel method for discrete prompt optimization
based on LLM-as-Judge is introduced. During optimization, this framework
leverages the LLM's ability to reflect on and exploit error feedback and
patterns, allowing for prompts that meet user needs and preferences while
saving response length. Lastly, we obtain CourseGPT-zh based on the open-source
LLM using parameter-efficient fine-tuning. Experimental results show that our
discrete prompt optimization framework effectively improves the response
quality of ChatGPT, and CourseGPT-zh exhibits strong professional capabilities
in specialized knowledge question-answering, significantly outperforming
comparable open-source models.

中文翻译:
以下是符合要求的学术摘要中文翻译：

大型语言模型（LLMs）在自然语言处理任务中展现出惊人能力，这激发了人们对其在专业化要求更高领域的应用兴趣。然而，通过API访问闭源模型的限制性，以及海量高质量教育数据收集的困难性，阻碍了面向多学科教育领域的大模型发展。针对这些挑战，我们提出CourseGPT-zh——一个支持定制化与低成本部署的课程导向教育大模型。为满足学科语料库的全面性与多样性需求，我们设计了融合提示优化的高质量问答语料蒸馏框架，能有效挖掘教材知识并增强其多样性。此外，考虑到大模型响应与用户需求的匹配度，我们创新性地提出基于LLM-as-Judge的离散提示优化方法。该框架在优化过程中利用大模型对错误反馈与模式的反思能力，在节省响应长度的同时生成符合用户需求和偏好的提示。最终，我们通过对开源模型进行参数高效微调获得CourseGPT-zh。实验表明：我们的离散提示优化框架能有效提升ChatGPT响应质量，且CourseGPT-zh在专业知识问答中展现出强劲的专业能力，显著优于同类开源模型。


2. 被动语态转换为中文主动表述（如"are posed"译为"阻碍了"）
3. 长难句合理切分（如将原文复合句拆分为多个中文短句）
4. 关键概念首次出现标注英文缩写（如"大型语言模型（LLMs）"）
5. 保留学术文本的客观严谨性，避免口语化表达）
