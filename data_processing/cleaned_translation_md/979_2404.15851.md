# Porting Large Language Models to Mobile Devices for Question Answering

链接: http://arxiv.org/abs/2404.15851v1

原文摘要:
Deploying Large Language Models (LLMs) on mobile devices makes all the
capabilities of natural language processing available on the device. An
important use case of LLMs is question answering, which can provide accurate
and contextually relevant answers to a wide array of user queries. We describe
how we managed to port state of the art LLMs to mobile devices, enabling them
to operate natively on the device. We employ the llama.cpp framework, a
flexible and self-contained C++ framework for LLM inference. We selected a
6-bit quantized version of the Orca-Mini-3B model with 3 billion parameters and
present the correct prompt format for this model. Experimental results show
that LLM inference runs in interactive speed on a Galaxy S21 smartphone and
that the model delivers high-quality answers to user queries related to
questions from different subjects like politics, geography or history.

中文翻译:
在移动设备上部署大型语言模型（LLMs）可将自然语言处理的所有能力集成至终端设备。LLMs的一个重要应用场景是问答系统，该功能能够针对各类用户查询提供精准且符合语境的答案。本文阐述了如何将前沿大型语言模型移植至移动设备，使其能在终端原生运行。我们采用llama.cpp框架——一个专为LLM推理设计的灵活、自包含C++框架，选取了参数规模达30亿的Orca-Mini-3B模型的6位量化版本，并给出了适用于该模型的正确提示格式。实验结果表明，在Galaxy S21智能手机上，LLM推理能以交互式速度运行，且该模型能针对政治、地理、历史等多学科相关问题为用户提供高质量解答。
