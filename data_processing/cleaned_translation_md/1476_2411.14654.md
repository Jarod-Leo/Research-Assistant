# Comparative Analysis of Pooling Mechanisms in LLMs: A Sentiment Analysis Perspective

链接: http://arxiv.org/abs/2411.14654v1

原文摘要:
Large Language Models (LLMs) have revolutionized natural language processing
(NLP) by delivering state-of-the-art performance across a variety of tasks.
Among these, Transformer-based models like BERT and GPT rely on pooling layers
to aggregate token-level embeddings into sentence-level representations. Common
pooling mechanisms such as Mean, Max, and Weighted Sum play a pivotal role in
this aggregation process. Despite their widespread use, the comparative
performance of these strategies on different LLM architectures remains
underexplored. To address this gap, this paper investigates the effects of
these pooling mechanisms on two prominent LLM families -- BERT and GPT, in the
context of sentence-level sentiment analysis. Comprehensive experiments reveal
that each pooling mechanism exhibits unique strengths and weaknesses depending
on the task's specific requirements. Our findings underline the importance of
selecting pooling methods tailored to the demands of particular applications,
prompting a re-evaluation of common assumptions regarding pooling operations.
By offering actionable insights, this study contributes to the optimization of
LLM-based models for downstream tasks.

中文翻译:
大型语言模型（LLMs）通过在各种自然语言处理（NLP）任务中实现最先进的性能，引发了该领域的革命性变革。其中，基于Transformer架构的模型（如BERT和GPT）依赖池化层将词元级嵌入聚合为句子级表征。均值池化、最大值池化和加权求和等常见机制在此聚合过程中起着关键作用。尽管这些策略被广泛采用，但不同LLM架构下各类池化方法的性能对比仍缺乏深入探索。为填补这一研究空白，本文以句子级情感分析任务为背景，系统研究了BERT和GPT两大主流模型家族中不同池化机制的影响。实验结果表明，根据任务的具体需求，每种池化机制都展现出独特的优势与局限性。我们的发现强调了针对特定应用场景选择适配池化方法的重要性，促使学界重新审视关于池化操作的常规假设。本研究通过提供可操作的见解，为优化基于LLM的下游任务模型提供了重要参考。

（翻译说明：采用学术论文摘要的规范表述方式，在保持专业性的同时确保行文流畅。关键术语如"pooling layers"统一译为"池化层"，"token-level embeddings"译为"词元级嵌入"符合NLP领域术语规范。通过拆分英文长句为中文短句（如将"Despite..."从句转为独立句），并运用"系统研究""结果表明"等学术表达，既准确传达原意又符合中文摘要写作习惯。最后"可操作的见解"等表述既保留原文"actionable insights"的隐喻，又确保学术文本的严谨性。）
