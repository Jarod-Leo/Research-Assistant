# Gradient-Free Adaptive Global Pruning for Pre-trained Language Models

链接: http://arxiv.org/abs/2402.17946v1

原文摘要:
The transformative impact of large language models (LLMs) like LLaMA and GPT
on natural language processing is countered by their prohibitive computational
demands. Pruning has emerged as a pivotal compression strategy, introducing
sparsity to enhance both memory and computational efficiency. Yet, traditional
global pruning is impractical for LLMs due to scalability issues, while local
pruning, despite its efficiency, leads to suboptimal solutions. Addressing
these challenges, we propose SparseLLM, a novel framework that redefines the
global pruning process into manageable, coordinated subproblems, allowing for
resource-efficient optimization with global optimality. SparseLLM's approach,
which conceptualizes LLMs as a chain of modular functions and leverages
auxiliary variables for problem decomposition, not only facilitates a pragmatic
application on LLMs but also demonstrates significant performance improvements,
particularly in high-sparsity regimes where it surpasses current
state-of-the-art methods.

中文翻译:
以下是符合学术规范的中文翻译：

大型语言模型（如LLaMA和GPT）对自然语言处理的变革性影响与其高昂的计算需求形成鲜明对比。剪枝技术作为一种关键压缩策略应运而生，通过引入稀疏性来提升内存和计算效率。然而，传统全局剪枝因可扩展性问题难以适用于大型语言模型，而局部剪枝虽具效率优势却会导致次优解。针对这些挑战，我们提出创新框架SparseLLM，通过将全局剪枝过程重构为可管理的协同子问题，实现资源高效的全局优化。该框架将语言模型概念化为模块化函数链，并利用辅助变量进行问题分解，不仅实现了在大型语言模型上的实用化应用，更展现出显著的性能提升——尤其在高度稀疏场景下，其表现超越当前最先进方法。

（翻译说明：
1. 专业术语统一处理："pruning"译为"剪枝"，"sparsity"译为"稀疏性"
2. 长句拆分重构：将原文复合句按中文表达习惯分解为多个短句
3. 被动语态转化："is conceptualized as"译为主动式"概念化为"
4. 学术用语规范："state-of-the-art"译为"最先进的"
5. 逻辑连接显化：通过破折号和括号保持原文的补充说明关系
6. 保持技术准确性：严格保持"global/local pruning"等核心概念的对应翻译）
