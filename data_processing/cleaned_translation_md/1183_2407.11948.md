# Rethinking Transformer-based Multi-document Summarization: An Empirical Investigation

链接: http://arxiv.org/abs/2407.11948v1

原文摘要:
The utilization of Transformer-based models prospers the growth of
multi-document summarization (MDS). Given the huge impact and widespread
adoption of Transformer-based models in various natural language processing
tasks, investigating their performance and behaviors in the context of MDS
becomes crucial for advancing the field and enhancing the quality of summary.
To thoroughly examine the behaviours of Transformer-based MDS models, this
paper presents five empirical studies on (1) measuring the impact of document
boundary separators quantitatively; (2) exploring the effectiveness of
different mainstream Transformer structures; (3) examining the sensitivity of
the encoder and decoder; (4) discussing different training strategies; and (5)
discovering the repetition in a summary generation. The experimental results on
prevalent MDS datasets and eleven evaluation metrics show the influence of
document boundary separators, the granularity of different level features and
different model training strategies. The results also reveal that the decoder
exhibits greater sensitivity to noises compared to the encoder. This
underscores the important role played by the decoder, suggesting a potential
direction for future research in MDS. Furthermore, the experimental results
indicate that the repetition problem in the generated summaries has
correlations with the high uncertainty scores.

中文翻译:
基于Transformer的模型应用推动了多文档摘要（MDS）领域的发展。鉴于该类模型在各类自然语言处理任务中的巨大影响力和广泛应用，研究其在MDS任务中的表现与行为特征，对推动领域发展及提升摘要质量具有重要意义。为系统探究基于Transformer的MDS模型行为特性，本文开展了五项实证研究：（1）定量评估文档边界分隔符的影响；（2）探索不同主流Transformer架构的有效性；（3）检验编码器与解码器的敏感性差异；（4）讨论不同训练策略；（5）探究摘要生成中的重复现象。通过在主流MDS数据集上采用十一项评估指标的实验结果表明：文档边界分隔符、不同层级特征的粒度以及模型训练策略均会产生显著影响。研究同时发现解码器对噪声的敏感度显著高于编码器，这揭示了解码器在MDS中的关键作用，为未来研究提供了潜在方向。此外，实验结果表明生成摘要中的重复现象与高不确定性评分存在相关性。
