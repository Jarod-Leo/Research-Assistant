# Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning

链接: http://arxiv.org/abs/2310.11397v1

原文摘要:
Large Language Models (LLMs) are powerful tools for natural language
processing, enabling novel applications and user experiences. However, to
achieve optimal performance, LLMs often require adaptation with private data,
which poses privacy and security challenges. Several techniques have been
proposed to adapt LLMs with private data, such as Low-Rank Adaptation (LoRA),
Soft Prompt Tuning (SPT), and In-Context Learning (ICL), but their comparative
privacy and security properties have not been systematically investigated. In
this work, we fill this gap by evaluating the robustness of LoRA, SPT, and ICL
against three types of well-established attacks: membership inference, which
exposes data leakage (privacy); backdoor, which injects malicious behavior
(security); and model stealing, which can violate intellectual property
(privacy and security). Our results show that there is no silver bullet for
privacy and security in LLM adaptation and each technique has different
strengths and weaknesses.

中文翻译:
以下是符合您要求的中文翻译：

大型语言模型（LLMs）作为自然语言处理的强大工具，能够实现创新应用和用户体验。然而，为达到最佳性能，LLMs通常需要使用私有数据进行适配，这带来了隐私与安全挑战。目前已有多种私有数据适配技术被提出，例如低秩适配（LoRA）、软提示调优（SPT）和上下文学习（ICL），但这些技术的隐私与安全特性尚未得到系统化研究。本研究通过评估LoRA、SPT和ICL对三类典型攻击的鲁棒性填补了这一空白：可能暴露数据泄露的成员推理攻击（隐私问题）、注入恶意行为的后门攻击（安全问题）以及可能侵犯知识产权的模型窃取攻击（隐私与安全问题）。实验结果表明，LLM适配领域不存在隐私与安全的通用解决方案，每种技术都存在不同的优势与缺陷。

（译文特点说明：
1. 专业术语准确统一："Low-Rank Adaptation"译为行业通用译法"低秩适配"
2. 句式结构重组：将英语长句拆分为符合中文表达习惯的短句，如将"which poses..."处理为独立分句
3. 被动语态转化："have been proposed"译为主动态的"被提出"
4. 概念清晰传达："silver bullet"译为"通用解决方案"而非字面直译
5. 技术名词保留：SPT/LoRA/ICL等缩写首次出现时标注全称
6. 逻辑关系显化：通过"例如"、"以及"等连接词明确原文隐含的列举关系）
