# UniTabE: Pretraining a Unified Tabular Encoder for Heterogeneous Tabular Data

链接: http://arxiv.org/abs/2307.09249v1

原文摘要:
Recent advancements in NLP have witnessed the groundbreaking impact of
pretrained models, yielding impressive outcomes across various tasks. This
study seeks to extend the power of pretraining methodologies to facilitating
the prediction over tables in data science, a domain traditionally overlooked,
yet inherently challenging due to the plethora of table schemas intrinsic to
different tasks. The primary research questions underpinning this work revolve
around the establishment of a universal pretraining protocol for tables with
varied structures, the generalizability and transferability of learned
knowledge across tasks, the adaptation to diverse downstream applications, and
the incorporation of incremental columns over time. In response to these
challenges, we introduce UniTabE, a straightforward yet effective method
designed to process tables in a uniform manner, devoid of constraints imposed
by specific table structures. UniTabE's core concept relies on representing
each basic table element with a module, termed TabUnit. This is subsequently
followed by a Transformer encoder to refine the representation. Moreover, our
model is designed to facilitate pretraining and finetuning through the
utilization of free-form prompts. In order to implement the pretraining phase,
we curated an expansive tabular dataset comprising approximately 13B samples,
meticulously gathered from the Kaggle platform. This research primarily centers
on classification and regression tasks involving tabular data, and conducts
rigorous experimental testing and analyses to validate the effectiveness of our
methodology. The experimental results demonstrate UniTabE's superior
performance against several baselines across massive benchmarks. This,
therefore, underscores UniTabE's potential to significantly enhance the
semantic representation of tabular data, thereby marking a significant stride
for tabular data analysis.

中文翻译:
自然语言处理领域的最新进展见证了预训练模型的突破性影响，其在各类任务中均展现出卓越性能。本研究旨在将预训练方法的强大能力拓展至数据科学中的表格预测领域——这一传统上被忽视却极具挑战性的方向，其难点主要源于不同任务中表格模式的多样性。本研究围绕以下核心问题展开：如何为异构结构的表格建立通用预训练框架；所学知识在不同任务间的泛化与迁移能力；对多样化下游应用的适应性；以及随时间推移新增列的处理方案。针对这些挑战，我们提出了UniTabE——一种简洁高效的方法，能够以统一方式处理各类表格，不受特定表格结构的限制。UniTabE的核心思想是通过称为TabUnit的模块化单元表征每个基础表格元素，再经由Transformer编码器进行表征优化。此外，该模型支持通过自由格式提示词实现预训练与微调。为实现预训练阶段，我们从Kaggle平台精心收集了约130亿样本的庞大规模表格数据集。本研究主要聚焦于表格数据的分类与回归任务，通过严谨的实验测试与分析验证了方法的有效性。实验结果表明，UniTabE在大量基准测试中均优于多个基线模型，显著提升了表格数据的语义表征能力，为表格数据分析领域迈出了重要一步。
