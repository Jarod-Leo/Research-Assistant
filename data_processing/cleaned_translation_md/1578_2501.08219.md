# Investigating Energy Efficiency and Performance Trade-offs in LLM Inference Across Tasks and DVFS Settings

链接: http://arxiv.org/abs/2501.08219v1

原文摘要:
Large language models (LLMs) have shown significant improvements in many
natural language processing (NLP) tasks, accelerating their rapid adoption
across many industries. These models are resource-intensive, requiring
extensive computational resources both during training and inference, leading
to increased energy consumption and negative environmental impact. As their
adoption accelerates, the sustainability of LLMs has become a critical issue,
necessitating strategies to optimize their runtime efficiency without
compromising performance. Hence, it is imperative to identify the parameters
that significantly influence the performance and energy efficiency of LLMs. To
that end, in this work, we investigate the effect of important parameters on
the performance and energy efficiency of LLMs during inference and examine
their trade-offs.
  First, we analyze how different types of models with varying numbers of
parameters and architectures perform on tasks like text generation, question
answering, and summarization by benchmarking LLMs such as Falcon-7B,
Mistral-7B-v0.1, T5-3B, GPT-2, GPT-J-6B, and GPT-Neo-2.7B. Second, we study
input and output sequence characteristics such as sequence length concerning
energy consumption, performance, and throughput. Finally, we explore the impact
of hardware-based power-saving techniques, i.e., Dynamic Voltage Frequency
Scaling (DVFS), on the models' latency and energy efficiency. Our extensive
benchmarking and statistical analysis reveal many interesting findings,
uncovering how specific optimizations can reduce energy consumption while
maintaining throughput and accuracy. This study provides actionable insights
for researchers and practitioners to design energy-efficient LLM inference
systems.

中文翻译:
以下是符合您要求的中文翻译：

大型语言模型（LLMs）在众多自然语言处理（NLP）任务中展现出显著性能提升，正加速渗透至各行业应用。然而这类模型对计算资源需求极高，在训练和推理阶段均需消耗大量算力，导致能耗激增并引发环境问题。随着应用规模扩大，LLMs的可持续性已成为关键挑战，亟需在不牺牲性能的前提下优化其运行时能效。为此，本研究系统探究了影响LLMs性能与能效的关键参数及其权衡关系。

首先，我们通过基准测试对比了不同参数量级和架构模型（包括Falcon-7B、Mistral-7B-v0.1、T5-3B、GPT-2、GPT-J-6B和GPT-Neo-2.7B）在文本生成、问答和摘要等任务中的表现。其次，量化分析了输入输出序列特征（如序列长度）对能耗、性能和吞吐量的影响规律。最后，探索了基于硬件的节能技术——动态电压频率调节（DVFS）对模型延迟和能效的作用机制。

通过大规模基准测试与统计分析，我们获得了多项重要发现：特定优化策略可在保持吞吐量与准确率的同时显著降低能耗。本研究为学术界和工业界设计高能效LLM推理系统提供了可落地的实践指导。

；保留所有技术术语首字母缩写（LLMs/DVFS）并添加中文注释；对"actionable insights"等概念进行本土化处理为"可落地的实践指导"）
