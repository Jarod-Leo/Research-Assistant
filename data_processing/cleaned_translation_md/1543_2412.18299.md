# M-Ped: Multi-Prompt Ensemble Decoding for Large Language Models

链接: http://arxiv.org/abs/2412.18299v1

原文摘要:
With the widespread application of Large Language Models (LLMs) in the field
of Natural Language Processing (NLP), enhancing their performance has become a
research hotspot. This paper presents a novel multi-prompt ensemble decoding
approach designed to bolster the generation quality of LLMs by leveraging the
aggregation of outcomes from multiple prompts. Given a unique input $X$, we
submit $n$ variations of prompts with $X$ to LLMs in batch mode to decode and
derive probability distributions. For each token prediction, we calculate the
ensemble probability by averaging the $n$ probability distributions within the
batch, utilizing this aggregated probability to generate the token. This
technique is dubbed Inner-Batch Ensemble. To facilitate efficient batch
inference, we implement a Left-Padding strategy to maintain uniform input
lengths across the n prompts. Through extensive experimentation on diverse NLP
tasks, including machine translation, code generation, and text simplification,
we demonstrate the efficacy of our method in enhancing LLM performance. The
results show substantial improvements in BLEU scores, pass@$k$ rates, and LENS
metrics over conventional methods.

中文翻译:
随着大语言模型（LLM）在自然语言处理（NLP）领域的广泛应用，提升其性能已成为研究热点。本文提出一种创新的多提示集成解码方法，通过聚合多个提示的生成结果来增强LLM的输出质量。给定特定输入$X$时，我们以批处理模式向LLM提交$X$的$n$种提示变体进行解码，获取对应的概率分布。在预测每个词元时，通过计算批次内$n$个概率分布的平均值得到集成概率，并基于该聚合概率生成最终词元。该技术被命名为"批内集成"。为实现高效批量推理，我们采用左填充策略确保$n$个提示的输入长度一致。通过在机器翻译、代码生成和文本简化等多样化NLP任务上的大量实验，验证了本方法对LLM性能的提升效果。实验结果表明，相较于传统方法，该方案在BLEU分数、pass@$k$通过率和LENS评估指标上均有显著提升。

（翻译说明：
1. 专业术语统一处理："Large Language Models"固定译为"大语言模型"，"multi-prompt ensemble decoding"译为"多提示集成解码"
2. 技术概念准确转化："Left-Padding strategy"译为专业术语"左填充策略"，"Inner-Batch Ensemble"采用直译加引号处理
3. 长句拆分重组：将原文复合长句按中文表达习惯分解为多个短句，如概率分布计算部分
4. 被动语态转化："we implement"等英文主动句式转换为中文常见的无主语句式
5. 指标名称保留：BLEU、pass@k等专业指标名称保留英文缩写形式
6. 逻辑关系显化：通过"通过...验证了..."等句式明确实验与结论的论证关系）
