# A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models

链接: http://arxiv.org/abs/2503.05613v1

原文摘要:
Large Language Models (LLMs) have revolutionized natural language processing,
yet their internal mechanisms remain largely opaque. Recently, mechanistic
interpretability has attracted significant attention from the research
community as a means to understand the inner workings of LLMs. Among various
mechanistic interpretability approaches, Sparse Autoencoders (SAEs) have
emerged as a particularly promising method due to their ability to disentangle
the complex, superimposed features within LLMs into more interpretable
components. This paper presents a comprehensive examination of SAEs as a
promising approach to interpreting and understanding LLMs. We provide a
systematic overview of SAE principles, architectures, and applications
specifically tailored for LLM analysis, covering theoretical foundations,
implementation strategies, and recent developments in sparsity mechanisms. We
also explore how SAEs can be leveraged to explain the internal workings of
LLMs, steer model behaviors in desired directions, and develop more transparent
training methodologies for future models. Despite the challenges that remain
around SAE implementation and scaling, they continue to provide valuable tools
for understanding the internal mechanisms of large language models.

中文翻译:
以下是符合您要求的中文翻译：

大型语言模型（LLMs）彻底改变了自然语言处理领域，但其内部机制仍具有高度不透明性。近年来，机械可解释性作为一种理解LLMs内部运作机制的方法，引起了研究界的广泛关注。在各种机械可解释性方法中，稀疏自编码器（SAEs）因其能将LLMs中复杂叠加的特征解耦为更具可解释性的组件，已成为最具前景的技术之一。本文对SAEs作为解释和理解LLMs的有效方法进行了全面考察，系统性地概述了专为LLM分析设计的SAE原理、架构及应用，涵盖理论基础、实施策略以及稀疏机制的最新进展。我们深入探讨了如何利用SAEs来：阐释LLMs的内部工作机制、引导模型行为朝向预期方向发展、并为未来模型开发更透明的训练方法。尽管SAE在实施和扩展方面仍存在挑战，但它持续为理解大型语言模型的内部机制提供了宝贵工具。

（翻译严格遵循了以下原则：
1. 专业术语准确统一（如mechanistic interpretability译为"机械可解释性"）
2. 长句合理切分（将原文复合句拆分为符合中文表达习惯的短句）
3. 被动语态转化（如"have emerged"译为"已成为"）
4. 学术风格保持（使用"解耦""可解释性"等规范术语）
5. 逻辑关系显化（通过冒号、分号等明确原文隐含的列举关系）
6. 文化适应性调整（"opaque"译为"不透明性"而非字面直译））
