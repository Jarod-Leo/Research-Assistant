# Prompt Tuning for Multi-View Graph Contrastive Learning

链接: http://arxiv.org/abs/2310.10362v1

原文摘要:
Graphs have become an important modeling tool for web applications, and Graph
Neural Networks (GNNs) have achieved great success in graph representation
learning. However, the performance of traditional GNNs heavily relies on a
large amount of supervision. Recently, ``pre-train, fine-tune'' has become the
paradigm to address the issues of label dependency and poor generalization.
However, the pre-training strategies vary for graphs with homophily and
heterophily, and the objectives for various downstream tasks also differ. This
leads to a gap between pretexts and downstream tasks, resulting in ``negative
transfer'' and poor performance. Inspired by prompt learning in Natural
Language Processing (NLP), many studies turn to bridge the gap and fully
leverage the pre-trained model. However, existing methods for graph prompting
are tailored to homophily, neglecting inherent heterophily on graphs.
Meanwhile, most of them rely on the randomly initialized prompts, which
negatively impact on the stability. Therefore, we propose Self-Prompt, a
prompting framework for graphs based on the model and data itself. We first
introduce asymmetric graph contrastive learning for pretext to address
heterophily and align the objectives of pretext and downstream tasks. Then we
reuse the component from pre-training phase as the self adapter and introduce
self-prompts based on graph itself for task adaptation. Finally, we conduct
extensive experiments on 11 benchmark datasets to demonstrate its superiority.
We provide our codes at 