# Quantifying Memorization of Domain-Specific Pre-trained Language Models using Japanese Newspaper and Paywalls

链接: http://arxiv.org/abs/2404.17143v1

原文摘要:
Dominant pre-trained language models (PLMs) have demonstrated the potential
risk of memorizing and outputting the training data. While this concern has
been discussed mainly in English, it is also practically important to focus on
domain-specific PLMs. In this study, we pre-trained domain-specific GPT-2
models using a limited corpus of Japanese newspaper articles and evaluated
their behavior. Experiments replicated the empirical finding that memorization
of PLMs is related to the duplication in the training data, model size, and
prompt length, in Japanese the same as in previous English studies.
Furthermore, we attempted membership inference attacks, demonstrating that the
training data can be detected even in Japanese, which is the same trend as in
English. The study warns that domain-specific PLMs, sometimes trained with
valuable private data, can ''copy and paste'' on a large scale.

中文翻译:
以下是符合要求的学术中文翻译：

主流预训练语言模型（PLMs）已被证实存在记忆并输出训练数据的潜在风险。尽管该问题目前主要在英语语境下讨论，但关注领域专用PLMs同样具有现实意义。本研究使用有限规模的日语新闻语料库预训练了领域专用GPT-2模型，并评估其行为特征。实验复现了PLMs记忆效应与训练数据重复率、模型规模及提示长度的相关性，证实日语环境下的表现与既往英语研究结论一致。此外，我们实施了成员推理攻击实验，证明即便在日语场景下同样可检测出训练数据，该趋势与英语研究相符。本研究警示：领域专用PLMs（有时使用具有价值的私有数据训练）可能引发大规模"复制粘贴"风险。

翻译说明：
1. 专业术语处理：PLMs、GPT-2等专业缩写保留英文形式，符合中文计算机领域论文惯例
2. 句式重构：将英语长句拆分为符合中文表达习惯的短句（如将"demonstrated that..."处理为"证实...该趋势..."）
3. 被动语态转换："can be detected"译为主动式"可检测出"
4. 文化适配："copy and paste"译为"复制粘贴"并添加引号，既保留原意又符合中文技术表达
5. 学术严谨性：使用"预训练""语料库""成员推理攻击"等规范学术译法
6. 逻辑连接：通过"此外""该趋势"等连接词保持论文逻辑连贯性
