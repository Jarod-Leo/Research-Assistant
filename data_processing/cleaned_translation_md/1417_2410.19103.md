# TesseraQ: Ultra Low-Bit LLM Post-Training Quantization with Block Reconstruction

链接: http://arxiv.org/abs/2410.19103v1

原文摘要:
Large language models (LLMs) have revolutionized natural language processing,
albeit at the cost of immense memory and computation requirements.
Post-training quantization (PTQ) is becoming the de facto method to reduce the
memory footprint and improve the inference throughput of LLMs. In this work, we
aim to push the upper limit of LLM PTQ by optimizing the weight rounding
parameters with the block reconstruction technique, a predominant method in
previous vision models. We propose TesseraQ, a new state-of-the-art PTQ
technique, to quantize the weights of LLMs to ultra-low bits. To effectively
optimize the rounding in LLMs and stabilize the reconstruction process, we
introduce progressive adaptive rounding. This approach iteratively transits the
soft rounding variables to hard variables during the reconstruction process.
Additionally, we optimize the dequantization scale parameters to fully leverage
the block reconstruction technique. We demonstrate that TesseraQ can be
seamlessly integrated with existing scaling or clipping-based PTQ algorithms
such as AWQ and OmniQuant, significantly enhancing their performance and
establishing a new state-of-the-art. For instance, when compared to AWQ,
TesseraQ improves the wikitext2 perplexity from 14.65 to 6.82 and average
downstream accuracy from 50.52 to 59.27 with 2-bit weight-only quantization of
LLaMA-2-7B. Across a range of quantization schemes, including W2A16, W3A16,
W3A3, and W4A4, TesseraQ consistently exhibits superior performance.

中文翻译:
大型语言模型（LLMs）彻底改变了自然语言处理领域，但其代价是巨大的内存和计算需求。训练后量化（PTQ）正成为减少LLM内存占用并提升推理吞吐量的事实标准方法。本研究旨在通过优化权重舍入参数（采用视觉模型中主流的块重建技术），突破LLM PTQ的性能上限。我们提出TesseraQ——一种新型的尖端PTQ技术，可将LLM权重量化为超低位宽。为有效优化LLM舍入操作并稳定重建过程，我们引入渐进式自适应舍入法，该方法在重建过程中将软舍入变量迭代转化为硬变量。此外，我们优化了解量化比例参数以充分发挥块重建技术的优势。实验表明，TesseraQ可与现有基于缩放或截断的PTQ算法（如AWQ和OmniQuant）无缝集成，显著提升其性能并确立新的技术标杆。例如在LLaMA-2-7B模型2比特权重量化中，相较AWQ方案，TesseraQ将wikitext2困惑度从14.65降至6.82，下游任务平均准确率从50.52%提升至59.27%。在W2A16、W3A16、W3A3及W4A4等多种量化方案下，TesseraQ均展现出持续优越的性能表现。


