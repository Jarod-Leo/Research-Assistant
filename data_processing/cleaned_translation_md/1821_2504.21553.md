# Precision Where It Matters: A Novel Spike Aware Mixed-Precision Quantization Strategy for LLaMA-based Language Models

链接: http://arxiv.org/abs/2504.21553v1

原文摘要:
Large Language Models (LLMs) have demonstrated remarkable capabilities in
various natural language processing tasks. However, their size presents
significant challenges for deployment and inference. This paper investigates
the quantization of LLMs, focusing on the LLaMA architecture and its
derivatives. We challenge existing assumptions about activation outliers in
LLMs and propose a novel mixed-precision quantization approach tailored for
LLaMA-like models. Our method leverages the observation that activation spikes
in LLaMA architectures are predominantly concentrated in specific projection
layers. By applying higher precision (FP16 or FP8) to these layers while
quantizing the rest of the model to lower bit-widths, we achieve superior
performance compared to existing quantization techniques. Experimental results
on LLaMA2, LLaMA3, and Mistral models demonstrate significant improvements in
perplexity and zero-shot accuracy, particularly for 8-bit per-tensor
quantization. Our approach outperforms general-purpose methods designed to
handle outliers across all architecture types, highlighting the benefits of
architecture-specific quantization strategies. This research contributes to the
ongoing efforts to make LLMs more efficient and deployable, potentially
enabling their use in resource-constrained environments. Our findings emphasize
the importance of considering model-specific characteristics in developing
effective quantization pipelines for state-of-the-art language models by
identifying and targeting a small number of projections that concentrate
activation spikes.

中文翻译:
以下是符合要求的学术论文摘要中文翻译：

大型语言模型（LLMs）在各类自然语言处理任务中展现出卓越能力，但其参数量级为部署与推理带来显著挑战。本文以LLaMA架构及其衍生模型为研究对象，对LLMs量化技术展开深入探讨。我们重新审视了现有关于LLM激活值异常点的理论假设，提出了一种专为类LLaMA模型设计的混合精度量化方法。该方法基于关键发现：LLaMA架构中的激活峰值集中出现在特定投影层。通过对这些关键层采用较高精度（FP16或FP8）处理，同时将模型其余部分量化为更低比特位宽，本方法在性能表现上超越了现有量化技术。基于LLaMA2、LLaMA3和Mistral模型的实验结果表明，该方法在困惑度和零样本准确率指标上实现显著提升，尤其在8比特张量量化场景中表现突出。相较于通用型异常值处理方法，我们的架构专用策略展现出明显优势，这凸显了针对特定模型特性开发量化方案的重要性。本研究通过精准识别并处理少数集中出现激活峰值的投影层，为提升前沿语言模型的量化效率提供了新思路，有助于推动LLMs在资源受限环境中的实际应用。研究成果强调：构建高效量化流程时，必须充分考虑模型特有的结构特征。
