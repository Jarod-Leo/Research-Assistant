# RCMHA: Relative Convolutional Multi-Head Attention for Natural Language Modelling

链接: http://arxiv.org/abs/2308.03429v1

原文摘要:
The Attention module finds common usage in language modeling, presenting
distinct challenges within the broader scope of Natural Language Processing.
Multi-Head Attention (MHA) employs an absolute positional encoding, which
imposes limitations on token length and entails substantial memory consumption
during the processing of embedded inputs. The current remedy proposed by
researchers involves the utilization of relative positional encoding, similar
to the approach adopted in Transformer-XL or Relative Multi-Head Attention
(RMHA), albeit the employed architecture consumes considerable memory
resources. To address these challenges, this study endeavors to refine MHA,
leveraging relative positional encoding in conjunction with the Depth-Wise
Convolutional Layer architecture, which promises heightened accuracy coupled
with minimized memory usage. The proposed RCMHA framework entails the
modification of two integral components: firstly, the application of the
Depth-Wise Convolutional Layer to the input embedding, encompassing Query, Key,
and Value parameters; secondly, the incorporation of Relative Positional
Encoding into the attention scoring phase, harmoniously integrated with Scaled
Dot-Product Attention. Empirical experiments underscore the advantages of
RCMHA, wherein it exhibits superior accuracy, boasting a score of 0.572 in
comparison to alternative attention modules such as MHA, Multi-DConv-Head
Attention (MDHA), and RMHA. Concerning memory utilization, RMHA emerges as the
most frugal, demonstrating an average consumption of 2.98 GB, surpassing RMHA
which necessitates 3.5 GB.

中文翻译:
注意力模块在语言建模中应用广泛，但在自然语言处理的更广泛领域中仍面临独特挑战。多头注意力机制（MHA）采用绝对位置编码，这种机制不仅限制了标记序列的长度，还会在处理嵌入输入时消耗大量内存。当前研究者提出的解决方案是采用相对位置编码，类似Transformer-XL或相对多头注意力（RMHA）所采用的方法，但现有架构仍存在内存资源消耗过大的问题。为应对这些挑战，本研究致力于改进MHA机制，通过结合深度可分离卷积架构与相对位置编码，在提升准确性的同时实现内存占用的优化。提出的RCMHA框架包含两大核心改进：首先对查询、键、值参数的输入嵌入应用深度可分离卷积层；其次将相对位置编码与缩放点积注意力机制有机融合至注意力评分阶段。实验数据表明，RCMHA相比MHA、多头深度卷积注意力（MDHA）和RMHA等其他注意力模块具有精度优势，其0.572的评分表现最佳。在内存消耗方面，RCMHA平均仅需2.98GB，显著优于需占用3.5GB内存的RMHA方案。


