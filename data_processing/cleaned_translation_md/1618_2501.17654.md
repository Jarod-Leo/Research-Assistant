# Exploring Vision Language Models for Multimodal and Multilingual Stance Detection

链接: http://arxiv.org/abs/2501.17654v1

原文摘要:
Social media's global reach amplifies the spread of information, highlighting
the need for robust Natural Language Processing tasks like stance detection
across languages and modalities. Prior research predominantly focuses on
text-only inputs, leaving multimodal scenarios, such as those involving both
images and text, relatively underexplored. Meanwhile, the prevalence of
multimodal posts has increased significantly in recent years. Although
state-of-the-art Vision-Language Models (VLMs) show promise, their performance
on multimodal and multilingual stance detection tasks remains largely
unexamined. This paper evaluates state-of-the-art VLMs on a newly extended
dataset covering seven languages and multimodal inputs, investigating their use
of visual cues, language-specific performance, and cross-modality interactions.
Our results show that VLMs generally rely more on text than images for stance
detection and this trend persists across languages. Additionally, VLMs rely
significantly more on text contained within the images than other visual
content. Regarding multilinguality, the models studied tend to generate
consistent predictions across languages whether they are explicitly
multilingual or not, although there are outliers that are incongruous with
macro F1, language support, and model size.

中文翻译:
社交媒体在全球范围内的广泛覆盖加速了信息传播，这凸显出对跨语言、跨模态的立场检测等自然语言处理任务的迫切需求。既往研究主要集中于纯文本输入场景，而对图像-文本等多模态情境的探索相对不足。近年来，多模态帖子的数量呈现显著增长态势。尽管前沿的视觉-语言模型（VLM）展现出应用潜力，但其在多模态与多语言立场检测任务中的性能表现尚未得到系统验证。本文基于新扩展的涵盖七种语言和多模态输入的数据集，对先进VLM模型进行评估，重点探究其对视觉线索的利用、语言特异性表现及跨模态交互机制。研究发现：1）VLM在立场检测中普遍更依赖文本而非图像，该趋势在不同语言中保持一致；2）相比其他视觉内容，模型明显更倾向于利用图像内嵌文本信息；3）就多语言性而言，无论是否显式支持多语言，所研究模型在不同语言间均能生成一致性预测，但存在与宏观F1分数、语言支持度和模型规模不相符的异常案例。
