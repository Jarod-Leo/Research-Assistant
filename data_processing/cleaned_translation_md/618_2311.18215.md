# Automatic Construction of a Korean Toxic Instruction Dataset for Ethical Tuning of Large Language Models

链接: http://arxiv.org/abs/2311.18215v1

原文摘要:
Caution: this paper may include material that could be offensive or
distressing.
  The advent of Large Language Models (LLMs) necessitates the development of
training approaches that mitigate the generation of unethical language and
aptly manage toxic user queries. Given the challenges related to human labor
and the scarcity of data, we present KoTox, comprising 39K unethical
instruction-output pairs. This collection of automatically generated toxic
instructions refines the training of LLMs and establishes a foundational
framework for improving LLMs' ethical awareness and response to various toxic
inputs, promoting more secure and responsible interactions in Natural Language
Processing (NLP) applications.

中文翻译:
警告：本文可能包含令人不适或不安的内容。

随着大语言模型（LLMs）的出现，亟需开发能够减少不道德语言生成、妥善处理恶意用户查询的训练方法。针对人工标注成本高昂及数据稀缺的挑战，我们提出了KoTox数据集——包含3.9万条不道德指令-输出对。这套自动生成的恶意指令集不仅能优化LLMs的训练，还为提升模型伦理认知能力、应对各类恶意输入建立了基础框架，从而推动自然语言处理（NLP）应用实现更安全、更负责任的交互。

（翻译说明：采用学术论文摘要的简洁风格，处理要点如下：
1. 将"Large Language Models"译为行业通用术语"大语言模型"并保留缩写LLMs
2. "toxic"根据上下文分别译为"不道德/恶意"以符合中文表达习惯
3. 被动语态转换为主动句式（如"necessitates"译为"亟需"）
4. 长句拆分重组，如最后一句通过破折号连接保持逻辑连贯
5. 专业术语"NLP"保留英文缩写并添加中文全称
6. 数字"39K"转换为中文计数习惯"3.9万条"）
