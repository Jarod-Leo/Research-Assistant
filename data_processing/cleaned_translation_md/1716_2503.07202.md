# A Zero-shot Learning Method Based on Large Language Models for Multi-modal Knowledge Graph Embedding

链接: http://arxiv.org/abs/2503.07202v1

原文摘要:
Zero-shot learning (ZL) is crucial for tasks involving unseen categories,
such as natural language processing, image classification, and cross-lingual
transfer.Current applications often fail to accurately infer and handle new
relations orentities involving unseen categories, severely limiting their
scalability and prac-ticality in open-domain scenarios. ZL learning faces the
challenge of effectivelytransferring semantic information of unseen categories
in multi-modal knowledgegraph (MMKG) embedding representation learning. In this
paper, we proposeZSLLM, a framework for zero-shot embedding learning of MMKGs
using largelanguage models (LLMs). We leverage textual modality information of
unseencategories as prompts to fully utilize the reasoning capabilities of
LLMs, enablingsemantic information transfer across different modalities for
unseen categories.Through model-based learning, the embedding representation of
unseen cate-gories in MMKG is enhanced. Extensive experiments conducted on
multiplereal-world datasets demonstrate the superiority of our approach
compared tostate-of-the-art methods.

中文翻译:
以下是符合学术规范的中文翻译：

零样本学习（ZL）对于涉及未知类别的任务至关重要，例如自然语言处理、图像分类和跨语言迁移。现有应用往往无法准确推断和处理涉及未知类别的新关系或实体，这严重限制了其在开放域场景中的扩展性和实用性。零样本学习面临的核心挑战是如何在多模态知识图谱（MMKG）嵌入表示学习中有效传递未知类别的语义信息。本文提出ZSLLM框架，通过大型语言模型（LLM）实现MMKG的零样本嵌入学习。我们利用未知类别的文本模态信息作为提示，充分发挥LLM的推理能力，实现跨模态的未知类别语义信息迁移。通过基于模型的学习，增强了MMKG中未知类别的嵌入表示能力。在多个真实数据集上的大量实验表明，本方法相较最先进技术具有显著优势。

（翻译说明：
1. 专业术语采用学界通用译法，如"zero-shot learning"译为"零样本学习"
2. 保留英文缩写首次出现时的全称标注（ZL/MMKG/LLM）
3. 长句按中文表达习惯拆分重组，如将"severely limiting..."独立成句
4. 学术用语规范化处理，如"state-of-the-art"译为"最先进技术"
5. 保持被动语态与主动语态的合理转换，符合中文科技论文表达习惯）
