# CRAFT: Extracting and Tuning Cultural Instructions from the Wild

链接: http://arxiv.org/abs/2405.03138v1

原文摘要:
Large language models (LLMs) have rapidly evolved as the foundation of
various natural language processing (NLP) applications. Despite their wide use
cases, their understanding of culturally-related concepts and reasoning remains
limited. Meantime, there is a significant need to enhance these models'
cultural reasoning capabilities, especially concerning underrepresented
regions. This paper introduces a novel pipeline for extracting high-quality,
culturally-related instruction tuning datasets from vast unstructured corpora.
We utilize a self-instruction generation pipeline to identify cultural concepts
and trigger instruction. By integrating with a general-purpose instruction
tuning dataset, our model demonstrates enhanced capabilities in recognizing and
understanding regional cultural nuances, thereby enhancing its reasoning
capabilities. We conduct experiments across three regions: Singapore, the
Philippines, and the United States, achieving performance improvement of up to
6%. Our research opens new avenues for extracting cultural instruction tuning
sets directly from unstructured data, setting a precedent for future
innovations in the field.

中文翻译:
以下是符合学术规范的中文翻译：

大型语言模型（LLMs）已迅速发展成为各类自然语言处理（NLP）应用的基础架构。尽管应用场景广泛，这些模型对文化相关概念的理解与推理能力仍存在局限。与此同时，业界亟需增强模型的文化推理能力——特别是针对代表性不足地区的文化特征。本文提出了一种创新流程，可从海量非结构化语料库中提取高质量的文化相关指令微调数据集。我们采用自指令生成流程来识别文化概念并触发指令生成，通过将其与通用指令微调数据集相结合，我们的模型在识别和理解地域文化特征方面展现出显著增强的能力，从而提升了文化推理性能。我们在新加坡、菲律宾和美国三个地区开展实验，实现了最高达6%的性能提升。本研究开创了直接从非结构化数据中提取文化指令微调数据集的新范式，为该领域的未来创新树立了先例。

（翻译说明：1. 专业术语如LLMs/NLP保留英文缩写并首次标注全称 2. "underrepresented regions"译为"代表性不足地区"符合社会学规范 3. "self-instruction generation pipeline"译为"自指令生成流程"准确传达技术含义 4. 被动语态转换为中文主动句式 5. 长句拆分符合中文表达习惯 6. "setting a precedent"译为"树立先例"保留学术严谨性）
