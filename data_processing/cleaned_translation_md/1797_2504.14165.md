# Self-Correction Makes LLMs Better Parsers

链接: http://arxiv.org/abs/2504.14165v1

原文摘要:
Large language models (LLMs) have achieved remarkable success across various
natural language processing (NLP) tasks. However, recent studies suggest that
they still face challenges in performing fundamental NLP tasks essential for
deep language understanding, particularly syntactic parsing. In this paper, we
conduct an in-depth analysis of LLM parsing capabilities, delving into the
specific shortcomings of their parsing results. We find that LLMs may stem from
limitations to fully leverage grammar rules in existing treebanks, which
restricts their capability to generate valid syntactic structures. To help LLMs
acquire knowledge without additional training, we propose a self-correction
method that leverages grammar rules from existing treebanks to guide LLMs in
correcting previous errors. Specifically, we automatically detect potential
errors and dynamically search for relevant rules, offering hints and examples
to guide LLMs in making corrections themselves. Experimental results on three
datasets with various LLMs, demonstrate that our method significantly improves
performance in both in-domain and cross-domain settings on the English and
Chinese datasets.

中文翻译:
以下是符合学术规范的中文翻译：

大型语言模型（LLMs）在各类自然语言处理（NLP）任务中取得了显著成就。然而最新研究表明，这些模型在执行关乎深层语言理解的基础NLP任务时仍存在挑战，特别是句法解析任务。本文通过深入分析LLMs的解析能力，系统探究了其解析结果的具体缺陷。我们发现，现有树库中语法规则利用不充分可能是导致LLMs生成有效句法结构能力受限的根本原因。为使LLMs无需额外训练即可获取知识，我们提出一种基于现有树库语法规则的自校正方法，通过引导模型自主修正错误。具体而言，该方法能自动检测潜在错误并动态检索相关规则，通过提供提示和示例指导LLMs完成自我修正。在包含英语和中文的三个数据集上进行的多模型实验表明，我们的方法在领域内和跨领域场景下均能显著提升性能。

（翻译说明：
1. 专业术语统一处理："treebanks"译为"树库"（语言学标准译法）
2. 被动语态转化："it is found that"转为主动句式"我们发现"
3. 长句拆分：将原文复合句按中文表达习惯分解为多个短句
4. 概念显化："self-correction method"增译为"自校正方法"以明确方法属性
5. 学术用语规范："cross-domain settings"规范译为"跨领域场景"
6. 保持客观性：避免添加原文没有的主观评价词
7. 逻辑连接：使用"具体而言"等衔接词保持论证连贯性）
