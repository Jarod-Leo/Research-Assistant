# TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling

链接: http://arxiv.org/abs/2504.07053v1

原文摘要:
Large Language Models (LLMs) excel in text-based natural language processing
tasks but remain constrained by their reliance on textual inputs and outputs.
To enable more natural human-LLM interaction, recent progress have focused on
deriving a spoken language model (SLM) that can not only listen but also
generate speech. To achieve this, a promising direction is to conduct
speech-text joint modeling. However, recent SLM still lag behind text LLM due
to the modality mismatch. One significant mismatch can be the sequence lengths
between speech and text tokens. To address this, we introduce Text-Aligned
Speech Tokenization and Embedding (TASTE), a method that directly addresses the
modality gap by aligning speech token with the corresponding text transcription
during the tokenization stage. We propose a method that can achieve this
through the special aggregation mechanism and with speech reconstruction as the
training objective. We conduct extensive experiments and show that TASTE can
preserve essential paralinguistic information while dramatically reducing the
token sequence length. Furthermore, by leveraging TASTE, we can adapt
text-based LLMs into effective SLMs with parameter-efficient fine-tuning
techniques such as Low-Rank Adaptation (LoRA). Experimental results on
benchmark tasks, including SALMON and StoryCloze, demonstrate that TASTE-based
SLMs perform similarly to previous full-finetuning methods. To our knowledge,
TASTE is the first end-to-end approach that utilizes a reconstruction objective
to automatically learn a text-aligned speech tokenization and embedding
suitable for spoken language modeling. Our demo, code, and models are publicly
available at 