# ScaMo: Exploring the Scaling Law in Autoregressive Motion Generation Model

链接: http://arxiv.org/abs/2412.14559v1

原文摘要:
The scaling law has been validated in various domains, such as natural
language processing (NLP) and massive computer vision tasks; however, its
application to motion generation remains largely unexplored. In this paper, we
introduce a scalable motion generation framework that includes the motion
tokenizer Motion FSQ-VAE and a text-prefix autoregressive transformer. Through
comprehensive experiments, we observe the scaling behavior of this system. For
the first time, we confirm the existence of scaling laws within the context of
motion generation. Specifically, our results demonstrate that the normalized
test loss of our prefix autoregressive models adheres to a logarithmic law in
relation to compute budgets. Furthermore, we also confirm the power law between
Non-Vocabulary Parameters, Vocabulary Parameters, and Data Tokens with respect
to compute budgets respectively. Leveraging the scaling law, we predict the
optimal transformer size, vocabulary size, and data requirements for a compute
budget of $1e18$. The test loss of the system, when trained with the optimal
model size, vocabulary size, and required data, aligns precisely with the
predicted test loss, thereby validating the scaling law.

中文翻译:
以下是符合学术规范的中文翻译：

缩放定律已在自然语言处理（NLP）和大规模计算机视觉任务等多个领域得到验证，但其在动作生成领域的应用仍属空白。本文提出一个可扩展的动作生成框架，包含动作分词器Motion FSQ-VAE和文本前缀自回归变换器。通过系统性实验，我们观察到该体系呈现的缩放规律。研究首次证实了动作生成领域存在缩放定律：具体而言，实验结果表明前缀自回归模型的归一化测试损失与计算预算之间遵循对数规律；同时验证了非词表参数量、词表参数量及数据标记量分别与计算预算之间存在的幂律关系。基于缩放定律，我们预测出1e18计算预算下最优变换器规模、词表大小及数据需求。当采用最优模型规模、词表大小及对应数据量进行训练时，系统测试损失与预测值完全吻合，从而验证了缩放定律的有效性。

（说明：翻译过程中进行了以下专业处理：
1. 专业术语统一："scaling law"译为"缩放定律"，"transformer"译为"变换器"（计算机领域标准译法）
2. 被动语态转化：将英文被动式转换为中文主动式表达
3. 长句拆分：将复合长句分解为符合中文表达习惯的短句
4. 单位规范：科学计数法"1e18"保留原格式
5. 概念准确："Vocabulary Parameters"译为"词表参数量"以区分于常规参数）
