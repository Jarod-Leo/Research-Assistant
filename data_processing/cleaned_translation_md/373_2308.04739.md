# Optimizing a Transformer-based network for a deep learning seismic processing workflow

链接: http://arxiv.org/abs/2308.04739v1

原文摘要:
StorSeismic is a recently introduced model based on the Transformer to adapt
to various seismic processing tasks through its pretraining and fine-tuning
training strategy. In the original implementation, StorSeismic utilized a
sinusoidal positional encoding and a conventional self-attention mechanism,
both borrowed from the natural language processing (NLP) applications. For
seismic processing they admitted good results, but also hinted to limitations
in efficiency and expressiveness. We propose modifications to these two key
components, by utilizing relative positional encoding and low-rank attention
matrices as replacements to the vanilla ones. The proposed changes are tested
on processing tasks applied to a realistic Marmousi and offshore field data as
a sequential strategy, starting from denoising, direct arrival removal,
multiple attenuation, and finally root-mean-squared velocity ($V_{RMS}$)
prediction for normal moveout (NMO) correction. We observe faster pretraining
and competitive results on the fine-tuning tasks and, additionally, fewer
parameters to train compared to the vanilla model.

中文翻译:
StorSeismic是近期提出的一种基于Transformer架构的模型，通过预训练与微调相结合的训练策略，可适配多种地震数据处理任务。在原始实现中，该模型沿用了自然语言处理（NLP）领域的正弦位置编码和传统自注意力机制，虽能获得良好处理效果，但也暴露出效率与表征能力方面的局限性。我们针对这两个核心组件提出改进方案：采用相对位置编码替代原始位置编码，并引入低秩注意力矩阵取代标准注意力机制。通过在Marmousi理论模型和海上实际资料上开展序列化处理测试（依次执行去噪、直达波压制、多次波衰减及均方根速度预测等任务，最终用于动校正处理），验证了改进方案能实现更快的预训练速度，在微调任务中保持竞争力，同时相比原模型具有更少的可训练参数。
