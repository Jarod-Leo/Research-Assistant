# Venturing into Uncharted Waters: The Navigation Compass from Transformer to Mamba

链接: http://arxiv.org/abs/2406.16722v1

原文摘要:
Transformer, a deep neural network architecture, has long dominated the field
of natural language processing and beyond. Nevertheless, the recent
introduction of Mamba challenges its supremacy, sparks considerable interest
among researchers, and gives rise to a series of Mamba-based models that have
exhibited notable potential. This survey paper orchestrates a comprehensive
discussion, diving into essential research dimensions, covering: (i) the
functioning of the Mamba mechanism and its foundation on the principles of
structured state space models; (ii) the proposed improvements and the
integration of Mamba with various networks, exploring its potential as a
substitute for Transformers; (iii) the combination of Transformers and Mamba to
compensate for each other's shortcomings. We have also made efforts to
interpret Mamba and Transformer in the framework of kernel functions, allowing
for a comparison of their mathematical nature within a unified context. Our
paper encompasses the vast majority of improvements related to Mamba to date.

中文翻译:
Transformer作为一种深度神经网络架构，长期以来在自然语言处理及其他领域占据主导地位。然而，近期Mamba架构的提出对其统治地位发起挑战，引发了研究界的广泛关注，并催生出一系列展现出显著潜力的Mamba衍生模型。本综述论文系统性地展开全面探讨，深入关键研究维度，涵盖：（i）Mamba机制的工作原理及其基于结构化状态空间模型的理论基础；（ii）现有改进方案及Mamba与各类网络的融合研究，探索其替代Transformer的潜力；（iii）Transformer与Mamba的互补结合以弥补彼此缺陷。我们还尝试在核函数框架下对二者进行数学阐释，使其数学本质能在统一语境下进行对比。本文囊括了迄今为止绝大多数Mamba相关改进研究。

（译文说明：采用学术论文摘要的标准表述方式，在保持专业性的同时确保行文流畅。关键术语如"structured state space models"译为"结构化状态空间模型"符合计算机领域术语规范；长句拆分符合中文表达习惯，如将原文三个并列维度处理为分号连接的排比结构；"kernel functions"译为"核函数"沿用机器学习领域通用译法；被动语态转换为主动表述，如"has long dominated"处理为"长期以来占据主导地位"更符合中文表达习惯。）
