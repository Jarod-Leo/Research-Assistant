# ReAGent: Towards A Model-agnostic Feature Attribution Method for Generative Language Models

链接: http://arxiv.org/abs/2402.00794v1

原文摘要:
Feature attribution methods (FAs), such as gradients and attention, are
widely employed approaches to derive the importance of all input features to
the model predictions. Existing work in natural language processing has mostly
focused on developing and testing FAs for encoder-only language models (LMs) in
classification tasks. However, it is unknown if it is faithful to use these FAs
for decoder-only models on text generation, due to the inherent differences
between model architectures and task settings respectively. Moreover, previous
work has demonstrated that there is no `one-wins-all' FA across models and
tasks. This makes the selection of a FA computationally expensive for large LMs
since input importance derivation often requires multiple forward and backward
passes including gradient computations that might be prohibitive even with
access to large compute. To address these issues, we present a model-agnostic
FA for generative LMs called Recursive Attribution Generator (ReAGent). Our
method updates the token importance distribution in a recursive manner. For
each update, we compute the difference in the probability distribution over the
vocabulary for predicting the next token between using the original input and
using a modified version where a part of the input is replaced with RoBERTa
predictions. Our intuition is that replacing an important token in the context
should have resulted in a larger change in the model's confidence in predicting
the token than replacing an unimportant token. Our method can be universally
applied to any generative LM without accessing internal model weights or
additional training and fine-tuning, as most other FAs require. We extensively
compare the faithfulness of ReAGent with seven popular FAs across six
decoder-only LMs of various sizes. The results show that our method
consistently provides more faithful token importance distributions.

中文翻译:
特征归因方法（如梯度和注意力机制）是用于推导所有输入特征对模型预测重要性的常用技术。当前自然语言处理领域的研究主要集中在为编码器架构的语言模型开发并测试其在分类任务中的特征归因方法。然而，由于模型架构与任务设置的本质差异，尚不确定这些方法是否可忠实应用于解码器架构模型在文本生成任务中的归因分析。此外，已有研究表明不存在适用于所有模型和任务的"通用最优"归因方法，这使得针对大语言模型选择特征归因方法的计算成本极高——因为输入重要性推导通常需要多次包含梯度计算的前向与反向传播，即使拥有强大算力也难以承受。

为解决这些问题，我们提出了一种模型无关的生成式语言模型特征归因方法——递归归因生成器（ReAGent）。该方法通过递归方式更新词元重要性分布：每次更新时，我们计算模型在两种情况下预测下一词元的词汇概率分布差异——使用原始输入与使用经过修改的输入（其中部分内容被RoBERTa预测结果替换）。我们的核心假设是：替换上下文中的重要词元相较于替换非重要词元，会导致模型预测置信度产生更显著变化。本方法无需访问模型内部权重或进行额外训练微调（这是大多数其他归因方法的要求），即可通用应用于任何生成式语言模型。我们系统比较了ReAGent与七种主流归因方法在六种不同规模解码器语言模型上的归因忠实性，结果表明我们的方法始终能提供更可靠的词元重要性分布。
