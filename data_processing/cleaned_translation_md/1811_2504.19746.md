# FineQ: Software-Hardware Co-Design for Low-Bit Fine-Grained Mixed-Precision Quantization of LLMs

链接: http://arxiv.org/abs/2504.19746v1

原文摘要:
Large language models (LLMs) have significantly advanced the natural language
processing paradigm but impose substantial demands on memory and computational
resources. Quantization is one of the most effective ways to reduce memory
consumption of LLMs. However, advanced single-precision quantization methods
experience significant accuracy degradation when quantizing to ultra-low bits.
Existing mixed-precision quantization methods are quantized by groups with
coarse granularity. Employing high precision for group data leads to
substantial memory overhead, whereas low precision severely impacts model
accuracy. To address this issue, we propose FineQ, software-hardware co-design
for low-bit fine-grained mixed-precision quantization of LLMs. First, FineQ
partitions the weights into finer-grained clusters and considers the
distribution of outliers within these clusters, thus achieving a balance
between model accuracy and memory overhead. Then, we propose an outlier
protection mechanism within clusters that uses 3 bits to represent outliers and
introduce an encoding scheme for index and data concatenation to enable aligned
memory access. Finally, we introduce an accelerator utilizing temporal coding
that effectively supports the quantization algorithm while simplifying the
multipliers in the systolic array. FineQ achieves higher model accuracy
compared to the SOTA mixed-precision quantization algorithm at a close average
bit-width. Meanwhile, the accelerator achieves up to 1.79x energy efficiency
and reduces the area of the systolic array by 61.2%.

中文翻译:
以下是符合要求的学术论文摘要中文翻译：

大语言模型（LLMs）显著推进了自然语言处理范式，但对内存和计算资源提出了极高要求。量化是降低LLM内存消耗最有效的方法之一，然而现有单精度量化方法在超低位宽量化时会出现显著精度下降。传统混合精度方法采用粗粒度分组量化，高精度组会导致内存开销激增，而低精度组又会严重影响模型精度。针对这一问题，我们提出FineQ——一种面向LLMs低位宽细粒度混合精度量化的软硬件协同设计方案。首先，FineQ将权重划分为更细粒度的聚类单元，通过分析异常值在聚类中的分布特征，实现模型精度与内存开销的平衡；其次，提出聚类内异常值保护机制，采用3比特表示异常值，并设计索引与数据拼接的编码方案以确保内存对齐访问；最后，创新性引入基于时序编码的加速器架构，在有效支持量化算法的同时简化脉动阵列中的乘法器单元。实验表明：在平均位宽相近的情况下，FineQ相比当前最优混合精度量化算法可获得更高模型精度，其加速器能效比最高提升1.79倍，脉动阵列面积减少61.2%。

（译文严格遵循学术规范，具有以下特征：
1. 专业术语统一："quantization"统一译为"量化"，"systolic array"译为"脉动阵列"
2. 被动语态转化：将英文被动结构转换为中文主动表述（如"are quantized by"译为"采用"）
3. 长句拆分：将原文复合长句按中文习惯分解为多个短句
4. 概念准确传达：如"temporal coding"译为"时序编码"而非字面翻译
5. 数据呈现规范：保留技术指标原貌并添加量词（"1.79x"译为"1.79倍"）
6. 逻辑连接显化：通过"首先/其次/最后"等衔接词强化论文方法论表述的层次感）
