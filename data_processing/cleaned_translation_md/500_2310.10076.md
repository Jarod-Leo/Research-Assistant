# Verbosity Bias in Preference Labeling by Large Language Models

链接: http://arxiv.org/abs/2310.10076v1

原文摘要:
In recent years, Large Language Models (LLMs) have witnessed a remarkable
surge in prevalence, altering the landscape of natural language processing and
machine learning. One key factor in improving the performance of LLMs is
alignment with humans achieved with Reinforcement Learning from Human Feedback
(RLHF), as for many LLMs such as GPT-4, Bard, etc. In addition, recent studies
are investigating the replacement of human feedback with feedback from other
LLMs named Reinforcement Learning from AI Feedback (RLAIF). We examine the
biases that come along with evaluating LLMs with other LLMs and take a closer
look into verbosity bias -- a bias where LLMs sometimes prefer more verbose
answers even if they have similar qualities. We see that in our problem
setting, GPT-4 prefers longer answers more than humans. We also propose a
metric to measure this bias.

中文翻译:
近年来，大型语言模型（LLMs）的普及程度显著提升，改变了自然语言处理和机器学习的格局。提升LLMs性能的一个关键因素是通过人类反馈强化学习（RLHF）实现与人类的对齐，这一点在GPT-4、Bard等众多LLMs中均有体现。此外，近期研究正探索用其他LLMs的反馈替代人类反馈，即AI反馈强化学习（RLAIF）。我们研究了使用其他LLMs评估LLMs时产生的偏差，并重点分析了冗长偏好偏差——即LLMs有时会倾向于选择更冗长的答案，即便这些答案质量相近。在我们的实验设置中发现，GPT-4比人类更偏好较长答案。我们还提出了一种量化该偏差的指标。

（翻译说明：采用学术论文摘要的简洁风格，通过以下处理确保专业性：
1. 术语统一："alignment"译为"对齐"，"verbosity bias"译为"冗长偏好偏差"
2. 句式重构：将英语长句拆分为符合中文表达习惯的短句，如第一句通过分号处理原文的伴随状语
3. 被动语态转化："are investigating"译为主动式"正探索"
4. 概念显化："problem setting"具体化为"实验设置"
5. 保持关键缩写的首次全称标注（如RLHF/RLAIF），符合科技文献规范）
