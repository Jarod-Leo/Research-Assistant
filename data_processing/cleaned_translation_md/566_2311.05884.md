# Hiformer: Heterogeneous Feature Interactions Learning with Transformers for Recommender Systems

链接: http://arxiv.org/abs/2311.05884v1

原文摘要:
Learning feature interaction is the critical backbone to building recommender
systems. In web-scale applications, learning feature interaction is extremely
challenging due to the sparse and large input feature space; meanwhile,
manually crafting effective feature interactions is infeasible because of the
exponential solution space. We propose to leverage a Transformer-based
architecture with attention layers to automatically capture feature
interactions. Transformer architectures have witnessed great success in many
domains, such as natural language processing and computer vision. However,
there has not been much adoption of Transformer architecture for feature
interaction modeling in industry. We aim at closing the gap. We identify two
key challenges for applying the vanilla Transformer architecture to web-scale
recommender systems: (1) Transformer architecture fails to capture the
heterogeneous feature interactions in the self-attention layer; (2) The serving
latency of Transformer architecture might be too high to be deployed in
web-scale recommender systems. We first propose a heterogeneous self-attention
layer, which is a simple yet effective modification to the self-attention layer
in Transformer, to take into account the heterogeneity of feature interactions.
We then introduce \textsc{Hiformer} (\textbf{H}eterogeneous
\textbf{I}nteraction Trans\textbf{former}) to further improve the model
expressiveness. With low-rank approximation and model pruning, \hiformer enjoys
fast inference for online deployment. Extensive offline experiment results
corroborates the effectiveness and efficiency of the \textsc{Hiformer} model.
We have successfully deployed the \textsc{Hiformer} model to a real world large
scale App ranking model at Google Play, with significant improvement in key
engagement metrics (up to +2.66\%).

中文翻译:
以下是符合要求的学术化中文翻译：

学习特征交互是构建推荐系统的核心基础。在Web级应用中，由于输入特征空间具有高稀疏性与超大规模特性，特征交互学习面临极大挑战；同时，由于解空间呈指数级增长，人工设计有效特征交互在实践中不可行。我们提出采用基于Transformer的注意力架构来自动捕捉特征交互。Transformer架构已在自然语言处理、计算机视觉等领域取得显著成功，但在工业界的特征交互建模领域尚未得到广泛应用。本研究致力于填补这一空白。

我们发现标准Transformer架构应用于Web级推荐系统时存在两大关键挑战：(1) 自注意力层难以捕捉异构特征交互；(2) 架构的服务延迟可能过高而无法满足线上部署需求。首先，我们提出异构自注意力层——这是对Transformer自注意力层的简洁而有效的改进，能够充分考虑特征交互的异质性。继而提出\textsc{Hiformer}模型（\textbf{异}构\textbf{交}互Trans\textbf{former}）以进一步增强模型表达能力。通过低秩近似和模型剪枝技术，\hiformer实现了高效的线上推理速度。大量离线实验结果验证了\textsc{Hiformer}模型的有效性与高效性。目前该模型已成功部署于Google Play商店的大规模应用排序系统，关键用户参与指标获得显著提升（最高达+2.66%）。


