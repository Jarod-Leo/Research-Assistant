# Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment

链接: http://arxiv.org/abs/2405.03594v1

原文摘要:
Large language models (LLMs) have revolutionized Natural Language Processing
(NLP), but their size creates computational bottlenecks. We introduce a novel
approach to create accurate, sparse foundational versions of performant LLMs
that achieve full accuracy recovery for fine-tuning tasks at up to 70%
sparsity. We achieve this for the LLaMA-2 7B model by combining the SparseGPT
one-shot pruning method and sparse pretraining of those models on a subset of
the SlimPajama dataset mixed with a Python subset of The Stack dataset. We
exhibit training acceleration due to sparsity on Cerebras CS-3 chips that
closely matches theoretical scaling. In addition, we establish inference
acceleration of up to 3x on CPUs by utilizing Neural Magic's DeepSparse engine
and 1.7x on GPUs through Neural Magic's nm-vllm engine. The above gains are
realized via sparsity alone, thus enabling further gains through additional use
of quantization. Specifically, we show a total speedup on CPUs for
sparse-quantized LLaMA models of up to 8.6x. We demonstrate these results
across diverse, challenging tasks, including chat, instruction following, code
generation, arithmetic reasoning, and summarization to prove their generality.
This work paves the way for rapidly creating smaller and faster LLMs without
sacrificing accuracy.

中文翻译:
大型语言模型（LLMs）彻底改变了自然语言处理（NLP）领域，但其庞大的规模也带来了计算瓶颈。我们提出了一种创新方法，能够为高性能LLMs构建精确的稀疏基础版本，在稀疏度高达70%的情况下仍能实现微调任务的完全准确率恢复。通过在LLaMA-2 7B模型上结合SparseGPT一次性剪枝技术，并基于SlimPajama数据集子集与The Stack数据集的Python子集进行混合稀疏预训练，我们实现了这一目标。实验显示，在Cerebras CS-3芯片上，稀疏化带来的训练加速效果与理论预测高度吻合。此外，通过Neural Magic的DeepSparse引擎，我们在CPU上实现了最高3倍的推理加速；借助nm-vllm引擎，在GPU上获得了1.7倍的加速效果。上述增益仅通过稀疏化实现，因此结合量化技术可进一步扩大优势——我们证实稀疏量化后的LLaMA模型在CPU上最高可实现8.6倍的整体加速。这些成果在对话、指令跟随、代码生成、算术推理和文本摘要等多样化挑战性任务中均得到验证，充分证明了该方法的普适性。本研究为快速构建更小、更快且不损失准确性的LLMs开辟了新途径。

（翻译说明：采用技术文献的严谨表述风格，通过以下处理确保专业性：
1. 术语统一："sparsity"译为"稀疏度"，"quantization"译为"量化"
2. 被动语态转化："are realized"译为"通过...实现"符合中文表达习惯
3. 长句拆分：将原文复合句分解为多个短句，如将方法描述与实验验证分开
4. 概念显化："one-shot pruning"译为"一次性剪枝"并保留技术品牌名称
5. 数据呈现：精确保持所有数值关系，如"1.7x"译为"1.7倍"
6. 逻辑连接：使用"此外""因此"等衔接词保持论证连贯性）
