# A Comparative Study of Open-Source Large Language Models, GPT-4 and Claude 2: Multiple-Choice Test Taking in Nephrology

链接: http://arxiv.org/abs/2308.04709v1

原文摘要:
In recent years, there have been significant breakthroughs in the field of
natural language processing, particularly with the development of large
language models (LLMs). These LLMs have showcased remarkable capabilities on
various benchmarks. In the healthcare field, the exact role LLMs and other
future AI models will play remains unclear. There is a potential for these
models in the future to be used as part of adaptive physician training, medical
co-pilot applications, and digital patient interaction scenarios. The ability
of AI models to participate in medical training and patient care will depend in
part on their mastery of the knowledge content of specific medical fields. This
study investigated the medical knowledge capability of LLMs, specifically in
the context of internal medicine subspecialty multiple-choice test-taking
ability. We compared the performance of several open-source LLMs (Koala 7B,
Falcon 7B, Stable-Vicuna 13B, and Orca Mini 13B), to GPT-4 and Claude 2 on
multiple-choice questions in the field of Nephrology. Nephrology was chosen as
an example of a particularly conceptually complex subspecialty field within
internal medicine. The study was conducted to evaluate the ability of LLM
models to provide correct answers to nephSAP (Nephrology Self-Assessment
Program) multiple-choice questions. The overall success of open-sourced LLMs in
answering the 858 nephSAP multiple-choice questions correctly was 17.1% -
25.5%. In contrast, Claude 2 answered 54.4% of the questions correctly, whereas
GPT-4 achieved a score of 73.3%. We show that current widely used open-sourced
LLMs do poorly in their ability for zero-shot reasoning when compared to GPT-4
and Claude 2. The findings of this study potentially have significant
implications for the future of subspecialty medical training and patient care.

中文翻译:
近年来，自然语言处理领域取得了重大突破，尤其是大语言模型（LLMs）的发展。这些大语言模型在各种基准测试中展现出卓越能力。在医疗健康领域，LLMs及未来其他AI模型将扮演的确切角色尚不明确。这些模型未来或可应用于适应性医师培训、医疗辅助应用及数字化患者互动场景。AI模型参与医学培训和患者护理的能力，部分取决于其对特定医学领域知识内容的掌握程度。

本研究评估了大语言模型在内科学亚专科选择题应试能力方面的医学知识水平。我们比较了多个开源LLM模型（Koala 7B、Falcon 7B、Stable-Vicuna 13B和Orca Mini 13B）与GPT-4、Claude 2在肾脏病学选择题上的表现。选择肾脏病学作为内科学中概念特别复杂的亚专业领域代表，通过让这些模型回答肾脏病学自我评估项目（nephSAP）的选择题来评估其能力。结果显示：开源LLM模型在858道nephSAP选择题中的总体正确率为17.1%-25.5%，而Claude 2正确率为54.4%，GPT-4则达到73.3%。研究表明，当前广泛使用的开源LLM在零样本推理能力上远逊于GPT-4和Claude 2。这些发现可能对亚专科医学培训及患者护理的未来发展具有重要启示意义。
