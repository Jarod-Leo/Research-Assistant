# Towards Effective Time-Aware Language Representation: Exploring Enhanced Temporal Understanding in Language Models

链接: http://arxiv.org/abs/2406.01863v1

原文摘要:
In the evolving field of Natural Language Processing (NLP), understanding the
temporal context of text is increasingly critical for applications requiring
advanced temporal reasoning. Traditional pre-trained language models like BERT,
which rely on synchronic document collections such as BookCorpus and Wikipedia,
often fall short in effectively capturing and leveraging temporal information.
To address this limitation, we introduce BiTimeBERT 2.0, a novel time-aware
language model pre-trained on a temporal news article collection. BiTimeBERT
2.0 incorporates temporal information through three innovative pre-training
objectives: Extended Time-Aware Masked Language Modeling (ETAMLM), Document
Dating (DD), and Time-Sensitive Entity Replacement (TSER). Each objective is
specifically designed to target a distinct dimension of temporal information:
ETAMLM enhances the model's understanding of temporal contexts and relations,
DD integrates document timestamps as explicit chronological markers, and TSER
focuses on the temporal dynamics of "Person" entities. Moreover, our refined
corpus preprocessing strategy reduces training time by nearly 53\%, making
BiTimeBERT 2.0 significantly more efficient while maintaining high performance.
Experimental results show that BiTimeBERT 2.0 achieves substantial improvements
across a broad range of time-related tasks and excels on datasets spanning
extensive temporal ranges. These findings underscore BiTimeBERT 2.0's potential
as a powerful tool for advancing temporal reasoning in NLP.

中文翻译:
在自然语言处理（NLP）快速发展的领域中，理解文本的时间语境对于需要高级时序推理的应用日益关键。传统预训练语言模型（如BERT）依赖BookCorpus和维基百科等共时性文档集合，往往难以有效捕捉和利用时间信息。为突破这一局限，我们推出BiTimeBERT 2.0——一种基于时序新闻文章集合预训练的新型时间感知语言模型。该模型通过三项创新预训练目标整合时间信息：扩展时间感知掩码语言建模（ETAMLM）、文档年代标注（DD）以及时间敏感实体替换（TSER）。每个目标分别针对时间信息的不同维度：ETAMLM增强模型对时序上下文和关系的理解，DD将文档时间戳作为显式时间标记，TSER则聚焦"人物"实体的时序动态特征。此外，我们优化的语料预处理策略使训练时间缩短近53%，在保持高性能的同时显著提升效率。实验结果表明，BiTimeBERT 2.0在各类时间相关任务中均取得显著提升，尤其在跨越长期时间范围的数据集上表现优异。这些发现印证了BiTimeBERT 2.0作为推进NLP时序推理强大工具的潜力。

（翻译说明：采用学术论文摘要的规范表述，通过以下处理实现专业性与可读性的平衡：
1. 术语统一："temporal context"译为"时间语境"，"chronological markers"译为"时间标记"
2. 长句拆分：将原文复合句重组为符合中文表达习惯的短句结构
3. 被动语态转化："are specifically designed to"译为主动式"分别针对"
4. 概念显化："Person entities"增译为"人物实体"以明确指代
5. 数据呈现：保留"53%"数字格式符合中文科技论文惯例
6. 逻辑衔接：使用"此外""尤其"等连接词保持论证连贯性）
