# Zero-shot Causal Graph Extrapolation from Text via LLMs

链接: http://arxiv.org/abs/2312.14670v1

原文摘要:
We evaluate the ability of large language models (LLMs) to infer causal
relations from natural language. Compared to traditional natural language
processing and deep learning techniques, LLMs show competitive performance in a
benchmark of pairwise relations without needing (explicit) training samples.
This motivates us to extend our approach to extrapolating causal graphs through
iterated pairwise queries. We perform a preliminary analysis on a benchmark of
biomedical abstracts with ground-truth causal graphs validated by experts. The
results are promising and support the adoption of LLMs for such a crucial step
in causal inference, especially in medical domains, where the amount of
scientific text to analyse might be huge, and the causal statements are often
implicit.

中文翻译:
我们评估了大语言模型（LLMs）从自然语言中推断因果关系的能力。与传统自然语言处理和深度学习技术相比，LLMs在无需（显式）训练样本的情况下，于成对关系基准测试中展现出具有竞争力的表现。这促使我们将该方法拓展至通过迭代式成对查询来推导因果图。我们在一个生物医学摘要基准集上进行了初步分析，该数据集包含经专家验证的真实因果图。实验结果令人鼓舞，支持采用LLMs来完成因果推理中的这一关键步骤——尤其在医学领域，待分析的科学文本体量可能极其庞大，且因果陈述往往具有隐含性。

（翻译说明：
1. 专业术语处理："ground-truth"译为"真实"，"biomedical abstracts"译为"生物医学摘要"符合学科惯例
2. 句式重构：将英语长句拆分为符合中文表达习惯的短句，如将"where"引导的从句独立成破折号补充说明
3. 被动语态转换："validated by experts"主动化为"经专家验证"
4. 概念显化："implicit"译为"隐含性"以保持学术文本的精确度
5. 保持技术严谨性："iterated pairwise queries"译为"迭代式成对查询"准确传达算法特征）
