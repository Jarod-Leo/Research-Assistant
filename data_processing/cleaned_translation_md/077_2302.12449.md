# SGL-PT: A Strong Graph Learner with Graph Prompt Tuning

链接: http://arxiv.org/abs/2302.12449v1

原文摘要:
Recently, much exertion has been paid to design graph self-supervised methods
to obtain generalized pre-trained models, and adapt pre-trained models onto
downstream tasks through fine-tuning. However, there exists an inherent gap
between pretext and downstream graph tasks, which insufficiently exerts the
ability of pre-trained models and even leads to negative transfer. Meanwhile,
prompt tuning has seen emerging success in natural language processing by
aligning pre-training and fine-tuning with consistent training objectives. In
this paper, we identify the challenges for graph prompt tuning: The first is
the lack of a strong and universal pre-training task across sundry pre-training
methods in graph domain. The second challenge lies in the difficulty of
designing a consistent training objective for both pre-training and downstream
tasks. To overcome above obstacles, we propose a novel framework named SGL-PT
which follows the learning strategy ``Pre-train, Prompt, and Predict''.
Specifically, we raise a strong and universal pre-training task coined as SGL
that acquires the complementary merits of generative and contrastive
self-supervised graph learning. And aiming for graph classification task, we
unify pre-training and fine-tuning by designing a novel verbalizer-free
prompting function, which reformulates the downstream task in a similar format
as pretext task. Empirical results show that our method surpasses other
baselines under unsupervised setting, and our prompt tuning method can greatly
facilitate models on biological datasets over fine-tuning methods.

中文翻译:
近年来，研究者们投入大量精力设计图自监督学习方法以获取通用预训练模型，并通过微调使预训练模型适配下游任务。然而，预训练任务与下游图任务之间存在固有差距，这不仅限制了预训练模型性能的充分发挥，甚至可能导致负迁移现象。与此同时，提示学习（prompt tuning）通过保持预训练与微调阶段目标的一致性，已在自然语言处理领域展现出显著成效。本文指出图提示学习面临的两大挑战：其一，图领域缺乏跨多种预训练方法的强通用预训练任务；其二，难以设计同时适用于预训练与下游任务的一致性训练目标。为突破这些障碍，我们提出名为SGL-PT的新型框架，遵循"预训练-提示-预测"的学习范式。具体而言，我们提出名为SGL的强通用预训练任务，融合生成式与对比式图自监督学习的互补优势。针对图分类任务，我们通过设计无需人工模板的提示函数，将下游任务重构为与预训练任务相似的格式，从而实现预训练与微调的统一。实验结果表明，在无监督设定下我们的方法显著优于基线模型，且相较于传统微调方法，我们的提示学习方法在生物数据集上能大幅提升模型性能。
