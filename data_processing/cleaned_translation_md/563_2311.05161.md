# Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization

链接: http://arxiv.org/abs/2311.05161v1

原文摘要:
Large Language Models (LLMs) are proficient in natural language processing
tasks, but their deployment is often restricted by extensive parameter sizes
and computational demands. This paper focuses on post-training quantization
(PTQ) in LLMs, specifically 4-bit weight and 8-bit activation (W4A8)
quantization, to enhance computational efficiency -- a topic less explored
compared to weight-only quantization. We present two innovative techniques:
activation-quantization-aware scaling (AQAS) and sequence-length-aware
calibration (SLAC) to enhance PTQ by considering the combined effects on
weights and activations and aligning calibration sequence lengths to target
tasks. Moreover, we introduce dINT, a hybrid data format combining integer and
denormal representations, to address the underflow issue in W4A8 quantization,
where small values are rounded to zero. Through rigorous evaluations of LLMs,
including OPT and LLaMA, we demonstrate that our techniques significantly boost
task accuracies to levels comparable with full-precision models. By developing
arithmetic units compatible with dINT, we further confirm that our methods
yield a 2$\times$ hardware efficiency improvement compared to 8-bit integer MAC
unit.

中文翻译:
以下是符合要求的学术中文翻译：

大型语言模型（LLMs）在自然语言处理任务中表现卓越，但其部署常受限于庞大的参数量与计算需求。本文重点研究LLMs的训练后量化（PTQ）技术，特别是4比特权重与8比特激活（W4A8）量化方案以提升计算效率——该领域相较于仅权重量化的研究尚不充分。我们提出两项创新技术：通过协同考虑权重与激活影响的"激活量化感知缩放"（AQAS）技术，以及通过校准序列长度匹配目标任务的"序列长度感知校准"（SLAC）方法。此外，针对W4A8量化中小数值被舍入为零的数值下溢问题，我们设计了融合整数与非规格化表示的混合数据格式dINT。基于OPT和LLaMA等LLMs的严格评估表明，我们的技术将任务准确率显著提升至与全精度模型相当的水平。通过开发兼容dINT的算术运算单元，进一步验证本方案相比8比特整数乘法累加单元可实现2倍的硬件能效提升。

翻译说明：
1. 专业术语处理：采用"训练后量化"（PTQ）、"非规格化表示"等标准译法，首次出现标注英文缩写
2. 技术概念显化：将"combined effects"译为"协同影响"，"underflow issue"译为"数值下溢问题"
3. 句式结构调整：拆分英文长句为中文短句（如第一句），将被动语态转为主动表述
4. 数据规范呈现：保留"2×"的数学表达形式，维持技术文档精确性
5. 学术风格统一：使用"本文""表明""验证"等科研论文惯用表述
6. 技术名词一致性：全篇统一"量化/激活/校准"等关键术语译法
