# Questioning Internal Knowledge Structure of Large Language Models Through the Lens of the Olympic Games

链接: http://arxiv.org/abs/2409.06518v1

原文摘要:
Large language models (LLMs) have become a dominant approach in natural
language processing, yet their internal knowledge structures remain largely
unexplored. In this paper, we analyze the internal knowledge structures of LLMs
using historical medal tallies from the Olympic Games. We task the models with
providing the medal counts for each team and identifying which teams achieved
specific rankings. Our results reveal that while state-of-the-art LLMs perform
remarkably well in reporting medal counts for individual teams, they struggle
significantly with questions about specific rankings. This suggests that the
internal knowledge structures of LLMs are fundamentally different from those of
humans, who can easily infer rankings from known medal counts. To support
further research, we publicly release our code, dataset, and model outputs.

中文翻译:
大语言模型（LLMs）已成为自然语言处理领域的主流方法，但其内部知识结构仍存在大量未知。本文通过奥林匹克运动会历史奖牌数据，对LLMs的内部知识结构进行系统性分析。我们要求模型完成两项任务：提供各代表队的奖牌总数，以及识别达到特定排名的队伍。研究发现，尽管最先进的LLMs在单个队伍的奖牌统计任务上表现优异，但在涉及具体排名的问题上却存在显著困难。这表明LLMs的内部知识结构与人类存在本质差异——人类能够轻松地从已知奖牌数推导出排名，而模型则表现出明显的认知偏差。为促进后续研究，我们公开了实验代码、数据集及模型输出结果。

（翻译说明：采用学术论文摘要的简洁风格，通过句式重组实现以下效果：
1. 将原文两个长句拆分为符合中文表达习惯的短句
2. "fundamentally different"译为"本质差异"突出核心发现
3. "struggle significantly"转化为"存在显著困难"保持学术客观性
4. 补充"认知偏差"作为"cannot infer"的隐含语义补偿
5. 使用破折号替代原文"who"的从句结构，增强逻辑衔接
6. 专业术语如"state-of-the-art"统一译为业界通用表述"最先进的"）
