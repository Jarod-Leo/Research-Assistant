# Hierarchical Pretraining on Multimodal Electronic Health Records

链接: http://arxiv.org/abs/2310.07871v1

原文摘要:
Pretraining has proven to be a powerful technique in natural language
processing (NLP), exhibiting remarkable success in various NLP downstream
tasks. However, in the medical domain, existing pretrained models on electronic
health records (EHR) fail to capture the hierarchical nature of EHR data,
limiting their generalization capability across diverse downstream tasks using
a single pretrained model. To tackle this challenge, this paper introduces a
novel, general, and unified pretraining framework called MEDHMP, specifically
designed for hierarchically multimodal EHR data. The effectiveness of the
proposed MEDHMP is demonstrated through experimental results on eight
downstream tasks spanning three levels. Comparisons against eighteen baselines
further highlight the efficacy of our approach.

中文翻译:
预训练已被证明是自然语言处理（NLP）领域的一项强大技术，在各种NLP下游任务中展现出显著成效。然而在医疗领域，现有基于电子健康记录（EHR）的预训练模型未能捕捉EHR数据的层次化特性，导致单一预训练模型在多样化下游任务中的泛化能力受限。为应对这一挑战，本文提出了一种新颖、通用且统一的预训练框架MEDHMP，专为层次化多模态EHR数据设计。通过在涵盖三个层级的八项下游任务上的实验结果，验证了所提MEDHMP框架的有效性。与十八个基线模型的对比进一步凸显了本方法的优越性。

（翻译说明：
1. 专业术语处理："hierarchically multimodal"译为"层次化多模态"，"downstream tasks"统一译为"下游任务"
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句，如第一句通过分号连接两个并列观点
3. 被动语态转换："is demonstrated"译为主动态的"验证了"
4. 概念显化："spanning three levels"补充译为"涵盖三个层级"以明确指代
5. 学术风格保持：使用"泛化能力""基线模型"等规范学术用语
6. 逻辑连接处理："However"译为"然而"并独立成句，体现转折关系）
