# LLM-Based Section Identifiers Excel on Open Source but Stumble in Real World Applications

链接: http://arxiv.org/abs/2404.16294v1

原文摘要:
Electronic health records (EHR) even though a boon for healthcare
practitioners, are growing convoluted and longer every day. Sifting around
these lengthy EHRs is taxing and becomes a cumbersome part of physician-patient
interaction. Several approaches have been proposed to help alleviate this
prevalent issue either via summarization or sectioning, however, only a few
approaches have truly been helpful in the past. With the rise of automated
methods, machine learning (ML) has shown promise in solving the task of
identifying relevant sections in EHR. However, most ML methods rely on labeled
data which is difficult to get in healthcare. Large language models (LLMs) on
the other hand, have performed impressive feats in natural language processing
(NLP), that too in a zero-shot manner, i.e. without any labeled data. To that
end, we propose using LLMs to identify relevant section headers. We find that
GPT-4 can effectively solve the task on both zero and few-shot settings as well
as segment dramatically better than state-of-the-art methods. Additionally, we
also annotate a much harder real world dataset and find that GPT-4 struggles to
perform well, alluding to further research and harder benchmarks.

中文翻译:
尽管电子健康记录（EHR）对医疗从业者而言是一大福音，但其内容正日益复杂冗长。从这些长篇累牍的EHR中筛选信息不仅耗费精力，更成为医患互动中的沉重负担。为缓解这一普遍问题，学界提出了通过摘要生成或章节划分的多种解决方案，但真正有效的方法屈指可数。随着自动化技术的兴起，机器学习（ML）在识别EHR相关章节任务中展现出潜力，然而大多数ML方法依赖标注数据——这在医疗领域获取难度极高。相比之下，大语言模型（LLMs）在自然语言处理（NLP）中展现出惊人能力，甚至能以零样本（即无需标注数据）方式完成任务。基于此，我们提出使用LLMs识别相关章节标题。研究发现，GPT-4在零样本和少样本场景下均能有效完成任务，其分割效果显著优于现有最优方法。此外，我们还标注了一个更具挑战性的真实世界数据集，发现GPT-4表现欠佳，这暗示着未来研究需要更严格的基准测试。

（翻译说明：采用学术论文的严谨表述风格，通过以下处理实现专业性与可读性的平衡：
1. 术语统一："zero-shot"译为"零样本"，"few-shot"译为"少样本"符合NLP领域惯例
2. 长句拆分：将原文复合句分解为符合中文表达习惯的短句，如将"are growing...day"处理为独立短句
3. 被动语态转化："have been proposed"译为主动态"学界提出"
4. 文化适配："boon"译为"福音"而非直译"恩惠"，更符合中文科技文本特征
5. 逻辑显化：通过"相比之下""基于此"等连接词保持论证链条清晰）
