# Graph Reasoning with Large Language Models via Pseudo-code Prompting

链接: http://arxiv.org/abs/2409.17906v1

原文摘要:
Large language models (LLMs) have recently achieved remarkable success in
various reasoning tasks in the field of natural language processing. This
success of LLMs has also motivated their use in graph-related tasks. Among
others, recent work has explored whether LLMs can solve graph problems such as
counting the number of connected components of a graph or computing the
shortest path distance between two nodes. Although LLMs possess preliminary
graph reasoning abilities, they might still struggle to solve some seemingly
simple problems. In this paper, we investigate whether prompting via
pseudo-code instructions can improve the performance of LLMs in solving graph
problems. Our experiments demonstrate that using pseudo-code instructions
generally improves the performance of all considered LLMs. The graphs,
pseudo-code prompts, and evaluation code are publicly available.

中文翻译:
近期，大型语言模型（LLMs）在自然语言处理领域的各类推理任务中取得了显著成功。这一成功也推动了LLMs在图相关任务中的应用。最新研究开始探索LLMs能否解决诸如计算图的连通分量数量、求解节点间最短路径距离等图论问题。尽管LLMs已具备初步的图推理能力，但在处理某些看似简单的问题时仍存在困难。本文通过实验验证：采用伪代码指令进行提示能否提升LLMs解决图问题的性能。结果表明，伪代码提示策略普遍提高了所有测试模型的性能。相关实验数据（包括图结构、伪代码提示模板及评估代码）已开源发布。

（翻译说明：  
1. 专业术语处理："connected components"译为"连通分量"（图论标准译法），"shortest path distance"译为"最短路径距离"  
2. 句式重构：将原文复合长句拆分为符合中文表达习惯的短句，如将"Although..."让步状语从句转换为"尽管...但..."结构  
3. 被动语态转换：将"has been explored"等被动式转为"开始探索"主动表达  
4. 学术规范：保留"LLMs"英文缩写（中文领域通用），补充"（大型语言模型）"说明  
5. 文化适配："publicly available"译为符合中文论文惯例的"已开源发布"而非直译"公开可用"）
