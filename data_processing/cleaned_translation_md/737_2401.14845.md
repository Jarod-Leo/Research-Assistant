# Adaptive Point Transformer

链接: http://arxiv.org/abs/2401.14845v1

原文摘要:
The recent surge in 3D data acquisition has spurred the development of
geometric deep learning models for point cloud processing, boosted by the
remarkable success of transformers in natural language processing. While point
cloud transformers (PTs) have achieved impressive results recently, their
quadratic scaling with respect to the point cloud size poses a significant
scalability challenge for real-world applications. To address this issue, we
propose the Adaptive Point Cloud Transformer (AdaPT), a standard PT model
augmented by an adaptive token selection mechanism. AdaPT dynamically reduces
the number of tokens during inference, enabling efficient processing of large
point clouds. Furthermore, we introduce a budget mechanism to flexibly adjust
the computational cost of the model at inference time without the need for
retraining or fine-tuning separate models. Our extensive experimental
evaluation on point cloud classification tasks demonstrates that AdaPT
significantly reduces computational complexity while maintaining competitive
accuracy compared to standard PTs. The code for AdaPT is made publicly
available.

中文翻译:
近年来，随着三维数据采集技术的蓬勃发展，受Transformer架构在自然语言处理领域取得突破性成功的启发，基于点云的几何深度学习模型研究迎来快速增长。尽管点云Transformer（PT）近期展现出卓越性能，但其计算复杂度随点云规模呈平方级增长的特性，严重制约了实际应用中的可扩展性。为此，我们提出自适应点云Transformer（AdaPT），通过在标准PT模型中引入动态令牌选择机制，实现推理过程中自适应缩减令牌数量，从而高效处理大规模点云数据。该框架创新性地采用预算调控机制，无需重新训练或部署多个独立模型，即可在推理阶段灵活调整计算开销。我们在点云分类任务上的大量实验表明：相较于传统PT模型，AdaPT在保持竞争力精度的同时显著降低了计算复杂度。本研究已公开全部实现代码。

（翻译说明：
1. 专业术语处理："geometric deep learning"译为"几何深度学习"，"token selection mechanism"译为"令牌选择机制"
2. 句式重构：将原文复合长句拆分为符合中文表达习惯的短句，如将"boosted by..."处理为独立分句
3. 被动语态转换："are made publicly available"主动化为"已公开"
4. 技术概念显化："quadratic scaling"译为"平方级增长"并补充"计算复杂度"主体
5. 逻辑连接优化：使用"为此""从而""相较于"等连接词保持论证连贯性
6. 学术风格保持：采用"该框架""本研究"等符合学术论文表达的措辞）
