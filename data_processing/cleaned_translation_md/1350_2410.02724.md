# Large Language Models as Markov Chains

链接: http://arxiv.org/abs/2410.02724v1

原文摘要:
Large language models (LLMs) are remarkably efficient across a wide range of
natural language processing tasks and well beyond them. However, a
comprehensive theoretical analysis of the LLMs' generalization capabilities
remains elusive. In our paper, we approach this task by drawing an equivalence
between autoregressive transformer-based language models and Markov chains
defined on a finite state space. This allows us to study the multi-step
inference mechanism of LLMs from first principles. We relate the obtained
results to the pathological behavior observed with LLMs such as repetitions and
incoherent replies with high temperature. Finally, we leverage the proposed
formalization to derive pre-training and in-context learning generalization
bounds for LLMs under realistic data and model assumptions. Experiments with
the most recent Llama and Gemma herds of models show that our theory correctly
captures their behavior in practice.

中文翻译:
以下是符合您要求的中文翻译：

大语言模型（LLMs）在自然语言处理领域乃至更广泛的任务中展现出卓越效能，然而其泛化能力的系统性理论分析仍属空白。本研究通过建立自回归Transformer语言模型与有限状态空间马尔可夫链的等价关系，从第一性原理出发探究LLMs的多步推理机制。所得理论结果可解释高温参数下模型出现的重复生成、语义不连贯等异常现象。基于该形式化框架，我们进一步推导出实际数据与模型假设条件下LLMs的预训练及上下文学习泛化边界。在最新Llama与Gemma系列模型上的实验表明，本理论能准确预测其实践表现。

（翻译严格遵循学术规范，采用专业术语统一原则："generalization capabilities"译为"泛化能力"，"autoregressive"译为"自回归"，"in-context learning"译为"上下文学习"等。通过拆分英语长句为中文短句结构（如将"drawing an equivalence between..."处理为独立分句），并保留"first principles"等专业表述的直译。在保持原文严谨性的同时，使用"空白""探究""框架"等符合中文论文摘要风格的措辞。）
