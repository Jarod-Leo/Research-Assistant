# Prada: Black-Box LLM Adaptation with Private Data on Resource-Constrained Devices

链接: http://arxiv.org/abs/2503.14932v1

原文摘要:
In recent years, Large Language Models (LLMs) have demonstrated remarkable
abilities in various natural language processing tasks. However, adapting these
models to specialized domains using private datasets stored on
resource-constrained edge devices, such as smartphones and personal computers,
remains challenging due to significant privacy concerns and limited
computational resources. Existing model adaptation methods either compromise
data privacy by requiring data transmission or jeopardize model privacy by
exposing proprietary LLM parameters. To address these challenges, we propose
Prada, a novel privacy-preserving and efficient black-box LLM adaptation system
using private on-device datasets. Prada employs a lightweight proxy model
fine-tuned with Low-Rank Adaptation (LoRA) locally on user devices. During
inference, Prada leverages the logits offset, i.e., difference in outputs
between the base and adapted proxy models, to iteratively refine outputs from a
remote black-box LLM. This offset-based adaptation approach preserves both data
privacy and model privacy, as there is no need to share sensitive data or
proprietary model parameters. Furthermore, we incorporate speculative decoding
to further speed up the inference process of Prada, making the system
practically deployable on bandwidth-constrained edge devices, enabling a more
practical deployment of Prada. Extensive experiments on various downstream
tasks demonstrate that Prada achieves performance comparable to centralized
fine-tuning methods while significantly reducing computational overhead by up
to 60% and communication costs by up to 80%.

中文翻译:
近年来，大型语言模型（LLM）在各类自然语言处理任务中展现出卓越能力。然而，当这些模型需要利用智能手机、个人电脑等资源受限的边缘设备上存储的私有数据进行领域适配时，严重的隐私顾虑和有限的计算资源仍构成重大挑战。现有模型适配方法要么因数据传输需求而损害数据隐私，要么因暴露专有LLM参数而危及模型隐私。为解决这些问题，我们提出Prada——一种基于私有设备端数据、兼顾隐私保护与高效执行的黑盒LLM适配系统。Prada通过在用户设备本地部署经低秩适配（LoRA）微调的轻量级代理模型，在推理阶段利用基础模型与适配后代理模型输出间的对数偏移量，对远程黑盒LLM的输出进行迭代优化。这种基于偏移量的适配方法无需共享敏感数据或专有模型参数，同时保护了数据隐私和模型隐私。此外，我们引入推测式解码技术进一步加速Prada的推理过程，使其能够在带宽受限的边缘设备上实现实际部署。多组下游任务实验表明，Prada在达到与集中式微调方法相当性能的同时，显著降低高达60%的计算开销和80%的通信成本。
