# On Transforming Reinforcement Learning by Transformer: The Development Trajectory

链接: http://arxiv.org/abs/2212.14164v1

原文摘要:
Transformer, originally devised for natural language processing, has also
attested significant success in computer vision. Thanks to its super expressive
power, researchers are investigating ways to deploy transformers to
reinforcement learning (RL) and the transformer-based models have manifested
their potential in representative RL benchmarks. In this paper, we collect and
dissect recent advances on transforming RL by transformer (transformer-based RL
or TRL), in order to explore its development trajectory and future trend. We
group existing developments in two categories: architecture enhancement and
trajectory optimization, and examine the main applications of TRL in robotic
manipulation, text-based games, navigation and autonomous driving. For
architecture enhancement, these methods consider how to apply the powerful
transformer structure to RL problems under the traditional RL framework, which
model agents and environments much more precisely than deep RL methods, but
they are still limited by the inherent defects of traditional RL algorithms,
such as bootstrapping and "deadly triad". For trajectory optimization, these
methods treat RL problems as sequence modeling and train a joint state-action
model over entire trajectories under the behavior cloning framework, which are
able to extract policies from static datasets and fully use the long-sequence
modeling capability of the transformer. Given these advancements, extensions
and challenges in TRL are reviewed and proposals about future direction are
discussed. We hope that this survey can provide a detailed introduction to TRL
and motivate future research in this rapidly developing field.

中文翻译:
以下是符合要求的学术中文翻译：

最初为自然语言处理设计的Transformer架构，在计算机视觉领域也已取得显著成功。凭借其卓越的表征能力，研究者们正探索如何将Transformer应用于强化学习（RL）领域，基于Transformer的模型已在多个代表性RL基准测试中展现出巨大潜力。本文系统梳理并剖析了Transformer革新强化学习（Transformer-based RL，简称TRL）的最新进展，旨在探究其发展轨迹与未来趋势。我们将现有研究归纳为两大方向：架构增强与轨迹优化，并考察TRL在机器人操控、文本游戏、导航及自动驾驶等领域的主要应用。

在架构增强方面，相关研究致力于将强大的Transformer结构融入传统RL框架，这类方法相比深度RL能更精确地建模智能体与环境，但仍受限于传统RL算法的固有缺陷（如自举问题与"致命三要素"）。在轨迹优化方面，这些方法将RL问题视为序列建模任务，在行为克隆框架下训练覆盖完整轨迹的状态-动作联合模型，既能从静态数据集中提取策略，又能充分发挥Transformer的长序列建模优势。

本文在综述TRL领域技术进步与现存挑战的基础上，进一步探讨了未来发展方向。期望本综述能为TRL研究提供详尽的领域导引，推动这一快速发展领域的后续探索。


2. 被动语态转化（英文被动句转为中文主动表述）
3. 长句拆分重组（如原文最后复合句分解为三个中文短句）
4. 概念显化（如"deadly triad"增译为"致命三要素"并保留引号）
5. 逻辑连接词优化（"Thanks to"转为"凭借"，"Given"转为"在...基础上"））
