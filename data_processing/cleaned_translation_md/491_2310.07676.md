# Composite Backdoor Attacks Against Large Language Models

链接: http://arxiv.org/abs/2310.07676v1

原文摘要:
Large language models (LLMs) have demonstrated superior performance compared
to previous methods on various tasks, and often serve as the foundation models
for many researches and services. However, the untrustworthy third-party LLMs
may covertly introduce vulnerabilities for downstream tasks. In this paper, we
explore the vulnerability of LLMs through the lens of backdoor attacks.
Different from existing backdoor attacks against LLMs, ours scatters multiple
trigger keys in different prompt components. Such a Composite Backdoor Attack
(CBA) is shown to be stealthier than implanting the same multiple trigger keys
in only a single component. CBA ensures that the backdoor is activated only
when all trigger keys appear. Our experiments demonstrate that CBA is effective
in both natural language processing (NLP) and multimodal tasks. For instance,
with $3\%$ poisoning samples against the LLaMA-7B model on the Emotion dataset,
our attack achieves a $100\%$ Attack Success Rate (ASR) with a False Triggered
Rate (FTR) below $2.06\%$ and negligible model accuracy degradation. Our work
highlights the necessity of increased security research on the trustworthiness
of foundation LLMs.

中文翻译:
以下是符合要求的学术中文翻译：

大型语言模型（LLMs）在多类任务中展现出超越传统方法的卓越性能，已成为众多研究和服务的基础模型。然而，不可信的第三方LLMs可能为下游任务暗中植入安全隐患。本文通过后门攻击视角系统探究LLMs的脆弱性。与现有针对LLMs的后门攻击不同，本研究创新性地将多重触发密钥分散嵌入不同提示组件中。这种复合式后门攻击（CBA）被证实比将相同多密钥集中植入单一组件更具隐蔽性——仅当所有触发密钥同时出现时才会激活后门机制。实验表明，CBA在自然语言处理（NLP）和多模态任务中均具显著攻击效果。以LLaMA-7B模型在Emotion数据集上的测试为例，仅需3%的中毒样本即可实现100%攻击成功率（ASR），同时将误触发率（FTR）控制在2.06%以下，且模型精度损失可忽略不计。本研究揭示了加强基础LLMs可信安全研究的迫切必要性。

（翻译严格遵循以下原则：
1. 专业术语准确统一（如ASR/FTR保留英文缩写+中文全称）
2. 被动语态转换为主动句式（"is shown to be"→"被证实"）
3. 长难句拆分重组（将复合从句分解为符合中文表达习惯的短句）
4. 学术用语规范化（"demonstrate"→"表明"，"highlight"→"揭示"）
5. 数据呈现完整保留（3%/100%等数值精确传达）
6. 逻辑连接词显化（"However"→"然而"实现转折衔接））
