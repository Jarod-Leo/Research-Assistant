# SELFormer: Molecular Representation Learning via SELFIES Language Models

链接: http://arxiv.org/abs/2304.04662v1

原文摘要:
Automated computational analysis of the vast chemical space is critical for
numerous fields of research such as drug discovery and material science.
Representation learning techniques have recently been employed with the primary
objective of generating compact and informative numerical expressions of
complex data. One approach to efficiently learn molecular representations is
processing string-based notations of chemicals via natural language processing
(NLP) algorithms. Majority of the methods proposed so far utilize SMILES
notations for this purpose; however, SMILES is associated with numerous
problems related to validity and robustness, which may prevent the model from
effectively uncovering the knowledge hidden in the data. In this study, we
propose SELFormer, a transformer architecture-based chemical language model
that utilizes a 100% valid, compact and expressive notation, SELFIES, as input,
in order to learn flexible and high-quality molecular representations.
SELFormer is pre-trained on two million drug-like compounds and fine-tuned for
diverse molecular property prediction tasks. Our performance evaluation has
revealed that, SELFormer outperforms all competing methods, including graph
learning-based approaches and SMILES-based chemical language models, on
predicting aqueous solubility of molecules and adverse drug reactions. We also
visualized molecular representations learned by SELFormer via dimensionality
reduction, which indicated that even the pre-trained model can discriminate
molecules with differing structural properties. We shared SELFormer as a
programmatic tool, together with its datasets and pre-trained models. Overall,
our research demonstrates the benefit of using the SELFIES notations in the
context of chemical language modeling and opens up new possibilities for the
design and discovery of novel drug candidates with desired features.

中文翻译:
以下是符合您要求的中文翻译：

化学空间的自动化计算分析对于药物研发和材料科学等诸多研究领域至关重要。表征学习技术近年被广泛应用，其主要目标是生成复杂数据的紧凑且信息丰富的数值表达。其中一种高效学习分子表征的方法是通过自然语言处理（NLP）算法处理基于字符串的化学式表示。目前大多数方法采用SMILES符号进行此类处理，但SMILES存在诸多有效性和鲁棒性问题，可能阻碍模型有效挖掘数据中的隐藏知识。本研究提出SELFormer——一种基于Transformer架构的化学语言模型，该模型采用100%有效、紧凑且高表达性的SELFIES符号作为输入，以学习灵活且高质量的分子表征。SELFormer在200万个类药化合物上进行预训练，并针对多种分子性质预测任务进行微调。性能评估表明，在预测分子水溶性和药物不良反应任务上，SELFormer优于所有竞争方法（包括基于图学习的方法和SMILES化学语言模型）。我们通过降维可视化SELFormer学习的分子表征，发现即使预训练模型也能区分具有不同结构特性的分子。本研究以编程工具形式公开了SELFormer及其数据集与预训练模型。总体而言，我们的研究证明了在化学语言建模中使用SELFIES符号的优势，并为设计发现具有特定功能的新药候选分子开辟了新途径。

翻译说明：
1. 专业术语处理：SMILES/SELFIES保留英文大写形式，Transformer架构等专业名词按计算机领域惯例处理
2. 长句拆分：将原文复合长句按中文表达习惯分解为多个短句，如将"which may prevent..."独立成句
3. 被动语态转换："is associated with"译为"存在"更符合中文表达
4. 概念显化处理："drug-like compounds"译为"类药化合物"比直译更专业
5. 逻辑连接优化："Overall"译为"总体而言"作为段落总结标志词
6. 技术表述统一："pre-trained model"统一译为"预训练模型"保持术语一致性
7. 数据呈现方式：将"two million"转换为"200万"符合中文数字表达规范
