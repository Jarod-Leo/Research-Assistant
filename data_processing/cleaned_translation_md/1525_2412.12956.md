# SnakModel: Lessons Learned from Training an Open Danish Large Language Model

链接: http://arxiv.org/abs/2412.12956v1

原文摘要:
We present SnakModel, a Danish large language model (LLM) based on Llama2-7B,
which we continuously pre-train on 13.6B Danish words, and further tune on 3.7M
Danish instructions. As best practices for creating LLMs for smaller language
communities have yet to be established, we examine the effects of early
modeling and training decisions on downstream performance throughout the entire
training pipeline, including (1) the creation of a strictly curated corpus of
Danish text from diverse sources; (2) the language modeling and
instruction-tuning training process itself, including the analysis of
intermediate training dynamics, and ablations across different hyperparameters;
(3) an evaluation on eight language and culturally-specific tasks. Across these
experiments SnakModel achieves the highest overall performance, outperforming
multiple contemporary Llama2-7B-based models. By making SnakModel, the majority
of our pre-training corpus, and the associated code available under open
licenses, we hope to foster further research and development in Danish Natural
Language Processing, and establish training guidelines for languages with
similar resource constraints.

中文翻译:
我们推出SnakModel——一个基于Llama2-7B架构的丹麦语大语言模型（LLM）。该模型通过持续预训练13.6B丹麦语词汇，并进一步使用370万条丹麦语指令进行微调。鉴于针对小语种社群构建LLM的最佳实践尚未确立，我们系统考察了整个训练流程中早期建模与训练决策对下游性能的影响，包括：（1）从多源数据构建严格筛选的丹麦语文本语料库；（2）语言建模与指令微调的训练过程本身，涵盖中间训练动态分析及不同超参数的消融实验；（3）在八项涉及语言特性与文化特征的任务上进行评估。实验表明SnakModel综合表现最优，优于多个同期基于Llama2-7B的对比模型。我们将SnakModel、大部分预训练语料及相关代码以开放许可形式发布，旨在推动丹麦自然语言处理领域的研究发展，并为具有类似资源限制的语言建立训练范式。
