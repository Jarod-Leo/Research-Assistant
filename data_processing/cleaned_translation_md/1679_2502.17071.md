# Systematic Weight Evaluation for Pruning Large Language Models: Enhancing Performance and Sustainability

链接: http://arxiv.org/abs/2502.17071v1

原文摘要:
The exponential growth of large language models (LLMs) like ChatGPT has
revolutionized artificial intelligence, offering unprecedented capabilities in
natural language processing. However, the extensive computational resources
required for training these models have significant environmental implications,
including high carbon emissions, energy consumption, and water usage. This
research presents a novel approach to LLM pruning, focusing on the systematic
evaluation of individual weight importance throughout the training process. By
monitoring parameter evolution over time, we propose a method that effectively
reduces model size without compromising performance. Extensive experiments with
both a scaled-down LLM and a large multimodal model reveal that moderate
pruning enhances efficiency and reduces loss, while excessive pruning
drastically deteriorates model performance. These findings highlight the
critical need for optimized AI models to ensure sustainable development,
balancing technological advancement with environmental responsibility.

中文翻译:
以下是符合要求的学术中文翻译：

大型语言模型（如ChatGPT）的指数级增长彻底改变了人工智能领域，为自然语言处理提供了前所未有的能力。然而，训练这些模型所需的大量计算资源会带来显著的环境影响，包括高碳排放、能源消耗和水资源消耗。本研究提出了一种创新的语言模型剪枝方法，其核心在于系统评估训练过程中各权重参数的重要性。通过持续追踪参数的动态演化规律，我们开发出一种能在保持模型性能的同时有效压缩模型规模的技术方案。基于小型语言模型和大型多模态模型的对比实验表明：适度剪枝可提升模型效率并降低损失值，而过度剪枝则会导致模型性能急剧下降。这些发现揭示了优化人工智能模型的紧迫性，强调必须在技术进步与环境责任之间取得平衡，以实现可持续发展。

（翻译说明：1. 专业术语统一处理如"pruning"译为"剪枝"；2. 被动语态转换为中文主动句式；3. 长难句拆分重组为符合中文表达习惯的短句；4. 关键概念如"multimodal model"采用学界通用译法"多模态模型"；5. 保持学术文本的客观严谨性，避免口语化表达）
