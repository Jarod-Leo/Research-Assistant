# Transformers Simulate MLE for Sequence Generation in Bayesian Networks

链接: http://arxiv.org/abs/2501.02547v1

原文摘要:
Transformers have achieved significant success in various fields, notably
excelling in tasks involving sequential data like natural language processing.
Despite these achievements, the theoretical understanding of transformers'
capabilities remains limited. In this paper, we investigate the theoretical
capabilities of transformers to autoregressively generate sequences in Bayesian
networks based on in-context maximum likelihood estimation (MLE). Specifically,
we consider a setting where a context is formed by a set of independent
sequences generated according to a Bayesian network. We demonstrate that there
exists a simple transformer model that can (i) estimate the conditional
probabilities of the Bayesian network according to the context, and (ii)
autoregressively generate a new sample according to the Bayesian network with
estimated conditional probabilities. We further demonstrate in extensive
experiments that such a transformer does not only exist in theory, but can also
be effectively obtained through training. Our analysis highlights the potential
of transformers to learn complex probabilistic models and contributes to a
better understanding of large language models as a powerful class of sequence
generators.

中文翻译:
Transformer模型已在多个领域取得显著成功，尤其在自然语言处理等序列数据处理任务中表现卓越。然而尽管成果丰硕，学界对其理论能力的认知仍存在局限。本文基于上下文最大似然估计（MLE）框架，系统研究了Transformer在贝叶斯网络中自回归生成序列的理论能力。具体而言，我们设定由贝叶斯网络生成的独立序列集合构成上下文环境，证明存在一种简单的Transformer模型能够实现：（1）根据上下文估计贝叶斯网络的条件概率；（2）基于估计的条件概率自回归生成符合该网络的新样本。我们通过大量实验进一步验证，此类Transformer不仅理论存在，更能通过训练有效获得。本研究揭示了Transformer学习复杂概率模型的潜力，为理解大语言模型作为强大序列生成器的内在机制提供了新的理论视角。
