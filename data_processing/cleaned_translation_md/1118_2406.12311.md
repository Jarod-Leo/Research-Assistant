# Mixture of Scales: Memory-Efficient Token-Adaptive Binarization for Large Language Models

链接: http://arxiv.org/abs/2406.12311v1

原文摘要:
Binarization, which converts weight parameters to binary values, has emerged
as an effective strategy to reduce the size of large language models (LLMs).
However, typical binarization techniques significantly diminish linguistic
effectiveness of LLMs. To address this issue, we introduce a novel binarization
technique called Mixture of Scales (BinaryMoS). Unlike conventional methods,
BinaryMoS employs multiple scaling experts for binary weights, dynamically
merging these experts for each token to adaptively generate scaling factors.
This token-adaptive approach boosts the representational power of binarized
LLMs by enabling contextual adjustments to the values of binary weights.
Moreover, because this adaptive process only involves the scaling factors
rather than the entire weight matrix, BinaryMoS maintains compression
efficiency similar to traditional static binarization methods. Our experimental
results reveal that BinaryMoS surpasses conventional binarization techniques in
various natural language processing tasks and even outperforms 2-bit
quantization methods, all while maintaining similar model size to static
binarization techniques.

中文翻译:
二值化技术通过将权重参数转换为二元数值，已成为缩减大语言模型（LLM）体积的有效策略。然而传统二值化方法会显著削弱大语言模型的语言处理效能。为解决这一问题，我们提出了一种创新性的"混合比例二值化"技术（BinaryMoS）。与常规方法不同，BinaryMoS采用多组比例专家处理二值化权重，并针对每个输入标记动态融合这些专家，从而自适应生成比例因子。这种标记自适应的机制通过上下文调节二值权重的数值，显著增强了二值化大语言模型的表征能力。值得注意的是，由于该自适应过程仅作用于比例因子而非整个权重矩阵，BinaryMoS仍能保持与传统静态二值化方法相近的压缩效率。实验结果表明，BinaryMoS在多种自然语言处理任务中不仅优于传统二值化技术，其性能甚至超越2位量化方法，同时保持与静态二值化技术相当的模型体积。
