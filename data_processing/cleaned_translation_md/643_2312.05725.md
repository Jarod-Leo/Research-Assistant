# FP8-BERT: Post-Training Quantization for Transformer

链接: http://arxiv.org/abs/2312.05725v1

原文摘要:
Transformer-based models, such as BERT, have been widely applied in a wide
range of natural language processing tasks. However, one inevitable side effect
is that they require massive memory storage and inference cost when deployed in
production. Quantization is one of the popularized ways to alleviate the cost.
However, the previous 8-bit quantization strategy based on INT8 data format
either suffers from the degradation of accuracy in a Post-Training Quantization
(PTQ) fashion or requires an expensive Quantization-Aware Training (QAT)
process. Recently, a new numeric format FP8 (i.e. floating-point of 8-bits) has
been proposed and supported in commercial AI computing platforms such as H100.
In this paper, we empirically validate the effectiveness of FP8 as a way to do
Post-Training Quantization without significant loss of accuracy, with a simple
calibration and format conversion process. We adopt the FP8 standard proposed
by NVIDIA Corp. (2022) in our extensive experiments of BERT variants on GLUE
and SQuAD v1.1 datasets, and show that PTQ with FP8 can significantly improve
the accuracy upon that with INT8, to the extent of the full-precision model.

中文翻译:
基于Transformer的模型（如BERT）已被广泛应用于各类自然语言处理任务中。然而这类模型在部署时不可避免地存在内存占用大、推理成本高的问题。量化技术是当前主流的解决方案之一，但传统的基于INT8数据格式的8位量化方案存在明显缺陷：采用训练后量化（PTQ）方式会导致精度显著下降，而量化感知训练（QAT）过程又需要高昂的计算成本。近期，新型数值格式FP8（8位浮点数）被提出并已在H100等商用AI计算平台获得支持。本文通过实证研究表明：采用简单的校准和格式转换流程，FP8能在训练后量化过程中保持模型精度无损。我们基于NVIDIA公司（2022）提出的FP8标准，在GLUE和SQuAD v1.1数据集上对多种BERT变体进行了大量实验，结果显示FP8训练后量化的准确率显著优于INT8方案，甚至能达到与全精度模型相当的水平。


