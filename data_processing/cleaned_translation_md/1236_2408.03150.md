# Conditioning LLMs with Emotion in Neural Machine Translation

链接: http://arxiv.org/abs/2408.03150v1

原文摘要:
Large Language Models (LLMs) have shown remarkable performance in Natural
Language Processing tasks, including Machine Translation (MT). In this work, we
propose a novel MT pipeline that integrates emotion information extracted from
a Speech Emotion Recognition (SER) model into LLMs to enhance translation
quality. We first fine-tune five existing LLMs on the Libri-trans dataset and
select the most performant model. Subsequently, we augment LLM prompts with
different dimensional emotions and train the selected LLM under these different
configurations. Our experiments reveal that integrating emotion information,
especially arousal, into LLM prompts leads to notable improvements in
translation quality.

中文翻译:
大语言模型（LLMs）在自然语言处理任务中展现出卓越性能，包括机器翻译（MT）领域。本研究提出了一种创新的机器翻译流程，通过将语音情感识别（SER）模型提取的情感信息整合到大语言模型中，从而提升翻译质量。我们首先在Libri-trans数据集上对五种现有大语言模型进行微调，并筛选出性能最优的模型。随后，我们在模型提示中融入不同维度的情感特征，并在这些差异化配置下训练选定的大语言模型。实验结果表明，将情感信息（特别是唤醒度维度）整合到模型提示中，能显著提高翻译质量。

（翻译说明：
1. 专业术语采用学界通用译法："Large Language Models"译为"大语言模型"，"arousal"译为"唤醒度"
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句，如将"integrate...into..."结构转换为"通过...整合..."的主动句式
3. 语态转换：将被动语态"are augmented"等转换为中文常用的主动表达"融入"
4. 术语统一性：保持"LLM"在全文中统一译为"大语言模型"
5. 文化适配："notable improvements"译为"显著提高"符合中文学术表达习惯）
