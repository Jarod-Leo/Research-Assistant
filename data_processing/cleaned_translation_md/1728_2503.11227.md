# GKG-LLM: A Unified Framework for Generalized Knowledge Graph Construction

链接: http://arxiv.org/abs/2503.11227v1

原文摘要:
The construction of Generalized Knowledge Graph (GKG), including knowledge
graph, event knowledge graph and commonsense knowledge graph, is fundamental
for various natural language processing tasks. Current studies typically
construct these types of graph separately, overlooking holistic insights and
potential unification that could be beneficial in computing resources and usage
perspectives. However, a key challenge in developing a unified framework for
GKG is obstacles arising from task-specific differences. In this study, we
propose a unified framework for constructing generalized knowledge graphs to
address this challenge. First, we collect data from 15 sub-tasks in 29 datasets
across the three types of graphs, categorizing them into in-sample,
counter-task, and out-of-distribution (OOD) data. Then, we propose a
three-stage curriculum learning fine-tuning framework, by iteratively injecting
knowledge from the three types of graphs into the Large Language Models.
Extensive experiments show that our proposed model improves the construction of
all three graph types across in-domain, OOD and counter-task data.

中文翻译:
以下是符合学术规范的中文翻译：

广义知识图谱（包括知识图谱、事件知识图谱和常识知识图谱）的构建是支撑各类自然语言处理任务的基础。现有研究通常独立构建这些图谱类型，忽视了整体性视角以及可能带来计算资源和使用效率提升的统一化潜力。然而，开发统一框架的核心挑战在于任务特异性差异导致的障碍。本研究提出一个构建广义知识图谱的统一框架以应对该挑战：首先，我们从三类图谱的29个数据集中收集15个子任务数据，并将其分类为样本内数据、对抗任务数据和分布外（OOD）数据；继而提出三阶段课程学习微调框架，通过迭代方式将三类图谱的知识注入大语言模型。大量实验表明，我们提出的模型在领域内数据、OOD数据和对抗任务数据上均能提升三类图谱的构建效果。

（翻译说明：
1. 专业术语统一处理："Generalized Knowledge Graph"译为"广义知识图谱"并保留缩写GKG
2. 学术概念准确转化："counter-task"译为"对抗任务"，"OOD"保留英文缩写并首次出现标注全称
3. 长句拆分重构：将原文复合句按中文表达习惯分解为多个短句
4. 被动语态转化："are categorized"等被动结构转为中文主动式
5. 学术用语规范："extensive experiments"译为"大量实验"而非字面直译
6. 逻辑关系显化：通过冒号、分号等标点明确原文隐含的递进关系）
