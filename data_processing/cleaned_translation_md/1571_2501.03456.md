# Text to Band Gap: Pre-trained Language Models as Encoders for Semiconductor Band Gap Prediction

链接: http://arxiv.org/abs/2501.03456v1

原文摘要:
In this study, we explore the use of a transformer-based language model as an
encoder to predict the band gaps of semiconductor materials directly from their
text descriptions. Quantum chemistry simulations, including Density Functional
Theory (DFT), are computationally intensive and time-consuming, which limits
their practicality for high-throughput material screening, particularly for
complex systems. Shallow machine learning (ML) models, while effective, often
require extensive data preprocessing to convert non-numerical material
properties into numerical inputs. In contrast, our approach leverages textual
data directly, bypassing the need for complex feature engineering. We generate
material descriptions in two formats: formatted strings combining features and
natural language text generated using the ChatGPT API. We demonstrate that the
RoBERTa model, pre-trained on natural language processing tasks, performs
effectively as an encoder for prediction tasks. With minimal fine-tuning, it
achieves a mean absolute error (MAE) of approximately 0.33 eV, performing
better than shallow machine learning models such as Support Vector Regression,
Random Forest, and XGBoost. Even when only the linear regression head is
trained while keeping the RoBERTa encoder layers frozen, the accuracy remains
nearly identical to that of the fully trained model. This demonstrates that the
pre-trained RoBERTa encoder is highly adaptable for processing domain-specific
text related to material properties, such as the band gap, significantly
reducing the need for extensive retraining. This study highlights the potential
of transformer-based language models to serve as efficient and versatile
encoders for semiconductor materials property prediction tasks.

中文翻译:
本研究探讨了基于Transformer架构的语言模型作为编码器，直接从半导体材料的文本描述预测其带隙的可行性。传统量子化学模拟方法（如密度泛函理论DFT）存在计算复杂度高、耗时长等局限，难以满足复杂体系的高通量材料筛选需求。虽然浅层机器学习模型（ML）表现良好，但通常需要大量数据预处理将非数值型材料特征转化为数值输入。相比之下，我们的方法直接利用文本数据，规避了复杂的特征工程过程。

我们采用两种格式生成材料描述：结合特征的结构化字符串，以及通过ChatGPT API生成的自然语言文本。实验表明，经过自然语言处理任务预训练的RoBERTa模型作为预测任务的编码器表现优异：仅需微调即可实现约0.33电子伏特的平均绝对误差（MAE），其性能优于支持向量回归、随机森林和XGBoost等浅层机器学习模型。值得注意的是，即使冻结RoBERTa编码器层仅训练线性回归头，模型精度仍与全参数训练相当。这证明预训练的RoBERTa编码器对带隙等材料特性相关领域文本具有极强的适应能力，可大幅降低模型再训练需求。

本研究表明，基于Transformer的语言模型有望成为半导体材料特性预测任务中高效且通用的编码器解决方案。
