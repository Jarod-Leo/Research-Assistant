# IberBench: LLM Evaluation on Iberian Languages

链接: http://arxiv.org/abs/2504.16921v1

原文摘要:
Large Language Models (LLMs) remain difficult to evaluate comprehensively,
particularly for languages other than English, where high-quality data is often
limited. Existing benchmarks and leaderboards are predominantly
English-centric, with only a few addressing other languages. These benchmarks
fall short in several key areas: they overlook the diversity of language
varieties, prioritize fundamental Natural Language Processing (NLP)
capabilities over tasks of industrial relevance, and are static. With these
aspects in mind, we present IberBench, a comprehensive and extensible benchmark
designed to assess LLM performance on both fundamental and industry-relevant
NLP tasks, in languages spoken across the Iberian Peninsula and Ibero-America.
IberBench integrates 101 datasets from evaluation campaigns and recent
benchmarks, covering 22 task categories such as sentiment and emotion analysis,
toxicity detection, and summarization. The benchmark addresses key limitations
in current evaluation practices, such as the lack of linguistic diversity and
static evaluation setups by enabling continual updates and community-driven
model and dataset submissions moderated by a committee of experts. We evaluate
23 LLMs ranging from 100 million to 14 billion parameters and provide empirical
insights into their strengths and limitations. Our findings indicate that (i)
LLMs perform worse on industry-relevant tasks than in fundamental ones, (ii)
performance is on average lower for Galician and Basque, (iii) some tasks show
results close to random, and (iv) in other tasks LLMs perform above random but
below shared task systems. IberBench offers open-source implementations for the
entire evaluation pipeline, including dataset normalization and hosting,
incremental evaluation of LLMs, and a publicly accessible leaderboard.

中文翻译:
以下是符合要求的学术化中文翻译：

大型语言模型（LLMs）的综合评估仍面临挑战，尤其在英语以外的语言领域——这些语言常面临高质量数据匮乏的问题。现有基准测试和排行榜主要聚焦英语，仅少数涉及其他语种。这些基准存在若干关键缺陷：忽视语言变体的多样性、偏重基础自然语言处理（NLP）能力而忽视工业级应用任务、且采用静态评估模式。基于此，我们提出IberBench——一个全面且可扩展的评估框架，旨在针对伊比利亚半岛及拉丁美洲地区语言，系统评估LLMs在基础NLP任务与工业级任务上的表现。

IberBench整合了来自评估竞赛和最新基准的101个数据集，涵盖22类任务（包括情感分析、毒性检测、文本摘要等）。该基准通过三项创新设计突破当前评估实践的局限：（1）支持持续更新与社区驱动的模型/数据集提交（由专家委员会审核）；（2）解决语言多样性缺失问题；（3）改进静态评估模式。我们对23个参数量从1亿至140亿不等的LLMs进行了实证评估，结果显示：（i）LLMs在工业级任务表现显著弱于基础任务；（ii）对加利西亚语和巴斯克语的平均处理能力较低；（iii）部分任务结果接近随机水平；（iv）某些任务虽优于随机基线但仍落后于专项系统。

IberBench开源了完整评估流程的实现方案，包含数据集标准化与托管、LLMs增量评估系统，以及公开的实时排行榜。所有资源可通过https://iberbench.linel.es获取。


