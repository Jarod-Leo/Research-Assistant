# Unraveling the Dominance of Large Language Models Over Transformer Models for Bangla Natural Language Inference: A Comprehensive Study

链接: http://arxiv.org/abs/2405.02937v1

原文摘要:
Natural Language Inference (NLI) is a cornerstone of Natural Language
Processing (NLP), providing insights into the entailment relationships between
text pairings. It is a critical component of Natural Language Understanding
(NLU), demonstrating the ability to extract information from spoken or written
interactions. NLI is mainly concerned with determining the entailment
relationship between two statements, known as the premise and hypothesis. When
the premise logically implies the hypothesis, the pair is labeled "entailment".
If the hypothesis contradicts the premise, the pair receives the
"contradiction" label. When there is insufficient evidence to establish a
connection, the pair is described as "neutral". Despite the success of Large
Language Models (LLMs) in various tasks, their effectiveness in NLI remains
constrained by issues like low-resource domain accuracy, model overconfidence,
and difficulty in capturing human judgment disagreements. This study addresses
the underexplored area of evaluating LLMs in low-resourced languages such as
Bengali. Through a comprehensive evaluation, we assess the performance of
prominent LLMs and state-of-the-art (SOTA) models in Bengali NLP tasks,
focusing on natural language inference. Utilizing the XNLI dataset, we conduct
zero-shot and few-shot evaluations, comparing LLMs like GPT-3.5 Turbo and
Gemini 1.5 Pro with models such as BanglaBERT, Bangla BERT Base, DistilBERT,
mBERT, and sahajBERT. Our findings reveal that while LLMs can achieve
comparable or superior performance to fine-tuned SOTA models in few-shot
scenarios, further research is necessary to enhance our understanding of LLMs
in languages with modest resources like Bengali. This study underscores the
importance of continued efforts in exploring LLM capabilities across diverse
linguistic contexts.

中文翻译:
自然语言推理（Natural Language Inference, NLI）是自然语言处理（NLP）的基石任务，通过分析文本对之间的蕴涵关系来揭示语言的内在逻辑。作为自然语言理解（NLU）的核心组成部分，NLI展现了从口头或书面交互中提取信息的能力。该任务主要致力于判定两个陈述（分别称为前提与假设）之间的逻辑关系：当前提能够推导出假设时，标注为"蕴涵"；当假设与前提矛盾时，标注为"矛盾"；当证据不足无法建立关联时，则标注为"中立"。尽管大语言模型（LLMs）在多项任务中表现卓越，但其在NLI任务中的效能仍受限于低资源领域准确率、模型过度自信以及难以捕捉人类判断分歧等问题。

本研究聚焦于LLMs在孟加拉语等低资源语言评估中的空白领域。通过系统化评估，我们对比了主流LLMs与前沿模型（SOTA）在孟加拉语NLP任务（特别是自然语言推理）中的表现。基于XNLI数据集，我们采用零样本和小样本评估方法，将GPT-3.5 Turbo、Gemini 1.5 Pro等LLMs与BanglaBERT、孟加拉语BERT基础模型、DistilBERT、mBERT及sahajBERT等模型进行对比。研究发现，在小样本场景下，LLMs能够达到甚至超越精调SOTA模型的性能，但对于孟加拉语等中等资源语言，仍需进一步研究来深化对LLMs的理解。本项研究强调了持续探索LLMs在多样化语言环境中应用能力的重要性。
