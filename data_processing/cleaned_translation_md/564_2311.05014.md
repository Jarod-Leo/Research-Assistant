# Interpreting Pretrained Language Models via Concept Bottlenecks

链接: http://arxiv.org/abs/2311.05014v1

原文摘要:
Pretrained language models (PLMs) have made significant strides in various
natural language processing tasks. However, the lack of interpretability due to
their ``black-box'' nature poses challenges for responsible implementation.
Although previous studies have attempted to improve interpretability by using,
e.g., attention weights in self-attention layers, these weights often lack
clarity, readability, and intuitiveness. In this research, we propose a novel
approach to interpreting PLMs by employing high-level, meaningful concepts that
are easily understandable for humans. For example, we learn the concept of
``Food'' and investigate how it influences the prediction of a model's
sentiment towards a restaurant review. We introduce C$^3$M, which combines
human-annotated and machine-generated concepts to extract hidden neurons
designed to encapsulate semantically meaningful and task-specific concepts.
Through empirical evaluations on real-world datasets, we manifest that our
approach offers valuable insights to interpret PLM behavior, helps diagnose
model failures, and enhances model robustness amidst noisy concept labels.

中文翻译:
预训练语言模型（PLMs）在各类自然语言处理任务中取得了显著进展。然而，由于其"黑盒"特性导致的解释性缺失，为负责任的应用落地带来了挑战。尽管先前研究尝试通过自注意力层中的注意力权重等方法来提升可解释性，但这些权重往往缺乏清晰性、可读性和直观性。本研究提出了一种创新方法，通过采用人类易于理解的高层次语义概念来解释PLMs。例如，我们学习"食物"这一概念，并探究其如何影响模型对餐厅评论的情感预测。我们提出的C$^3$M框架融合人工标注与机器生成的概念，专门设计用于提取能封装语义明确且任务相关概念的隐藏神经元。基于真实数据集的实证评估表明，该方法能有效解释PLM行为机制，辅助诊断模型故障，并在存在噪声概念标签的情况下增强模型鲁棒性。

（翻译说明：
1. 专业术语处理：PLMs采用中文全称+括号标注英文缩写的形式；self-attention layers译为技术界通用译名"自注意力层"
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句，如将"which combines..."定语从句转换为独立分句
3. 概念显化："black-box"译为加引号的"黑盒"以保留原文隐喻特征
4. 技术表述准确性：attention weights严格对应"注意力权重"，neuron采用计算机领域通用译法"神经元"
5. 被动语态转换：将"are easily understandable"等被动结构转为中文主动表述
6. 术语一致性：全文统一"可解释性""鲁棒性"等关键术语译法
7. 文化适配：restaurant review根据中文语境译为"餐厅评论"而非字面的"餐馆评论"）
