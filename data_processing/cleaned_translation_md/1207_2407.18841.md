# QT-TDM: Planning with Transformer Dynamics Model and Autoregressive Q-Learning

链接: http://arxiv.org/abs/2407.18841v1

原文摘要:
Inspired by the success of the Transformer architecture in natural language
processing and computer vision, we investigate the use of Transformers in
Reinforcement Learning (RL), specifically in modeling the environment's
dynamics using Transformer Dynamics Models (TDMs). We evaluate the capabilities
of TDMs for continuous control in real-time planning scenarios with Model
Predictive Control (MPC). While Transformers excel in long-horizon prediction,
their tokenization mechanism and autoregressive nature lead to costly planning
over long horizons, especially as the environment's dimensionality increases.
To alleviate this issue, we use a TDM for short-term planning, and learn an
autoregressive discrete Q-function using a separate Q-Transformer (QT) model to
estimate a long-term return beyond the short-horizon planning. Our proposed
method, QT-TDM, integrates the robust predictive capabilities of Transformers
as dynamics models with the efficacy of a model-free Q-Transformer to mitigate
the computational burden associated with real-time planning. Experiments in
diverse state-based continuous control tasks show that QT-TDM is superior in
performance and sample efficiency compared to existing Transformer-based RL
models while achieving fast and computationally efficient inference.

中文翻译:
受Transformer架构在自然语言处理和计算机视觉领域取得成功的启发，我们探索了Transformer在强化学习（RL）中的应用，特别是通过Transformer动力学模型（TDM）对环境动态进行建模。我们评估了TDM在模型预测控制（MPC）框架下实时连续控制任务中的表现。虽然Transformer擅长长时程预测，但其标记化机制和自回归特性会导致长时程规划的计算成本高昂，尤其在环境维度增加时更为显著。为缓解这一问题，我们采用TDM进行短期规划，同时通过独立的Q-Transformer（QT）模型学习自回归离散Q函数来估计超出短期规划范围的长时程回报。我们提出的QT-TDM方法将Transformer作为动力学模型的强大预测能力，与无模型Q-Transformer的高效性相结合，从而减轻实时规划的计算负担。在多种基于状态的连续控制任务实验中，QT-TDM在性能和样本效率上均优于现有基于Transformer的强化学习模型，同时实现了快速且计算高效的推理。
