# BrainNPT: Pre-training of Transformer networks for brain network classification

链接: http://arxiv.org/abs/2305.01666v1

原文摘要:
Deep learning methods have advanced quickly in brain imaging analysis over
the past few years, but they are usually restricted by the limited labeled
data. Pre-trained model on unlabeled data has presented promising improvement
in feature learning in many domains, including natural language processing and
computer vision. However, this technique is under-explored in brain network
analysis. In this paper, we focused on pre-training methods with Transformer
networks to leverage existing unlabeled data for brain functional network
classification. First, we proposed a Transformer-based neural network, named as
BrainNPT, for brain functional network classification. The proposed method
leveraged <cls> token as a classification embedding vector for the Transformer
model to effectively capture the representation of brain network. Second, we
proposed a pre-training framework for BrainNPT model to leverage unlabeled
brain network data to learn the structure information of brain networks. The
results of classification experiments demonstrated the BrainNPT model without
pre-training achieved the best performance with the state-of-the-art models,
and the BrainNPT model with pre-training strongly outperformed the
state-of-the-art models. The pre-training BrainNPT model improved 8.75% of
accuracy compared with the model without pre-training. We further compared the
pre-training strategies, analyzed the influence of the parameters of the model,
and interpreted the trained model.

中文翻译:
以下是符合要求的学术中文翻译：

深度学习方法在过去几年中推动了脑成像分析的快速发展，但这些方法通常受限于标记数据的稀缺性。基于无标记数据的预训练模型已在自然语言处理和计算机视觉等多个领域展现出卓越的特征学习能力，然而该技术在脑网络分析领域尚未得到充分探索。本研究聚焦Transformer网络的预训练方法，旨在利用现有无标记数据提升脑功能网络分类性能。首先，我们提出名为BrainNPT的基于Transformer的神经网络架构，通过引入<cls>标记作为分类嵌入向量，使模型能有效捕捉脑网络表征特征。其次，我们设计了BrainNPT的预训练框架，利用无标记脑网络数据学习网络结构信息。分类实验结果表明：未经预训练的BrainNPT模型已达到当前最先进模型的性能水平，而经过预训练的BrainNPT模型则显著超越所有基线模型——预训练使模型准确率提升8.75%。研究进一步对比了不同预训练策略，分析了模型参数影响，并对训练完成的模型进行了解释性分析。

（译文严格遵循学术规范，具有以下特点：
1. 专业术语统一："Transformer networks"译为"Transformer网络"，"pre-training"统一为"预训练"
2. 被动语态转化：将英文被动式转换为中文主动式（如"are restricted by"译为"受限于"）
3. 长句拆分：将原文复合长句按中文习惯分解为多个短句
4. 概念准确传达："classification embedding vector"译为"分类嵌入向量"保持技术准确性
5. 数据呈现规范：百分比数字保留原文精确值"8.75%"
6. 学术用语："state-of-the-art models"译为"最先进模型"符合国内学术惯例）
