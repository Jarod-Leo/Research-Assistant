# Positional Knowledge is All You Need: Position-induced Transformer (PiT) for Operator Learning

链接: http://arxiv.org/abs/2405.09285v1

原文摘要:
Operator learning for Partial Differential Equations (PDEs) is rapidly
emerging as a promising approach for surrogate modeling of intricate systems.
Transformers with the self-attention mechanism$\unicode{x2013}$a powerful tool
originally designed for natural language processing$\unicode{x2013}$have
recently been adapted for operator learning. However, they confront challenges,
including high computational demands and limited interpretability. This raises
a critical question: Is there a more efficient attention mechanism for
Transformer-based operator learning? This paper proposes the Position-induced
Transformer (PiT), built on an innovative position-attention mechanism, which
demonstrates significant advantages over the classical self-attention in
operator learning. Position-attention draws inspiration from numerical methods
for PDEs. Different from self-attention, position-attention is induced by only
the spatial interrelations of sampling positions for input functions of the
operators, and does not rely on the input function values themselves, thereby
greatly boosting efficiency. PiT exhibits superior performance over current
state-of-the-art neural operators in a variety of complex operator learning
tasks across diverse PDE benchmarks. Additionally, PiT possesses an enhanced
discretization convergence feature, compared to the widely-used Fourier neural
operator.

中文翻译:
偏微分方程（PDE）的算子学习正迅速成为复杂系统代理建模的重要方法。最初为自然语言处理设计的强大工具——基于自注意力机制的Transformer——近期已被应用于算子学习领域。然而，该方法仍面临计算需求高、可解释性有限等挑战，这引发了一个关键问题：是否存在更高效的注意力机制适用于基于Transformer的算子学习？本文提出位置诱导Transformer（PiT），其创新性地构建于位置注意力机制之上，在算子学习中展现出显著优于传统自注意力的特性。该位置注意力机制从偏微分方程数值解法中获得启发，与自注意力的本质区别在于：其注意力权重仅由算子输入函数采样点的空间相互关系决定，完全不依赖于输入函数值本身，从而大幅提升计算效率。在多种偏微分方程基准测试的复杂算子学习任务中，PiT均表现出超越当前最先进神经算子模型的性能优势。此外，与广泛应用的傅里叶神经算子相比，PiT还具有更强的离散化收敛特性。

（翻译说明：
1. 专业术语处理："operator learning"译为"算子学习"，"surrogate modeling"译为"代理建模"，保持学术规范性
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句，如将"Transformers with..."处理为独立分句
3. 被动语态转换：将"have recently been adapted"译为主动态"近期已被应用"
4. 概念显化："self-attention mechanism"补充说明为"自注意力机制"
5. 技术细节处理：准确翻译"discretization convergence feature"为"离散化收敛特性"
6. 文化适配：保留"Transformer/PiT"等专业缩写，首次出现时标注全称）
