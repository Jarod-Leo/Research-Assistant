# PIS: Linking Importance Sampling and Attention Mechanisms for Efficient Prompt Compression

链接: http://arxiv.org/abs/2504.16574v1

原文摘要:
Large language models (LLMs) have achieved remarkable progress, demonstrating
unprecedented capabilities across various natural language processing tasks.
However, the high costs associated with such exceptional performance limit the
widespread adoption of LLMs, highlighting the need for prompt compression.
Existing prompt compression methods primarily rely on heuristic truncation or
abstractive summarization techniques, which fundamentally overlook the
intrinsic mechanisms of LLMs and lack a systematic evaluation of token
importance for generation. In this work, we introduce Prompt Importance
Sampling (PIS), a novel compression framework that dynamically compresses
prompts by sampling important tokens based on the analysis of attention scores
of hidden states. PIS employs a dual-level compression mechanism: 1) at the
token level, we quantify saliency using LLM-native attention scores and
implement adaptive compression through a lightweight 9-layer reinforcement
learning (RL) network; 2) at the semantic level, we propose a Russian roulette
sampling strategy for sentence-level importance sampling. Comprehensive
evaluations across multiple domain benchmarks demonstrate that our method
achieves state-of-the-art compression performance. Notably, our framework
serendipitously enhances reasoning efficiency through optimized context
structuring. This work advances prompt engineering by offering both theoretical
grounding and practical efficiency in context management for LLMs.

中文翻译:
以下是符合您要求的中文翻译：

大型语言模型（LLMs）已取得显著进展，在各种自然语言处理任务中展现出前所未有的能力。然而，这种卓越性能伴随的高昂成本限制了LLMs的广泛应用，凸显出提示词压缩的必要性。现有压缩方法主要依赖启发式截断或摘要生成技术，这些方法本质上忽视了LLMs的内在机制，且缺乏对生成过程中词元重要性的系统评估。本研究提出提示词重要性采样（PIS），这是一种通过分析隐藏状态注意力分数来动态采样重要词元的新型压缩框架。PIS采用双级压缩机制：1）在词元层面，我们利用LLM原生注意力分数量化显著性，并通过轻量级9层强化学习（RL）网络实现自适应压缩；2）在语义层面，我们提出俄罗斯轮盘赌采样策略进行句子级重要性采样。跨多领域基准的全面评估表明，本方法实现了最先进的压缩性能。值得注意的是，该框架通过优化上下文结构，意外提升了推理效率。本研究为LLMs的上下文管理提供了理论基础和实践效能，推动了提示词工程的发展。

翻译说明：
1. 专业术语处理：LLMs统一译为"大型语言模型"，"attention scores"译为"注意力分数"，"reinforcement learning"译为"强化学习"等
2. 技术概念保留："Russian roulette sampling"采用意译+注释的"俄罗斯轮盘赌采样策略"
3. 长句拆分：将原文复合长句按中文习惯分解为多个短句，如将"which fundamentally..."独立成句
4. 被动语态转换："are primarily relied"转为主动式"主要依赖"
5. 学术风格保持：使用"本研究"、"该框架"等符合学术论文表述的用语
6. 文化适配："serendipitously"译为"意外"而非直译，更符合中文表达习惯
