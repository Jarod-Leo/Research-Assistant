# Pre-trained Language Models and Few-shot Learning for Medical Entity Extraction

链接: http://arxiv.org/abs/2504.04385v1

原文摘要:
This study proposes a medical entity extraction method based on Transformer
to enhance the information extraction capability of medical literature.
Considering the professionalism and complexity of medical texts, we compare the
performance of different pre-trained language models (BERT, BioBERT,
PubMedBERT, ClinicalBERT) in medical entity extraction tasks. Experimental
results show that PubMedBERT achieves the best performance (F1-score = 88.8%),
indicating that a language model pre-trained on biomedical literature is more
effective in the medical domain. In addition, we analyze the impact of
different entity extraction methods (CRF, Span-based, Seq2Seq) and find that
the Span-based approach performs best in medical entity extraction tasks
(F1-score = 88.6%). It demonstrates superior accuracy in identifying entity
boundaries. In low-resource scenarios, we further explore the application of
Few-shot Learning in medical entity extraction. Experimental results show that
even with only 10-shot training samples, the model achieves an F1-score of
79.1%, verifying the effectiveness of Few-shot Learning under limited data
conditions. This study confirms that the combination of pre-trained language
models and Few-shot Learning can enhance the accuracy of medical entity
extraction. Future research can integrate knowledge graphs and active learning
strategies to improve the model's generalization and stability, providing a
more effective solution for medical NLP research. Keywords- Natural Language
Processing, medical named entity recognition, pre-trained language model,
Few-shot Learning, information extraction, deep learning

中文翻译:
本研究提出了一种基于Transformer的医疗实体抽取方法，旨在提升医学文献的信息抽取能力。针对医学文本的专业性和复杂性，我们对比了不同预训练语言模型（BERT、BioBERT、PubMedBERT、ClinicalBERT）在医疗实体抽取任务中的表现。实验结果表明，基于生物医学文献预训练的PubMedBERT模型取得最优性能（F1值=88.8%），证实领域适配的预训练模型在医疗领域更具优势。此外，我们分析了不同实体抽取方法（CRF、基于片段的方法、序列到序列模型）的效果，发现基于片段的方法在医疗实体抽取任务中表现最佳（F1值=88.6%），其识别实体边界具有更高准确性。在低资源场景下，我们进一步探索了小样本学习在医疗实体抽取中的应用，实验显示仅需10个训练样本即可达到79.1%的F1值，验证了小样本学习在数据受限条件下的有效性。本研究证实预训练语言模型与小样本学习的结合能有效提升医疗实体抽取精度，未来研究可融合知识图谱和主动学习策略以增强模型泛化性和稳定性，为医疗自然语言处理研究提供更优解决方案。

关键词：自然语言处理，医疗命名实体识别，预训练语言模型，小样本学习，信息抽取，深度学习
