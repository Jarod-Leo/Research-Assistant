# PAFT: A Parallel Training Paradigm for Effective LLM Fine-Tuning

链接: http://arxiv.org/abs/2406.17923v1

原文摘要:
Large language models (LLMs) have shown remarkable abilities in diverse
natural language processing (NLP) tasks. The LLMs generally undergo supervised
fine-tuning (SFT) followed by preference alignment to be usable in downstream
applications. However, this sequential training pipeline leads to alignment tax
that degrades the LLM performance.
  This paper introduces PAFT, a new PArallel training paradigm for effective
LLM Fine-Tuning, which independently performs SFT and preference alignment
(e.g., DPO and ORPO, etc.) with the same pre-trained model on respective
datasets. The model produced by SFT and the model from preference alignment are
then merged into a final model by parameter fusing for use in downstream
applications. This work reveals important findings that preference alignment
like DPO naturally results in a sparse model while SFT leads to a natural dense
model which needs to be sparsified for effective model merging. This paper
introduces an effective interference resolution which reduces the redundancy by
sparsifying the delta parameters. The LLM resulted from the new training
paradigm achieved Rank #1 on the HuggingFace Open LLM Leaderboard.
Comprehensive evaluation shows the effectiveness of the parallel training
paradigm.

中文翻译:
以下是符合要求的学术中文翻译：

大语言模型（LLMs）在多样化自然语言处理（NLP）任务中展现出卓越能力。当前主流方法通常采用监督微调（SFT）与偏好对齐（如DPO、ORPO等）的串行训练流程，但这种顺序训练模式会导致"对齐税"现象，从而降低模型性能。

本文提出PAFT——一种并行的LLM高效微调范式，其创新性地使用同一预训练模型分别在SFT数据集和偏好对齐数据集上独立训练。随后通过参数融合技术将两个训练分支的模型合并为最终下游应用模型。研究发现：DPO等偏好对齐方法会自然产生稀疏化模型，而SFT则生成稠密模型——后者需进行稀疏化处理才能实现有效融合。为此，本文提出基于delta参数稀疏化的干扰消除方法，通过消除冗余参数提升合并效果。采用该并行训练范式得到的LLM在HuggingFace开放大模型排行榜上位列第一，全面评估验证了该范式的有效性。

翻译说明：
1. 专业术语处理：LLM/NLP/DPO等专业缩写首次出现时保留英文全称，SFT等通用术语直接使用中文译名
2. 句式重构：将原文复合句拆分为符合中文表达习惯的短句，如将"which independently..."处理为独立分句
3. 学术表达："alignment tax"译为专业术语"对齐税"，"parameter fusing"译为"参数融合技术"
4. 被动语态转化："are merged"译为主动态的"合并为"
5. 文化适配：排行榜名称保留"HuggingFace"品牌名但补充说明其性质
6. 逻辑显化：通过破折号和冒号等标点强化技术路线的逻辑关系
7. 术语统一性：全文保持"稀疏化/稠密"的对应译法一致性
