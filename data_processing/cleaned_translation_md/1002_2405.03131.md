# WDMoE: Wireless Distributed Large Language Models with Mixture of Experts

链接: http://arxiv.org/abs/2405.03131v1

原文摘要:
Large Language Models (LLMs) have achieved significant success in various
natural language processing tasks, but how wireless communications can support
LLMs has not been extensively studied. In this paper, we propose a wireless
distributed LLMs paradigm based on Mixture of Experts (MoE), named WDMoE,
deploying LLMs collaboratively across edge servers of base station (BS) and
mobile devices in the wireless communications system. Specifically, we
decompose the MoE layer in LLMs by deploying the gating network and the
preceding neural network layer at BS, while distributing the expert networks
across the devices. This arrangement leverages the parallel capabilities of
expert networks on distributed devices. Moreover, to overcome the instability
of wireless communications, we design an expert selection policy by taking into
account both the performance of the model and the end-to-end latency, which
includes both transmission delay and inference delay. Evaluations conducted
across various LLMs and multiple datasets demonstrate that WDMoE not only
outperforms existing models, such as Llama 2 with 70 billion parameters, but
also significantly reduces end-to-end latency.

中文翻译:
以下是符合要求的学术性中文翻译：

【译文】
大语言模型（LLMs）在各类自然语言处理任务中取得了显著成功，但无线通信如何支持LLMs尚未得到充分研究。本文提出一种基于专家混合系统（MoE）的无线分布式LLMs范式WDMoE，将LLMs协同部署于无线通信系统的基站边缘服务器与移动设备。具体而言，我们通过将MoE层中的门控网络及前置神经网络层部署在基站，同时将专家网络分布式部署于终端设备，从而实现对LLMs中MoE层的解构。这种架构充分利用了分布式设备上专家网络的并行计算能力。此外，为克服无线通信的不稳定性，我们设计了一种综合考虑模型性能与端到端延迟（包含传输时延与推理时延）的专家选择策略。在不同LLMs和多种数据集上的评估表明，WDMoE不仅性能优于现有模型（如700亿参数的Llama 2），还能显著降低端到端延迟。

【翻译要点说明】
1. 专业术语处理：
- "Mixture of Experts"译为"专家混合系统"（计算机领域标准译法）
- "gating network"保留专业术语特征译为"门控网络"
- "end-to-end latency"译为"端到端延迟"符合通信工程术语规范

2. 句式重构：
- 将原文复合长句拆分为符合中文表达习惯的短句（如第二句的拆分）
- 被动语态转换为主动表述（如"are distributed"译为"分布式部署"）
- 增补逻辑连接词"从而"、"此外"等增强连贯性

3. 技术准确性：
- "edge servers of base station"精确译为"基站边缘服务器"
- "transmission delay and inference delay"分别译为"传输时延"与"推理时延"（专业术语对应）

4. 学术风格保持：
- 保留"本文"、"研究表明"等学术论文惯用表述
- 数字单位统一使用中文"亿"（如70 billion译为70亿）
- 技术名词首次出现标注英文缩写（LLMs/MoE）
