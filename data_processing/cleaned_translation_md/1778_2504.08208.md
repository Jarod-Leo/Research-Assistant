# How Good Are Large Language Models for Course Recommendation in MOOCs?

链接: http://arxiv.org/abs/2504.08208v1

原文摘要:
Large Language Models (LLMs) have made significant strides in natural
language processing and are increasingly being integrated into recommendation
systems. However, their potential in educational recommendation systems has yet
to be fully explored. This paper investigates the use of LLMs as a
general-purpose recommendation model, leveraging their vast knowledge derived
from large-scale corpora for course recommendation tasks. We explore a variety
of approaches, ranging from prompt-based methods to more advanced fine-tuning
techniques, and compare their performance against traditional recommendation
models. Extensive experiments were conducted on a real-world MOOC dataset,
evaluating using LLMs as course recommendation systems across key dimensions
such as accuracy, diversity, and novelty. Our results demonstrate that LLMs can
achieve good performance comparable to traditional models, highlighting their
potential to enhance educational recommendation systems. These findings pave
the way for further exploration and development of LLM-based approaches in the
context of educational recommendations.

中文翻译:
大语言模型（LLMs）在自然语言处理领域取得显著进展，并日益被整合到推荐系统中。然而，其在教育推荐系统中的潜力尚未得到充分探索。本文研究将LLMs作为通用推荐模型的可行性，利用其从大规模语料库中获取的丰富知识来完成课程推荐任务。我们探索了从基于提示的方法到更高级微调技术等多种实现路径，并将其性能与传统推荐模型进行对比。通过在真实慕课数据集上的大量实验，我们从准确性、多样性和新颖性等关键维度评估了LLMs作为课程推荐系统的表现。结果表明，LLMs能达到与传统模型相当的良好性能，凸显了其提升教育推荐系统的潜力。这些发现为基于LLM的教育推荐方法的进一步探索与发展奠定了基础。

（翻译说明：采用学术论文摘要的标准表述方式，对专业术语如"fine-tuning"译为"微调"、"MOOC"译为"慕课"保持一致性；将英文长句合理切分为符合中文表达习惯的短句；通过"可行性"、"实现路径"等措辞准确传达原文的研究方法论；使用"凸显"、"奠定基础"等动词短语体现学术文本的严谨性；在保持专业性的同时确保行文流畅，如"大量实验"比直译"广泛实验"更符合中文表达习惯。）
