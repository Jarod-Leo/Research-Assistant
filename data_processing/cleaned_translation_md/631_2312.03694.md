# Parameter-Efficient Transfer Learning of Audio Spectrogram Transformers

链接: http://arxiv.org/abs/2312.03694v2

原文摘要:
Parameter-efficient transfer learning (PETL) methods have emerged as a solid
alternative to the standard full fine-tuning approach. They only train a few
extra parameters for each downstream task, without sacrificing performance and
dispensing with the issue of storing a copy of the pre-trained model for each
task. For audio classification tasks, the Audio Spectrogram Transformer (AST)
model shows impressive results. However, surprisingly, how to efficiently adapt
it to several downstream tasks has not been tackled before. In this paper, we
bridge this gap and present a detailed investigation of common PETL methods for
the adaptation of the AST model to audio/speech tasks. Furthermore, we propose
a new adapter design that exploits the convolution module of the Conformer
model, leading to superior performance over the standard PETL approaches and
surpassing or achieving performance parity with full fine-tuning by updating
only 0.29% of the parameters. Finally, we provide ablation studies revealing
that our proposed adapter: 1) proves to be effective in few-shot efficient
transfer learning, 2) attains optimal results regardless of the amount of the
allocated parameters, and 3) can be applied to other pre-trained models.

中文翻译:
参数高效迁移学习（PETL）方法已成为标准全微调方法的有力替代方案。这类方法仅需为每个下游任务训练少量额外参数，在保持性能优势的同时，避免了为每个任务存储预训练模型副本的问题。在音频分类任务中，音频频谱变换器（AST）模型展现出卓越性能，但令人惊讶的是，如何高效适配该模型至多个下游任务的研究尚属空白。本文填补了这一研究缺口，系统考察了AST模型在音频/语音任务适配中常见PETL方法的应用效果。我们进一步提出一种新型适配器设计，该设计利用Conformer模型的卷积模块，在仅更新0.29%参数的情况下，其性能不仅超越标准PETL方法，更达到或超过全微调水平。消融研究表明：1）该适配器在小样本高效迁移学习中表现优异；2）其性能不受参数分配量的影响；3）可拓展应用于其他预训练模型。
