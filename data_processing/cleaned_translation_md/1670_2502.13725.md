# Adapting Large Language Models for Time Series Modeling via a Novel Parameter-efficient Adaptation Method

链接: http://arxiv.org/abs/2502.13725v1

原文摘要:
Time series modeling holds significant importance in many real-world
applications and has been extensively studied. While pre-trained foundation
models have made impressive strides in the fields of natural language
processing (NLP) and computer vision (CV), their development in time series
domains has been constrained by data sparsity. A series of recent studies have
demonstrated that large language models (LLMs) possess robust pattern
recognition and reasoning abilities over complex sequences of tokens. However,
the current literature have yet striked a high-quality balance between (a)
effectively aligning the time series and natural language modalities, and (b)
keeping the inference efficiency. To address the above issues, we now propose
the Time-LlaMA framework. Time-LlaMA first converts the time series input into
token embeddings through a linear tokenization mechanism. Second, the time
series token embeddings are aligned with the text prompts. Third, to further
adapt the LLM backbone for time series modeling, we have developed a dynamic
low-rank adaptation technique (D-LoRA). D-LoRA dynamically chooses the most
suitable LoRA modules at each layer of the Transformer backbone for each time
series input, enhancing the model's predictive capabilities. Our experimental
results on an extensive collection of challenging real-world time series tasks
confirm that our proposed method achieves the state-of-the-art (SOTA)
performance.

中文翻译:
时间序列建模在众多现实应用中具有重要意义，并已得到广泛研究。尽管预训练基础模型在自然语言处理（NLP）和计算机视觉（CV）领域取得了显著进展，但其在时间序列领域的发展一直受限于数据稀疏性。近期一系列研究表明，大型语言模型（LLMs）对复杂标记序列具有强大的模式识别与推理能力。然而现有研究尚未在以下两方面实现高质量平衡：（a）有效对齐时间序列与自然语言模态；（b）保持推理效率。针对上述问题，本文提出Time-LlaMA框架。该框架首先通过线性标记化机制将时间序列输入转化为标记嵌入，随后将时间序列标记嵌入与文本提示进行对齐。为进一步适配LLM主干网络的时间序列建模需求，我们开发了动态低秩自适应技术（D-LoRA）。该技术根据每个时间序列输入动态选择Transformer主干网络每层最合适的LoRA模块，从而增强模型预测能力。我们在大量具有挑战性的现实世界时间序列任务上的实验结果表明，所提方法实现了最先进的（SOTA）性能。


