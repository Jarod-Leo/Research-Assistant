# MoSECroT: Model Stitching with Static Word Embeddings for Crosslingual Zero-shot Transfer

链接: http://arxiv.org/abs/2401.04821v1

原文摘要:
Transformer-based pre-trained language models (PLMs) have achieved remarkable
performance in various natural language processing (NLP) tasks. However,
pre-training such models can take considerable resources that are almost only
available to high-resource languages. On the contrary, static word embeddings
are easier to train in terms of computing resources and the amount of data
required. In this paper, we introduce MoSECroT Model Stitching with Static Word
Embeddings for Crosslingual Zero-shot Transfer), a novel and challenging task
that is especially relevant to low-resource languages for which static word
embeddings are available. To tackle the task, we present the first framework
that leverages relative representations to construct a common space for the
embeddings of a source language PLM and the static word embeddings of a target
language. In this way, we can train the PLM on source-language training data
and perform zero-shot transfer to the target language by simply swapping the
embedding layer. However, through extensive experiments on two classification
datasets, we show that although our proposed framework is competitive with weak
baselines when addressing MoSECroT, it fails to achieve competitive results
compared with some strong baselines. In this paper, we attempt to explain this
negative result and provide several thoughts on possible improvement.

中文翻译:
基于Transformer的预训练语言模型（PLMs）在各种自然语言处理（NLP）任务中取得了显著成效。然而，此类模型的预训练过程需要消耗大量计算资源，目前几乎仅在高资源语言中得以实现。相比之下，静态词向量在计算资源和数据需求量方面更易于训练。本文提出MoSECroT（基于静态词向量的跨语言零样本迁移模型拼接），这项新颖且具有挑战性的任务尤其适用于已具备静态词向量资源的低资源语言。为应对该任务，我们首次提出利用相对表征构建共享空间的框架，将源语言PLM的词向量与目标语言的静态词向量映射至统一空间。通过这种方式，我们可以在源语言训练数据上训练PLM，仅需替换嵌入层即可实现向目标语言的零样本迁移。然而，通过在两个分类数据集上的大量实验表明：虽然我们提出的框架在处理MoSECroT任务时能与弱基线模型竞争，但相较于某些强基线模型仍无法取得优势结果。本文尝试对这一负面结果进行解释，并就可能的改进方向提出若干思考。

（翻译说明：
1. 专业术语处理："static word embeddings"统一译为"静态词向量"，"zero-shot transfer"译为"零样本迁移"符合NLP领域惯例
2. 被动语态转换：将英文被动式"can be trained"等转化为中文主动态"更易于训练"
3. 长句拆分：将原文复合长句拆分为符合中文表达习惯的短句，如框架描述部分
4. 概念显化："relative representations"译为"相对表征"并补充"映射至统一空间"以明确技术内涵
5. 学术用语规范："competitive with"译为"能与...竞争"保留原文比较级语义
6. 标题翻译策略：MoSECroT采用首译标注+括号说明的形式，兼顾术语统一性与读者理解）
