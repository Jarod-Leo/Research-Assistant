# Combining Aggregated Attention and Transformer Architecture for Accurate and Efficient Performance of Spiking Neural Networks

链接: http://arxiv.org/abs/2412.13553v1

原文摘要:
Spiking Neural Networks have attracted significant attention in recent years
due to their distinctive low-power characteristics. Meanwhile, Transformer
models, known for their powerful self-attention mechanisms and parallel
processing capabilities, have demonstrated exceptional performance across
various domains, including natural language processing and computer vision.
Despite the significant advantages of both SNNs and Transformers, directly
combining the low-power benefits of SNNs with the high performance of
Transformers remains challenging. Specifically, while the sparse computing mode
of SNNs contributes to reduced energy consumption, traditional attention
mechanisms depend on dense matrix computations and complex softmax operations.
This reliance poses significant challenges for effective execution in low-power
scenarios. Given the tremendous success of Transformers in deep learning, it is
a necessary step to explore the integration of SNNs and Transformers to harness
the strengths of both. In this paper, we propose a novel model architecture,
Spike Aggregation Transformer (SAFormer), that integrates the low-power
characteristics of SNNs with the high-performance advantages of Transformer
models. The core contribution of SAFormer lies in the design of the Spike
Aggregated Self-Attention (SASA) mechanism, which significantly simplifies the
computation process by calculating attention weights using only the spike
matrices query and key, thereby effectively reducing energy consumption.
Additionally, we introduce a Depthwise Convolution Module (DWC) to enhance the
feature extraction capabilities, further improving overall accuracy. We
evaluated and demonstrated that SAFormer outperforms state-of-the-art SNNs in
both accuracy and energy consumption, highlighting its significant advantages
in low-power and high-performance computing.

中文翻译:
近年来，脉冲神经网络（SNN）因其独特的低功耗特性受到广泛关注。与此同时，以强大自注意力机制和并行处理能力著称的Transformer模型，在自然语言处理、计算机视觉等多个领域展现出卓越性能。尽管SNN与Transformer各具优势，但如何将SNN的低功耗特性与Transformer的高性能直接结合仍面临挑战。具体而言，SNN的稀疏计算模式虽有助于降低能耗，但传统注意力机制依赖密集矩阵运算和复杂的softmax操作，这种特性对低功耗场景下的高效执行构成了重大障碍。鉴于Transformer在深度学习领域的巨大成功，探索SNN与Transformer的融合以兼取二者之长具有重要研究价值。

本文提出了一种新型模型架构——脉冲聚合Transformer（SAFormer），成功将SNN的低功耗特性与Transformer的高性能优势相结合。其核心创新在于设计了脉冲聚合自注意力机制（SASA），仅通过脉冲矩阵query和key计算注意力权重，大幅简化运算流程，从而有效降低能耗。此外，我们还引入深度卷积模块（DWC）以增强特征提取能力，进一步提升模型整体精度。实验证明，SAFormer在精度与能耗方面均优于当前最先进的SNN模型，充分展现了其在低功耗高性能计算领域的显著优势。
