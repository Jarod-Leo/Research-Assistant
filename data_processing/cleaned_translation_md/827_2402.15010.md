# How Important Is Tokenization in French Medical Masked Language Models?

链接: http://arxiv.org/abs/2402.15010v1

原文摘要:
Subword tokenization has become the prevailing standard in the field of
natural language processing (NLP) over recent years, primarily due to the
widespread utilization of pre-trained language models. This shift began with
Byte-Pair Encoding (BPE) and was later followed by the adoption of
SentencePiece and WordPiece. While subword tokenization consistently
outperforms character and word-level tokenization, the precise factors
contributing to its success remain unclear. Key aspects such as the optimal
segmentation granularity for diverse tasks and languages, the influence of data
sources on tokenizers, and the role of morphological information in
Indo-European languages remain insufficiently explored. This is particularly
pertinent for biomedical terminology, characterized by specific rules governing
morpheme combinations. Despite the agglutinative nature of biomedical
terminology, existing language models do not explicitly incorporate this
knowledge, leading to inconsistent tokenization strategies for common terms. In
this paper, we seek to delve into the complexities of subword tokenization in
French biomedical domain across a variety of NLP tasks and pinpoint areas where
further enhancements can be made. We analyze classical tokenization algorithms,
including BPE and SentencePiece, and introduce an original tokenization
strategy that integrates morpheme-enriched word segmentation into existing
tokenization methods.

中文翻译:
近年来，子词切分（subword tokenization）凭借预训练语言模型的广泛应用，已成为自然语言处理领域的主流标准。这一变革始于字节对编码（BPE），随后又衍生出SentencePiece和WordPiece等方案。尽管子词切分的表现始终优于字符级和词级切分，但其成功的确切因素仍不明确。诸如不同任务和语言的最佳切分粒度、数据源对分词器的影响、印欧语系中形态学信息的作用等关键问题，目前仍缺乏深入探究。这一现象在生物医学术语领域尤为突出——该领域的术语构词遵循特定的语素组合规则。尽管生物医学术语具有粘着特性，现有语言模型却未显式融入这类知识，导致常见术语的切分策略存在不一致性。本文旨在深入探究法语生物医学领域子词切分在各种自然语言处理任务中的复杂性，并指出有待改进的方向。我们分析了BPE和SentencePiece等经典分词算法，并提出了一种创新策略：将融合语素知识的词汇切分方法整合至现有分词体系中。
