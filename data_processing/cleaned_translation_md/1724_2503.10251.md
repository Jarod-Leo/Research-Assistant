# Numerical Error Analysis of Large Language Models

链接: http://arxiv.org/abs/2503.10251v1

原文摘要:
Large language models based on transformer architectures have become integral
to state-of-the-art natural language processing applications. However, their
training remains computationally expensive and exhibits instabilities, some of
which are expected to be caused by finite-precision computations. We provide a
theoretical analysis of the impact of round-off errors within the forward pass
of a transformer architecture which yields fundamental bounds for these
effects. In addition, we conduct a series of numerical experiments which
demonstrate the practical relevance of our bounds. Our results yield concrete
guidelines for choosing hyperparameters that mitigate round-off errors, leading
to more robust and stable inference.

中文翻译:
基于Transformer架构的大规模语言模型已成为当前自然语言处理应用的核心组件。然而，这类模型的训练过程仍存在计算成本高昂和训练不稳定的问题，其中部分不稳定性可归因于有限精度计算带来的影响。本研究通过理论分析揭示了Transformer前向传播过程中舍入误差的作用机制，并推导出这些误差影响的基础性边界条件。此外，我们通过一系列数值实验验证了所提出边界条件的实际相关性。研究结果最终形成了可缓解舍入误差的超参数选择准则，为提升模型推理过程的鲁棒性和稳定性提供了具体指导。

（翻译说明：
1. 专业术语处理："transformer architectures"保留技术特征译为"Transformer架构"，"finite-precision computations"译为专业术语"有限精度计算"
2. 长句拆分：将原文复合句按中文表达习惯拆分为多个短句，如将理论分析部分拆分为"揭示机制"和"推导边界"两个层次
3. 被动语态转换：将"are expected to be caused by"等被动结构转换为"可归因于"的主动表述
4. 概念显化："yields fundamental bounds"译为"推导出基础性边界条件"以突出学术严谨性
5. 动态对等："concrete guidelines"译为"具体指导"而非字面直译，更符合中文技术文献表述习惯
6. 逻辑衔接：通过"此外""最终"等连接词保持论文摘要的论证逻辑链）
