# Extracting Paragraphs from LLM Token Activations

链接: http://arxiv.org/abs/2409.06328v1

原文摘要:
Generative large language models (LLMs) excel in natural language processing
tasks, yet their inner workings remain underexplored beyond token-level
predictions. This study investigates the degree to which these models decide
the content of a paragraph at its onset, shedding light on their contextual
understanding. By examining the information encoded in single-token
activations, specifically the "\textbackslash n\textbackslash n" double newline
token, we demonstrate that patching these activations can transfer significant
information about the context of the following paragraph, providing further
insights into the model's capacity to plan ahead.

中文翻译:
生成式大型语言模型（LLMs）在自然语言处理任务中表现卓越，但除词汇层面的预测外，其内部工作机制仍缺乏深入探索。本研究通过探究这些模型在段落起始阶段对内容的确立程度，揭示其上下文理解能力。通过分析单标记激活（特别是"\n\n"双换行符标记）中编码的信息，我们证明修补这些激活可以传递关于后续段落语境的重要信息，从而进一步阐明模型的前瞻性规划能力。

（翻译说明：
1. 专业术语处理："Generative large language models"译为"生成式大型语言模型"，"token-level predictions"译为"词汇层面预测"，均采用计算机领域通用译法
2. 被动语态转换：将"their inner workings remain underexplored"主动化为"缺乏深入探索"，符合中文表达习惯
3. 长句拆分：将原文复合句拆分为多个短句，如把"shedding light..."独立成短句"揭示其..."
4. 符号处理：保留技术符号"\n\n"的原始形式，并添加中文说明"双换行符标记"
5. 概念一致性："plan ahead"统一译为"前瞻性规划"，与认知科学术语保持一致）
