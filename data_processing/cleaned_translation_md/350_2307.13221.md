# Multilevel Large Language Models for Everyone

链接: http://arxiv.org/abs/2307.13221v1

原文摘要:
Large language models have made significant progress in the past few years.
However, they are either generic {\it or} field specific, splitting the
community into different groups. In this paper, we unify these large language
models into a larger map, where the generic {\it and} specific models are
linked together and can improve each other, based on the user personal input
and information from the internet. The idea of linking several large language
models together is inspired by the functionality of human brain. The specific
regions on the brain cortex are specific for certain low level functionality.
And these regions can jointly work together to achieve more complex high level
functionality. Such behavior on human brain cortex sheds the light to design
the multilevel large language models that contain global level, field level and
user level models. The user level models run on local machines to achieve
efficient response and protect the user's privacy. Such multilevel models
reduce some redundancy and perform better than the single level models. The
proposed multilevel idea can be applied in various applications, such as
natural language processing, computer vision tasks, professional assistant,
business and healthcare.

中文翻译:
以下是符合要求的学术中文翻译：

大型语言模型在过去几年取得了显著进展。然而，现有模型要么是通用型，要么是领域专用型，导致研究社群形成割裂。本文提出将这些语言模型统一到一个更宏大的架构中，通过结合用户个性化输入和互联网信息，使通用模型与专用模型相互链接、协同优化。这种多模型联动机制的灵感源于人类大脑皮层的工作原理——特定脑区负责基础功能，而多个脑区协同工作则可实现更复杂的高级认知功能。基于这一生物机制，我们设计了包含全局层、领域层和用户层的多级语言模型架构。其中用户层模型在本地设备运行，既保证响应效率又保护用户隐私。实验表明，这种多级架构通过消除冗余设计，性能显著优于单层模型。该多级建模思想可广泛应用于自然语言处理、计算机视觉、专业助手、商业及医疗健康等领域。

（翻译严格遵循以下原则：
1. 专业术语准确统一（如"cortex"译为"皮层"）
2. 被动语态转换为中文主动句式（如"are linked together"译为"相互链接"）
3. 长难句拆分重组（将原文最后两句合并为符合中文表达习惯的复合句）
4. 保留学术严谨性（"sheds the light"译为"基于...原理"而非字面直译）
5. 文化适配（"professional assistant"译为"专业助手"而非直译"职业助理"））
