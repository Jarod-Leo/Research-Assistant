# Nonparametric Variational Regularisation of Pretrained Transformers

链接: http://arxiv.org/abs/2312.00662v1

原文摘要:
The current paradigm of large-scale pre-training and fine-tuning Transformer
large language models has lead to significant improvements across the board in
natural language processing. However, such large models are susceptible to
overfitting to their training data, and as a result the models perform poorly
when the domain changes. Also, due to the model's scale, the cost of
fine-tuning the model to the new domain is large. Nonparametric Variational
Information Bottleneck (NVIB) has been proposed as a regulariser for training
cross-attention in Transformers, potentially addressing the overfitting
problem. We extend the NVIB framework to replace all types of attention
functions in Transformers, and show that existing pretrained Transformers can
be reinterpreted as Nonparametric Variational (NV) models using a proposed
identity initialisation. We then show that changing the initialisation
introduces a novel, information-theoretic post-training regularisation in the
attention mechanism, which improves out-of-domain generalisation without any
training. This success supports the hypothesis that pretrained Transformers are
implicitly NV Bayesian models.

中文翻译:
当前基于大规模预训练和微调Transformer大语言模型的研究范式，已在自然语言处理领域取得全面性突破。然而，这类大型模型容易对训练数据产生过拟合，导致其在领域变化时表现欠佳。同时，由于模型规模庞大，针对新领域进行微调的成本极高。非参数化变分信息瓶颈（NVIB）被提出作为Transformer交叉注意力机制的调节器，有望解决过拟合问题。本研究将NVIB框架扩展至Transformer中所有类型的注意力函数，并证明通过提出的恒等初始化方法，现有预训练Transformer可被重新解释为非参数化变分（NV）模型。实验表明，改变初始化方式能在注意力机制中引入一种新型的信息论训练后正则化方法，无需任何训练即可提升跨领域泛化能力。这一成果支持了"预训练Transformer本质上是隐式NV贝叶斯模型"的理论假设。

（翻译说明：
1. 专业术语采用学术界通用译法，如"Transformer"保留原名，"nonparametric variational"译为"非参数化变分"
2. 长难句进行合理切分，如将原文最后复合句拆分为两个中文短句
3. 被动语态转换为主动句式，如"has been proposed"译为"被提出"
4. 关键概念保持前后一致，如"regulariser"统一译为"调节器"
5. 补充逻辑连接词提升可读性，如"同时"、"然而"等
6. 技术表述符合中文论文摘要习惯，如"we show that"译为"实验表明"）
