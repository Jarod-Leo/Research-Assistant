# How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias

链接: http://arxiv.org/abs/2505.00926v1

原文摘要:
Language recognition tasks are fundamental in natural language processing
(NLP) and have been widely used to benchmark the performance of large language
models (LLMs). These tasks also play a crucial role in explaining the working
mechanisms of transformers. In this work, we focus on two representative tasks
in the category of regular language recognition, known as `even pairs' and
`parity check', the aim of which is to determine whether the occurrences of
certain subsequences in a given sequence are even. Our goal is to explore how a
one-layer transformer, consisting of an attention layer followed by a linear
layer, learns to solve these tasks by theoretically analyzing its training
dynamics under gradient descent. While even pairs can be solved directly by a
one-layer transformer, parity check need to be solved by integrating
Chain-of-Thought (CoT), either into the inference stage of a transformer
well-trained for the even pairs task, or into the training of a one-layer
transformer. For both problems, our analysis shows that the joint training of
attention and linear layers exhibits two distinct phases. In the first phase,
the attention layer grows rapidly, mapping data sequences into separable
vectors. In the second phase, the attention layer becomes stable, while the
linear layer grows logarithmically and approaches in direction to a max-margin
hyperplane that correctly separates the attention layer outputs into positive
and negative samples, and the loss decreases at a rate of $O(1/t)$. Our
experiments validate those theoretical results.

中文翻译:
语言识别任务是自然语言处理（NLP）领域的基础课题，常被用于评估大语言模型（LLMs）的性能表现，同时对解释Transformer模型的工作机制具有关键作用。本研究聚焦于正则语言识别中的两个代表性任务——"偶数对"与"奇偶校验"，其核心目标是判断给定序列中特定子序列的出现次数是否为偶数。我们旨在通过理论分析单层Transformer（由注意力层与线性层构成）在梯度下降训练过程中的动态特性，探究其学习解决这些任务的机制。研究发现：虽然"偶数对"任务可直接由单层Transformer解决，但"奇偶校验"任务需要引入思维链（CoT）机制——要么将其集成至已完成"偶数对"训练的Transformer推理阶段，要么在单层Transformer训练过程中直接融合CoT。理论分析表明，两个任务的注意力层与线性层联合训练均呈现两个显著阶段：第一阶段注意力层快速生长，将数据序列映射为可分离向量；第二阶段注意力层趋于稳定，而线性层呈对数级增长并逐渐逼近最大间隔超平面方向，从而将注意力层输出正确划分为正负样本，此时损失函数以$O(1/t)$速率下降。实验数据验证了上述理论结论。
