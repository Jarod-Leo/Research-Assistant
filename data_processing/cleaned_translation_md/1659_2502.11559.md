# Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models

链接: http://arxiv.org/abs/2502.11559v1

原文摘要:
Pre-training large language models (LLMs) on vast text corpora enhances
natural language processing capabilities but risks encoding social biases,
particularly gender bias. While parameter-modification methods like fine-tuning
mitigate bias, they are resource-intensive, unsuitable for closed-source
models, and lack adaptability to evolving societal norms. Instruction-based
approaches offer flexibility but often compromise task performance. To address
these limitations, we propose $\textit{FaIRMaker}$, an automated and
model-independent framework that employs an $\textbf{auto-search and
refinement}$ paradigm to adaptively generate Fairwords, which act as
instructions integrated into input queries to reduce gender bias and enhance
response quality. Extensive experiments demonstrate that $\textit{FaIRMaker}$
automatically searches for and dynamically refines Fairwords, effectively
mitigating gender bias while preserving task integrity and ensuring
compatibility with both API-based and open-source LLMs.

中文翻译:
以下是对英文论文摘要的中文翻译：

【摘要】  
在大规模文本语料库上预训练大语言模型（LLM）虽能增强自然语言处理能力，但也存在编码社会偏见（尤其是性别偏见）的风险。现有参数修改方法（如微调）虽可缓解偏见，但存在资源消耗大、不适用于闭源模型、难以适应动态社会规范等局限；基于指令的方法虽具灵活性，却常以牺牲任务性能为代价。为此，我们提出$\textit{FaIRMaker}$——一种自动化且与模型无关的框架，采用$\textbf{自动搜索与优化}$范式，自适应生成"公平词"（Fairwords）作为指令嵌入输入查询，从而在降低性别偏见的同时提升响应质量。大量实验表明，$\textit{FaIRMaker}$能自动搜索并动态优化公平词，在保持任务完整性的前提下有效缓解性别偏见，且兼容基于API的LLM与开源模型。


