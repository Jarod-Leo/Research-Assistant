# Multilingual Prompts in LLM-Based Recommenders: Performance Across Languages

链接: http://arxiv.org/abs/2409.07604v1

原文摘要:
Large language models (LLMs) are increasingly used in natural language
processing tasks. Recommender systems traditionally use methods such as
collaborative filtering and matrix factorization, as well as advanced
techniques like deep learning and reinforcement learning. Although language
models have been applied in recommendation, the recent trend have focused on
leveraging the generative capabilities of LLMs for more personalized
suggestions. While current research focuses on English due to its resource
richness, this work explores the impact of non-English prompts on
recommendation performance. Using OpenP5, a platform for developing and
evaluating LLM-based recommendations, we expanded its English prompt templates
to include Spanish and Turkish. Evaluation on three real-world datasets, namely
ML1M, LastFM, and Amazon-Beauty, showed that usage of non-English prompts
generally reduce performance, especially in less-resourced languages like
Turkish. We also retrained an LLM-based recommender model with multilingual
prompts to analyze performance variations. Retraining with multilingual prompts
resulted in more balanced performance across languages, but slightly reduced
English performance. This work highlights the need for diverse language support
in LLM-based recommenders and suggests future research on creating evaluation
datasets, using newer models and additional languages.

中文翻译:
以下是符合要求的学术中文翻译：

大语言模型（LLMs）在自然语言处理任务中的应用日益广泛。传统推荐系统通常采用协同过滤、矩阵分解等方法，以及深度学习和强化学习等先进技术。尽管语言模型已在推荐领域有所应用，但当前研究趋势更侧重于利用LLMs的生成能力来提供更个性化的推荐。鉴于英语资源的丰富性，现有研究多聚焦于英语场景，而本研究则探究了非英语提示对推荐性能的影响。

基于OpenP5（一个用于开发和评估LLM推荐系统的平台），我们将其英文提示模板扩展至西班牙语和土耳其语。在ML1M、LastFM和Amazon-Beauty三个真实数据集上的评估表明：使用非英语提示通常会导致性能下降，尤其在土耳其语等资源较少的语言中更为明显。我们还通过多语言提示重训练了基于LLM的推荐模型以分析性能变化。实验发现，多语言提示重训练虽能实现跨语言性能的均衡化，但会轻微降低英语场景的表现。

本研究揭示了LLM推荐系统需要多样化语言支持的重要性，并建议未来研究应着眼于：构建多语言评估数据集、采用更新颖的模型架构以及扩展更多语言场景。
