# Controlled Low-Rank Adaptation with Subspace Regularization for Continued Training on Large Language Models

链接: http://arxiv.org/abs/2410.16801v1

原文摘要:
Large language models (LLMs) exhibit remarkable capabilities in natural
language processing but face catastrophic forgetting when learning new tasks,
where adaptation to a new domain leads to a substantial decline in performance
on previous tasks. In this paper, we propose Controlled LoRA (CLoRA), a
sub-space regularization method on LoRA structure. Aiming to reduce the scale
of output change while introduce minimal constraint on model capacity, CLoRA
imposes constraint on the direction of updating matrix's null space.
Experimental results on one-stage LLM finetuning tasks and continual learning
settings highlight the superority of CLoRA as a effective parameter efficient
finetuning method with catastrophic forgetting mitigating.Further investigation
for model parameters indicates that CLoRA effectively balances the trade-off
between model capacity and degree of forgetting.

中文翻译:
以下是符合学术规范的中文翻译：

大语言模型（LLMs）在自然语言处理方面展现出卓越能力，但在学习新任务时面临灾难性遗忘问题——当模型适应新领域时，其在先前任务上的性能会显著下降。本文提出受控低秩自适应（CLoRA），一种基于LoRA结构的子空间正则化方法。该方法通过在更新矩阵的零空间方向上施加约束，旨在实现输出变化规模最小化的同时，对模型能力施加极低限制。在单阶段大语言模型微调任务和持续学习场景下的实验结果表明，CLoRA作为一种参数高效的微调方法，在缓解灾难性遗忘方面具有显著优势。对模型参数的进一步分析表明，CLoRA能有效平衡模型能力与遗忘程度之间的权衡关系。

（翻译说明：
1. 专业术语处理：LLMs统一译为"大语言模型"，LoRA采用技术界通用译名"低秩自适应"
2. 被动语态转换："are faced with"转为主动式"面临"
3. 长句拆分：将原文复合句按中文习惯分解为多个短句
4. 概念显化："one-stage"补充说明为"单阶段"
5. 学术表达："highlight the superiority"译为"具有显著优势"而非字面直译
6. 术语一致性："model capacity"全文统一译为"模型能力"）
