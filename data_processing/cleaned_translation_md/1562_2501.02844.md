# Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text Classification

链接: http://arxiv.org/abs/2501.02844v1

原文摘要:
Text classification is a fundamental task in data mining, pivotal to various
applications such as tabular understanding and recommendation. Although neural
network-based models, such as CNN and BERT, have demonstrated remarkable
performance in text classification, their effectiveness heavily relies on
abundant labeled training data. This dependency makes these models less
effective in dynamic few-shot text classification, where labeled data is
scarce, and new target labels frequently appear based on application needs.
Recently, large language models (LLMs) have shown promise due to their
extensive pretraining and contextual understanding ability. Current approaches
provide LLMs with text inputs, candidate labels, and additional side
information (e.g., descriptions) to classify texts. However, their
effectiveness is hindered by the increased input size and the noise introduced
through side information processing. To address these limitations, we propose a
graph-based online retrieval-augmented generation framework, namely GORAG, for
dynamic few-shot text classification. Rather than treating each input
independently, GORAG constructs and maintains a weighted graph by extracting
side information across all target texts. In this graph, text keywords and
labels are represented as nodes, with edges indicating the correlations between
them. To model these correlations, GORAG employs an edge weighting mechanism to
prioritize the importance and reliability of extracted information and
dynamically retrieves relevant context using a minimum-cost spanning tree
tailored for each text input. Empirical evaluations demonstrate that GORAG
outperforms existing approaches by providing more comprehensive and precise
contextual information.

中文翻译:
文本分类是数据挖掘中的基础任务，对表格理解和推荐系统等应用至关重要。尽管基于神经网络的模型（如CNN和BERT）在文本分类中表现出色，但其效果高度依赖大量标注训练数据。这种依赖性使得这些模型在动态少样本文本分类场景中表现欠佳——此类场景标注数据稀缺，且需根据应用需求频繁新增目标标签。近年来，大语言模型（LLMs）凭借其广泛的预训练和上下文理解能力展现出优势。现有方法通过向LLMs提供文本输入、候选标签及附加信息（如描述）来实现分类，但其效果受限于输入规模的扩大和附加信息处理引入的噪声。为突破这些限制，我们提出基于图结构的在线检索增强生成框架GORAG，专为动态少样本文本分类设计。不同于独立处理每个输入，GORAG通过提取所有目标文本的附加信息构建并维护加权图：其中文本关键词和标签作为节点，边表示其关联关系。为建模这些关联，GORAG采用边权重机制评估提取信息的重要性和可靠性，并基于为每个文本定制的最小生成树动态检索相关上下文。实验证明，GORAG通过提供更全面精准的上下文信息，性能优于现有方法。

（翻译说明：采用技术文献的简洁风格，处理长句时通过破折号和冒号保持原文逻辑层次；"dynamic few-shot"译为"动态少样本"符合领域术语；"weighted graph"等专业表述保留原意；通过"构建并维护"等四字结构提升中文节奏感；最后一句"Empirical evaluations"译为"实验证明"符合中文论文表述习惯。）
