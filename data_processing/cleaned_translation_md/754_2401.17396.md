# Fine-tuning Transformer-based Encoder for Turkish Language Understanding Tasks

链接: http://arxiv.org/abs/2401.17396v1

原文摘要:
Deep learning-based and lately Transformer-based language models have been
dominating the studies of natural language processing in the last years. Thanks
to their accurate and fast fine-tuning characteristics, they have outperformed
traditional machine learning-based approaches and achieved state-of-the-art
results for many challenging natural language understanding (NLU) problems.
Recent studies showed that the Transformer-based models such as BERT, which is
Bidirectional Encoder Representations from Transformers, have reached
impressive achievements on many tasks. Moreover, thanks to their transfer
learning capacity, these architectures allow us to transfer pre-built models
and fine-tune them to specific NLU tasks such as question answering. In this
study, we provide a Transformer-based model and a baseline benchmark for the
Turkish Language. We successfully fine-tuned a Turkish BERT model, namely
BERTurk that is trained with base settings, to many downstream tasks and
evaluated with a the Turkish Benchmark dataset. We showed that our studies
significantly outperformed other existing baseline approaches for Named-Entity
Recognition, Sentiment Analysis, Question Answering and Text Classification in
Turkish Language. We publicly released these four fine-tuned models and
resources in reproducibility and with the view of supporting other Turkish
researchers and applications.

中文翻译:
基于深度学习及近期基于Transformer的语言模型在过去几年中主导了自然语言处理领域的研究。凭借其精准高效的微调特性，这些模型不仅超越了传统机器学习方法，更在众多具有挑战性的自然语言理解（NLU）任务中取得了最先进的成果。研究表明，基于Transformer的模型（如BERT——来自Transformer的双向编码器表示）已在多项任务中取得显著成就。此外，得益于其迁移学习能力，此类架构支持将预训练模型迁移并微调至特定NLU任务（如问答系统）。

本研究针对土耳其语提出了一个基于Transformer的模型及基准测试体系。我们成功将土耳其语BERT模型（采用基础配置训练的BERTurk）微调适配至多个下游任务，并使用土耳其语基准数据集进行评估。实验证明，该模型在土耳其语的命名实体识别、情感分析、问答系统和文本分类任务中显著优于现有基线方法。为促进研究可复现性并支持土耳其语相关研究与应用，我们已公开释放这四种微调模型及配套资源。
