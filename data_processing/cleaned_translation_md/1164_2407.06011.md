# Igea: a Decoder-Only Language Model for Biomedical Text Generation in Italian

链接: http://arxiv.org/abs/2407.06011v1

原文摘要:
The development of domain-specific language models has significantly advanced
natural language processing applications in various specialized fields,
particularly in biomedicine. However, the focus has largely been on
English-language models, leaving a gap for less-resourced languages such as
Italian. This paper introduces Igea, the first decoder-only language model
designed explicitly for biomedical text generation in Italian. Built on the
Minerva model and continually pretrained on a diverse corpus of Italian medical
texts, Igea is available in three model sizes: 350 million, 1 billion, and 3
billion parameters. The models aim to balance computational efficiency and
performance, addressing the challenges of managing the peculiarities of medical
terminology in Italian. We evaluate Igea using a mix of in-domain biomedical
corpora and general-purpose benchmarks, highlighting its efficacy and retention
of general knowledge even after the domain-specific training. This paper
discusses the model's development and evaluation, providing a foundation for
future advancements in Italian biomedical NLP.

中文翻译:
领域专用语言模型的发展显著推动了自然语言处理在各专业领域的应用，尤其在生物医学领域表现突出。然而现有研究主要集中于英语模型，导致意大利语等资源相对匮乏的语言存在明显空白。本文首次提出Igea——专为意大利语生物医学文本生成设计的纯解码器语言模型。该模型基于Minerva架构，通过持续预训练整合了多样化的意大利医学文本语料，提供3.5亿、10亿和30亿参数三种规格，在计算效率与性能之间实现平衡，有效应对意大利医学术语的特殊性挑战。我们采用生物医学领域语料库与通用基准相结合的评估方式，证明该模型在保持通用知识的同时展现出优异的专业性能。本文详细阐述模型的开发与评估过程，为意大利语生物医学自然语言处理的后续发展奠定基础。

（翻译说明：采用学术论文摘要的标准表述方式，通过以下处理确保专业性与可读性：
1. 专业术语准确对应："decoder-only"译为"纯解码器"，"continually pretrained"译为"持续预训练"
2. 长句拆分重组：将原文复合句按中文表达习惯分解为多个短句，如参数规格部分单独成句
3. 逻辑显化：添加"有效应对"等连接词强化论证逻辑
4. 被动语态转化："is evaluated"转为主动式"我们采用...评估方式"
5. 文化适配："less-resourced languages"译为"资源相对匮乏的语言"更符合中文表达）
