# A Survey of Vision Transformers in Autonomous Driving: Current Trends and Future Directions

链接: http://arxiv.org/abs/2403.07542v1

原文摘要:
This survey explores the adaptation of visual transformer models in
Autonomous Driving, a transition inspired by their success in Natural Language
Processing. Surpassing traditional Recurrent Neural Networks in tasks like
sequential image processing and outperforming Convolutional Neural Networks in
global context capture, as evidenced in complex scene recognition, Transformers
are gaining traction in computer vision. These capabilities are crucial in
Autonomous Driving for real-time, dynamic visual scene processing. Our survey
provides a comprehensive overview of Vision Transformer applications in
Autonomous Driving, focusing on foundational concepts such as self-attention,
multi-head attention, and encoder-decoder architecture. We cover applications
in object detection, segmentation, pedestrian detection, lane detection, and
more, comparing their architectural merits and limitations. The survey
concludes with future research directions, highlighting the growing role of
Vision Transformers in Autonomous Driving.

中文翻译:
本综述探讨了视觉Transformer模型在自动驾驶领域的适应性应用，这一转变源于其在自然语言处理中的成功实践。研究显示，Transformer在序列图像处理任务中超越了传统循环神经网络，在复杂场景识别等需要全局上下文捕捉的任务中表现优于卷积神经网络，因而在计算机视觉领域日益受到重视。这些特性对于自动驾驶实时动态视觉场景处理至关重要。本文全面综述了视觉Transformer在自动驾驶中的应用，重点解析自注意力机制、多头注意力及编码器-解码器架构等核心概念。研究涵盖目标检测、图像分割、行人识别、车道检测等具体应用场景，对比分析了不同架构的优势与局限。最后展望未来研究方向，强调视觉Transformer在自动驾驶领域日益重要的作用。

（翻译说明：采用学术文献的规范表达，保持术语一致性；将英语长句合理切分为符合中文阅读习惯的短句；"self-attention"等专业术语采用业界通用译法；通过"源于"、"显示"等措辞实现英文被动语态的主动化转换；使用"日益"等程度副词准确传达"growing"的渐进语义）
