# Improving Natural Language Inference in Arabic using Transformer Models and Linguistically Informed Pre-Training

链接: http://arxiv.org/abs/2307.14666v1

原文摘要:
This paper addresses the classification of Arabic text data in the field of
Natural Language Processing (NLP), with a particular focus on Natural Language
Inference (NLI) and Contradiction Detection (CD). Arabic is considered a
resource-poor language, meaning that there are few data sets available, which
leads to limited availability of NLP methods. To overcome this limitation, we
create a dedicated data set from publicly available resources. Subsequently,
transformer-based machine learning models are being trained and evaluated. We
find that a language-specific model (AraBERT) performs competitively with
state-of-the-art multilingual approaches, when we apply linguistically informed
pre-training methods such as Named Entity Recognition (NER). To our knowledge,
this is the first large-scale evaluation for this task in Arabic, as well as
the first application of multi-task pre-training in this context.

中文翻译:
本文针对自然语言处理（NLP）领域的阿拉伯语文本分类问题展开研究，重点关注自然语言推理（NLI）和矛盾检测（CD）任务。阿拉伯语被视为资源稀缺型语言，现有数据集稀少，导致相关NLP方法发展受限。为突破这一限制，我们基于公开资源构建了专用数据集，随后对基于Transformer架构的机器学习模型进行训练与评估。研究发现，当采用命名实体识别（NER）等语言学知识增强的预训练方法时，专用阿拉伯语模型（AraBERT）的表现足以媲美最先进的多语言模型。据我们所知，这是阿拉伯语领域首次针对该任务开展的大规模评估，也是多任务预训练方法在此背景下的首次应用。
