# Dynamic data sampler for cross-language transfer learning in large language models

链接: http://arxiv.org/abs/2405.10626v1

原文摘要:
Large Language Models (LLMs) have gained significant attention in the field
of natural language processing (NLP) due to their wide range of applications.
However, training LLMs for languages other than English poses significant
challenges, due to the difficulty in acquiring large-scale corpus and the
requisite computing resources. In this paper, we propose ChatFlow, a
cross-language transfer-based LLM, to address these challenges and train large
Chinese language models in a cost-effective manner. We employ a mix of Chinese,
English, and parallel corpus to continuously train the LLaMA2 model, aiming to
align cross-language representations and facilitate the knowledge transfer
specifically to the Chinese language model. In addition, we use a dynamic data
sampler to progressively transition the model from unsupervised pre-training to
supervised fine-tuning. Experimental results demonstrate that our approach
accelerates model convergence and achieves superior performance. We evaluate
ChatFlow on popular Chinese and English benchmarks, the results indicate that
it outperforms other Chinese models post-trained on LLaMA-2-7B.

中文翻译:
以下是符合学术规范的中文翻译：

大型语言模型（LLMs）因其广泛的应用价值在自然语言处理（NLP）领域获得显著关注。然而针对非英语语种训练LLMs存在重大挑战，主要源于大规模语料获取困难与所需计算资源庞大。本文提出ChatFlow——一种基于跨语言迁移的LLM解决方案，以经济高效的方式训练中文大语言模型。我们采用中英文混合语料与平行语料对LLaMA2模型进行持续训练，旨在实现跨语言表征对齐并促进向中文模型的知识迁移。此外，通过动态数据采样器实现模型从无监督预训练到有监督微调的渐进式过渡。实验结果表明，该方法能加速模型收敛并获得更优性能。我们在主流中英文基准测试上评估ChatFlow，结果显示其性能优于其他基于LLaMA-2-7B进行后训练的中文模型。

（翻译说明：
1. 专业术语统一处理：LLMs/Large Language Models统一译为"大型语言模型"并首次出现标注英文缩写
2. 被动语态转化："are trained"等被动结构转换为中文主动表达
3. 长句拆分：将原文复合句按中文表达习惯分解为多个短句
4. 概念准确传达："cross-language transfer-based"译为"基于跨语言迁移的"保持学术精确性
5. 技术表述规范："unsupervised pre-training/supervised fine-tuning"对应标准译法"无监督预训练/有监督微调"
6. 数据呈现方式：保留原始模型名称"LLaMA-2-7B"等技术参数不变）
