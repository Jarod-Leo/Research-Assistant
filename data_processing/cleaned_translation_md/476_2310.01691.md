# Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across Language Models

链接: http://arxiv.org/abs/2310.01691v1

原文摘要:
Prompt tuning in natural language processing (NLP) has become an increasingly
popular method for adapting large language models to specific tasks. However,
the transferability of these prompts, especially continuous prompts, between
different models remains a challenge. In this work, we propose a zero-shot
continuous prompt transfer method, where source prompts are encoded into
relative space and the corresponding target prompts are searched for
transferring to target models. Experimental results confirm the effectiveness
of our method, showing that 'task semantics' in continuous prompts can be
generalized across various language models. Moreover, we find that combining
'task semantics' from multiple source models can further enhance the
generalizability of transfer.

中文翻译:
自然语言处理（NLP）中的提示调优技术已成为适配大语言模型至特定任务的日益流行的方法。然而，这些提示（尤其是连续型提示）在不同模型间的可迁移性仍存在挑战。本研究提出一种零样本连续提示迁移方法：通过将源提示编码至相对空间，并搜索对应目标提示以实现跨模型迁移。实验结果证实了该方法的有效性，表明连续提示中的"任务语义"可在不同语言模型间实现泛化。此外，我们发现整合多个源模型的"任务语义"能进一步提升迁移的泛化能力。

（翻译说明：
1. 专业术语处理："prompt tuning"译为"提示调优"，"continuous prompts"译为"连续型提示"，保持学术规范性
2. 被动语态转换：将英文被动结构"are encoded"等转换为中文主动式表达
3. 概念一致性："task semantics"统一译为"任务语义"并保留引号强调
4. 长句拆分：将原文复合句按中文表达习惯分解为短句
5. 术语补充：在"零样本"后添加"连续提示迁移方法"使概念更完整
6. 逻辑显化：通过冒号和引号等标点增强技术表述的清晰度）
