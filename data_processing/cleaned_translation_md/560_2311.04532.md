# Evaluating Diverse Large Language Models for Automatic and General Bug Reproduction

链接: http://arxiv.org/abs/2311.04532v2

原文摘要:
Bug reproduction is a critical developer activity that is also challenging to
automate, as bug reports are often in natural language and thus can be
difficult to transform to test cases consistently. As a result, existing
techniques mostly focused on crash bugs, which are easier to automatically
detect and verify. In this work, we overcome this limitation by using large
language models (LLMs), which have been demonstrated to be adept at natural
language processing and code generation. By prompting LLMs to generate
bug-reproducing tests, and via a post-processing pipeline to automatically
identify promising generated tests, our proposed technique LIBRO could
successfully reproduce about one-third of all bugs in the widely used Defects4J
benchmark. Furthermore, our extensive evaluation on 15 LLMs, including 11
open-source LLMs, suggests that open-source LLMs also demonstrate substantial
potential, with the StarCoder LLM achieving 70% of the reproduction performance
of the closed-source OpenAI LLM code-davinci-002 on the large Defects4J
benchmark, and 90% of performance on a held-out bug dataset likely not part of
any LLM's training data. In addition, our experiments on LLMs of different
sizes show that bug reproduction using LIBRO improves as LLM size increases,
providing information as to which LLMs can be used with the LIBRO pipeline.

中文翻译:
以下是符合要求的学术化中文翻译：

错误复现是开发者的一项重要活动，但由于错误报告通常采用自然语言描述，难以稳定地转化为测试用例，该过程的自动化一直面临挑战。现有技术大多局限于崩溃型错误，这类错误更易被自动检测和验证。本研究通过利用大型语言模型（LLM）突破了这一局限——LLM已被证明在自然语言处理和代码生成方面具有卓越能力。我们提出的LIBRO技术通过提示LLM生成错误复现测试用例，并采用后处理流程自动筛选有效测试，在广泛使用的Defects4J基准测试中成功复现了约三分之一的所有错误。

我们对15个LLM（含11个开源模型）的深入评估表明：开源LLM同样展现出显著潜力，StarCoder模型在Defects4J基准上的复现性能达到闭源模型code-davinci-002的70%，在未参与任何LLM训练数据的保留错误数据集上更达到90%的性能。此外，针对不同规模LLM的实验显示：随着模型规模增大，LIBRO的错误复现效果持续提升，这为选择适用于LIBRO流程的LLM提供了参考依据。


