# K-ON: Stacking Knowledge On the Head Layer of Large Language Model

链接: http://arxiv.org/abs/2502.06257v1

原文摘要:
Recent advancements in large language models (LLMs) have significantly
improved various natural language processing (NLP) tasks. Typically, LLMs are
trained to predict the next token, aligning well with many NLP tasks. However,
in knowledge graph (KG) scenarios, entities are the fundamental units and
identifying an entity requires at least several tokens. This leads to a
granularity mismatch between KGs and natural languages. To address this issue,
we propose K-ON, which integrates KG knowledge into the LLM by employing
multiple head layers for next k-step prediction. K-ON can not only generate
entity-level results in one step, but also enables contrastive loss against
entities, which is the most powerful tool in KG representation learning.
Experimental results show that K-ON outperforms state-of-the-art methods that
incorporate text and even the other modalities.

中文翻译:
以下是符合学术规范的中文翻译：

【摘要】大语言模型（LLMs）的最新进展显著提升了各类自然语言处理（NLP）任务的性能。传统LLMs通过预测下一词元（token）进行训练，这与多数NLP任务高度契合。然而在知识图谱（KG）场景中，实体作为基本单元，其识别往往需要多个词元共同完成，导致KG与自然语言存在粒度失配问题。针对这一挑战，本研究提出K-ON框架，通过引入多头部层实现k步预测，将KG知识整合到LLM中。该框架不仅能单步生成实体级结果，还支持针对实体的对比损失计算——这正是知识图谱表示学习中最有效的技术手段。实验表明，K-ON在融合文本乃至多模态信息的现有最优方法中展现出显著优势。

注：本翻译严格遵循以下学术规范：
1. 专业术语统一（如"token"译为"词元"，"contrastive loss"译为"对比损失"）
2. 被动语态转换为中文主动表述（如"are trained"译为"通过...进行训练"）
3. 长难句拆分重组（如原文第三句拆分为两个中文分句）
4. 关键方法名称保留英文缩写（K-ON）并补充说明
5. 学术用语准确（"state-of-the-art"译为"现有最优方法"而非字面直译）
