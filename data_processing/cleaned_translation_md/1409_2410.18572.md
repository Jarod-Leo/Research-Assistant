# Taipan: Efficient and Expressive State Space Language Models with Selective Attention

链接: http://arxiv.org/abs/2410.18572v1

原文摘要:
Efficient long-context language modeling remains a significant challenge in
Natural Language Processing (NLP). While Transformers dominate language tasks,
they struggle with long sequences due to quadratic computational complexity in
training and linearly scaling memory costs during inference. Recent State Space
Models (SSMs) such as Mamba offer alternatives with constant memory usage, but
they underperform in tasks requiring extensive in-context retrieval. We
introduce Taipan, a novel hybrid architecture that combines Mamba-2 with
Selective Attention Layers (SALs). These SALs identify tokens requiring
long-range interactions, remove less important features, and then augment their
representations using the attention module. This approach balances Mamba's
efficiency with Transformer-like performance in memory-intensive tasks. By
constraining the attention budget, Taipan extends accurate predictions to
context lengths of up to 1 million tokens while preserving computational
efficiency. Our experiments demonstrate Taipan's superior performance across
various scales and tasks, offering a promising solution for efficient
long-context language modeling.

中文翻译:
高效的长上下文语言建模一直是自然语言处理（NLP）领域的重大挑战。尽管Transformer模型在语言任务中占据主导地位，但其训练时的二次计算复杂度和推理时线性增长的内存开销，使其难以处理长序列。近期提出的状态空间模型（如Mamba）虽能保持恒定内存占用，但在需要大量上下文检索的任务中表现欠佳。我们提出Taipan——一种创新混合架构，将Mamba-2与选择性注意力层（SALs）相结合。这些SALs能识别需要长距离交互的关键词元，过滤次要特征，并通过注意力模块增强其表征。该方法既保持了Mamba的高效性，又在内存密集型任务中实现了类Transformer的性能。通过约束注意力计算量，Taipan在保持计算效率的同时，可将准确预测的上下文长度扩展至100万词元。实验结果表明，Taipan在不同规模和任务中均展现出卓越性能，为高效长上下文语言建模提供了极具前景的解决方案。

（翻译说明：采用技术文档的严谨风格，通过以下处理确保专业性：
1. 术语统一："tokens"译为"词元"（NLP领域标准译法），"quadratic computational complexity"译为"二次计算复杂度"
2. 句式重构：将英语长句拆解为符合中文表达习惯的短句，如原文第三句通过分号转换为并列结构
3. 概念显化："Selective Attention Layers"增译为"选择性注意力层"并保留缩写SALs
4. 动态对等："underperform"译为"表现欠佳"而非字面直译，更符合学术评价语境
5. 数据呈现：规范处理数字单位"1 million tokens"译为"100万词元"）
