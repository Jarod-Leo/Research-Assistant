# EdgeShard: Efficient LLM Inference via Collaborative Edge Computing

链接: http://arxiv.org/abs/2405.14371v1

原文摘要:
Large language models (LLMs) have shown great potential in natural language
processing and content generation. However, current LLMs heavily rely on cloud
computing, leading to prolonged latency, high bandwidth cost, and privacy
concerns. Edge computing is promising to address such concerns by deploying
LLMs on edge devices, closer to data sources. Some works try to leverage model
quantization to reduce the model size to fit the resource-constraint edge
devices, but they lead to accuracy loss. Other works use cloud-edge
collaboration, suffering from unstable network connections. In this work, we
leverage collaborative edge computing to facilitate the collaboration among
edge devices and cloud servers for jointly performing efficient LLM inference.
We propose a general framework to partition the LLM model into shards and
deploy on distributed devices. To achieve efficient LLM inference, we formulate
an adaptive joint device selection and model partition problem and design an
efficient dynamic programming algorithm to optimize the inference latency and
throughput, respectively. Experiments of Llama2 serial models on a
heterogeneous physical prototype demonstrate that EdgeShard achieves up to 50%
latency reduction and 2x throughput improvement over baseline methods.

中文翻译:
以下是符合要求的学术论文摘要中文翻译：

大语言模型（LLMs）在自然语言处理与内容生成领域展现出巨大潜力。然而，现有LLMs严重依赖云计算，导致推理延迟高、带宽成本大且存在隐私隐患。边缘计算通过将LLMs部署在更接近数据源的边缘设备上，有望解决这些问题。现有方案中，模型量化方法虽能缩减模型尺寸以适应资源受限的边缘设备，但会导致精度损失；而云边协同方案则受限于网络连接的不稳定性。本研究创新性地采用协同边缘计算架构，通过边缘设备与云服务器的高效协作实现LLM推理优化。我们提出通用框架将LLM模型划分为分片并分布式部署，进而构建自适应联合设备选择与模型划分优化问题，分别设计基于动态规划的高效算法来优化推理延迟与吞吐量。在异构物理原型上进行的Llama2系列模型实验表明，EdgeShard方案相较基线方法最高可实现50%的延迟降低与2倍的吞吐量提升。

翻译说明：
1. 专业术语处理：LLMs统一译为"大语言模型"，"edge computing"译为"边缘计算"，"model quantization"译为"模型量化"等
2. 被动语态转换："are deployed"译为主动态"分布式部署"
3. 长句拆分：将原文复合长句拆分为符合中文表达习惯的短句结构
4. 概念显化："baseline methods"译为"基线方法"而非字面直译
5. 数据呈现：精确保留"50%"和"2倍"等量化指标
6. 学术风格：使用"本研究"、"构建"、"相较"等学术用语
7. 逻辑衔接：通过"然而"、"进而"等连接词保持论证连贯性
