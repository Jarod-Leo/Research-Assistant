# MoE$^2$: Optimizing Collaborative Inference for Edge Large Language Models

链接: http://arxiv.org/abs/2501.09410v1

原文摘要:
Large language models (LLMs) have demonstrated remarkable capabilities across
a wide range of natural language processing tasks. Exploiting the heterogeneous
capabilities of edge LLMs is crucial for diverse emerging applications, as it
enables greater cost-effectiveness and reduced latency. In this work, we
introduce \textit{Mixture-of-Edge-Experts (MoE$^2$)}, a novel collaborative
inference framework for edge LLMs. We formulate the joint gating and expert
selection problem to optimize inference performance under energy and latency
constraints. Unlike conventional MoE problems, LLM expert selection is
significantly more challenging due to the combinatorial nature and the
heterogeneity of edge LLMs across various attributes. To this end, we propose a
two-level expert selection mechanism through which we uncover an
optimality-preserving property of gating parameters across expert selections.
This property enables the decomposition of the training and selection
processes, significantly reducing complexity. Furthermore, we leverage the
objective's monotonicity and design a discrete monotonic optimization algorithm
for optimal expert selection. We implement edge servers with NVIDIA Jetson AGX
Orins and NVIDIA RTX 4090 GPUs, and perform extensive experiments. Our results
validate that performance improvements of various LLM models and show that our
MoE$^2$ method can achieve optimal trade-offs among different delay and energy
budgets, and outperforms baselines under various system resource constraints.

中文翻译:
以下是符合要求的学术摘要中文翻译：

【译文】
大语言模型（LLMs）已在各类自然语言处理任务中展现出卓越能力。充分利用边缘LLMs的异构能力对多样化新兴应用至关重要，因其能实现更高成本效益并降低延迟。本文提出\textit{边缘专家混合框架（MoE$^2$）}，这是一种创新的边缘LLMs协同推理框架。我们将联合门控与专家选择问题建模为能量和延迟约束下的推理性能优化问题。与传统MoE问题不同，由于组合复杂性及边缘LLMs在多维属性上的异构性，LLM专家选择面临更大挑战。为此，我们设计了两级专家选择机制，通过该机制揭示了门控参数在专家选择中具有保持最优性的特性。该特性实现了训练与选择过程的解耦，显著降低了复杂度。此外，利用目标函数的单调性，我们设计了离散单调优化算法以实现最优专家选择。基于NVIDIA Jetson AGX Orin和RTX 4090 GPU搭建边缘服务器开展实验，结果表明：多种LLM模型均获得性能提升，MoE$^2$方法能在不同延迟与能耗预算下实现最优权衡，且在各类系统资源约束条件下均优于基线方法。

【关键术语处理】
1. "heterogeneous capabilities"译为"异构能力"（计算机领域标准术语）
2. "cost-effectiveness"译为"成本效益"（经济学常用译法）
3. "combinatorial nature"译为"组合复杂性"（算法复杂度标准表述）
4. "monotonic optimization"译为"单调优化"（数学优化标准术语）
5. "optimality-preserving property"译为"保持最优性的特性"（保留数学证明特性）

【学术规范】
• 保留原文技术符号（MoE$^2$）
• 专业术语首次出现标注英文原词（LLMs）
• 被动语态转换为中文主动表述（如"is formulated"译为"建模为"）
• 长难句拆分重组（如将原文60词长句拆分为三个中文分句）
