# EMRModel: A Large Language Model for Extracting Medical Consultation Dialogues into Structured Medical Records

链接: http://arxiv.org/abs/2504.16448v1

原文摘要:
Medical consultation dialogues contain critical clinical information, yet
their unstructured nature hinders effective utilization in diagnosis and
treatment. Traditional methods, relying on rule-based or shallow machine
learning techniques, struggle to capture deep and implicit semantics. Recently,
large pre-trained language models and Low-Rank Adaptation (LoRA), a lightweight
fine-tuning method, have shown promise for structured information extraction.
We propose EMRModel, a novel approach that integrates LoRA-based fine-tuning
with code-style prompt design, aiming to efficiently convert medical
consultation dialogues into structured electronic medical records (EMRs).
Additionally, we construct a high-quality, realistically grounded dataset of
medical consultation dialogues with detailed annotations. Furthermore, we
introduce a fine-grained evaluation benchmark for medical consultation
information extraction and provide a systematic evaluation methodology,
advancing the optimization of medical natural language processing (NLP) models.
Experimental results show EMRModel achieves an F1 score of 88.1%, improving
by49.5% over standard pre-trained models. Compared to traditional LoRA
fine-tuning methods, our model shows superior performance, highlighting its
effectiveness in structured medical record extraction tasks.

中文翻译:
以下是符合学术规范的中文翻译：

医疗问诊对话蕴含关键临床信息，但其非结构化特性阻碍了诊疗过程中的有效利用。传统基于规则或浅层机器学习的方法难以捕捉深层隐含语义。近年来，大规模预训练语言模型与轻量化微调方法低秩自适应（LoRA）在结构化信息抽取方面展现出潜力。我们提出EMRModel创新方案，通过融合LoRA微调与代码式提示设计，实现医疗对话向结构化电子病历（EMR）的高效转化。本研究还构建了高质量、贴近真实场景的医疗问诊对话数据集，并提供精细化标注。此外，我们设计了细粒度的医疗问诊信息抽取评估基准，提出系统化的评测方法，推动医疗自然语言处理模型的优化。实验表明EMRModel的F1值达到88.1%，较标准预训练模型提升49.5%。与传统LoRA微调方法相比，本模型展现出更优性能，凸显其在结构化病历抽取任务中的有效性。

（翻译说明：
1. 专业术语处理："Low-Rank Adaptation"保留技术缩写"LoRA"并补充中文全称
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句，如将"aiming to..."独立成句
3. 概念显化："code-style prompt design"译为"代码式提示设计"以保持技术准确性
4. 数据呈现：保留精确数值"88.1%"和"49.5%"，符合学术论文规范
5. 被动语态转换：将"have shown promise"等被动表达转为中文主动语态
6. 逻辑连接：通过"此外""还"等连接词保持论文的学术连贯性）
