# PromptOptMe: Error-Aware Prompt Compression for LLM-based MT Evaluation Metrics

链接: http://arxiv.org/abs/2412.16120v1

原文摘要:
Evaluating the quality of machine-generated natural language content is a
challenging task in Natural Language Processing (NLP). Recently, large language
models (LLMs) like GPT-4 have been employed for this purpose, but they are
computationally expensive due to the extensive token usage required by complex
evaluation prompts. In this paper, we propose a prompt optimization approach
that uses a smaller, fine-tuned language model to compress input data for
evaluation prompt, thus reducing token usage and computational cost when using
larger LLMs for downstream evaluation. Our method involves a two-stage
fine-tuning process: supervised fine-tuning followed by preference optimization
to refine the model's outputs based on human preferences. We focus on Machine
Translation (MT) evaluation and utilize the GEMBA-MQM metric as a starting
point. Our results show a $2.37\times$ reduction in token usage without any
loss in evaluation quality. This work makes state-of-the-art LLM-based metrics
like GEMBA-MQM more cost-effective and efficient, enhancing their accessibility
for broader use.

中文翻译:
评估机器生成的自然语言内容质量是自然语言处理（NLP）领域的一项挑战性任务。近年来，虽然GPT-4等大语言模型（LLMs）被用于此项工作，但由于复杂评估提示需要消耗大量token，其计算成本高昂。本文提出一种提示优化方法：通过微调的小型语言模型压缩评估提示的输入数据，从而在使用大型LLMs进行下游评估时降低token消耗与计算成本。该方法采用两阶段微调流程——先进行监督式微调，再通过偏好优化根据人类反馈精炼模型输出。我们以机器翻译（MT）评估为应用场景，以GEMBA-MQM指标为基准，实验表明在保持评估质量不变的前提下实现了2.37倍的token用量缩减。这项研究使GEMBA-MQM等基于LLM的尖端评估指标更具成本效益，提升了其在更广泛场景中的可用性。

（翻译说明：
1. 专业术语处理："fine-tuned"译为"微调"，"token"保留英文形式，符合NLP领域惯例
2. 长句拆分：将原文复合句按中文表达习惯分解为多个短句，如第二句拆分为"虽然...但..."结构
3. 被动语态转换："are employed"译为主动态"被用于"，符合中文叙述习惯
4. 数字表达：保留"$2.37\times$"的数学表达形式，确保技术准确性
5. 概念显化："preference optimization"增译为"偏好优化根据人类反馈"，明确技术内涵
6. 术语一致性：保持"GEMBA-MQM"等专有名词原文形态，确保学术严谨性）
