# Accelerating Retrieval-Augmented Language Model Serving with Speculation

链接: http://arxiv.org/abs/2401.14021v1

原文摘要:
Retrieval-augmented language models (RaLM) have demonstrated the potential to
solve knowledge-intensive natural language processing (NLP) tasks by combining
a non-parametric knowledge base with a parametric language model. Instead of
fine-tuning a fully parametric model, RaLM excels at its low-cost adaptation to
the latest data and better source attribution mechanisms. Among various RaLM
approaches, iterative RaLM delivers a better generation quality due to a more
frequent interaction between the retriever and the language model. Despite the
benefits, iterative RaLM usually encounters high overheads due to the frequent
retrieval step. To this end, we propose RaLMSpec, a speculation-inspired
framework that provides generic speed-up over iterative RaLM while preserving
the same model outputs through speculative retrieval and batched verification.
By further incorporating prefetching, optimal speculation stride scheduler, and
asynchronous verification, RaLMSpec can automatically exploit the acceleration
potential to the fullest. For naive iterative RaLM serving, extensive
evaluations over three language models on four downstream QA datasets
demonstrate that RaLMSpec can achieve a speed-up ratio of 1.75-2.39x,
1.04-1.39x, and 1.31-1.77x when the retriever is an exact dense retriever,
approximate dense retriever, and sparse retriever respectively compared with
the baseline. For KNN-LM serving, RaLMSpec can achieve a speed-up ratio up to
7.59x and 2.45x when the retriever is an exact dense retriever and approximate
dense retriever, respectively, compared with the baseline.

中文翻译:
检索增强语言模型（RaLM）通过将非参数化知识库与参数化语言模型相结合，已展现出解决知识密集型自然语言处理（NLP）任务的潜力。相较于微调全参数化模型，RaLM的优势在于能以更低成本适配最新数据，并提供更优的溯源机制。在各类RaLM方法中，迭代式RaLM因检索器与语言模型间更频繁的交互而具有更优的生成质量。然而这种优势伴随高昂开销——频繁检索步骤导致系统负担加重。为此，我们提出RaLMSpec框架，其受推测执行思想启发，通过推测式检索与批量验证机制，在保持模型输出一致性的前提下为迭代式RaLM提供通用加速方案。该框架进一步整合预取技术、最优推测跨度调度器及异步验证机制，可自动实现最大化的加速潜力。针对基础迭代式RaLM服务的实验表明：在四个下游QA数据集上对三种语言模型的广泛测试中，当检索器分别为精确稠密检索器、近似稠密检索器和稀疏检索器时，RaLMSpec相较基线可实现1.75-2.39倍、1.04-1.39倍和1.31-1.77倍的加速比。在KNN-LM服务场景下，当采用精确稠密检索器与近似稠密检索器时，加速比最高可达7.59倍与2.45倍。
