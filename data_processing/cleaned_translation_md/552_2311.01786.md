# TCM-GPT: Efficient Pre-training of Large Language Models for Domain Adaptation in Traditional Chinese Medicine

链接: http://arxiv.org/abs/2311.01786v1

原文摘要:
Pre-training and fine-tuning have emerged as a promising paradigm across
various natural language processing (NLP) tasks. The effectiveness of
pretrained large language models (LLM) has witnessed further enhancement,
holding potential for applications in the field of medicine, particularly in
the context of Traditional Chinese Medicine (TCM). However, the application of
these general models to specific domains often yields suboptimal results,
primarily due to challenges like lack of domain knowledge, unique objectives,
and computational efficiency. Furthermore, their effectiveness in specialized
domains, such as Traditional Chinese Medicine, requires comprehensive
evaluation. To address the above issues, we propose a novel domain specific
TCMDA (TCM Domain Adaptation) approach, efficient pre-training with
domain-specific corpus. Specifically, we first construct a large TCM-specific
corpus, TCM-Corpus-1B, by identifying domain keywords and retreving from
general corpus. Then, our TCMDA leverages the LoRA which freezes the pretrained
model's weights and uses rank decomposition matrices to efficiently train
specific dense layers for pre-training and fine-tuning, efficiently aligning
the model with TCM-related tasks, namely TCM-GPT-7B. We further conducted
extensive experiments on two TCM tasks, including TCM examination and TCM
diagnosis. TCM-GPT-7B archived the best performance across both datasets,
outperforming other models by relative increments of 17% and 12% in accuracy,
respectively. To the best of our knowledge, our study represents the pioneering
validation of domain adaptation of a large language model with 7 billion
parameters in TCM domain. We will release both TCMCorpus-1B and TCM-GPT-7B
model once accepted to facilitate interdisciplinary development in TCM and NLP,
serving as the foundation for further study.

中文翻译:
预训练与微调已成为自然语言处理（NLP）领域各类任务的通用范式。预训练大语言模型（LLM）的效能持续提升，为医学领域（尤其是中医药领域）的应用创造了可能。然而，这类通用模型在垂直领域的直接应用往往表现欠佳，主要归因于领域知识缺失、目标特异性及计算效率等挑战。此外，其在中医药等专业领域的适用性仍需系统评估。针对上述问题，本研究提出创新的领域自适应方法TCMDA（中医药领域适配），通过领域语料库实现高效预训练。具体而言，我们首先通过领域关键词筛选与通用语料库检索，构建了规模达10亿字符的中医药专用语料库TCM-Corpus-1B。随后，TCMDA采用LoRA技术冻结预训练模型权重，利用秩分解矩阵高效训练特定稠密层，实现预训练与微调过程，最终产出与中医药任务高度适配的TCM-GPT-7B模型。我们在中医药资格考试与中医诊断两项任务上展开实验验证，TCM-GPT-7B在两项任务准确率上分别以17%和12%的相对优势超越基线模型。据我们所知，这是首个在中医药领域完成70亿参数大模型领域适配的实证研究。论文录用后，我们将公开TCM-Corpus-1B语料库与TCM-GPT-7B模型，以促进中医药与NLP的跨学科发展，为后续研究奠定基础。  


