# T-former: An Efficient Transformer for Image Inpainting

链接: http://arxiv.org/abs/2305.07239v1

原文摘要:
Benefiting from powerful convolutional neural networks (CNNs), learning-based
image inpainting methods have made significant breakthroughs over the years.
However, some nature of CNNs (e.g. local prior, spatially shared parameters)
limit the performance in the face of broken images with diverse and complex
forms. Recently, a class of attention-based network architectures, called
transformer, has shown significant performance on natural language processing
fields and high-level vision tasks. Compared with CNNs, attention operators are
better at long-range modeling and have dynamic weights, but their computational
complexity is quadratic in spatial resolution, and thus less suitable for
applications involving higher resolution images, such as image inpainting. In
this paper, we design a novel attention linearly related to the resolution
according to Taylor expansion. And based on this attention, a network called
$T$-former is designed for image inpainting. Experiments on several benchmark
datasets demonstrate that our proposed method achieves state-of-the-art
accuracy while maintaining a relatively low number of parameters and
computational complexity. The code can be found at
\href{