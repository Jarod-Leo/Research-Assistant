# What Causes Knowledge Loss in Multilingual Language Models?

链接: http://arxiv.org/abs/2504.20356v1

原文摘要:
Cross-lingual transfer in natural language processing (NLP) models enhances
multilingual performance by leveraging shared linguistic knowledge. However,
traditional methods that process all data simultaneously often fail to mimic
real-world scenarios, leading to challenges like catastrophic forgetting, where
fine-tuning on new tasks degrades performance on previously learned ones. Our
study explores this issue in multilingual contexts, focusing on linguistic
differences affecting representational learning rather than just model
parameters. We experiment with 52 languages using LoRA adapters of varying
ranks to evaluate non-shared, partially shared, and fully shared parameters.
Our aim is to see if parameter sharing through adapters can mitigate forgetting
while preserving prior knowledge. We find that languages using non-Latin
scripts are more susceptible to catastrophic forgetting, whereas those written
in Latin script facilitate more effective cross-lingual transfer.

中文翻译:
自然语言处理（NLP）模型中的跨语言迁移通过共享语言学知识来提升多语言性能。然而，传统方法同时处理所有数据的模式往往难以模拟真实场景，这会导致灾难性遗忘等挑战——即在新任务上的微调会损害模型先前习得任务的性能。我们的研究在多语言语境中探讨这一问题，重点关注影响表征学习的语言差异，而非仅关注模型参数。我们采用不同秩的LoRA适配器对52种语言进行实验，评估非共享、部分共享和完全共享参数的效果，旨在探究适配器的参数共享机制是否能在保留先验知识的同时缓解遗忘现象。研究发现：使用非拉丁文字的语言更易遭受灾难性遗忘，而采用拉丁文字的语言则能促成更有效的跨语言迁移。
