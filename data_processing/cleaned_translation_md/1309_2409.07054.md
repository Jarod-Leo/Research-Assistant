# Native vs Non-Native Language Prompting: A Comparative Analysis

链接: http://arxiv.org/abs/2409.07054v1

原文摘要:
Large language models (LLMs) have shown remarkable abilities in different
fields, including standard Natural Language Processing (NLP) tasks. To elicit
knowledge from LLMs, prompts play a key role, consisting of natural language
instructions. Most open and closed source LLMs are trained on available labeled
and unlabeled resources--digital content such as text, images, audio, and
videos. Hence, these models have better knowledge for high-resourced languages
but struggle with low-resourced languages. Since prompts play a crucial role in
understanding their capabilities, the language used for prompts remains an
important research question. Although there has been significant research in
this area, it is still limited, and less has been explored for medium to
low-resourced languages. In this study, we investigate different prompting
strategies (native vs. non-native) on 11 different NLP tasks associated with 12
different Arabic datasets (9.7K data points). In total, we conducted 197
experiments involving 3 LLMs, 12 datasets, and 3 prompting strategies. Our
findings suggest that, on average, the non-native prompt performs the best,
followed by mixed and native prompts.

中文翻译:
大型语言模型（LLMs）在不同领域展现出卓越能力，包括标准自然语言处理（NLP）任务。为激发LLMs的知识潜能，由自然语言指令构成的提示词（prompts）起着关键作用。当前多数开源与闭源LLMs基于现有标注/未标注资源（如文本、图像、音频、视频等数字内容）训练，因此这些模型对高资源语言表现优异，但在低资源语言上表现欠佳。由于提示词对理解模型能力至关重要，其使用语言的选择仍是重要研究课题。尽管该领域已有显著研究成果，但针对中低资源语言的探索仍然有限。本研究通过12个阿拉伯语数据集（含9.7K数据点）关联的11项NLP任务，系统考察了不同提示策略（母语vs.非母语）的效果。总计完成197组实验，涵盖3种LLMs、12个数据集及3种提示策略。实验结果表明：非母语提示策略平均表现最优，混合策略次之，母语策略居末。


2. "labeled/unlabeled resources"译为"标注/未标注资源"（机器学习标准术语）
3. "high/low-resourced languages"译为"高/低资源语言"（计算语言学通用表述）
4. 长难句按中文习惯拆分为短句，如将原文最后两句话合并重组为因果句式
5. 数字单位"9.7K"转换为中文计数习惯"9.7千"，并规范表述为"9.7K数据点"）
