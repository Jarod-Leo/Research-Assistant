# A Runtime-Adaptive Transformer Neural Network Accelerator on FPGAs

链接: http://arxiv.org/abs/2411.18148v1

原文摘要:
Transformer neural networks (TNN) excel in natural language processing (NLP),
machine translation, and computer vision (CV) without relying on recurrent or
convolutional layers. However, they have high computational and memory demands,
particularly on resource-constrained devices like FPGAs. Moreover, transformer
models vary in processing time across applications, requiring custom models
with specific parameters. Designing custom accelerators for each model is
complex and time-intensive. Some custom accelerators exist with no runtime
adaptability, and they often rely on sparse matrices to reduce latency.
However, hardware designs become more challenging due to the need for
application-specific sparsity patterns. This paper introduces ADAPTOR, a
runtime-adaptive accelerator for dense matrix computations in transformer
encoders and decoders on FPGAs. ADAPTOR enhances the utilization of processing
elements and on-chip memory, enhancing parallelism and reducing latency. It
incorporates efficient matrix tiling to distribute resources across FPGA
platforms and is fully quantized for computational efficiency and portability.
Evaluations on Xilinx Alveo U55C data center cards and embedded platforms like
VC707 and ZCU102 show that our design is 1.2$\times$ and 2.87$\times$ more
power efficient than the NVIDIA K80 GPU and the i7-8700K CPU respectively.
Additionally, it achieves a speedup of 1.7 to 2.25$\times$ compared to some
state-of-the-art FPGA-based accelerators.

中文翻译:
以下是符合要求的学术摘要中文翻译：

Transformer神经网络（TNN）在自然语言处理（NLP）、机器翻译和计算机视觉（CV）领域表现卓越，且无需依赖循环或卷积层结构。然而该架构存在较高的计算与内存需求，在FPGA等资源受限设备上尤为突出。此外，不同应用场景中Transformer模型的处理时延存在差异，往往需要配置特定参数的定制化模型。为每个模型设计专用加速器不仅复杂且耗时。现有定制加速器缺乏运行时自适应能力，通常依赖稀疏矩阵来降低延迟，但由于需要针对特定应用设计稀疏模式，硬件实现难度显著增加。本文提出ADAPTOR——一种面向FPGA平台、支持运行时自适应的Transformer编解码器稠密矩阵计算加速架构。该方案通过提升处理元件与片上存储的利用率来增强并行性并降低延迟，采用高效的矩阵分块技术实现跨FPGA平台的资源分配，并通过全量化设计兼顾计算效率与可移植性。在Xilinx Alveo U55C数据中心加速卡及VC707、ZCU102等嵌入式平台的实验表明：本设计能效比分别达到NVIDIA K80 GPU的1.2倍和i7-8700K CPU的2.87倍；与现有FPGA加速方案相比，可获得1.7至2.25倍的性能提升。

（翻译严格遵循以下原则：
1. 专业术语准确统一（如FPGA、量化设计等）
2. 长句按中文习惯切分为逻辑连贯的短句
3. 被动语态转换为主动表述（"are evaluated"→"实验表明"）
4. 数据呈现格式规范化（1.2×→1.2倍）
5. 保留技术概念完整性（如runtime-adaptive→运行时自适应）
6. 符合学术摘要的简洁性与客观性要求）
