# Adapting Large Language Models for Document-Level Machine Translation

链接: http://arxiv.org/abs/2401.06468v1

原文摘要:
Large language models (LLMs) have significantly advanced various natural
language processing (NLP) tasks. Recent research indicates that
moderately-sized LLMs often outperform larger ones after task-specific
fine-tuning. This study focuses on adapting LLMs for document-level machine
translation (DocMT) for specific language pairs. We first investigate the
impact of prompt strategies on translation performance and then conduct
extensive experiments using two fine-tuning methods, three LLM backbones, and
18 translation tasks across nine language pairs. Our results show that
specialized models can sometimes surpass GPT-4 in translation performance but
still face issues like off-target translation due to error propagation in
decoding. We provide an in-depth analysis of these LLMs tailored for DocMT,
examining translation errors, discourse phenomena, strategies for training and
inference, the data efficiency of parallel documents, recent test set
evaluations, and zero-shot crosslingual transfer. Our findings highlight the
strengths and limitations of LLM-based DocMT models and provide a foundation
for future research.

中文翻译:
以下是符合要求的学术中文翻译：

大型语言模型（LLMs）显著推动了各类自然语言处理（NLP）任务的发展。最新研究表明，经过特定任务微调的中等规模LLMs往往能超越更大规模的模型。本研究专注于将LLMs应用于特定语言对的文档级机器翻译（DocMT）任务。我们首先探究提示策略对翻译性能的影响，随后采用两种微调方法、三种LLM基础架构，在九组语言对的18项翻译任务上展开系统实验。结果表明：专用模型在翻译性能上有时能超越GPT-4，但仍存在解码过程中错误传播导致的译文偏离等问题。我们对这些DocMT专用LLMs进行了深度分析，包括：翻译错误类型、篇章现象处理、训练与推理策略、平行语料的数据效率、最新测试集评估以及零样本跨语言迁移能力。本研究不仅揭示了基于LLM的DocMT模型优势与局限，也为后续研究奠定了重要基础。

（翻译说明：严格遵循学术规范，采用术语统一原则，如"fine-tuning"统一译为"微调"；通过拆分英文长句为中文短句结构（如将原文最后两句话重组为三个分句）；保留专业缩写（LLMs/DocMT）并保持首次出现时全称标注；使用"探究""展开""揭示"等符合中文论文表达的动词；通过冒号引导实现原文分析性内容的逻辑分层）
