# United We Pretrain, Divided We Fail! Representation Learning for Time Series by Pretraining on 75 Datasets at Once

链接: http://arxiv.org/abs/2402.15404v1

原文摘要:
In natural language processing and vision, pretraining is utilized to learn
effective representations. Unfortunately, the success of pretraining does not
easily carry over to time series due to potential mismatch between sources and
target. Actually, common belief is that multi-dataset pretraining does not work
for time series! Au contraire, we introduce a new self-supervised contrastive
pretraining approach to learn one encoding from many unlabeled and diverse time
series datasets, so that the single learned representation can then be reused
in several target domains for, say, classification. Specifically, we propose
the XD-MixUp interpolation method and the Soft Interpolation Contextual
Contrasting (SICC) loss. Empirically, this outperforms both supervised training
and other self-supervised pretraining methods when finetuning on low-data
regimes. This disproves the common belief: We can actually learn from multiple
time series datasets, even from 75 at once.

中文翻译:
在自然语言处理与视觉领域，预训练技术被广泛用于学习有效表征。然而由于源域与目标域之间可能存在不匹配，这种成功经验难以直接迁移至时间序列数据。事实上，学界普遍认为多数据集预训练对时间序列无效！与此相反，我们提出了一种新型自监督对比预训练方法，能够从多个未标注的多样化时间序列数据集中学习统一编码，使得习得的单一表征可复用于分类等多种下游任务。具体而言，我们提出了XD-MixUp插值方法和软插值上下文对比损失函数（SICC）。实证研究表明，在低数据量场景进行微调时，该方法性能优于监督训练及其他自监督预训练方案。这一发现颠覆了传统认知：我们确实能够从多个时间序列数据集（甚至同时从75个数据集）中有效学习表征。

（翻译说明：
1. 专业术语处理："pretraining"译为"预训练"，"self-supervised contrastive"译为"自监督对比"，"fine-tuning"译为"微调"等保持学术规范性
2. 句式重构：将英文长句拆解为符合中文表达习惯的短句，如将"learn one encoding from..."处理为"能够从...学习统一编码"
3. 文化适配："Au contraire"采用意译"与此相反"，保留原文反驳语气
4. 技术概念显化：括号补充说明"（甚至同时从75个数据集）"增强可读性
5. 被动语态转化：将"can be reused"主动化为"可复用"符合中文表达习惯
6. 术语统一性：全篇保持"表征/编码/数据集"等术语的一致性）
