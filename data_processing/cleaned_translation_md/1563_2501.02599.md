# Empowering Bengali Education with AI: Solving Bengali Math Word Problems through Transformer Models

链接: http://arxiv.org/abs/2501.02599v1

原文摘要:
Mathematical word problems (MWPs) involve the task of converting textual
descriptions into mathematical equations. This poses a significant challenge in
natural language processing, particularly for low-resource languages such as
Bengali. This paper addresses this challenge by developing an innovative
approach to solving Bengali MWPs using transformer-based models, including
Basic Transformer, mT5, BanglaT5, and mBART50. To support this effort, the
"PatiGonit" dataset was introduced, containing 10,000 Bengali math problems,
and these models were fine-tuned to translate the word problems into equations
accurately. The evaluation revealed that the mT5 model achieved the highest
accuracy of 97.30%, demonstrating the effectiveness of transformer models in
this domain. This research marks a significant step forward in Bengali natural
language processing, offering valuable methodologies and resources for
educational AI tools. By improving math education, it also supports the
development of advanced problem-solving skills for Bengali-speaking students.

中文翻译:
数学应用题（MWPs）的核心任务是将文字描述转化为数学方程式。这一课题对自然语言处理领域构成重大挑战，尤其在孟加拉语等低资源语言中更为突出。本研究提出创新性解决方案，采用基于Transformer的模型（包括基础Transformer、mT5、BanglaT5和mBART50）来处理孟加拉语数学应用题。为支持研究，团队构建了包含10,000道题目的"PatiGonit"数据集，并通过微调使模型能够准确将文字问题转化为数学方程。评估结果显示，mT5模型以97.30%的准确率表现最优，充分验证了Transformer模型在该领域的有效性。本项研究标志着孟加拉语自然语言处理的重要进展，为教育类人工智能工具提供了宝贵的方法论和资源。通过提升数学教育质量，该成果还将助力孟加拉语学生培养高阶问题解决能力。
