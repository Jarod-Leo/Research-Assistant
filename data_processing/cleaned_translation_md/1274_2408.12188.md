# Reasoning Factual Knowledge in Structured Data with Large Language Models

链接: http://arxiv.org/abs/2408.12188v1

原文摘要:
Large language models (LLMs) have made remarkable progress in various natural
language processing tasks as a benefit of their capability to comprehend and
reason with factual knowledge. However, a significant amount of factual
knowledge is stored in structured data, which possesses unique characteristics
that differ from the unstructured texts used for pretraining. This difference
can introduce imperceptible inference parameter deviations, posing challenges
for LLMs in effectively utilizing and reasoning with structured data to
accurately infer factual knowledge. To this end, we propose a benchmark named
StructFact, to evaluate the structural reasoning capabilities of LLMs in
inferring factual knowledge. StructFact comprises 8,340 factual questions
encompassing various tasks, domains, timelines, and regions. This benchmark
allows us to investigate the capability of LLMs across five factual tasks
derived from the unique characteristics of structural facts. Extensive
experiments on a set of LLMs with different training strategies reveal the
limitations of current LLMs in inferring factual knowledge from structured
data. We present this benchmark as a compass to navigate the strengths and
weaknesses of LLMs in reasoning with structured data for knowledge-sensitive
tasks, and to encourage advancements in related real-world applications. Please
find our code at 