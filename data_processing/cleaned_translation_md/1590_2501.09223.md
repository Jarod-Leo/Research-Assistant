# Foundations of Large Language Models

链接: http://arxiv.org/abs/2501.09223v1

原文摘要:
This is a book about large language models. As indicated by the title, it
primarily focuses on foundational concepts rather than comprehensive coverage
of all cutting-edge technologies. The book is structured into four main
chapters, each exploring a key area: pre-training, generative models, prompting
techniques, and alignment methods. It is intended for college students,
professionals, and practitioners in natural language processing and related
fields, and can serve as a reference for anyone interested in large language
models.

中文翻译:
这是一本关于大语言模型的著作。如书名所示，本书主要聚焦基础性概念，而非全面涵盖所有前沿技术。全书分为四个核心章节，分别探讨预训练、生成模型、提示工程和对齐方法等关键领域。本书面向自然语言处理及相关领域的高校学生、专业人士和实践者，也可作为任何对大语言模型感兴趣人士的参考读物。

（译文特点说明：
1. 专业术语准确对应："pre-training"译作"预训练"，"prompting techniques"采用行业通用译法"提示工程"
2. 句式结构重组：将原文复合句拆分为符合中文表达习惯的短句，如首句处理为判断句式
3. 语序优化调整：将"intended for"引导的目的状语提前，符合中文先背景后重点的表达逻辑
4. 文化适配处理："college students"扩展为"高校学生"更符合中文教育体系表述
5. 专业领域惯用语："serve as a reference"译为"参考读物"符合学术出版语境）
