# MatrixFlow: System-Accelerator co-design for high-performance transformer applications

链接: http://arxiv.org/abs/2503.05290v1

原文摘要:
Transformers are central to advances in artificial intelligence (AI),
excelling in fields ranging from computer vision to natural language
processing. Despite their success, their large parameter count and
computational demands challenge efficient acceleration. To address these
limitations, this paper proposes MatrixFlow, a novel co-designed
system-accelerator architecture based on a loosely coupled systolic array
including a new software mapping approach for efficient transformer code
execution. MatrixFlow is co-optimized via a novel dataflow-based matrix
multiplication technique that reduces memory overhead. These innovations
significantly improve data throughput, which is critical for handling the
extensive computations required by transformers. We validate our approach
through full system simulation using gem5 across various BERT and ViT
Transformer models featuring different data types, demonstrating significant
application-wide speed-ups. Our method achieves up to a 22x improvement
compared to a many-core CPU system, and outperforms the closest
state-of-the-art loosely-coupled and tightly-coupled accelerators by over 5x
and 8x, respectively.

中文翻译:
以下是符合学术规范的中文翻译：

Transformer模型是推动人工智能发展的核心技术，在计算机视觉和自然语言处理等领域表现卓越。然而其庞大的参数量与计算需求对高效加速提出了严峻挑战。针对这些局限性，本文提出MatrixFlow——一种基于松散耦合脉动阵列的协同设计系统-加速器架构，该架构包含创新的软件映射方法以实现高效的Transformer代码执行。通过基于新型数据流的矩阵乘法技术进行协同优化，MatrixFlow显著降低了内存开销。这些创新设计大幅提升了数据吞吐量，这对处理Transformer所需的海量计算至关重要。我们采用gem5全系统仿真平台，在不同数据类型的BERT和ViT Transformer模型上验证了该方案，实验结果表明其能带来显著的全局加速效果：相比众核CPU系统实现最高22倍性能提升，较最先进的松散耦合与紧耦合加速器方案分别超出5倍和8倍以上。

（翻译说明：
1. 专业术语处理：Transformer/BERT/ViT等专有名词保留英文，gem5保持小写
2. 被动语态转换："are validated"译为主动式"验证了"
3. 长句拆分：将原文复合长句按中文习惯分解为多个短句
4. 数据呈现：规范处理"22x/5x/8x"为"22倍/5倍/8倍"
5. 学术用语："state-of-the-art"译为"最先进的"符合中文论文惯例）
