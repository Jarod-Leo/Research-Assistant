# RoBERTweet: A BERT Language Model for Romanian Tweets

链接: http://arxiv.org/abs/2306.06598v1

原文摘要:
Developing natural language processing (NLP) systems for social media
analysis remains an important topic in artificial intelligence research. This
article introduces RoBERTweet, the first Transformer architecture trained on
Romanian tweets. Our RoBERTweet comes in two versions, following the base and
large architectures of BERT. The corpus used for pre-training the models
represents a novelty for the Romanian NLP community and consists of all tweets
collected from 2008 to 2022. Experiments show that RoBERTweet models outperform
the previous general-domain Romanian and multilingual language models on three
NLP tasks with tweet inputs: emotion detection, sexist language identification,
and named entity recognition. We make our models and the newly created corpus
of Romanian tweets freely available.

中文翻译:
以下是符合学术规范的中文翻译：

面向社交媒体分析的自然语言处理系统开发仍是人工智能研究的重要课题。本文介绍了首个基于罗马尼亚语推文训练的Transformer架构——RoBERTweet。我们的RoBERTweet遵循BERT的基础版和大型版架构，提供两种版本。用于模型预训练的语料库对罗马尼亚NLP研究领域具有创新意义，包含2008至2022年间采集的所有推文。实验表明，在涉及推文输入的三个NLP任务（情绪检测、性别歧视语言识别和命名实体识别）上，RoBERTweet模型性能优于先前通用领域的罗马尼亚语及多语言模型。我们公开了所有模型及新构建的罗马尼亚语推文语料库。

（翻译说明：
1. 专业术语规范处理：NLP统一译为"自然语言处理"，Transformer保留原名，BERT/RoBERTweet等模型名称不作翻译
2. 长句拆分重组：将原文复合句按中文表达习惯分解为多个短句
3. 被动语态转换："are collected"译为主动态的"采集"
4. 学术用语规范："outperform"译为"性能优于"而非字面直译
5. 时间表述本地化："2008 to 2022"译为"2008至2022年间"
6. 保留关键数据完整性：所有技术参数与原文严格对应）
