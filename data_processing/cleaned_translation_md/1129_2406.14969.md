# Uni-Mol2: Exploring Molecular Pretraining Model at Scale

链接: http://arxiv.org/abs/2406.14969v1

原文摘要:
In recent years, pretraining models have made significant advancements in the
fields of natural language processing (NLP), computer vision (CV), and life
sciences. The significant advancements in NLP and CV are predominantly driven
by the expansion of model parameters and data size, a phenomenon now recognized
as the scaling laws. However, research exploring scaling law in molecular
pretraining models remains unexplored. In this work, we present Uni-Mol2 , an
innovative molecular pretraining model that leverages a two-track transformer
to effectively integrate features at the atomic level, graph level, and
geometry structure level. Along with this, we systematically investigate the
scaling law within molecular pretraining models, characterizing the power-law
correlations between validation loss and model size, dataset size, and
computational resources. Consequently, we successfully scale Uni-Mol2 to 1.1
billion parameters through pretraining on 800 million conformations, making it
the largest molecular pretraining model to date. Extensive experiments show
consistent improvement in the downstream tasks as the model size grows. The
Uni-Mol2 with 1.1B parameters also outperforms existing methods, achieving an
average 27% improvement on the QM9 and 14% on COMPAS-1D dataset.

中文翻译:
近年来，预训练模型在自然语言处理（NLP）、计算机视觉（CV）和生命科学领域取得了显著进展。NLP与CV领域的重大突破主要源于模型参数量和数据规模的扩大，这一现象现已被归纳为缩放定律。然而，针对分子预训练模型的缩放规律探索仍属空白。本研究提出Uni-Mol2——一种创新的分子预训练模型，通过双轨Transformer架构有效整合原子级、图级和几何结构级特征。基于此，我们系统研究了分子预训练中的缩放定律，揭示了验证损失与模型规模、数据集大小及计算资源之间的幂律关系。最终，我们通过对8亿个分子构象进行预训练，成功将Uni-Mol2参数量扩展至11亿，使其成为目前规模最大的分子预训练模型。大量实验表明，随着模型规模增长，下游任务性能持续提升。11亿参数的Uni-Mol2在QM9和COMPAS-1D数据集上分别实现了27%和14%的平均性能提升，显著优于现有方法。


