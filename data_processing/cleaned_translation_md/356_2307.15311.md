# TrafficSafetyGPT: Tuning a Pre-trained Large Language Model to a Domain-Specific Expert in Transportation Safety

链接: http://arxiv.org/abs/2307.15311v1

原文摘要:
Large Language Models (LLMs) have shown remarkable effectiveness in various
general-domain natural language processing (NLP) tasks. However, their
performance in transportation safety domain tasks has been suboptimal,
primarily attributed to the requirement for specialized transportation safety
expertise in generating accurate responses [1]. To address this challenge, we
introduce TrafficSafetyGPT, a novel LLAMA-based model, which has undergone
supervised fine-tuning using TrafficSafety-2K dataset which has human labels
from government produced guiding books and ChatGPT-generated instruction-output
pairs. Our proposed TrafficSafetyGPT model and TrafficSafety-2K train dataset
are accessible at 