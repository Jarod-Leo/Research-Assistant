# Faster Convergence for Transformer Fine-tuning with Line Search Methods

链接: http://arxiv.org/abs/2403.18506v1

原文摘要:
Recent works have shown that line search methods greatly increase performance
of traditional stochastic gradient descent methods on a variety of datasets and
architectures [1], [2]. In this work we succeed in extending line search
methods to the novel and highly popular Transformer architecture and dataset
domains in natural language processing. More specifically, we combine the
Armijo line search with the Adam optimizer and extend it by subdividing the
networks architecture into sensible units and perform the line search
separately on these local units. Our optimization method outperforms the
traditional Adam optimizer and achieves significant performance improvements
for small data sets or small training budgets, while performing equal or better
for other tested cases. Our work is publicly available as a python package,
which provides a hyperparameter-free pytorch optimizer that is compatible with
arbitrary network architectures.

中文翻译:
近期研究表明，线搜索方法能显著提升传统随机梯度下降算法在多种数据集和模型架构下的性能表现[1][2]。本研究成功将线搜索方法拓展至自然语言处理领域新兴且广受欢迎的Transformer架构及数据集领域。具体而言，我们将Armijo线搜索与Adam优化器相结合，通过将网络架构划分为合理单元并在这些局部单元上分别执行线搜索来实现方法创新。我们的优化方法在小型数据集或有限训练预算条件下显著优于传统Adam优化器，在其他测试场景中表现相当或更优。本研究成果已开源为Python工具包，提供免超参调优的pytorch优化器，可兼容任意网络架构。


