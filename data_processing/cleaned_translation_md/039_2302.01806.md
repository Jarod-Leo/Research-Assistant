# Mitigating Data Scarcity for Large Language Models

链接: http://arxiv.org/abs/2302.01806v1

原文摘要:
In recent years, pretrained neural language models (PNLMs) have taken the
field of natural language processing by storm, achieving new benchmarks and
state-of-the-art performances. These models often rely heavily on annotated
data, which may not always be available. Data scarcity are commonly found in
specialized domains, such as medical, or in low-resource languages that are
underexplored by AI research. In this dissertation, we focus on mitigating data
scarcity using data augmentation and neural ensemble learning techniques for
neural language models. In both research directions, we implement neural
network algorithms and evaluate their impact on assisting neural language
models in downstream NLP tasks. Specifically, for data augmentation, we explore
two techniques: 1) creating positive training data by moving an answer span
around its original context and 2) using text simplification techniques to
introduce a variety of writing styles to the original training data. Our
results indicate that these simple and effective solutions improve the
performance of neural language models considerably in low-resource NLP domains
and tasks. For neural ensemble learning, we use a multilabel neural classifier
to select the best prediction outcome from a variety of individual pretrained
neural language models trained for a low-resource medical text simplification
task.

中文翻译:
近年来，预训练神经语言模型（PNLMs）以破竹之势席卷自然语言处理领域，不断刷新基准测试并实现最先进的性能表现。这类模型通常高度依赖标注数据，但此类数据往往难以获取。数据稀缺现象在医疗等专业领域以及人工智能研究尚未充分探索的低资源语言中尤为普遍。本论文聚焦于通过数据增强和神经集成学习技术来缓解神经语言模型面临的数据稀缺问题。在这两个研究方向中，我们实现了神经网络算法，并评估了它们对辅助神经语言模型完成下游NLP任务的影响。具体而言，在数据增强方面，我们探索了两种技术：1）通过将答案片段在其原始上下文中移位来生成正向训练数据；2）运用文本简化技术为原始训练数据引入多样化的文本风格。研究结果表明，这些简单高效的解决方案能显著提升神经语言模型在低资源NLP领域及任务中的表现。在神经集成学习方面，我们采用多标签神经分类器，从多个针对低资源医疗文本简化任务单独训练的预训练神经语言模型中筛选最优预测结果。
