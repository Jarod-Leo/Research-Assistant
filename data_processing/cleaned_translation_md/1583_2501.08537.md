# Complexity Control Facilitates Reasoning-Based Compositional Generalization in Transformers

链接: http://arxiv.org/abs/2501.08537v1

原文摘要:
Transformers have demonstrated impressive capabilities across various tasks,
yet their performance on compositional problems remains a subject of debate. In
this study, we investigate the internal mechanisms underlying Transformers'
behavior in compositional tasks. We find that complexity control strategies
significantly influence whether the model learns primitive-level rules that
generalize out-of-distribution (reasoning-based solutions) or relies solely on
memorized mappings (memory-based solutions). By applying masking strategies to
the model's information circuits and employing multiple complexity metrics, we
reveal distinct internal working mechanisms associated with different solution
types. Further analysis reveals that reasoning-based solutions exhibit a lower
complexity bias, which aligns with the well-studied neuron condensation
phenomenon. This lower complexity bias is hypothesized to be the key factor
enabling these solutions to learn reasoning rules. We validate these
conclusions across multiple real-world datasets, including image generation and
natural language processing tasks, confirming the broad applicability of our
findings.

中文翻译:
以下是符合学术规范的中文翻译：

Transformer模型已在多种任务中展现出卓越性能，但其在组合性问题上的表现仍存在争议。本研究揭示了Transformer处理组合任务时的内部机制：复杂度控制策略会显著影响模型是学习具有分布外泛化能力的基元级规则（基于推理的解决方案），还是仅依赖记忆映射（基于记忆的解决方案）。通过采用信息通路掩蔽策略并结合多维度复杂度度量，我们发现了与不同解决方案类型相对应的内部工作机制差异。进一步分析表明，基于推理的解决方案呈现出更低的复杂度偏好，这与学界已深入研究的神经元凝聚现象相吻合。我们推测这种低复杂度偏好正是模型习得推理规则的关键因素。研究结论在图像生成和自然语言处理等多个现实数据集上得到验证，证实了发现的普适性。

翻译说明：
1. 专业术语处理：保持"Transformer"、"out-of-distribution"等术语的学术规范性，采用"分布外泛化"等学界通用译法
2. 句式重构：将英语长句拆分为符合中文表达习惯的短句，如将"we find that..."处理为发现性结论句式
3. 逻辑显化：通过冒号、括号等标点明确原文隐含的逻辑关系
4. 概念对应："neuron condensation phenomenon"译为"神经元凝聚现象"符合认知科学术语规范
5. 学术风格：使用"揭示""证实""推测"等科研论文常用动词，保持学术严谨性
6. 文化适配："broad applicability"译为"普适性"更符合中文论文表述习惯
