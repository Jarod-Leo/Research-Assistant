# Catastrophic Forgetting in LLMs: A Comparative Analysis Across Language Tasks

链接: http://arxiv.org/abs/2504.01241v1

原文摘要:
Large Language Models (LLMs) have significantly advanced Natural Language
Processing (NLP), particularly in Natural Language Understanding (NLU) tasks.
As we progress toward an agentic world where LLM-based agents autonomously
handle specialized tasks, it becomes crucial for these models to adapt to new
tasks without forgetting previously learned information - a challenge known as
catastrophic forgetting. This study evaluates the continual fine-tuning of
various open-source LLMs with different parameter sizes (specifically models
under 10 billion parameters) on key NLU tasks from the GLUE benchmark,
including SST-2, MRPC, CoLA, and MNLI. By employing prompt engineering and
task-specific adjustments, we assess and compare the models' abilities to
retain prior knowledge while learning new tasks. Our results indicate that
models such as Phi-3.5-mini exhibit minimal forgetting while maintaining strong
learning capabilities, making them well-suited for continual learning
environments. Additionally, models like Orca-2-7b and Qwen2.5-7B demonstrate
impressive learning abilities and overall performance after fine-tuning. This
work contributes to understanding catastrophic forgetting in LLMs and
highlights prompting engineering to optimize model performance for continual
learning scenarios.

中文翻译:
以下是符合学术规范的中文翻译：

大语言模型（LLMs）显著推动了自然语言处理（NLP）领域的发展，尤其在自然语言理解（NLU）任务中表现突出。随着我们迈向由LLM智能体自主处理专项任务的代理化世界，这些模型必须能在适应新任务时不遗忘已习得知识——这一挑战被称为灾难性遗忘。本研究评估了多种开源LLM（参数规模均小于100亿）在GLUE基准关键NLU任务（包括SST-2、MRPC、CoLA和MNLI）上的持续微调表现。通过采用提示工程和任务特定调整，我们系统评估比较了模型在学习新任务时保留原有知识的能力。实验结果表明，Phi-3.5-mini等模型在保持强大学习能力的同时仅产生极少量遗忘，特别适合持续学习环境；而Orca-2-7b和Qwen2.5-7B等模型经微调后则展现出卓越的学习能力和综合性能。本研究不仅深化了对LLM灾难性遗忘现象的理解，还揭示了提示工程对持续学习场景下模型性能优化的关键作用。

（翻译说明：1. 专业术语严格对应；2. 长句按中文习惯切分重组；3. 被动语态转为主动表述；4. 保留GLUE等专有名词原称；5. 模型名称维持英文规范写法；6. 学术表述符合中文论文摘要特征）
