# LegalPro-BERT: Classification of Legal Provisions by fine-tuning BERT Large Language Model

链接: http://arxiv.org/abs/2404.10097v1

原文摘要:
A contract is a type of legal document commonly used in organizations.
Contract review is an integral and repetitive process to avoid business risk
and liability. Contract analysis requires the identification and classification
of key provisions and paragraphs within an agreement. Identification and
validation of contract clauses can be a time-consuming and challenging task
demanding the services of trained and expensive lawyers, paralegals or other
legal assistants. Classification of legal provisions in contracts using
artificial intelligence and natural language processing is complex due to the
requirement of domain-specialized legal language for model training and the
scarcity of sufficient labeled data in the legal domain. Using general-purpose
models is not effective in this context due to the use of specialized legal
vocabulary in contracts which may not be recognized by a general model. To
address this problem, we propose the use of a pre-trained large language model
which is subsequently calibrated on legal taxonomy. We propose LegalPro-BERT, a
BERT transformer architecture model that we fine-tune to efficiently handle
classification task for legal provisions. We conducted experiments to measure
and compare metrics with current benchmark results. We found that LegalPro-BERT
outperforms the previous benchmark used for comparison in this research.

中文翻译:
合同是各类组织中广泛使用的法律文书。合同审查作为规避商业风险与责任的必要环节，是一个具有重复性的工作流程。合同分析的核心在于识别并分类协议中的关键条款与段落，然而条款的识别与验证往往耗时费力，需要依赖受过专业训练且费用高昂的律师、法律助理或其他法律从业者。由于法律领域需要专业术语进行模型训练，且标注数据相对匮乏，运用人工智能和自然语言处理技术对合同法律条款进行分类具有较高复杂性。通用模型在此场景下效果欠佳，因其难以识别合同中使用的专业法律术语。针对这一问题，我们提出采用经过法律分类体系校准的预训练大语言模型解决方案。本文提出的LegalPro-BERT模型基于BERT变换器架构，通过微调使其高效处理法律条款分类任务。我们通过实验测量各项指标并与现有基准结果进行比较，发现LegalPro-BERT在本研究对比的所有基准测试中均表现出更优性能。

（翻译说明：采用法律文书专业用语如"条款""法律从业者"等；将被动语态转换为中文主动表达；拆分英文长句为符合中文阅读习惯的短句；保留专业术语BERT/LegalPro-BERT等原名称；通过"具有""往往"等措辞保持学术文本严谨性；使用"针对""解决方案"等体现研究问题导向的表述）
