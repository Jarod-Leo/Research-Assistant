# Small Models, Big Impact: Efficient Corpus and Graph-Based Adaptation of Small Multilingual Language Models for Low-Resource Languages

链接: http://arxiv.org/abs/2502.10140v1

原文摘要:
Low-resource languages (LRLs) face significant challenges in natural language
processing (NLP) due to limited data. While current state-of-the-art large
language models (LLMs) still struggle with LRLs, smaller multilingual models
(mLMs) such as mBERT and XLM-R offer greater promise due to a better fit of
their capacity to low training data sizes. This study systematically
investigates parameter-efficient adapter-based methods for adapting mLMs to
LRLs, evaluating three architectures: Sequential Bottleneck, Invertible
Bottleneck, and Low-Rank Adaptation. Using unstructured text from GlotCC and
structured knowledge from ConceptNet, we show that small adaptation datasets
(e.g., up to 1 GB of free-text or a few MB of knowledge graph data) yield gains
in intrinsic (masked language modeling) and extrinsic tasks (topic
classification, sentiment analysis, and named entity recognition). We find that
Sequential Bottleneck adapters excel in language modeling, while Invertible
Bottleneck adapters slightly outperform other methods on downstream tasks due
to better embedding alignment and larger parameter counts. Adapter-based
methods match or outperform full fine-tuning while using far fewer parameters,
and smaller mLMs prove more effective for LRLs than massive LLMs like LLaMA-3,
GPT-4, and DeepSeek-R1-based distilled models. While adaptation improves
performance, pre-training data size remains the dominant factor, especially for
languages with extensive pre-training coverage.

中文翻译:
以下是符合学术规范的中文翻译：

低资源语言在自然语言处理中面临数据稀缺的显著挑战。尽管当前最先进的大语言模型对低资源语言的处理能力仍显不足，但mBERT、XLM-R等小型多语言模型因其参数量与有限训练数据更匹配而展现出更大潜力。本研究系统探究了基于适配器的参数高效迁移学习方法，评估了三种架构：序列瓶颈适配器、可逆瓶颈适配器及低秩自适应适配器。利用GlotCC的非结构化文本和ConceptNet的结构化知识，我们证明小规模适配数据集（如1GB自由文本或数MB知识图谱数据）即可在内部任务（掩码语言建模）和外部任务（主题分类、情感分析、命名实体识别）上取得提升。研究发现：序列瓶颈适配器在语言建模任务中表现最优，而可逆瓶颈适配器凭借更好的嵌入对齐和更多参数量，在下游任务中略胜一筹。基于适配器的方法在使用极少参数的情况下，性能达到或超越全参数微调；且小型多语言模型对低资源语言的处理效果显著优于LLaMA-3、GPT-4及基于DeepSeek-R1的蒸馏模型等大型语言模型。值得注意的是，虽然适配训练能提升性能，但预训练数据规模仍是主导因素，特别是对预训练覆盖较广的语言而言。

（翻译说明：
1. 专业术语处理：LRLs统一译为"低资源语言"，LLMs/mLMs保留英文缩写但补充全称说明
2. 技术概念转换："parameter-efficient"译为"参数高效"，"adapter-based methods"译为"基于适配器的方法"符合NLP领域术语
3. 长句拆分：将原文复合句按中文表达习惯分解为多个短句，如将"While current..."处理为转折关系复句
4. 被动语态转换："are evaluated"等被动式转为中文主动表述
5. 数据规范：精确保持"1GB"等计量单位与数字准确性
6. 学术风格：使用"显著挑战""略胜一筹"等符合学术文本的措辞，避免口语化表达）
