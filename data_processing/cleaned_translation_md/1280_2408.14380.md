# Probing Causality Manipulation of Large Language Models

链接: http://arxiv.org/abs/2408.14380v1

原文摘要:
Large language models (LLMs) have shown various ability on natural language
processing, including problems about causality. It is not intuitive for LLMs to
command causality, since pretrained models usually work on statistical
associations, and do not focus on causes and effects in sentences. So that
probing internal manipulation of causality is necessary for LLMs. This paper
proposes a novel approach to probe causality manipulation hierarchically, by
providing different shortcuts to models and observe behaviors. We exploit
retrieval augmented generation (RAG) and in-context learning (ICL) for models
on a designed causality classification task. We conduct experiments on
mainstream LLMs, including GPT-4 and some smaller and domain-specific models.
Our results suggest that LLMs can detect entities related to causality and
recognize direct causal relationships. However, LLMs lack specialized cognition
for causality, merely treating them as part of the global semantic of the
sentence.

中文翻译:
大语言模型（LLMs）在自然语言处理领域展现出多样能力，包括因果关联问题的处理。由于预训练模型通常基于统计关联运作，而非专注于句子中的因果关系，因此LLMs掌握因果性并非直观。这就有必要探究LLMs内部的因果机制操控。本文提出一种分层探测因果操控的新方法，通过为模型提供不同"捷径"并观察其行为展开研究。我们在设计的因果分类任务中，采用检索增强生成（RAG）和上下文学习（ICL）技术对模型进行测试。实验对象涵盖GPT-4等主流大模型及若干小型领域专用模型。结果表明：LLMs能够识别与因果相关的实体并判断直接因果关系，但缺乏对因果关系的专门化认知，仅将其视为句子全局语义的组成部分。
