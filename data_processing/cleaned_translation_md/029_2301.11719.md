# Incorporating Knowledge into Document Summarization: an Application of Prefix-Tuning on GPT-2

链接: http://arxiv.org/abs/2301.11719v1

原文摘要:
Despite the great development of document summarisation techniques nowadays,
factual inconsistencies between the generated summaries and the original texts
still occur from time to time. This study explores the possibility of adopting
prompts to incorporate factual knowledge into generated summaries. We
specifically study prefix-tuning that uses a set of trainable continuous prefix
prompts together with discrete natural language prompts to aid summary
generation. Experimental results demonstrate that the trainable prefixes can
help the summarisation model extract information from discrete prompts
precisely, thus generating knowledge-preserving summaries that are factually
consistent with the discrete prompts. The ROUGE improvements of the generated
summaries indicate that explicitly adding factual knowledge into the
summarisation process could boost the overall performance, showing great
potential for applying it to other natural language processing tasks.

中文翻译:
尽管当前文档摘要技术已取得长足发展，生成摘要与原文之间仍不时存在事实性不一致的问题。本研究探索了采用提示机制将事实性知识融入生成摘要的可能性，重点研究了前缀调优技术——该方法通过组合可训练的连续前缀提示与离散的自然语言提示来辅助摘要生成。实验结果表明，可训练前缀能有效帮助摘要模型精准提取离散提示中的信息，从而生成与离散提示保持事实一致的知识保留型摘要。生成摘要的ROUGE指标提升表明，在摘要过程中显式加入事实性知识可提升整体性能，这为将其应用于其他自然语言处理任务展现了巨大潜力。

（翻译说明：
1. 专业术语处理："prefix-tuning"译为"前缀调优"，"discrete prompts"译为"离散提示"，保持计算机领域术语规范
2. 句式重构：将英语长句拆分为符合中文表达习惯的短句，如原文第二句拆分为主从复合结构
3. 被动语态转换："be factually consistent with"译为主动式"保持事实一致"
4. 概念显化："knowledge-preserving"意译为"知识保留型"而非直译，更符合中文技术文献表述
5. 数据指标保留：ROUGE作为国际通用指标名称保留不译
6. 学术风格保持：使用"显式""展现潜力"等符合学术论文表达的措辞）
