# Sometimes Painful but Certainly Promising: Feasibility and Trade-offs of Language Model Inference at the Edge

链接: http://arxiv.org/abs/2503.09114v1

原文摘要:
The rapid rise of Language Models (LMs) has expanded the capabilities of
natural language processing, powering applications from text generation to
complex decision-making. While state-of-the-art LMs often boast hundreds of
billions of parameters and are primarily deployed in data centers, recent
trends show a growing focus on compact models-typically under 10 billion
parameters-enabled by techniques such as quantization and other model
compression techniques. This shift paves the way for LMs on edge devices,
offering potential benefits such as enhanced privacy, reduced latency, and
improved data sovereignty. However, the inherent complexity of even these
smaller models, combined with the limited computing resources of edge hardware,
raises critical questions about the practical trade-offs in executing LM
inference outside the cloud. To address these challenges, we present a
comprehensive evaluation of generative LM inference on representative CPU-based
and GPU-accelerated edge devices. Our study measures key performance
indicators-including memory usage, inference speed, and energy
consumption-across various device configurations. Additionally, we examine
throughput-energy trade-offs, cost considerations, and usability, alongside an
assessment of qualitative model performance. While quantization helps mitigate
memory overhead, it does not fully eliminate resource bottlenecks, especially
for larger models. Our findings quantify the memory and energy constraints that
must be considered for practical real-world deployments, offering concrete
insights into the trade-offs between model size, inference performance, and
efficiency. The exploration of LMs at the edge is still in its early stages. We
hope this study provides a foundation for future research, guiding the
refinement of models, the enhancement of inference efficiency, and the
advancement of edge-centric AI systems.

中文翻译:
语言模型（LMs）的迅速崛起拓展了自然语言处理的能力边界，其应用场景已从文本生成延伸至复杂决策。尽管最先进的语言模型通常拥有数千亿参数且主要部署在数据中心，但当前趋势显示，通过量化和模型压缩等技术实现的紧凑型模型（通常参数规模在百亿以下）正日益受到关注。这一转变为边缘设备部署语言模型铺平了道路，有望带来增强隐私保护、降低延迟、提升数据主权等优势。然而，即便这些小型模型也具备固有复杂性，加之边缘硬件有限的计算资源，使得在云端之外执行语言模型推理时面临关键的实际权衡问题。

为应对这些挑战，本研究对基于CPU和GPU加速的代表性边缘设备开展生成式语言模型推理的全面评估。我们测量了不同设备配置下的关键性能指标，包括内存占用、推理速度和能耗，同时分析了吞吐量与能耗的权衡关系、成本考量及可用性，并对模型定性表现进行评估。研究发现：量化技术虽能缓解内存压力，但无法完全消除资源瓶颈（尤其对较大模型而言）。我们的实验数据量化了实际部署中必须考虑的内存与能耗限制，为模型规模、推理性能与效率之间的权衡提供了具体依据。

边缘计算环境下的语言模型探索仍处于早期阶段。本研究旨在为未来研究奠定基础，推动模型优化、推理效率提升及边缘人工智能系统的发展。
