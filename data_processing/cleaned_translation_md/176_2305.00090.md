# NLNDE at SemEval-2023 Task 12: Adaptive Pretraining and Source Language Selection for Low-Resource Multilingual Sentiment Analysis

链接: http://arxiv.org/abs/2305.00090v1

原文摘要:
This paper describes our system developed for the SemEval-2023 Task 12
"Sentiment Analysis for Low-resource African Languages using Twitter Dataset".
Sentiment analysis is one of the most widely studied applications in natural
language processing. However, most prior work still focuses on a small number
of high-resource languages. Building reliable sentiment analysis systems for
low-resource languages remains challenging, due to the limited training data in
this task. In this work, we propose to leverage language-adaptive and
task-adaptive pretraining on African texts and study transfer learning with
source language selection on top of an African language-centric pretrained
language model. Our key findings are: (1) Adapting the pretrained model to the
target language and task using a small yet relevant corpus improves performance
remarkably by more than 10 F1 score points. (2) Selecting source languages with
positive transfer gains during training can avoid harmful interference from
dissimilar languages, leading to better results in multilingual and
cross-lingual settings. In the shared task, our system wins 8 out of 15 tracks
and, in particular, performs best in the multilingual evaluation.

中文翻译:
本文介绍了我们为SemEval-2023任务12"基于推特数据集的非洲低资源语言情感分析"开发的系统。情感分析是自然语言处理中研究最广泛的应用之一，但现有工作仍主要集中于少数高资源语言。由于训练数据有限，为低资源语言构建可靠的情感分析系统仍具挑战性。本研究提出对非洲语料进行语言自适应和任务自适应的预训练，并在以非洲语言为核心的预训练语言模型基础上，研究源语言选择对迁移学习的影响。主要发现包括：（1）使用小规模但相关的语料对预训练模型进行目标语言和任务适配，可使F1值显著提升10分以上；（2）在训练过程中选择具有正向迁移增益的源语言，可避免不相似语言的有害干扰，从而在多语言和跨语言场景中获得更好效果。在竞赛中，我们的系统在15个赛道中赢得8项冠军，尤其在多语言评估中表现最佳。
