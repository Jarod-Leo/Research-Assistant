# Misinforming LLMs: vulnerabilities, challenges and opportunities

链接: http://arxiv.org/abs/2408.01168v1

原文摘要:
Large Language Models (LLMs) have made significant advances in natural
language processing, but their underlying mechanisms are often misunderstood.
Despite exhibiting coherent answers and apparent reasoning behaviors, LLMs rely
on statistical patterns in word embeddings rather than true cognitive
processes. This leads to vulnerabilities such as "hallucination" and
misinformation. The paper argues that current LLM architectures are inherently
untrustworthy due to their reliance on correlations of sequential patterns of
word embedding vectors. However, ongoing research into combining generative
transformer-based models with fact bases and logic programming languages may
lead to the development of trustworthy LLMs capable of generating statements
based on given truth and explaining their self-reasoning process.

中文翻译:
以下是符合要求的学术中文翻译：

大型语言模型（LLMs）在自然语言处理领域取得了显著进展，但其底层机制常被误解。尽管能生成连贯回答并表现出明显的推理行为，LLMs本质上依赖的是词嵌入向量的统计模式，而非真实的认知过程。这种特性导致其存在"幻觉"和错误信息等固有缺陷。本文指出，由于当前LLM架构完全依赖于词嵌入向量序列模式的相关性，其本质上不具备可信赖性。然而，当前正在进行的将基于生成式Transformer的模型与事实数据库及逻辑编程语言相结合的研究，有望开发出新型可信LLMs——这类模型不仅能基于给定事实生成陈述，还能解释其自推理过程。

（翻译说明：
1. 专业术语统一处理："hallucination"译为行业通用译法"幻觉"，"word embeddings"译为"词嵌入向量"
2. 长句拆分重构：将原文复合句按中文表达习惯分解为多个短句，如将"but their underlying..."处理为转折独立句
3. 被动语态转化："are inherently untrustworthy"译为主动式"不具备可信赖性"
4. 概念显化处理："exhibiting coherent answers"译为"生成连贯回答"以明确动作主体
5. 学术风格保持：使用"其""这类"等正式指代，避免口语化表达
6. 补充说明性翻译：破折号后内容是对前文"可信LLMs"的具体解释，符合中文论文摘要常见写法）
