# Intra-Layer Recurrence in Transformers for Language Modeling

链接: http://arxiv.org/abs/2505.01855v1

原文摘要:
Transformer models have established new benchmarks in natural language
processing; however, their increasing depth results in substantial growth in
parameter counts. While existing recurrent transformer methods address this
issue by reprocessing layers multiple times, they often apply recurrence
indiscriminately across entire blocks of layers. In this work, we investigate
Intra-Layer Recurrence (ILR), a more targeted approach that applies recurrence
selectively to individual layers within a single forward pass. Our experiments
show that allocating more iterations to earlier layers yields optimal results.
These findings suggest that ILR offers a promising direction for optimizing
recurrent structures in transformer architectures.

中文翻译:
Transformer模型在自然语言处理领域树立了新的性能标杆，但其不断增加的深度导致参数量急剧膨胀。现有循环Transformer方法虽通过多次重处理层来缓解这一问题，却往往不加区分地对整个层块实施循环机制。本研究提出了一种更具针对性的方法——层内循环（ILR），该方法在前向传播过程中选择性地对单个层实施循环处理。实验表明，将更多迭代次数分配给早期层能获得最佳效果。这些发现证明，ILR为优化Transformer架构中的循环结构提供了极具前景的研究方向。

（翻译说明：
1. 专业术语处理："benchmarks"译为"性能标杆"符合计算机领域表述，"parameter counts"译为"参数量"更符合中文习惯
2. 句式重构：将原文复合句拆分为符合中文表达习惯的短句，如将"they often apply..."独立成句
3. 被动语态转换：将"recurrence is applied"主动化为"实施循环处理"
4. 概念准确传达："single forward pass"专业译为"前向传播过程"
5. 学术风格保持：使用"本研究""实验表明"等规范学术表达
6. 逻辑衔接优化：通过"虽...却..."等连词增强行文连贯性）
