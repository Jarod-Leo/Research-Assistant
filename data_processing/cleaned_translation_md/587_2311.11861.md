# Generating Valid and Natural Adversarial Examples with Large Language Models

链接: http://arxiv.org/abs/2311.11861v1

原文摘要:
Deep learning-based natural language processing (NLP) models, particularly
pre-trained language models (PLMs), have been revealed to be vulnerable to
adversarial attacks. However, the adversarial examples generated by many
mainstream word-level adversarial attack models are neither valid nor natural,
leading to the loss of semantic maintenance, grammaticality, and human
imperceptibility. Based on the exceptional capacity of language understanding
and generation of large language models (LLMs), we propose LLM-Attack, which
aims at generating both valid and natural adversarial examples with LLMs. The
method consists of two stages: word importance ranking (which searches for the
most vulnerable words) and word synonym replacement (which substitutes them
with their synonyms obtained from LLMs). Experimental results on the Movie
Review (MR), IMDB, and Yelp Review Polarity datasets against the baseline
adversarial attack models illustrate the effectiveness of LLM-Attack, and it
outperforms the baselines in human and GPT-4 evaluation by a significant
margin. The model can generate adversarial examples that are typically valid
and natural, with the preservation of semantic meaning, grammaticality, and
human imperceptibility.

中文翻译:
基于深度学习的自然语言处理（NLP）模型，尤其是预训练语言模型（PLMs），已被证实易受对抗性攻击。然而，许多主流词级对抗攻击模型生成的对抗样本往往缺乏有效性和自然性，导致语义保持、语法规范及人类不易察觉性等特性缺失。基于大语言模型（LLMs）卓越的语言理解与生成能力，我们提出LLM-Attack方法，旨在利用LLMs生成既有效又自然的对抗样本。该方法包含两个阶段：词重要性排序（定位最脆弱的词汇）和同义词替换（通过LLMs获取的同义词进行替换）。在Movie Review（MR）、IMDB和Yelp Review Polarity数据集上针对基线对抗攻击模型的实验结果表明，LLM-Attack显著优于基线模型，在人工评估和GPT-4评估中均展现出明显优势。该模型生成的对抗样本通常具有有效性、自然性，并能保持语义一致性、语法正确性及人类不易察觉性。
