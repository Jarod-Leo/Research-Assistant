# GET: A Generative EEG Transformer for continuous context-based neural

链接: http://arxiv.org/abs/2406.03115v1

原文摘要:
Generating continuous electroencephalography (EEG) signals through advanced
artificial neural networks presents a novel opportunity to enhance
brain-computer interface (BCI) technology. This capability has the potential to
significantly enhance applications ranging from simulating dynamic brain
activity and data augmentation to improving real-time epilepsy detection and
BCI inference. By harnessing generative transformer neural networks,
specifically designed for EEG signal generation, we can revolutionize the
interpretation and interaction with neural data. Generative AI has demonstrated
significant success across various domains, from natural language processing
(NLP) and computer vision to content creation in visual arts and music. It
distinguishes itself by using large-scale datasets to construct context windows
during pre-training, a technique that has proven particularly effective in NLP,
where models are fine-tuned for specific downstream tasks after extensive
foundational training. However, the application of generative AI in the field
of BCIs, particularly through the development of continuous, context-rich
neural signal generators, has been limited. To address this, we introduce the
Generative EEG Transformer (GET), a model leveraging transformer architecture
tailored for EEG data. The GET model is pre-trained on diverse EEG datasets,
including motor imagery and alpha wave datasets, enabling it to produce
high-fidelity neural signals that maintain contextual integrity. Our empirical
findings indicate that GET not only faithfully reproduces the frequency
spectrum of the training data and input prompts but also robustly generates
continuous neural signals. By adopting the successful training strategies of
the NLP domain for BCIs, the GET sets a new standard for the development and
application of neural signal generation technologies.

中文翻译:
通过先进的人工神经网络生成连续脑电图（EEG）信号，为提升脑机接口（BCI）技术开辟了新途径。这一突破性能力有望显著增强多项应用场景——从模拟动态大脑活动、数据增强到优化实时癫痫检测及BCI推理系统。我们采用专为EEG信号生成设计的生成式Transformer神经网络，或将彻底革新神经数据的解读与交互方式。

生成式AI已在自然语言处理（NLP）、计算机视觉、视觉艺术与音乐创作等领域取得显著成就。其核心优势在于预训练阶段利用大规模数据集构建上下文窗口，这一技术在NLP领域尤为成功——模型经过基础训练后可通过微调适配具体下游任务。然而在BCI领域，特别是开发能产生连续、富含上下文信息的神经信号生成器方面，生成式AI的应用仍属空白。

为此，我们提出生成式EEG Transformer（GET）模型。该模型采用适配EEG数据的Transformer架构，在运动想象和α波等多类EEG数据集上进行预训练，可生成保持上下文完整性的高保真神经信号。实验结果表明：GET不仅能精准复现训练数据与输入提示的频谱特征，更能稳定生成连续神经信号。通过将NLP领域的成功训练策略迁移至BCI领域，GET为神经信号生成技术的研发与应用树立了新标杆。
