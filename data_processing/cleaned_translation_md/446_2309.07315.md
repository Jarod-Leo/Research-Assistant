# Traveling Words: A Geometric Interpretation of Transformers

链接: http://arxiv.org/abs/2309.07315v1

原文摘要:
Transformers have significantly advanced the field of natural language
processing, but comprehending their internal mechanisms remains a challenge. In
this paper, we introduce a novel geometric perspective that elucidates the
inner mechanisms of transformer operations. Our primary contribution is
illustrating how layer normalization confines the latent features to a
hyper-sphere, subsequently enabling attention to mold the semantic
representation of words on this surface. This geometric viewpoint seamlessly
connects established properties such as iterative refinement and contextual
embeddings. We validate our insights by probing a pre-trained 124M parameter
GPT-2 model. Our findings reveal clear query-key attention patterns in early
layers and build upon prior observations regarding the subject-specific nature
of attention heads at deeper layers. Harnessing these geometric insights, we
present an intuitive understanding of transformers, depicting them as processes
that model the trajectory of word particles along the hyper-sphere.

中文翻译:
Transformer模型显著推动了自然语言处理领域的发展，但其内部运作机制的理解仍存在挑战。本文提出一种新颖的几何视角，用以阐释Transformer操作的内部机理。我们的核心贡献在于揭示层归一化如何将潜在特征约束在超球面上，进而使注意力机制能够在此曲面塑造词语的语义表征。这一几何视角有机整合了迭代优化、上下文嵌入等已知特性。我们通过分析预训练的1.24亿参数GPT-2模型验证了这些发现：早期层呈现清晰的查询-键注意力模式，而深层注意力头则延续了先前研究观察到的主题特异性。基于这些几何洞见，我们提出了对Transformer的直观理解——将其视为模拟词语粒子沿超球面运动轨迹的动态过程。

（翻译说明：
1. 专业术语处理："hyper-sphere"译为"超球面"，"layer normalization"保留专业表述"层归一化"
2. 句式重构：将英语长句拆分为符合中文表达习惯的短句，如原文最后一句分译为两个层次
3. 概念显化："iterative refinement"译为"迭代优化"，"contextual embeddings"译为"上下文嵌入"
4. 动态表达："model the trajectory"译为"模拟...运动轨迹"，通过动词化处理增强可读性
5. 学术风格：使用"机理""表征""洞见"等符合中文论文摘要的学术词汇
6. 数据规范：124M统一转换为"1.24亿"符合中文数字表达习惯）
