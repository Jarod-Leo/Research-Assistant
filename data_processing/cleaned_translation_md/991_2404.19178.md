# Revenge of the Fallen? Recurrent Models Match Transformers at Predicting Human Language Comprehension Metrics

链接: http://arxiv.org/abs/2404.19178v1

原文摘要:
Transformers have generally supplanted recurrent neural networks as the
dominant architecture for both natural language processing tasks and for
modelling the effect of predictability on online human language comprehension.
However, two recently developed recurrent model architectures, RWKV and Mamba,
appear to perform natural language tasks comparably to or better than
transformers of equivalent scale. In this paper, we show that contemporary
recurrent models are now also able to match - and in some cases, exceed - the
performance of comparably sized transformers at modeling online human language
comprehension. This suggests that transformer language models are not uniquely
suited to this task, and opens up new directions for debates about the extent
to which architectural features of language models make them better or worse
models of human language comprehension.

中文翻译:
Transformer模型已普遍取代循环神经网络，成为自然语言处理任务及预测性对人类在线语言理解影响建模的主导架构。然而，近期开发的两种循环模型架构——RWKV和Mamba——在自然语言任务上的表现已可比拟甚至超越同等规模的Transformer模型。本文研究表明，当代循环模型在模拟人类在线语言理解任务时，其性能已能与同体量Transformer模型匹敌（某些情况下甚至更优）。这一发现表明Transformer语言模型并非该任务的唯一适用架构，同时也为新一轮学术讨论开辟了空间：语言模型的架构特征究竟在何种程度上使其更优或更劣于人类语言理解的建模。
