# Empowering Few-Shot Relation Extraction with The Integration of Traditional RE Methods and Large Language Models

链接: http://arxiv.org/abs/2407.08967v1

原文摘要:
Few-Shot Relation Extraction (FSRE), a subtask of Relation Extraction (RE)
that utilizes limited training instances, appeals to more researchers in
Natural Language Processing (NLP) due to its capability to extract textual
information in extremely low-resource scenarios. The primary methodologies
employed for FSRE have been fine-tuning or prompt tuning techniques based on
Pre-trained Language Models (PLMs). Recently, the emergence of Large Language
Models (LLMs) has prompted numerous researchers to explore FSRE through
In-Context Learning (ICL). However, there are substantial limitations
associated with methods based on either traditional RE models or LLMs.
Traditional RE models are hampered by a lack of necessary prior knowledge,
while LLMs fall short in their task-specific capabilities for RE. To address
these shortcomings, we propose a Dual-System Augmented Relation Extractor
(DSARE), which synergistically combines traditional RE models with LLMs.
Specifically, DSARE innovatively injects the prior knowledge of LLMs into
traditional RE models, and conversely enhances LLMs' task-specific aptitude for
RE through relation extraction augmentation. Moreover, an Integrated Prediction
module is employed to jointly consider these two respective predictions and
derive the final results. Extensive experiments demonstrate the efficacy of our
proposed method.

中文翻译:
以下是符合要求的学术中文翻译：

小样本关系抽取（Few-Shot Relation Extraction, FSRE）作为关系抽取（RE）中利用有限训练实例的子任务，因其能在极低资源场景下提取文本信息的能力，日益受到自然语言处理（NLP）领域研究者的关注。当前FSRE主要采用基于预训练语言模型（PLMs）的微调或提示调优技术。随着大语言模型（LLMs）的出现，许多研究者开始通过上下文学习（ICL）探索FSRE的实现。然而，无论是传统RE模型还是LLMs方案都存在显著局限：传统RE模型受限于先验知识不足，而LLMs则缺乏针对关系抽取的任务专项能力。为克服这些缺陷，我们提出双系统增强关系抽取器（Dual-System Augmented Relation Extractor, DSARE），通过协同整合传统RE模型与LLMs实现优势互补。具体而言，DSARE创新性地将LLMs的先验知识注入传统RE模型，同时通过关系抽取增强机制提升LLMs的任务专项能力。此外，系统采用集成预测模块综合考量双系统的预测结果以生成最终判定。大量实验验证了本方法的有效性。

翻译说明：
1. 专业术语处理：采用"小样本关系抽取"、"上下文学习"等学界通用译法，首字母缩写在首次出现时保留英文全称
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句（如将"which synergistically..."处理为独立分句）
3. 被动语态转换：将"are hampered by"等被动结构转为"受限于"等主动表述
4. 学术风格保持：使用"协同整合"、"优势互补"等学术用语，避免口语化表达
5. 逻辑显化：通过"具体而言"、"此外"等连接词强化论文的逻辑递进关系
6. 术语一致性：全篇统一"LLMs"、"RE"等术语的译法
