# When Every Token Counts: Optimal Segmentation for Low-Resource Language Models

链接: http://arxiv.org/abs/2412.06926v1

原文摘要:
Traditional greedy tokenization methods have been a critical step in Natural
Language Processing (NLP), influencing how text is converted into tokens and
directly impacting model performance. While subword tokenizers like Byte-Pair
Encoding (BPE) are widely used, questions remain about their optimality across
model scales and languages. In this work, we demonstrate through extensive
experiments that an optimal BPE configuration significantly reduces token count
compared to greedy segmentation, yielding improvements in token-saving
percentages and performance benefits, particularly for smaller models. We
evaluate tokenization performance across various intrinsic and extrinsic tasks,
including generation and classification. Our findings suggest that
compression-optimized tokenization strategies could provide substantial
advantages for multilingual and low-resource language applications,
highlighting a promising direction for further research and inclusive NLP.

中文翻译:
传统贪婪分词方法一直是自然语言处理（NLP）中的关键环节，其将文本转化为词元的方式直接影响模型性能。尽管字节对编码（BPE）等子词分词器已被广泛采用，但针对不同模型规模与语言场景的最优分词策略仍存疑问。本研究通过大量实验证明：相较于贪婪分割策略，优化配置的BPE方案能显著降低词元数量，在词元节约百分比和模型性能（尤其是小规模模型）方面均带来提升。我们从生成、分类等内外任务维度评估了分词性能，结果表明：以压缩优化为导向的分词策略可为多语言及低资源语言应用带来显著优势，这为未来构建包容性NLP体系指明了有价值的研究方向。

（翻译说明：
1. 专业术语处理："tokenization"译为"分词"，"token"译为"词元"符合NLP领域惯例
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句，如将"questions remain..."处理为转折句式
3. 概念显化："intrinsic and extrinsic tasks"意译为"内外任务维度"以保持专业性的同时提升可读性
4. 动态对等："inclusive NLP"译为"包容性NLP体系"准确传达原文的社会技术内涵
5. 数据呈现："token-saving percentages"译为"词元节约百分比"保持量化表述的精确性
6. 学术风格：使用"本研究""结果表明"等符合中文论文摘要的规范表述）
