# CAPO: Cost-Aware Prompt Optimization

链接: http://arxiv.org/abs/2504.16005v1

原文摘要:
Large language models (LLMs) have revolutionized natural language processing
by solving a wide range of tasks simply guided by a prompt. Yet their
performance is highly sensitive to prompt formulation. While automated prompt
optimization addresses this challenge by finding optimal prompts, current
methods require a substantial number of LLM calls and input tokens, making
prompt optimization expensive. We introduce CAPO (Cost-Aware Prompt
Optimization), an algorithm that enhances prompt optimization efficiency by
integrating AutoML techniques. CAPO is an evolutionary approach with LLMs as
operators, incorporating racing to save evaluations and multi-objective
optimization to balance performance with prompt length. It jointly optimizes
instructions and few-shot examples while leveraging task descriptions for
improved robustness. Our extensive experiments across diverse datasets and LLMs
demonstrate that CAPO outperforms state-of-the-art discrete prompt optimization
methods in 11/15 cases with improvements up to 21%p. Our algorithm achieves
better performances already with smaller budgets, saves evaluations through
racing, and decreases average prompt length via a length penalty, making it
both cost-efficient and cost-aware. Even without few-shot examples, CAPO
outperforms its competitors and generally remains robust to initial prompts.
CAPO represents an important step toward making prompt optimization more
powerful and accessible by improving cost-efficiency.

中文翻译:
以下是符合您要求的中文翻译：

大型语言模型（LLMs）通过仅需提示引导即可解决各类任务，彻底改变了自然语言处理领域。然而其性能表现对提示表述极为敏感。虽然自动提示优化技术能通过寻找最优提示来解决这一问题，但现有方法需要消耗大量LLM调用次数和输入标记，导致优化成本高昂。我们提出CAPO（成本感知型提示优化算法），该算法通过整合自动机器学习（AutoML）技术来提升提示优化效率。CAPO是一种以LLM为操作符的进化算法，其特点包括：采用竞速机制减少评估次数，运用多目标优化平衡性能与提示长度，能同步优化指令说明和少样本示例，并利用任务描述提升鲁棒性。我们在多样化数据集和不同LLM上的实验表明，CAPO在11/15的案例中优于当前最先进的离散提示优化方法，最高提升达21个百分点。该算法能在较小预算下实现更优性能，通过竞速机制节省评估次数，并利用长度惩罚降低平均提示长度，兼具成本效益与成本感知特性。即使不使用少样本示例，CAPO仍能超越同类方法，且对初始提示普遍保持稳健性。通过显著提升成本效益，CAPO为推动提示优化技术迈向更强大、更易用的未来迈出了重要一步。

翻译说明：
1. 专业术语处理：LLMs统一译为"大型语言模型"，AutoML保留英文缩写但补充全称"自动机器学习"，"few-shot examples"译为专业术语"少样本示例"
2. 技术概念转化："racing"译为"竞速机制"，"multi-objective optimization"译为"多目标优化"，符合中文计算机领域表述习惯
3. 句式重构：将英语长句拆分为符合中文表达习惯的短句，如将"an evolutionary approach with LLMs as operators"处理为"以LLM为操作符的进化算法"
4. 数据呈现优化："21%p"译为"21个百分点"，符合中文数据表述规范
5. 被动语态转换："is highly sensitive"译为主动式"对...极为敏感"
6. 学术风格保持：使用"鲁棒性""同步优化""成本效益"等学术用语，确保专业度
7. 逻辑连接处理：通过"其特点包括""并利用"等连接词保持论证连贯性
