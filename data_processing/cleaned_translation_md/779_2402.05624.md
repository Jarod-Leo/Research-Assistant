# Efficient Models for the Detection of Hate, Abuse and Profanity

链接: http://arxiv.org/abs/2402.05624v1

原文摘要:
Large Language Models (LLMs) are the cornerstone for many Natural Language
Processing (NLP) tasks like sentiment analysis, document classification, named
entity recognition, question answering, summarization, etc. LLMs are often
trained on data which originates from the web. This data is prone to having
content with Hate, Abuse and Profanity (HAP). For a detailed definition of HAP,
please refer to the Appendix. Due to the LLMs being exposed to HAP content
during training, the models learn it and may then generate hateful or profane
content. For example, when the open-source RoBERTa model (specifically, the
RoBERTA base model) from the HuggingFace (HF) Transformers library is prompted
to replace the mask token in `I do not know that Persian people are that MASK`
it returns the word `stupid` with the highest score. This is unacceptable in
civil discourse.The detection of Hate, Abuse and Profanity in text is a vital
component of creating civil and unbiased LLMs, which is needed not only for
English, but for all languages. In this article, we briefly describe the
creation of HAP detectors and various ways of using them to make models civil
and acceptable in the output they generate.

中文翻译:
以下是符合要求的学术中文翻译：

大型语言模型（LLMs）已成为情感分析、文档分类、命名实体识别、问答系统、文本摘要等自然语言处理（NLP）任务的核心技术。这类模型通常基于网络数据进行训练，而这些数据往往包含仇恨言论、侮辱性内容和污秽用语（Hate, Abuse and Profanity, HAP）。关于HAP的详细定义请参阅附录。由于LLMs在训练过程中接触了HAP内容，模型可能习得此类表达并生成具有攻击性或低俗的内容。例如，当要求HuggingFace（HF）Transformers库中的开源RoBERTa模型（具体为RoBERTa-base版本）补全句子"I do not know that Persian people are that MASK"时，模型会以最高置信度返回"stupid"一词——这在文明对话中是完全不可接受的。

文本中仇恨言论、侮辱性内容和污秽用语的检测是构建文明、无偏见大型语言模型的关键环节，这一需求不仅适用于英语，也涵盖所有语言。本文简要阐述了HAP检测器的构建方法，以及如何运用这些检测器来确保模型输出内容符合文明规范与社会接受度。


