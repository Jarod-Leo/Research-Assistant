# Elo Uncovered: Robustness and Best Practices in Language Model Evaluation

链接: http://arxiv.org/abs/2311.17295v1

原文摘要:
In Natural Language Processing (NLP), the Elo rating system, originally
designed for ranking players in dynamic games such as chess, is increasingly
being used to evaluate Large Language Models (LLMs) through "A vs B" paired
comparisons. However, while popular, the system's suitability for assessing
entities with constant skill levels, such as LLMs, remains relatively
unexplored. We study two fundamental axioms that evaluation methods should
adhere to: reliability and transitivity. We conduct extensive evaluation of Elo
behaviour, illustrating that individual Elo computations exhibit volatility and
delving into the impact of varying the Elo rating system's hyperparameters. We
show that these axioms are not always satisfied raising questions about the
reliability of current comparative evaluations of LLMs. If the current use of
Elo scores is intended to substitute the costly head-to-head comparison of
LLMs, it is crucial to ensure the ranking is as robust as possible. Guided by
the axioms, our findings offer concrete guidelines for enhancing the
reliability of LLM evaluation methods, suggesting a need for reassessment of
existing comparative approaches.

中文翻译:
在自然语言处理（NLP）领域，原本为国际象棋等动态博弈设计的Elo评分系统，正日益频繁地通过"A vs B"配对比较方式用于评估大语言模型（LLMs）。然而尽管该方法广受欢迎，这种系统对于LLMs这类技能水平恒定的实体的适用性仍缺乏充分研究。我们针对评估方法应遵循的两项基本准则——可靠性与传递性展开研究，通过大量实验分析Elo系统的行为特征，证明单个Elo计算结果存在波动性，并深入探究不同超参数设置对系统的影响。研究表明这些准则并非总能得到满足，这对当前LLMs比较评估的可靠性提出了质疑。若当前Elo评分的应用旨在替代成本高昂的LLMs直接对抗比较，那么确保排名系统的稳健性至关重要。基于这些准则，我们的研究结果为提升LLM评估方法的可靠性提供了具体指导，表明有必要对现有比较评估方法进行重新审视。

（翻译说明：
1. 专业术语处理：保留"NLP/LLMs"等缩写首次出现时的全称，采用"大语言模型"等学界通用译法
2. 长句拆分：将原文复合句按中文表达习惯分解为多个短句，如将"illustrating that..."独立成句
3. 被动语态转换：将"remain unexplored"等被动结构转为"缺乏充分研究"的主动表述
4. 概念对等："axioms"译为"准则"而非字面"公理"，更符合计算机领域语境
5. 逻辑显化：添加"研究表明"等衔接词，使论证关系更清晰
6. 术语统一：全篇保持"可靠性/robustness"等核心概念译法一致性）
