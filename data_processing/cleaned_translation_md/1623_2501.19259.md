# Neuro-LIFT: A Neuromorphic, LLM-based Interactive Framework for Autonomous Drone FlighT at the Edge

链接: http://arxiv.org/abs/2501.19259v1

原文摘要:
The integration of human-intuitive interactions into autonomous systems has
been limited. Traditional Natural Language Processing (NLP) systems struggle
with context and intent understanding, severely restricting human-robot
interaction. Recent advancements in Large Language Models (LLMs) have
transformed this dynamic, allowing for intuitive and high-level communication
through speech and text, and bridging the gap between human commands and
robotic actions. Additionally, autonomous navigation has emerged as a central
focus in robotics research, with artificial intelligence (AI) increasingly
being leveraged to enhance these systems. However, existing AI-based navigation
algorithms face significant challenges in latency-critical tasks where rapid
decision-making is critical. Traditional frame-based vision systems, while
effective for high-level decision-making, suffer from high energy consumption
and latency, limiting their applicability in real-time scenarios. Neuromorphic
vision systems, combining event-based cameras and spiking neural networks
(SNNs), offer a promising alternative by enabling energy-efficient, low-latency
navigation. Despite their potential, real-world implementations of these
systems, particularly on physical platforms such as drones, remain scarce. In
this work, we present Neuro-LIFT, a real-time neuromorphic navigation framework
implemented on a Parrot Bebop2 quadrotor. Leveraging an LLM for natural
language processing, Neuro-LIFT translates human speech into high-level
planning commands which are then autonomously executed using event-based
neuromorphic vision and physics-driven planning. Our framework demonstrates its
capabilities in navigating in a dynamic environment, avoiding obstacles, and
adapting to human instructions in real-time.

中文翻译:
以下是符合学术规范的中文翻译：

将人类直觉交互融入自主系统的研究一直存在局限。传统自然语言处理（NLP）系统在语境与意图理解方面表现欠佳，严重制约了人机交互发展。随着大语言模型（LLM）的突破性进展，这种局面得以改变——通过语音与文本实现的直觉式高层级交流，成功弥合了人类指令与机器人行动之间的鸿沟。与此同时，自主导航已成为机器人研究的核心方向，人工智能（AI）技术正被广泛用于增强此类系统。然而现有基于AI的导航算法在需要快速决策的延迟敏感任务中面临重大挑战：传统帧式视觉系统虽擅长高层决策，却存在高能耗与高延迟缺陷，难以满足实时场景需求。神经形态视觉系统通过结合事件相机与脉冲神经网络（SNN），为构建高能效、低延迟的导航方案提供了新思路。尽管潜力显著，这类系统（尤其是无人机等实体平台）的实际应用仍属空白。本研究提出Neuro-LIFT框架——基于Parrot Bebop2四旋翼平台实现的实时神经形态导航系统：利用LLM解析自然语言，将人类语音转换为高层规划指令，继而通过事件驱动型神经形态视觉与物理约束规划实现自主执行。实验证明，该框架能在动态环境中实时完成导航避障，并自适应响应人类指令。

（翻译要点说明：
1. 专业术语统一处理：LLM/SNN等缩写首次出现标注全称
2. 长句拆分重构：将原文复合句按中文习惯分解为短句群
3. 被动语态转化："are leveraged"等转换为主动式表述
4. 概念显化处理："physics-driven planning"译为"物理约束规划"以明确内涵
5. 逻辑连接强化：添加破折号、冒号等符号保持论证连贯性
6. 学术风格保持：使用"本研究""实验证明"等规范表述）
