# RedWhale: An Adapted Korean LLM Through Efficient Continual Pretraining

链接: http://arxiv.org/abs/2408.11294v1

原文摘要:
The field of Natural Language Processing (NLP) has seen significant
advancements with the development of Large Language Models (LLMs). However,
much of this research remains focused on English, often overlooking
low-resource languages like Korean. This oversight presents challenges due to
the unique non-alphabetic token structure of Korean and the substantial memory
and computational demands required for LLM training, which frequently lead to
memory constraints and out-of-memory errors. To address these issues, we
present RedWhale, a model specifically tailored for Korean language processing.
RedWhale is developed using an efficient continual pretraining approach that
includes a comprehensive Korean corpus preprocessing pipeline, a specialized
tokenizer, an optimized model initialization technique, and a multistage
pretraining strategy. These innovations collectively reduce training time and
computational costs while maintaining high levels of accuracy and
comprehension. By leveraging cross-lingual transfer learning, which exploits
shared linguistic similarities across languages, RedWhale builds on English
models to enhance Korean language processing. Experimental results demonstrate
that RedWhale outperforms other leading models on Korean NLP benchmarks,
including the Korean Balanced Evaluation of Significant Tasks (KoBEST), showing
superior understanding and generation of Korean text. Furthermore, RedWhale
showed no signs of convergence even after pretraining on 9.7 billion tokens,
indicating the potential for further improvements with additional training.
This work represents a significant advancement in bridging the linguistic
divide, particularly in enhancing NLP capabilities for the Korean language.

中文翻译:
自然语言处理（NLP）领域随着大语言模型（LLMs）的发展取得了显著进步。然而，现有研究大多集中于英语，往往忽视韩语等低资源语言。这种忽视带来了特殊挑战：韩语独特的非字母化标记结构、LLM训练所需的高内存与计算需求，常导致内存限制和溢出错误。为此，我们提出专为韩语处理优化的RedWhale模型。该模型采用高效持续预训练方法，包含完整的韩语语料预处理流程、专用分词器、优化的模型初始化技术和多阶段预训练策略。这些创新在保持高准确性与理解力的同时，显著降低了训练时间和计算成本。通过利用跨语言迁移学习挖掘语言间的共享特征，RedWhale基于英语模型实现了韩语处理能力的提升。实验结果表明，在KoBEST等韩语NLP基准测试中，RedWhale在文本理解与生成方面均优于其他主流模型。值得注意的是，即便在97亿标记量的预训练后，模型仍未出现收敛迹象，预示着持续训练的改进潜力。本研究为弥合语言鸿沟作出了重要贡献，特别是在提升韩语NLP能力方面具有突破性意义。  


