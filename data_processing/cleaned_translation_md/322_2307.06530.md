# Exploring the Integration of Large Language Models into Automatic Speech Recognition Systems: An Empirical Study

链接: http://arxiv.org/abs/2307.06530v1

原文摘要:
This paper explores the integration of Large Language Models (LLMs) into
Automatic Speech Recognition (ASR) systems to improve transcription accuracy.
The increasing sophistication of LLMs, with their in-context learning
capabilities and instruction-following behavior, has drawn significant
attention in the field of Natural Language Processing (NLP). Our primary focus
is to investigate the potential of using an LLM's in-context learning
capabilities to enhance the performance of ASR systems, which currently face
challenges such as ambient noise, speaker accents, and complex linguistic
contexts. We designed a study using the Aishell-1 and LibriSpeech datasets,
with ChatGPT and GPT-4 serving as benchmarks for LLM capabilities.
Unfortunately, our initial experiments did not yield promising results,
indicating the complexity of leveraging LLM's in-context learning for ASR
applications. Despite further exploration with varied settings and models, the
corrected sentences from the LLMs frequently resulted in higher Word Error
Rates (WER), demonstrating the limitations of LLMs in speech applications. This
paper provides a detailed overview of these experiments, their results, and
implications, establishing that using LLMs' in-context learning capabilities to
correct potential errors in speech recognition transcriptions is still a
challenging task at the current stage.

中文翻译:
本文探讨了将大语言模型（LLMs）整合到自动语音识别（ASR）系统中以提高转录准确率的方法。随着LLMs在上下文学习能力和指令跟随行为方面的日益成熟，其在自然语言处理（NLP）领域引起了广泛关注。我们重点研究了利用LLM的上下文学习能力来提升ASR系统性能的潜力——当前ASR系统仍面临环境噪声、说话者口音和复杂语言语境等挑战。基于Aishell-1和LibriSpeech数据集，我们以ChatGPT和GPT-4作为LLM能力基准展开了实验研究。然而初步实验未能取得预期效果，这表明利用LLM的上下文学习优化ASR应用具有较高复杂性。尽管后续尝试了多种参数设置和模型调整，但经LLM修正后的句子反而频繁出现更高的词错误率（WER），揭示了LLM在语音应用中的局限性。本文详细阐述了实验设计、结果及其深层意义，证实现阶段利用LLM的上下文学习能力来修正语音识别转录错误仍是一项具有挑战性的任务。  

（翻译说明：  
1. 专业术语采用学界通用译法，如"in-context learning"译为"上下文学习"  
2. 长句按中文表达习惯拆分重组，如将"which currently face..."处理为破折号补充说明  
3. 被动语态转换为主动表述，如"has drawn significant attention"译为"引起了广泛关注"  
4. 关键指标"Word Error Rates"保留英文缩写WER并补充中文全称  
5. 学术表述保持严谨性，如"demonstrating the limitations"译为"揭示了局限性"而非口语化表达）
