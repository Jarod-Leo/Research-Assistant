# Breaking Memory Limits: Gradient Wavelet Transform Enhances LLMs Training

链接: http://arxiv.org/abs/2501.07237v1

原文摘要:
Large language models (LLMs) have shown impressive performance across a range
of natural language processing tasks. However, their vast number of parameters
introduces significant memory challenges during training, particularly when
using memory-intensive optimizers like Adam. Existing memory-efficient
algorithms often rely on techniques such as singular value decomposition
projection or weight freezing. While these approaches help alleviate memory
constraints, they generally produce suboptimal results compared to full-rank
updates. In this paper, we investigate the memory-efficient method beyond
low-rank training, proposing a novel solution called Gradient Wavelet Transform
(GWT), which applies wavelet transforms to gradients in order to significantly
reduce the memory requirements for maintaining optimizer states. We demonstrate
that GWT can be seamlessly integrated with memory-intensive optimizers,
enabling efficient training without sacrificing performance. Through extensive
experiments on both pre-training and fine-tuning tasks, we show that GWT
achieves state-of-the-art performance compared with advanced memory-efficient
optimizers and full-rank approaches in terms of both memory usage and training
performance.

中文翻译:
以下是符合您要求的中文翻译：

大语言模型（LLMs）在一系列自然语言处理任务中展现出卓越性能，但其海量参数在训练过程中（尤其是使用Adam等内存密集型优化器时）会带来显著的内存挑战。现有内存优化算法通常依赖奇异值分解投影或权重冻结等技术，虽然这些方法能缓解内存压力，但与全秩更新相比往往产生次优结果。本文探索了超越低秩训练的内存优化方法，提出创新性解决方案——梯度小波变换（GWT），通过对梯度施加小波变换来大幅降低优化器状态的内存需求。我们证明GWT可与内存密集型优化器无缝集成，在保证性能的同时实现高效训练。通过在预训练和微调任务上的大量实验表明，相较于先进的内存优化器和全秩方法，GWT在内存占用和训练性能方面均达到最先进水平。

（翻译严格遵循以下原则：
1. 专业术语准确统一（如LLMs/大语言模型、Adam优化器等）
2. 长句合理切分，符合中文表达习惯
3. 被动语态转换为主动句式（如"are often relied on"→"通常依赖"）
4. 关键概念首次出现标注英文缩写（GWT）
5. 保持学术论文的严谨性，避免口语化表达
6. 技术动作表述准确（如"weight freezing"译为"权重冻结"而非字面直译））
