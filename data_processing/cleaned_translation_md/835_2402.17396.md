# Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of Prompting Strategies

链接: http://arxiv.org/abs/2402.17396v1

原文摘要:
Large Language Models (LLMs) have revolutionized the field of Natural
Language Processing thanks to their ability to reuse knowledge acquired on
massive text corpora on a wide variety of downstream tasks, with minimal (if
any) tuning steps. At the same time, it has been repeatedly shown that LLMs
lack systematic generalization, which allows to extrapolate the learned
statistical regularities outside the training distribution. In this work, we
offer a systematic benchmarking of GPT-4, one of the most advanced LLMs
available, on three algorithmic tasks characterized by the possibility to
control the problem difficulty with two parameters. We compare the performance
of GPT-4 with that of its predecessor (GPT-3.5) and with a variant of the
Transformer-Encoder architecture recently introduced to solve similar tasks,
the Neural Data Router. We find that the deployment of advanced prompting
techniques allows GPT-4 to reach superior accuracy on all tasks, demonstrating
that state-of-the-art LLMs constitute a very strong baseline also in
challenging tasks that require systematic generalization.

中文翻译:
以下是符合您要求的中文翻译：

大型语言模型（LLMs）通过将海量文本语料库中习得的知识迁移至各类下游任务的能力（仅需极少量调优步骤甚至无需调整），彻底革新了自然语言处理领域。然而研究反复表明，这类模型缺乏系统性泛化能力——即无法将习得的统计规律外推至训练分布之外。本研究针对当前最先进的LLM之一GPT-4，在三个可通过双参数控制难度的算法任务上展开系统性基准测试。我们将GPT-4的表现与其前代模型（GPT-3.5）以及专为类似任务设计的神经数据路由架构（Neural Data Router）进行对比。研究发现，采用先进提示工程技术后，GPT-4在所有任务中均展现出更优的准确率，这表明即使在需要系统性泛化的挑战性任务中，最先进的大型语言模型同样能成为极具竞争力的基准模型。

翻译说明：
1. 专业术语处理：LLMs统一译为"大型语言模型"，Transformer-Encoder保留技术特征译为"架构"
2. 长句拆分：将原文复合长句按中文表达习惯分解为多个短句，如第一句拆分为因果关系的两个分句
3. 被动语态转换："it has been shown"译为主动式"研究表明"
4. 概念显化处理："systematic generalization"增译为"系统性泛化能力"并补充破折号解释
5. 技术表述优化："prompting techniques"译为"提示工程技术"以体现专业度
6. 逻辑连接处理：通过"然而""针对""将"等连接词保持论证脉络清晰
7. 文化适配：保留"GPT-4"等技术代号，括号注释保持原文补充说明功能
