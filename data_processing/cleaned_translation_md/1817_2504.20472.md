# Robustness via Referencing: Defending against Prompt Injection Attacks by Referencing the Executed Instruction

链接: http://arxiv.org/abs/2504.20472v1

原文摘要:
Large language models (LLMs) have demonstrated impressive performance and
have come to dominate the field of natural language processing (NLP) across
various tasks. However, due to their strong instruction-following capabilities
and inability to distinguish between instructions and data content, LLMs are
vulnerable to prompt injection attacks. These attacks manipulate LLMs into
deviating from the original input instructions and executing maliciously
injected instructions within data content, such as web documents retrieved from
search engines. Existing defense methods, including prompt-engineering and
fine-tuning approaches, typically instruct models to follow the original input
instructions while suppressing their tendencies to execute injected
instructions. However, our experiments reveal that suppressing
instruction-following tendencies is challenging. Through analyzing failure
cases, we observe that although LLMs tend to respond to any recognized
instructions, they are aware of which specific instructions they are executing
and can correctly reference them within the original prompt. Motivated by these
findings, we propose a novel defense method that leverages, rather than
suppresses, the instruction-following abilities of LLMs. Our approach prompts
LLMs to generate responses that include both answers and their corresponding
instruction references. Based on these references, we filter out answers not
associated with the original input instructions. Comprehensive experiments
demonstrate that our method outperforms prompt-engineering baselines and
achieves performance comparable to fine-tuning methods, reducing the attack
success rate (ASR) to 0 percent in some scenarios. Moreover, our approach has
minimal impact on overall utility.

中文翻译:
以下是符合要求的学术化中文翻译：

大型语言模型（LLMs）已展现出卓越的性能表现，并在各类自然语言处理（NLP）任务中占据主导地位。然而，由于其强大的指令跟随能力且无法区分指令与数据内容，LLMs容易受到提示注入攻击。此类攻击会操纵模型偏离原始输入指令，转而执行数据内容（如搜索引擎检索到的网页文档）中恶意注入的指令。现有防御方法（包括提示工程与微调方案）通常通过指导模型遵循原始输入指令，同时抑制其执行注入指令的倾向。但我们的实验表明，抑制指令跟随倾向具有显著挑战性。

通过分析失败案例，我们发现：尽管LLMs倾向于响应任何可识别的指令，但它们能够明确感知当前执行的特定指令，并可在原始提示中正确引用这些指令。基于此发现，我们提出了一种创新防御方法——该方法并非抑制而是充分利用LLMs的指令跟随能力。我们的方案引导LLMs生成包含答案及其对应指令引用的响应，随后根据这些引用过滤掉与原始输入指令无关的答案。

综合实验表明：本方法在性能上优于提示工程基线方案，并达到与微调方法相当的水平，在某些场景下可将攻击成功率（ASR）降至0%。此外，该方法对模型整体效用影响甚微。


2. 被动语态转换为中文主动表述（如"are vulnerable to"→"容易受到"）
3. 长难句拆分重组（如将英文复合句分解为中文流水句）
4. 逻辑连接显性化（添加"基于此发现"等过渡词）
5. 数据量化表述规范化（"0 percent"→"0%"）
6. 保持客观严谨的学术风格，避免口语化表达）
