# Confidant: Customizing Transformer-based LLMs via Collaborative Edge Training

链接: http://arxiv.org/abs/2311.13381v1

原文摘要:
Transformer-based large language models (LLMs) have demonstrated impressive
capabilities in a variety of natural language processing (NLP) tasks.
Nonetheless, it is challenging to deploy and fine-tune LLMs on mobile edge
devices with limited computing, memory, and energy budgets. In this paper, we
propose Confidant, a multi-backend collaborative training framework for
customizing state-of-the-art LLMs on commodity mobile devices like smartphones.
Confidant partitions an LLM into several sub-models so that each fits into a
mobile device's memory. A pipeline parallel training mechanism is further
developed to ensure fast and efficient distributed training. In addition, we
propose a novel backend scheduler to allocate different attention heads to
heterogeneous compute hardware, including mobile CPU and GPUs, to maximize the
compute resource utilization on each edge device. Our preliminary experimental
results show that Confidant achieves at most 45.3% memory reduction and 8.03x
inference speedup in practical settings.

中文翻译:
基于Transformer架构的大语言模型（LLMs）已在各类自然语言处理（NLP）任务中展现出卓越性能。然而在计算、内存和能耗资源有限的移动边缘设备上部署和微调这类模型仍面临巨大挑战。本文提出Confidant——一个面向智能手机等商用移动设备的多后端协同训练框架，旨在实现尖端大语言模型的定制化部署。该框架通过将大语言模型拆分为多个适配移动设备内存容量的子模型，并创新性地开发了流水线并行训练机制以确保高效分布式训练。此外，我们设计了一种新型后端调度器，能够将不同注意力头动态分配给移动端CPU与GPU等异构计算硬件，从而最大化边缘设备的计算资源利用率。初步实验表明，Confidant在实际应用场景中最高可实现45.3%的内存占用削减和8.03倍的推理加速。
