# Reducing the Transformer Architecture to a Minimum

链接: http://arxiv.org/abs/2410.13732v1

原文摘要:
Transformers are a widespread and successful model architecture, particularly
in Natural Language Processing (NLP) and Computer Vision (CV). The essential
innovation of this architecture is the Attention Mechanism, which solves the
problem of extracting relevant context information from long sequences in NLP
and realistic scenes in CV. A classical neural network component, a Multi-Layer
Perceptron (MLP), complements the attention mechanism. Its necessity is
frequently justified by its capability of modeling nonlinear relationships.
However, the attention mechanism itself is nonlinear through its internal use
of similarity measures. A possible hypothesis is that this nonlinearity is
sufficient for modeling typical application problems. As the MLPs usually
contain the most trainable parameters of the whole model, their omission would
substantially reduce the parameter set size. Further components can also be
reorganized to reduce the number of parameters. Under some conditions, query
and key matrices can be collapsed into a single matrix of the same size. The
same is true about value and projection matrices, which can also be omitted
without eliminating the substance of the attention mechanism. Initially, the
similarity measure was defined asymmetrically, with peculiar properties such as
that a token is possibly dissimilar to itself. A possible symmetric definition
requires only half of the parameters. We have laid the groundwork by testing
widespread CV benchmarks: MNIST and CIFAR-10. The tests have shown that
simplified transformer architectures (a) without MLP, (b) with collapsed
matrices, and (c) symmetric similarity matrices exhibit similar performance as
the original architecture, saving up to 90% of parameters without hurting the
classification performance.

中文翻译:
以下是符合学术规范的中文翻译：

Transformer是一种广泛应用且成效显著的模型架构，尤其在自然语言处理（NLP）和计算机视觉（CV）领域。该架构的核心创新在于注意力机制，它有效解决了从NLP长序列和CV真实场景中提取相关上下文信息的问题。传统神经网络组件多层感知机（MLP）通常作为注意力机制的补充模块，其必要性常被解释为具有建模非线性关系的能力。然而注意力机制本身通过内部相似度计算已具备非线性特性，我们推测这种非线性足以应对典型应用场景的建模需求。由于MLP通常包含整个模型最多的可训练参数，移除该模块将显著减少参数量。其他组件亦可进行结构调整以压缩参数规模：在某些条件下，查询矩阵和键矩阵可合并为同尺寸的单一矩阵；同理，值矩阵与投影矩阵也可在不影响注意力机制核心功能的前提下省略。原始相似度度量采用非对称定义，存在特定标记可能与自身不相似等特殊性质；若改用对称定义则仅需半数参数。我们通过在MNIST和CIFAR-10等主流CV基准测试上验证发现：简化版Transformer架构在（a）去除MLP、（b）矩阵合并、（c）对称相似度矩阵等条件下，分类性能与原架构相当，却能减少高达90%的参数量。

（翻译说明：
1. 专业术语统一处理：如"Attention Mechanism"统一译为"注意力机制"，"Multi-Layer Perceptron"采用学界通用缩写"MLP"
2. 长句拆分重构：将原文复合句按中文表达习惯分解为多个短句，如将"However..."开始的转折长句拆分为两个语义单元
3. 被动语态转化："can be collapsed"等被动结构转换为中文主动表述"可合并为"
4. 学术表述规范：使用"建模""参数量""基准测试"等符合计算机学科规范的表述
5. 逻辑关系显化：通过"由于""若""却能"等连接词明确原文隐含的因果关系和对比关系
6. 文化适配：保留"MNIST/CIFAR-10"等国际通用数据集名称不翻译，符合国内学术惯例）
