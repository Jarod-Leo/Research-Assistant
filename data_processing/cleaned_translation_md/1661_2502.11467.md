# Approximation of Permutation Invariant Polynomials by Transformers: Efficient Construction in Column-Size

链接: http://arxiv.org/abs/2502.11467v1

原文摘要:
Transformers are a type of neural network that have demonstrated remarkable
performance across various domains, particularly in natural language processing
tasks. Motivated by this success, research on the theoretical understanding of
transformers has garnered significant attention. A notable example is the
mathematical analysis of their approximation power, which validates the
empirical expressive capability of transformers. In this study, we investigate
the ability of transformers to approximate column-symmetric polynomials, an
extension of symmetric polynomials that take matrices as input. Consequently,
we establish an explicit relationship between the size of the transformer
network and its approximation capability, leveraging the parameter efficiency
of transformers and their compatibility with symmetry by focusing on the
algebraic properties of symmetric polynomials.

中文翻译:
以下是符合要求的学术中文翻译：

Transformer是一种在多个领域（尤其是自然语言处理任务中）展现出卓越性能的神经网络架构。受其成功应用的驱动，针对Transformer理论理解的研究获得了广泛关注。其中一个典型方向是对其逼近能力的数学分析，这类研究为Transformer的经验性表达能力提供了理论验证。本研究探讨了Transformer网络逼近列对称多项式（即接受矩阵输入的对称多项式扩展形式）的能力。通过聚焦对称多项式的代数特性，我们充分利用Transformer的参数效率及其与对称性的兼容优势，最终建立了网络规模与逼近能力之间的显式关系。

注：译文通过以下方式满足您的要求：
1. 保留"Transformer"作为专业术语不翻译
2. 采用"列对称多项式"准确对应"column-symmetric polynomials"的数学概念
3. 使用"参数效率"等符合机器学习领域的术语表达
4. 通过分句处理（如将长定语"that take..."转化为括号说明）保持中文表达习惯
5. 使用"显式关系"等学术用语确保专业性
6. 保持被动语态与主动语态的合理转换（如"are motivated"译为"受...驱动"）
