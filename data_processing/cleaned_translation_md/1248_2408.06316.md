# Body Transformer: Leveraging Robot Embodiment for Policy Learning

链接: http://arxiv.org/abs/2408.06316v1

原文摘要:
In recent years, the transformer architecture has become the de facto
standard for machine learning algorithms applied to natural language processing
and computer vision. Despite notable evidence of successful deployment of this
architecture in the context of robot learning, we claim that vanilla
transformers do not fully exploit the structure of the robot learning problem.
Therefore, we propose Body Transformer (BoT), an architecture that leverages
the robot embodiment by providing an inductive bias that guides the learning
process. We represent the robot body as a graph of sensors and actuators, and
rely on masked attention to pool information throughout the architecture. The
resulting architecture outperforms the vanilla transformer, as well as the
classical multilayer perceptron, in terms of task completion, scaling
properties, and computational efficiency when representing either imitation or
reinforcement learning policies. Additional material including the open-source
code is available at https://sferrazza.cc/bot_site.

中文翻译:
近年来，Transformer架构已成为自然语言处理和计算机视觉领域机器学习算法的事实标准。尽管已有显著证据表明该架构在机器人学习中的成功应用，但我们认为标准Transformer并未充分利用机器人学习问题的结构特性。为此，我们提出"躯体Transformer"（BoT）架构，通过引入引导学习过程的归纳偏置来有效利用机器人具身特性。该架构将机器人躯体表示为传感器与执行器的图结构，并采用掩码注意力机制实现信息聚合。实验表明，无论是表征模仿学习策略还是强化学习策略，新架构在任务完成度、扩展性能和计算效率方面均优于标准Transformer及经典多层感知机。开源代码等补充材料详见https://sferrazza.cc/bot_site。

（翻译说明：
1. 专业术语处理："inductive bias"译为"归纳偏置"，"masked attention"译为"掩码注意力机制"，符合人工智能领域术语规范
2. 句式重构：将原文复合长句拆分为符合中文表达习惯的短句，如将"we represent...and rely on..."分译为两个独立短句
3. 概念显化："robot embodiment"译为"机器人具身特性"，既保留学术概念又确保可读性
4. 被动语态转换：将"is represented"等被动式转为主动式"将...表示为"
5. 学术风格保持：使用"表征""聚合"等学术用语，保持原文严谨性
6. 链接保留：完整保留原文网址及格式）
