# SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models

链接: http://arxiv.org/abs/2503.07605v1

原文摘要:
Large Language Models have achieved remarkable success across various natural
language processing tasks, yet their high computational cost during inference
remains a major bottleneck. This paper introduces Sparse Expert Activation
Pruning (SEAP), a training-free pruning method that selectively retains
task-relevant parameters to reduce inference overhead. Inspired by the
clustering patterns of hidden states and activations in LLMs, SEAP identifies
task-specific expert activation patterns and prunes the model while preserving
task performance and enhancing computational efficiency. Experimental results
demonstrate that SEAP significantly reduces computational overhead while
maintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both
WandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2%
performance drop compared to the dense model. These findings highlight SEAP's
scalability and effectiveness, making it a promising approach for optimizing
large-scale LLMs.

中文翻译:
以下是符合要求的学术论文摘要翻译：

大型语言模型在各类自然语言处理任务中取得了显著成功，但其推理过程中的高计算成本仍是主要瓶颈。本文提出稀疏专家激活剪枝（SEAP），这是一种无需重新训练的剪枝方法，通过选择性保留任务相关参数来降低推理开销。受大型语言模型中隐藏状态与激活聚类模式的启发，SEAP能识别任务特定的专家激活模式，在保持任务性能的同时提升计算效率。实验结果表明，SEAP在维持竞争力准确率的前提下显著降低了计算开销。值得注意的是，在50%剪枝率下，SEAP性能超越WandA和FLAP方法20%以上；在20%剪枝率时，相较稠密模型仅产生2.2%的性能下降。这些发现证明了SEAP的可扩展性与有效性，使其成为优化大规模语言模型的前沿方法。

（翻译严格遵循以下原则：
1. 专业术语统一："pruning"译为"剪枝"，"dense model"译为"稠密模型"
2. 被动语态转化："are inspired by"译为"受...启发"的主动句式
3. 长句拆分：将原文复合句分解为符合中文表达习惯的短句
4. 数据呈现：精确保留"20%"等数值及比较关系
5. 学术风格：使用"显著""相较""前沿方法"等规范学术用语
6. 机构名称保留：WandA/FLAP等算法名维持英文原名）
