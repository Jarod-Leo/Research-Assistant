# Transformers Can Overcome the Curse of Dimensionality: A Theoretical Study from an Approximation Perspective

链接: http://arxiv.org/abs/2504.13558v1

原文摘要:
The Transformer model is widely used in various application areas of machine
learning, such as natural language processing. This paper investigates the
approximation of the H\"older continuous function class
$\mathcal{H}_{Q}^{\beta}\left([0,1]^{d\times n},\mathbb{R}^{d\times n}\right)$
by Transformers and constructs several Transformers that can overcome the curse
of dimensionality. These Transformers consist of one self-attention layer with
one head and the softmax function as the activation function, along with
several feedforward layers. For example, to achieve an approximation accuracy
of $\epsilon$, if the activation functions of the feedforward layers in the
Transformer are ReLU and floor, only
$\mathcal{O}\left(\log\frac{1}{\epsilon}\right)$ layers of feedforward layers
are needed, with widths of these layers not exceeding
$\mathcal{O}\left(\frac{1}{\epsilon^{2/\beta}}\log\frac{1}{\epsilon}\right)$.
If other activation functions are allowed in the feedforward layers, the width
of the feedforward layers can be further reduced to a constant. These results
demonstrate that Transformers have a strong expressive capability. The
construction in this paper is based on the Kolmogorov-Arnold Representation
Theorem and does not require the concept of contextual mapping, hence our proof
is more intuitively clear compared to previous Transformer approximation works.
Additionally, the translation technique proposed in this paper helps to apply
the previous approximation results of feedforward neural networks to
Transformer research.

中文翻译:
Transformer模型被广泛应用于机器学习的各个应用领域，例如自然语言处理。本文研究了Transformer对H\"older连续函数类$\mathcal{H}_{Q}^{\beta}\left([0,1]^{d\times n},\mathbb{R}^{d\times n}\right)$的逼近能力，并构建了若干能克服维度诅咒的Transformer结构。这些Transformer由一个使用softmax作为激活函数的单头自注意力层和若干前馈层组成。例如，为实现$\epsilon$逼近精度，若Transformer中前馈层的激活函数采用ReLU和floor函数，则仅需$\mathcal{O}\left(\log\frac{1}{\epsilon}\right)$层前馈层，且各层宽度不超过$\mathcal{O}\left(\frac{1}{\epsilon^{2/\beta}}\log\frac{1}{\epsilon}\right)$；若允许前馈层使用其他激活函数，其宽度可进一步缩减为常数。这些结果表明Transformer具有强大的表达能力。本文的构造基于Kolmogorov-Arnold表示定理，无需借助上下文映射概念，因此相比以往Transformer逼近工作的证明更具直观清晰性。此外，本文提出的平移技术有助于将前馈神经网络已有的逼近结果应用于Transformer研究。


