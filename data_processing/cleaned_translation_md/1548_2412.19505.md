# DrivingWorld: ConstructingWorld Model for Autonomous Driving via Video GPT

链接: http://arxiv.org/abs/2412.19505v1

原文摘要:
Recent successes in autoregressive (AR) generation models, such as the GPT
series in natural language processing, have motivated efforts to replicate this
success in visual tasks. Some works attempt to extend this approach to
autonomous driving by building video-based world models capable of generating
realistic future video sequences and predicting ego states. However, prior
works tend to produce unsatisfactory results, as the classic GPT framework is
designed to handle 1D contextual information, such as text, and lacks the
inherent ability to model the spatial and temporal dynamics essential for video
generation. In this paper, we present DrivingWorld, a GPT-style world model for
autonomous driving, featuring several spatial-temporal fusion mechanisms. This
design enables effective modeling of both spatial and temporal dynamics,
facilitating high-fidelity, long-duration video generation. Specifically, we
propose a next-state prediction strategy to model temporal coherence between
consecutive frames and apply a next-token prediction strategy to capture
spatial information within each frame. To further enhance generalization
ability, we propose a novel masking strategy and reweighting strategy for token
prediction to mitigate long-term drifting issues and enable precise control.
Our work demonstrates the ability to produce high-fidelity and consistent video
clips of over 40 seconds in duration, which is over 2 times longer than
state-of-the-art driving world models. Experiments show that, in contrast to
prior works, our method achieves superior visual quality and significantly more
accurate controllable future video generation. Our code is available at
