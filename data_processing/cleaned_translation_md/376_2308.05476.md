# Exploring Machine Learning and Transformer-based Approaches for Deceptive Text Classification: A Comparative Analysis

链接: http://arxiv.org/abs/2308.05476v2

原文摘要:
Deceptive text classification is a critical task in natural language
processing that aims to identify deceptive o fraudulent content. This study
presents a comparative analysis of machine learning and transformer-based
approaches for deceptive text classification. We investigate the effectiveness
of traditional machine learning algorithms and state-of-the-art transformer
models, such as BERT, XLNET, DistilBERT, and RoBERTa, in detecting deceptive
text. A labeled dataset consisting of deceptive and non-deceptive texts is used
for training and evaluation purposes. Through extensive experimentation, we
compare the performance metrics, including accuracy, precision, recall, and F1
score, of the different approaches. The results of this study shed light on the
strengths and limitations of machine learning and transformer-based methods for
deceptive text classification, enabling researchers and practitioners to make
informed decisions when dealing with deceptive content.

中文翻译:
以下是符合要求的学术摘要中文翻译：

【译文】
欺骗性文本分类是自然语言处理领域的关键任务，旨在识别具有欺骗性或欺诈性的内容。本研究对基于机器学习与Transformer架构的欺骗性文本分类方法进行了对比分析。我们探究了传统机器学习算法与前沿Transformer模型（包括BERT、XLNET、DistilBERT和RoBERTa）在检测欺骗性文本方面的有效性。实验采用包含欺骗性与非欺骗性文本的标注数据集进行训练与评估。通过大量实验，我们对比了不同方法在准确率、精确率、召回率和F1值等性能指标上的表现。研究结果揭示了机器学习与Transformer方法在欺骗性文本分类中的优势与局限，为研究者和实践者处理欺骗性内容提供了决策依据。

【翻译要点说明】
1. 专业术语处理：
- "transformer-based approaches"译为"基于Transformer架构的方法"，符合计算机领域术语规范
- "state-of-the-art"译为"前沿"而非字面直译，更符合中文表达习惯

2. 句式重构：
- 将原文复合长句拆分为符合中文表达习惯的短句，如将"Through extensive..."独立成句
- 被动语态转换："A labeled dataset is used"译为主动式"实验采用..."

3. 学术规范：
- 保留所有技术术语首字母大写（如BERT等模型名称）
- 关键指标翻译采用领域通用译法（"F1 score"统一译为"F1值"）

4. 逻辑显化：
- "shed light on"译为"揭示"而非字面直译，更准确传达研究价值
- 结尾"informed decisions"译为"决策依据"，突出研究实用性

译文在保持学术严谨性的同时，通过合理的语序调整和术语统一，确保了专业性与可读性的平衡。
