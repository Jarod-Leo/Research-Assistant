# Unified CNNs and transformers underlying learning mechanism reveals multi-head attention modus vivendi

链接: http://arxiv.org/abs/2501.12900v1

原文摘要:
Convolutional neural networks (CNNs) evaluate short-range correlations in
input images which progress along the layers, whereas vision transformer (ViT)
architectures evaluate long-range correlations, using repeated transformer
encoders composed of fully connected layers. Both are designed to solve complex
classification tasks but from different perspectives. This study demonstrates
that CNNs and ViT architectures stem from a unified underlying learning
mechanism, which quantitatively measures the single-nodal performance (SNP) of
each node in feedforward (FF) and multi-head attention (MHA) sub-blocks. Each
node identifies small clusters of possible output labels, with additional noise
represented as labels outside these clusters. These features are progressively
sharpened along the transformer encoders, enhancing the signal-to-noise ratio.
This unified underlying learning mechanism leads to two main findings. First,
it enables an efficient applied nodal diagonal connection (ANDC) pruning
technique without affecting the accuracy. Second, based on the SNP, spontaneous
symmetry breaking occurs among the MHA heads, such that each head focuses its
attention on a subset of labels through cooperation among its SNPs.
Consequently, each head becomes an expert in recognizing its designated labels,
representing a quantitative MHA modus vivendi mechanism. This statistical
mechanics inspired viewpoint enables to reveal macroscopic behavior of the
entire network from the microscopic performance of each node. These results are
based on a compact convolutional transformer architecture trained on the
CIFAR-100 and Flowers-102 datasets and call for their extension to other
architectures and applications, such as natural language processing.

中文翻译:
卷积神经网络（CNN）通过逐层递进的方式评估输入图像的短程相关性，而视觉变换器（ViT）架构则采用由全连接层构成的重复变换器编码器来评估长程相关性。两者虽设计初衷不同，但均致力于解决复杂分类任务。本研究表明，CNN与ViT架构源于统一的基础学习机制——该机制通过定量测量前馈（FF）子块和多头注意力（MHA）子块中每个节点的单节点性能（SNP），揭示出每个节点会识别出若干可能的输出标签小簇，并将簇外标签视为噪声。这些特征沿变换器编码器逐级锐化，从而提升信噪比。

这一统一机制引发出两项重要发现：首先，基于该机制可实施高效的应用节点对角连接（ANDC）剪枝技术，且不影响模型精度；其次，根据SNP原理，MHA头间会发生自发对称性破缺，使得各注意力头通过SNP协同作用聚焦于特定标签子集。由此每个头都成为识别其专属标签的"专家"，形成定量化的MHA共存机制。这种受统计力学启发的视角，使得从微观节点性能推演整个网络的宏观行为成为可能。研究结论基于在CIFAR-100和Flowers-102数据集上训练的紧凑卷积变换器架构得出，未来可扩展至其他架构及自然语言处理等应用领域。


