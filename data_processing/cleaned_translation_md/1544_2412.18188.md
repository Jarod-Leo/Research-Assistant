# On the Applicability of Zero-Shot Cross-Lingual Transfer Learning for Sentiment Classification in Distant Language Pairs

链接: http://arxiv.org/abs/2412.18188v1

原文摘要:
This research explores the applicability of cross-lingual transfer learning
from English to Japanese and Indonesian using the XLM-R pre-trained model. The
results are compared with several previous works, either by models using a
similar zero-shot approach or a fully-supervised approach, to provide an
overview of the zero-shot transfer learning approach's capability using XLM-R
in comparison with existing models. Our models achieve the best result in one
Japanese dataset and comparable results in other datasets in Japanese and
Indonesian languages without being trained using the target language.
Furthermore, the results suggest that it is possible to train a multi-lingual
model, instead of one model for each language, and achieve promising results.

中文翻译:
本研究探讨了基于XLM-R预训练模型实现英语到日语及印度尼西亚语的跨语言迁移学习适用性。通过将实验结果与采用类似零样本学习方法或全监督方法的现有模型进行对比，系统评估了XLM-R在零样本迁移学习中的性能表现。我们的模型在未经目标语言训练的情况下，在一个日语数据集上取得了最优结果，在其他日语及印度尼西亚语数据集上也获得了可比性能。研究结果表明：相较于为每种语言单独构建模型，训练单一多语言模型同样能取得理想效果。
