# Stance Detection on Social Media with Fine-Tuned Large Language Models

链接: http://arxiv.org/abs/2404.12171v1

原文摘要:
Stance detection, a key task in natural language processing, determines an
author's viewpoint based on textual analysis. This study evaluates the
evolution of stance detection methods, transitioning from early machine
learning approaches to the groundbreaking BERT model, and eventually to modern
Large Language Models (LLMs) such as ChatGPT, LLaMa-2, and Mistral-7B. While
ChatGPT's closed-source nature and associated costs present challenges, the
open-source models like LLaMa-2 and Mistral-7B offers an encouraging
alternative. Initially, our research focused on fine-tuning ChatGPT, LLaMa-2,
and Mistral-7B using several publicly available datasets. Subsequently, to
provide a comprehensive comparison, we assess the performance of these models
in zero-shot and few-shot learning scenarios. The results underscore the
exceptional ability of LLMs in accurately detecting stance, with all tested
models surpassing existing benchmarks. Notably, LLaMa-2 and Mistral-7B
demonstrate remarkable efficiency and potential for stance detection, despite
their smaller sizes compared to ChatGPT. This study emphasizes the potential of
LLMs in stance detection and calls for more extensive research in this field.

中文翻译:
立场检测作为自然语言处理的核心任务，旨在通过文本分析识别作者的观点倾向。本研究系统评估了立场检测方法的演进历程：从早期的机器学习方法，到具有里程碑意义的BERT模型，直至ChatGPT、LLaMa-2和Mistral-7B等现代大语言模型（LLMs）。尽管ChatGPT的闭源特性及使用成本构成实践挑战，但LLaMa-2与Mistral-7B等开源模型提供了令人振奋的替代方案。研究首先基于多个公开数据集对ChatGPT、LLaMa-2和Mistral-7B进行微调训练，随后通过零样本和小样本学习场景下的性能评估展开全面对比。实验结果表明，所有测试的大语言模型均超越现有基准，展现出卓越的立场检测精度。值得注意的是，LLaMa-2和Mistral-7B在模型体积显著小于ChatGPT的情况下，仍表现出优异的检测效率与发展潜力。本研究不仅证实了大语言模型在立场检测领域的应用价值，更呼吁学界开展更深入的探索。  

（翻译说明：  
1. 专业术语统一："stance detection"译为"立场检测"，"zero-shot/few-shot"采用通用译法"零样本/小样本"  
2. 长句拆分：将原文复合句按中文表达习惯分解为多个短句，如第二句拆分为演进历程描述+模型特性对比  
3. 被动语态转化："are assessed"转为主动式"展开全面对比"  
4. 概念显化处理："benchmarks"具体化为"现有基准"，"potential"根据语境分别译为"潜力/应用价值"  
5. 学术风格保持：使用"旨在""显著""卓越"等符合论文摘要规范的措辞  
6. 逻辑连接强化：通过"尽管""值得注意的是"等衔接词保持论证连贯性）
