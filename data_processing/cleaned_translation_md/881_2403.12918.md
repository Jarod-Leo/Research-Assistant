# Generalizable and Stable Finetuning of Pretrained Language Models on Low-Resource Texts

链接: http://arxiv.org/abs/2403.12918v1

原文摘要:
Pretrained Language Models (PLMs) have advanced Natural Language Processing
(NLP) tasks significantly, but finetuning PLMs on low-resource datasets poses
significant challenges such as instability and overfitting. Previous methods
tackle these issues by finetuning a strategically chosen subnetwork on a
downstream task, while keeping the remaining weights fixed to the pretrained
weights. However, they rely on a suboptimal criteria for sub-network selection,
leading to suboptimal solutions. To address these limitations, we propose a
regularization method based on attention-guided weight mixup for finetuning
PLMs. Our approach represents each network weight as a mixup of task-specific
weight and pretrained weight, controlled by a learnable attention parameter,
providing finer control over sub-network selection. Furthermore, we employ a
bi-level optimization (BLO) based framework on two separate splits of the
training dataset, improving generalization and combating overfitting. We
validate the efficacy of our proposed method through extensive experiments,
demonstrating its superiority over previous methods, particularly in the
context of finetuning PLMs on low-resource datasets.

中文翻译:
以下是符合学术规范的中文翻译：

预训练语言模型（PLMs）显著推动了自然语言处理（NLP）任务的发展，但在低资源数据集上微调PLMs存在显著挑战，如训练不稳定和过拟合问题。现有方法通过在下游任务中微调策略性选择的子网络（同时固定其余参数为预训练权重）来解决这些问题，但其子网络选择标准存在不足，导致次优解。为突破这些限制，我们提出一种基于注意力引导权重混合的PLMs微调正则化方法。该方法将每个网络权重表示为任务特定权重与预训练权重的混合，混合比例由可学习的注意力参数控制，从而实现对子网络选择的精细调控。此外，我们基于训练数据的双重划分采用双层优化（BLO）框架，有效提升模型泛化能力并抑制过拟合。通过大量实验验证，本方法在低资源数据集上的PLMs微调任务中展现出显著优势，其性能超越现有最佳方法。

（翻译说明：
1. 专业术语处理：PLMs/NLP/BLO等首字母缩略词保留英文缩写并添加中文全称
2. 被动语态转换："are represented as"译为主动态"将...表示为"
3. 长句拆分：将原文复合句拆分为符合中文表达习惯的短句结构
4. 学术表达："suboptimal solutions"译为"次优解"而非字面直译
5. 概念一致性："attention-guided weight mixup"统一译为"注意力引导权重混合"
6. 逻辑显化：通过"从而/此外"等连接词明确原文隐含的逻辑关系）
