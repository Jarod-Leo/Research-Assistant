# Evaluating LLMs and Pre-trained Models for Text Summarization Across Diverse Datasets

链接: http://arxiv.org/abs/2502.19339v1

原文摘要:
Text summarization plays a crucial role in natural language processing by
condensing large volumes of text into concise and coherent summaries. As
digital content continues to grow rapidly and the demand for effective
information retrieval increases, text summarization has become a focal point of
research in recent years. This study offers a thorough evaluation of four
leading pre-trained and open-source large language models: BART, FLAN-T5,
LLaMA-3-8B, and Gemma-7B, across five diverse datasets CNN/DM, Gigaword, News
Summary, XSum, and BBC News. The evaluation employs widely recognized automatic
metrics, including ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, and METEOR, to assess
the models' capabilities in generating coherent and informative summaries. The
results reveal the comparative strengths and limitations of these models in
processing various text types.

中文翻译:
文本摘要作为自然语言处理中的关键技术，通过将海量文本压缩为简洁连贯的概要，正发挥着日益重要的作用。随着数字内容呈指数级增长以及高效信息检索需求的激增，文本摘要研究已成为近年来的学术焦点。本研究对BART、FLAN-T5、LLaMA-3-8B和Gemma-7B四种前沿预训练开源大语言模型进行了系统评估，测试范围涵盖CNN/DM、Gigaword、News Summary、XSum和BBC News五个差异化数据集。采用ROUGE-1、ROUGE-2、ROUGE-L、BERTScore和METEOR等业界公认的自动评价指标，全面考察各模型生成连贯性摘要与信息浓缩的能力。实验结果清晰揭示了这些模型在处理不同类型文本时的相对优势与局限性。

（翻译说明：采用学术论文的严谨表述风格，通过以下处理实现专业性与可读性的平衡：
1. 专业术语准确对应："pre-trained"译为"预训练"，"automatic metrics"译为"自动评价指标"
2. 长句拆分重组：将原文复合句分解为符合中文表达习惯的短句结构
3. 概念显化处理："digital content continues to grow rapidly"意译为"数字内容呈指数级增长"增强表现力
4. 术语统一性：保持"coherent summaries"统一译为"连贯性摘要"
5. 被动语态转化：将英文被动式"has become a focal point"主动化为"已成为...焦点"
6. 数据规范呈现：模型名称与数据集名称保留英文原名符合学术惯例）
