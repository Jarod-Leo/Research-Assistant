# A review on the use of large language models as virtual tutors

链接: http://arxiv.org/abs/2405.11983v1

原文摘要:
Transformer architectures contribute to managing long-term dependencies for
Natural Language Processing, representing one of the most recent changes in the
field. These architectures are the basis of the innovative, cutting-edge Large
Language Models (LLMs) that have produced a huge buzz in several fields and
industrial sectors, among the ones education stands out. Accordingly, these
generative Artificial Intelligence-based solutions have directed the change in
techniques and the evolution in educational methods and contents, along with
network infrastructure, towards high-quality learning. Given the popularity of
LLMs, this review seeks to provide a comprehensive overview of those solutions
designed specifically to generate and evaluate educational materials and which
involve students and teachers in their design or experimental plan. To the best
of our knowledge, this is the first review of educational applications (e.g.,
student assessment) of LLMs. As expected, the most common role of these systems
is as virtual tutors for automatic question generation. Moreover, the most
popular models are GTP-3 and BERT. However, due to the continuous launch of new
generative models, new works are expected to be published shortly.

中文翻译:
Transformer架构为自然语言处理中的长程依赖关系管理提供了创新解决方案，成为该领域最具突破性的技术变革之一。作为前沿大型语言模型（LLMs）的核心基础，这些架构已在多个领域引发巨大反响，其中教育领域的应用尤为突出。基于生成式人工智能的解决方案正推动着教学技术、教育方法、内容体系以及网络基础设施的全面革新，助力高质量教育发展。

鉴于LLMs的广泛影响力，本文综述旨在系统梳理那些专门用于生成和评估教学材料、并在设计或实验环节纳入师生参与的解决方案。据我们所知，这是首篇聚焦LLMs教育应用（如学生评估）的综述研究。研究发现，这类系统最常见的应用场景是作为自动试题生成的虚拟导师，其中GPT-3和BERT是最主流的模型。值得注意的是，随着新型生成模型的持续涌现，预计短期内将有更多相关研究成果问世。


