# LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset

链接: http://arxiv.org/abs/2402.09391v1

原文摘要:
Chemistry plays a crucial role in many domains, such as drug discovery and
material science. While large language models (LLMs) such as GPT-4 exhibit
remarkable capabilities on natural language processing tasks, existing research
indicates that their performance on chemistry tasks is discouragingly low. In
this paper, however, we demonstrate that our developed LLMs can achieve very
strong results on a comprehensive set of chemistry tasks, outperforming the
most advanced GPT-4 and Claude 3 Opus by a substantial margin. To accomplish
this, we propose SMolInstruct, a large-scale, comprehensive, and high-quality
dataset for instruction tuning. It contains 14 selected chemistry tasks and
over three million samples, laying a solid foundation for training and
evaluating LLMs for chemistry. Using SMolInstruct, we fine-tune a set of
open-source LLMs, among which, we find that Mistral serves as the best base
model for chemistry tasks. Our analysis further demonstrates the critical role
of the proposed dataset in driving the performance improvements.

中文翻译:
化学在药物研发、材料科学等诸多领域具有关键作用。尽管GPT-4等大语言模型在自然语言处理任务中展现出卓越能力，但现有研究表明其在化学任务上的表现不尽如人意。本文通过实证研究发现，我们开发的大语言模型能在综合性化学任务集上取得显著优于当前最先进的GPT-4和Claude 3 Opus的强劲表现。为此，我们提出了SMolInstruct——一个大规模、全面且高质量的指令微调数据集。该数据集涵盖14个精选化学任务及超300万样本，为化学领域大语言模型的训练与评估奠定了坚实基础。基于SMolInstruct对多个开源大语言模型进行微调后，我们发现Mistral是化学任务的最佳基础模型。进一步分析证实，所构建数据集对模型性能提升具有决定性作用。

（翻译说明：采用学术论文摘要的规范表述，通过以下处理实现专业性与可读性平衡：
1. 专业术语准确对应："instruction tuning"译为"指令微调"，"base model"译为"基础模型"
2. 长句拆分重组：将原文复合句按中文习惯分解为短句，如将"To accomplish this..."独立成句
3. 逻辑显化处理：添加"为此"等连接词明确研究动机
4. 被动语态转化："it contains..."转为主动式"该数据集涵盖..."
5. 数据呈现优化："over three million"译为"超300万"符合中文计量习惯
6. 学术用语规范："demonstrate"译为"证实"而非口语化的"展示"）
