# WenyanGPT: A Large Language Model for Classical Chinese Tasks

链接: http://arxiv.org/abs/2504.20609v1

原文摘要:
Classical Chinese, as the core carrier of Chinese culture, plays a crucial
role in the inheritance and study of ancient literature. However, existing
natural language processing models primarily optimize for Modern Chinese,
resulting in inadequate performance on Classical Chinese. This paper presents a
comprehensive solution for Classical Chinese language processing. By continuing
pre-training and instruction fine-tuning on the LLaMA3-8B-Chinese model, we
construct a large language model, WenyanGPT, which is specifically designed for
Classical Chinese tasks. Additionally, we develop an evaluation benchmark
dataset, WenyanBENCH. Experimental results on WenyanBENCH demonstrate that
WenyanGPT significantly outperforms current advanced LLMs in various Classical
Chinese tasks. We make the model's training data, instruction fine-tuning
data\footnote, and evaluation benchmark dataset publicly available to promote
further research and development in the field of Classical Chinese processing.

中文翻译:
文言文作为中华文化的核心载体，在古代文献传承与研究领域具有关键作用。然而现有自然语言处理模型主要针对现代汉语优化，在文言文任务上表现欠佳。本文提出一套完整的文言文处理解决方案：基于LLaMA3-8B-Chinese模型进行持续预训练与指令微调，构建了专攻文言文任务的大语言模型"文渊GPT"（WenyanGPT）。同时开发了评测基准数据集"文渊评测"（WenyanBENCH）。实验结果表明，文渊GPT在文言文各项任务上的表现显著优于当前先进大语言模型。我们公开了模型训练数据、指令微调数据\footnote及评测基准数据集，以推动文言文处理领域的深入研究与发展。


