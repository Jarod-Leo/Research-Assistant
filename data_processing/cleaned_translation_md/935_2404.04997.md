# Adapting LLMs for Efficient Context Processing through Soft Prompt Compression

链接: http://arxiv.org/abs/2404.04997v1

原文摘要:
The rapid advancement of Large Language Models (LLMs) has inaugurated a
transformative epoch in natural language processing, fostering unprecedented
proficiency in text generation, comprehension, and contextual scrutiny.
Nevertheless, effectively handling extensive contexts, crucial for myriad
applications, poses a formidable obstacle owing to the intrinsic constraints of
the models' context window sizes and the computational burdens entailed by
their operations. This investigation presents an innovative framework that
strategically tailors LLMs for streamlined context processing by harnessing the
synergies among natural language summarization, soft prompt compression, and
augmented utility preservation mechanisms. Our methodology, dubbed
SoftPromptComp, amalgamates natural language prompts extracted from
summarization methodologies with dynamically generated soft prompts to forge a
concise yet semantically robust depiction of protracted contexts. This
depiction undergoes further refinement via a weighting mechanism optimizing
information retention and utility for subsequent tasks. We substantiate that
our framework markedly diminishes computational overhead and enhances LLMs'
efficacy across various benchmarks, while upholding or even augmenting the
caliber of the produced content. By amalgamating soft prompt compression with
sophisticated summarization, SoftPromptComp confronts the dual challenges of
managing lengthy contexts and ensuring model scalability. Our findings point
towards a propitious trajectory for augmenting LLMs' applicability and
efficiency, rendering them more versatile and pragmatic for real-world
applications. This research enriches the ongoing discourse on optimizing
language models, providing insights into the potency of soft prompts and
summarization techniques as pivotal instruments for the forthcoming generation
of NLP solutions.

中文翻译:
**译文：**  
大型语言模型（LLM）的快速发展开创了自然语言处理的变革时代，其在文本生成、语义理解和上下文分析方面展现出前所未有的能力。然而，受限于模型上下文窗口的固有约束及计算负担，如何高效处理长上下文（这一关键需求对众多应用至关重要）仍面临巨大挑战。本研究提出一种创新框架，通过整合自然语言摘要、软提示压缩与增强的效用保留机制，战略性地优化LLM以实现高效上下文处理。  

我们的方法名为**SoftPromptComp**，其核心在于融合摘要技术提取的自然语言提示与动态生成的软提示，从而构建出既简洁又语义丰富的长上下文表征。该表征通过加权机制进一步优化，以最大化后续任务中的信息保留与实用性。实验证明，该框架能显著降低计算开销，并在多项基准测试中提升LLM性能，同时保持甚至提高生成内容的质量。  

通过将软提示压缩与高级摘要技术相结合，SoftPromptComp有效应对了长上下文处理与模型可扩展性的双重挑战。研究结果表明，这一框架为增强LLM的适用性与效率开辟了可行路径，使其在现实应用中更具通用性与实用性。本研究成果为语言模型优化领域的持续探索提供了新视角，揭示了软提示与摘要技术作为下一代NLP解决方案关键工具的潜力。  

**注：**  
1. 采用学术论文常见的被动语态转主动语态处理（如"is substantiated"译为"实验证明"）  
2. 专业术语统一（如"soft prompt compression"固定译为"软提示压缩"）  
3. 长难句拆分重组（如原文首句拆分为因果逻辑更清晰的中文短句）  
4. 关键创新点名称"SoftPromptComp"保留英文并加粗，符合计算机领域惯例
