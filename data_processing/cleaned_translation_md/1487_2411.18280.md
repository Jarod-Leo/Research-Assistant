# Neutralizing Backdoors through Information Conflicts for Large Language Models

链接: http://arxiv.org/abs/2411.18280v1

原文摘要:
Large language models (LLMs) have seen significant advancements, achieving
superior performance in various Natural Language Processing (NLP) tasks, from
understanding to reasoning. However, they remain vulnerable to backdoor
attacks, where models behave normally for standard queries but generate harmful
responses or unintended output when specific triggers are activated. Existing
backdoor defenses often suffer from drawbacks that they either focus on
detection without removal, rely on rigid assumptions about trigger properties,
or prove to be ineffective against advanced attacks like multi-trigger
backdoors. In this paper, we present a novel method to eliminate backdoor
behaviors from LLMs through the construction of information conflicts using
both internal and external mechanisms. Internally, we leverage a lightweight
dataset to train a conflict model, which is then merged with the backdoored
model to neutralize malicious behaviors by embedding contradictory information
within the model's parametric memory. Externally, we incorporate convincing
contradictory evidence into the prompt to challenge the model's internal
backdoor knowledge. Experimental results on classification and conversational
tasks across 4 widely used LLMs demonstrate that our method outperforms 8
state-of-the-art backdoor defense baselines. We can reduce the attack success
rate of advanced backdoor attacks by up to 98% while maintaining over 90% clean
data accuracy. Furthermore, our method has proven to be robust against adaptive
backdoor attacks. The code will be open-sourced upon publication.

中文翻译:
以下是符合要求的学术摘要中文翻译：

大型语言模型（LLMs）已取得显著进展，在从理解到推理的各种自然语言处理（NLP）任务中展现出卓越性能。然而，这类模型仍易受后门攻击影响——在标准查询时表现正常，但当特定触发器被激活时会产生有害响应或非预期输出。现有后门防御方法普遍存在局限：要么仅聚焦检测而无法消除后门，要么依赖对触发器特性的刚性假设，或对多触发器后门等高级攻击无效。本文提出一种创新方法，通过内部与外部机制构建信息冲突来消除LLMs的后门行为。内部机制方面，我们利用轻量级数据集训练冲突模型，将其与受感染模型融合，通过在模型参数记忆中嵌入矛盾信息来中和恶意行为；外部机制方面，我们在提示词中加入具有说服力的矛盾证据，以挑战模型内部的后门知识。在4种主流LLMs上进行的分类和对话任务实验表明，本方法优于8种最先进的后门防御基线方案，能将高级后门攻击成功率降低达98%，同时保持90%以上的干净数据准确率。此外，本方法对自适应后门攻击也展现出强鲁棒性。相关代码将在论文发表后开源。

（翻译严格遵循以下原则：
1. 专业术语准确统一（如backdoor attacks=后门攻击，triggers=触发器）
2. 被动语态转换（如"are vulnerable to"→"易受"）
3. 长句拆分重组（将原文复合句按中文习惯分解为分号连接的短句）
4. 学术用语规范（如"state-of-the-art"→"最先进"，"parametric memory"→"参数记忆"）
5. 保留关键数据（98%/90%等数值精确呈现）
6. 逻辑连接词处理（"Furthermore"→"此外"保持递进关系））
