# Perturbed examples reveal invariances shared by language models

链接: http://arxiv.org/abs/2311.04166v1

原文摘要:
The rapid growth in natural language processing (NLP) research has led to
numerous new models, outpacing our understanding of how they compare to
established ones. One major reason for this difficulty is saturating
benchmarks, which may not well reflect differences in model performance in the
wild. In this work, we introduce a novel framework to compare two NLP models by
revealing their shared invariance to interpretable input perturbations
targeting a specific linguistic capability. Via experiments on models from the
same and different architecture families, this framework offers insights about
how changes in models (e.g., distillation, size increase) affect linguistic
capabilities. Furthermore, our framework enables evaluation of invariances
between commercial black-box models (e.g., InstructGPT family) and models that
are better understood (e.g., GPT-2). Across experiments, we observe that large
language models share many invariances encoded by models of various sizes,
whereas the invariances by large models are only shared by other large models.
Possessing a wide variety of invariances may be key to the recent successes of
large language models, and our framework can shed light on the types of
invariances retained or emerging in new models. We make the code publicly
available.

中文翻译:
自然语言处理（NLP）研究的快速发展催生了大量新模型，其涌现速度已超越我们对这些模型与传统模型差异的认知。造成这一困境的主要原因是现有基准测试趋于饱和，可能无法有效反映模型在真实场景中的性能差异。本研究提出了一种创新框架，通过揭示两种NLP模型对特定语言能力靶向的可解释输入扰动所表现出的共有不变性来进行模型比较。通过对同架构家族及跨架构家族模型的实验，该框架揭示了模型变化（如知识蒸馏、规模扩大）对语言能力的影响机制。更重要的是，我们的框架能够评估商业黑箱模型（如InstructGPT系列）与可解释性更强的模型（如GPT-2）之间的不变性关联。实验结果表明：大型语言模型普遍具备各类规模模型共有的不变性特征，而大型模型特有的不变性仅存在于同类大模型中。拥有广泛的不变性特征可能是大语言模型近期取得成功的关键因素，我们的框架能够揭示新模型保留或涌现的不变性类型。相关代码已开源发布。  

（翻译说明：  
1. 专业术语处理："invariance"译为"不变性"符合机器学习领域术语规范  
2. 长句拆分：将原文复合句拆分为符合中文表达习惯的短句结构  
3. 被动语态转换："are shared by"等被动结构转换为主动句式  
4. 概念显化："in the wild"意译为"真实场景"而非字面直译  
5. 逻辑衔接：通过"更重要的是"等连接词保持学术文本的论证逻辑  
6. 文化适配："black-box"采用"黑箱"这一中文科技文献常用表述）
