# Selection-p: Self-Supervised Task-Agnostic Prompt Compression for Faithfulness and Transferability

链接: http://arxiv.org/abs/2410.11786v1

原文摘要:
Large Language Models (LLMs) have demonstrated impressive capabilities in a
wide range of natural language processing tasks when leveraging in-context
learning. To mitigate the additional computational and financial costs
associated with in-context learning, several prompt compression methods have
been proposed to compress the in-context learning prompts. Despite their
success, these methods face challenges with transferability due to
model-specific compression, or rely on external training data, such as GPT-4.
In this paper, we investigate the ability of LLMs to develop a unified
compression method that discretizes uninformative tokens, utilizing a
self-supervised pre-training technique. By introducing a small number of
parameters during the continual pre-training, the proposed Selection-p produces
a probability for each input token, indicating whether to preserve or discard
it. Experiments show Selection-p achieves state-of-the-art performance across
numerous classification tasks, achieving compression rates of up to 10 times
while experiencing only a marginal 0.8% decrease in performance. Moreover, it
exhibits superior transferability to different models compared to prior work.
Additionally, we further analyze how Selection-p helps maintain performance on
in-context learning with long contexts.

中文翻译:
大语言模型（LLMs）在利用上下文学习时，已展现出处理各类自然语言任务的卓越能力。为降低上下文学习带来的额外计算与成本负担，研究者们提出了多种提示压缩方法。尽管现有方法成效显著，但仍面临两大挑战：因模型特异性压缩导致的迁移性不足，或依赖外部训练数据（如GPT-4）。本文探索了LLMs通过自监督预训练技术开发统一压缩方法的能力，该方法通过离散化非信息性标记来实现。通过在持续预训练阶段引入少量参数，我们提出的Selection-p方法能为每个输入标记生成保留或丢弃的概率值。实验表明，Selection-p在多项分类任务中达到最先进性能，在实现最高10倍压缩率的同时，性能损失仅0.8%。相较于现有研究，该方法展现出更优异的跨模型迁移能力。此外，我们还深入分析了Selection-p如何帮助模型在长上下文学习中保持性能稳定性。

（翻译说明：采用技术论文的标准表述方式，通过以下处理确保专业性与可读性：
1. 专业术语统一："in-context learning"译为"上下文学习"，"self-supervised"译为"自监督"
2. 长句拆分：将原文复合句按中文习惯分解为多个短句，如将"utilizing..."独立成短句
3. 被动语态转化："have been proposed"转为主动态"研究者们提出了"
4. 概念显化："discretizes uninformative tokens"译为"离散化非信息性标记"而非字面直译
5. 数据呈现优化：保留具体数值"10倍/0.8%"，符合中文科技论文表述规范
6. 逻辑连接处理："Moreover"转为"相较于"，更符合中文递进关系表达）
