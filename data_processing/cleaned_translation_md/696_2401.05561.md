# TrustLLM: Trustworthiness in Large Language Models

链接: http://arxiv.org/abs/2401.05561v1

原文摘要:
Large language models (LLMs), exemplified by ChatGPT, have gained
considerable attention for their excellent natural language processing
capabilities. Nonetheless, these LLMs present many challenges, particularly in
the realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs
emerges as an important topic. This paper introduces TrustLLM, a comprehensive
study of trustworthiness in LLMs, including principles for different dimensions
of trustworthiness, established benchmark, evaluation, and analysis of
trustworthiness for mainstream LLMs, and discussion of open challenges and
future directions. Specifically, we first propose a set of principles for
trustworthy LLMs that span eight different dimensions. Based on these
principles, we further establish a benchmark across six dimensions including
truthfulness, safety, fairness, robustness, privacy, and machine ethics. We
then present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of
over 30 datasets. Our findings firstly show that in general trustworthiness and
utility (i.e., functional effectiveness) are positively related. Secondly, our
observations reveal that proprietary LLMs generally outperform most open-source
counterparts in terms of trustworthiness, raising concerns about the potential
risks of widely accessible open-source LLMs. However, a few open-source LLMs
come very close to proprietary ones. Thirdly, it is important to note that some
LLMs may be overly calibrated towards exhibiting trustworthiness, to the extent
that they compromise their utility by mistakenly treating benign prompts as
harmful and consequently not responding. Finally, we emphasize the importance
of ensuring transparency not only in the models themselves but also in the
technologies that underpin trustworthiness. Knowing the specific trustworthy
technologies that have been employed is crucial for analyzing their
effectiveness.

中文翻译:
以ChatGPT为代表的大型语言模型（LLM）凭借其卓越的自然语言处理能力获得了广泛关注。然而这类模型也带来了诸多挑战，尤其在可信度方面存在显著问题，因此确保LLM的可信性成为重要议题。本文提出TrustLLM研究框架，对LLM可信度开展全面研究，包括多维度可信原则制定、主流LLM可信度基准建立与评估分析，以及开放挑战与未来方向的探讨。具体而言，我们首先提出涵盖八个维度的可信LLM原则体系，并基于这些原则构建了包含真实性、安全性、公平性、鲁棒性、隐私性和机器伦理六大维度的评估基准。随后我们通过超过30个数据集对16个主流LLM进行了可信度评估研究，主要发现如下：首先，整体而言模型可信度与实用性（即功能有效性）呈正相关；其次，商业闭源LLM在可信度方面普遍优于多数开源模型，这引发了关于广泛流通的开源LLM潜在风险的担忧，但少数开源模型表现已接近商业模型；第三需注意的是，部分LLM可能过度校准可信度表现，以致将良性指令误判为有害请求而拒绝响应，反而损害了其实用性；最后我们强调，不仅需要模型本身的透明度，支撑可信度的技术透明度同样重要，了解具体采用的可信技术对分析其有效性至关重要。
