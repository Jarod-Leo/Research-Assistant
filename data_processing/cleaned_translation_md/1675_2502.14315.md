# Unveiling Cultural Blind Spots: Analyzing the Limitations of mLLMs in Procedural Text Comprehension

链接: http://arxiv.org/abs/2502.14315v1

原文摘要:
Despite the impressive performance of multilingual large language models
(mLLMs) in various natural language processing tasks, their ability to
understand procedural texts, particularly those with culture-specific content,
remains largely unexplored. Texts describing cultural procedures, including
rituals, traditional craftsmanship, and social etiquette, require an inherent
understanding of cultural context, presenting a significant challenge for
mLLMs. In this work, we introduce CAPTex, a benchmark designed to evaluate
mLLMs' ability to process and reason about culturally diverse procedural texts
across multiple languages using various methodologies to assess their
performance. Our findings indicate that (1) mLLMs face difficulties with
culturally contextualized procedural texts, showing notable performance
declines in low-resource languages, (2) model performance fluctuates across
cultural domains, with some areas presenting greater difficulties, and (3)
language models exhibit better performance on multiple-choice tasks within
conversational frameworks compared to direct questioning. These results
underscore the current limitations of mLLMs in handling culturally nuanced
procedural texts and highlight the need for culturally aware benchmarks like
CAPTex to enhance their adaptability and comprehension across diverse
linguistic and cultural landscapes.

中文翻译:
尽管多语言大语言模型（mLLMs）在各种自然语言处理任务中表现卓越，但其对程序性文本——尤其是蕴含文化特定内容文本的理解能力仍待深入探索。描述文化相关程序（包括仪式、传统工艺、社交礼仪等）的文本需要内在的文化语境理解能力，这对mLLMs构成了重大挑战。本研究推出CAPTex评估基准，旨在通过多种方法论评估mLLMs处理及推理跨语言文化多样性程序性文本的能力。研究发现：（1）mLLMs在处理文化语境化的程序性文本时存在困难，尤其在低资源语言中表现显著下降；（2）模型表现随文化领域波动，某些领域挑战更大；（3）与直接提问相比，语言模型在对话框架下的多选题任务中表现更优。这些结果揭示了当前mLLMs在处理文化敏感程序性文本时的局限性，并凸显了需要CAPTex这类文化感知基准来提升模型在多元语言文化环境中的适应性与理解力。
