# Revisiting Topic-Guided Language Models

链接: http://arxiv.org/abs/2312.02331v1

原文摘要:
A recent line of work in natural language processing has aimed to combine
language models and topic models. These topic-guided language models augment
neural language models with topic models, unsupervised learning methods that
can discover document-level patterns of word use. This paper compares the
effectiveness of these methods in a standardized setting. We study four
topic-guided language models and two baselines, evaluating the held-out
predictive performance of each model on four corpora. Surprisingly, we find
that none of these methods outperform a standard LSTM language model baseline,
and most fail to learn good topics. Further, we train a probe of the neural
language model that shows that the baseline's hidden states already encode
topic information. We make public all code used for this study.

中文翻译:
近期自然语言处理领域的一系列研究致力于将语言模型与主题模型相结合。这些主题引导的语言模型通过主题模型（一种能发现文档层面词汇使用模式的无监督学习方法）来增强神经语言模型。本文在标准化环境下比较了这些方法的有效性，研究了四种主题引导语言模型和两种基线模型，并在四个语料库上评估了每个模型在保留测试集上的预测性能。令人惊讶的是，我们发现这些方法均未能超越标准LSTM语言模型基线，且大多数方法无法学习到优质主题。进一步地，我们通过神经语言模型的探测实验表明，基线模型的隐藏状态本身已编码了主题信息。本研究使用的全部代码已公开。
