# Seeing Eye to AI: Human Alignment via Gaze-Based Response Rewards for Large Language Models

链接: http://arxiv.org/abs/2410.01532v1

原文摘要:
Advancements in Natural Language Processing (NLP), have led to the emergence
of Large Language Models (LLMs) such as GPT, Llama, Claude, and Gemini, which
excel across a range of tasks but require extensive fine-tuning to align their
outputs with human expectations. A widely used method for achieving this
alignment is Reinforcement Learning from Human Feedback (RLHF), which, despite
its success, faces challenges in accurately modelling human preferences. In
this paper, we introduce GazeReward, a novel framework that integrates implicit
feedback -- and specifically eye-tracking (ET) data -- into the Reward Model
(RM). In addition, we explore how ET-based features can provide insights into
user preferences. Through ablation studies we test our framework with different
integration methods, LLMs, and ET generator models, demonstrating that our
approach significantly improves the accuracy of the RM on established human
preference datasets. This work advances the ongoing discussion on optimizing AI
alignment with human values, exploring the potential of cognitive data for
shaping future NLP research.

中文翻译:
自然语言处理（NLP）领域的进步催生了GPT、Llama、Claude和Gemini等大型语言模型（LLMs），这些模型虽在多项任务中表现卓越，但需要大量微调才能使其输出符合人类预期。实现这种对齐的常用方法是基于人类反馈的强化学习（RLHF），尽管该方法成效显著，但在精确建模人类偏好方面仍面临挑战。本文提出GazeReward创新框架，将眼动追踪（ET）数据等隐式反馈整合至奖励模型（RM）中。我们进一步探究基于ET的特征如何揭示用户偏好，通过消融实验测试了不同整合方法、LLMs及ET生成模型的效果，证明该框架显著提升了RM在权威人类偏好数据集上的准确率。本研究通过探索认知数据潜力，为优化人工智能与人类价值观对齐的持续讨论提供了新思路，为未来NLP研究指明了方向。

（翻译说明：采用学术论文摘要的标准结构，保持专业术语一致性；将英语长句拆解为符合中文表达习惯的短句；"ablation studies"译为"消融实验"符合机器学习领域术语规范；"advances the ongoing discussion"意译为"为持续讨论提供了新思路"实现动态对等；通过"探究""证明""指明"等动词保持学术文本的严谨性；最后一句采用分号结构处理英文原句的嵌套关系，确保中文行文流畅。）
