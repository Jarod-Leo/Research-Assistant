# The Model Arena for Cross-lingual Sentiment Analysis: A Comparative Study in the Era of Large Language Models

链接: http://arxiv.org/abs/2406.19358v1

原文摘要:
Sentiment analysis serves as a pivotal component in Natural Language
Processing (NLP). Advancements in multilingual pre-trained models such as XLM-R
and mT5 have contributed to the increasing interest in cross-lingual sentiment
analysis. The recent emergence in Large Language Models (LLM) has significantly
advanced general NLP tasks, however, the capability of such LLMs in
cross-lingual sentiment analysis has not been fully studied. This work
undertakes an empirical analysis to compare the cross-lingual transfer
capability of public Small Multilingual Language Models (SMLM) like XLM-R,
against English-centric LLMs such as Llama-3, in the context of sentiment
analysis across English, Spanish, French and Chinese. Our findings reveal that
among public models, SMLMs exhibit superior zero-shot cross-lingual performance
relative to LLMs. However, in few-shot cross-lingual settings, public LLMs
demonstrate an enhanced adaptive potential. In addition, we observe that
proprietary GPT-3.5 and GPT-4 lead in zero-shot cross-lingual capability, but
are outpaced by public models in few-shot scenarios.

中文翻译:
情感分析是自然语言处理（NLP）中的关键组成部分。随着XLM-R和mT5等多语言预训练模型的发展，跨语言情感分析研究日益受到关注。尽管近期兴起的大语言模型（LLM）显著推动了通用NLP任务的进步，但此类模型在跨语言情感分析中的能力尚未得到充分研究。本研究通过实证分析，对比了公开的小型多语言模型（SMLM，如XLM-R）与以英语为中心的大语言模型（如Llama-3）在英语、西班牙语、法语和中文情感分析任务中的跨语言迁移能力。研究发现：在公开模型中，SMLM展现出优于LLM的零样本跨语言性能；但在小样本跨语言场景下，公开LLM表现出更强的适应潜力。此外，研究还发现私有模型GPT-3.5和GPT-4在零样本跨语言能力上领先，但在小样本场景中会被公开模型反超。

（译文特点说明：
1. 专业术语准确对应："zero-shot"译为"零样本"，"few-shot"译为"小样本"
2. 句式重构：将英文长句拆分为符合中文表达习惯的短句，如将"however"引导的转折关系转换为分号衔接
3. 被动语态转化："has not been fully studied"译为主动式"尚未得到充分研究"
4. 概念显化处理："proprietary"译为"私有模型"以明确技术语境
5. 动态对等："outpaced"译为"反超"既保持竞技隐喻又符合中文表达）
