# One Arrow, Many Targets: Probing LLMs for Multi-Attribute Controllable Text Summarization

链接: http://arxiv.org/abs/2411.01213v1

原文摘要:
Text summarization is a well-established task within the natural language
processing (NLP) community. However, the focus on controllable summarization
tailored to user requirements is gaining traction only recently. While several
efforts explore controllability in text summarization, the investigation of
Multi-Attribute Controllable Summarization (MACS) remains limited. This work
addresses this gap by examining the MACS task through the lens of large
language models (LLMs), using various learning paradigms, particularly low-rank
adapters. We experiment with different popular adapter fine-tuning strategies
to assess the effectiveness of the resulting models in retaining cues and
patterns associated with multiple controllable attributes. Additionally, we
propose and evaluate a novel hierarchical adapter fusion technique to integrate
learnings from two distinct controllable attributes. Subsquently, we present
our findings, discuss the challenges encountered, and suggest potential avenues
for advancing the MACS task.

中文翻译:
文本摘要是自然语言处理（NLP）领域一项成熟的研究任务。然而，针对用户需求定制的可控文本摘要技术直到最近才受到广泛关注。尽管已有若干研究探索文本摘要的可控性，但针对多属性可控摘要（MACS）的研究仍存在明显空白。本研究通过大语言模型（LLMs）的视角，采用多种学习范式（尤其是低秩适配器技术）来填补这一研究缺口。我们通过实验对比了多种主流适配器微调策略，评估所得模型在保留多可控属性关联特征与模式方面的有效性。此外，我们提出并评估了一种新颖的分层适配器融合技术，用于整合两种不同可控属性的学习成果。最后，我们呈现了实验发现，探讨了研究过程中遇到的挑战，并为推进MACS任务的发展提出了潜在研究方向。

（翻译说明：
1. 专业术语处理："low-rank adapters"译为技术界通用译法"低秩适配器"，"fine-tuning"统一译为"微调"
2. 句式重构：将英语长句拆分为符合中文表达习惯的短句，如第一句通过"然而"转折连接两个分句
3. 被动语态转换：将"the investigation...remains limited"等被动结构转换为中文主动表述
4. 概念显化："Subsquently"隐含的递进关系通过"最后"明确呈现
5. 术语一致性：全文统一"MACS"译名为"多属性可控摘要"，首次出现标注英文缩写
6. 学术风格保持：使用"评估""探讨""呈现"等学术用语，避免口语化表达）
