# What Makes Good Examples for Visual In-Context Learning?

链接: http://arxiv.org/abs/2301.13670v2

原文摘要:
Large-scale models trained on broad data have recently become the mainstream
architecture in computer vision due to their strong generalization performance.
In this paper, the main focus is on an emergent ability in large vision models,
known as in-context learning, which allows inference on unseen tasks by
conditioning on in-context examples (a.k.a.~prompt) without updating the model
parameters. This concept has been well-known in natural language processing but
has only been studied very recently for large vision models. We for the first
time provide a comprehensive investigation on the impact of in-context examples
in computer vision, and find that the performance is highly sensitive to the
choice of in-context examples. To overcome the problem, we propose a prompt
retrieval framework to automate the selection of in-context examples.
Specifically, we present (1) an unsupervised prompt retrieval method based on
nearest example search using an off-the-shelf model, and (2) a supervised
prompt retrieval method, which trains a neural network to choose examples that
directly maximize in-context learning performance. The results demonstrate that
our methods can bring non-trivial improvements to visual in-context learning in
comparison to the commonly-used random selection.

中文翻译:
基于海量数据训练的大规模模型凭借其强大的泛化性能，近期已成为计算机视觉领域的主流架构。本文重点研究大视觉模型中涌现的一种新能力——上下文学习，该能力通过基于上下文示例（即提示）进行推理而无需更新模型参数，即可处理未见过的任务。这一概念在自然语言处理领域已广为人知，但在大视觉模型中的研究才刚刚起步。我们首次系统性地探究了上下文示例对计算机视觉任务的影响，发现模型性能对示例选择极为敏感。为解决该问题，我们提出了一种提示检索框架来自动化选择上下文示例。具体而言，我们开发了：（1）基于现成模型的最近邻搜索无监督提示检索方法；（2）监督式提示检索方法，通过训练神经网络直接选择能最大化上下文学习性能的示例。实验结果表明，相较于常用的随机选择方法，我们的方案能为视觉上下文学习带来显著提升。
