# Beyond Fine-Tuning: Effective Strategies for Mitigating Hallucinations in Large Language Models for Data Analytics

链接: http://arxiv.org/abs/2410.20024v1

原文摘要:
Large Language Models (LLMs) have become increasingly important in natural
language processing, enabling advanced data analytics through natural language
queries. However, these models often generate "hallucinations"-inaccurate or
fabricated information-that can undermine their reliability in critical
data-driven decision-making. Addressing the challenge of hallucinations is
essential to improve the accuracy and trustworthiness of LLMs in processing
natural language queries. This research focuses on mitigating hallucinations in
LLMs, specifically within the context of data analytics. We introduce and
evaluate four targeted strategies: Structured Output Generation, Strict Rules
Enforcement, System Prompt Enhancements, and Semantic Layer Integration. Our
findings show that these methods are more effective than traditional
fine-tuning approaches in reducing hallucinations, offering a more reliable
framework for deploying LLMs in natural language queries for data analytics.
This research demonstrates the potential of these strategies to enhance the
accuracy of LLM-driven data queries, ensuring more dependable results in
data-driven environments.

中文翻译:
以下是符合您要求的中文翻译：

大型语言模型（LLMs）在自然语言处理领域日益重要，能够通过自然语言查询实现高级数据分析。然而，这些模型常产生"幻觉"——即不准确或虚构的信息——这可能危及关键数据驱动决策中的可靠性。解决幻觉问题对于提升LLMs处理自然语言查询的准确性与可信度至关重要。本研究专注于缓解数据分析场景下LLMs的幻觉现象，提出并评估了四种针对性策略：结构化输出生成、严格规则执行、系统提示增强和语义层集成。实验结果表明，这些方法在减少幻觉方面比传统微调策略更有效，为数据分析领域的自然语言查询提供了更可靠的LLM部署框架。本研究证明了这些策略在提升LLM驱动数据查询准确性方面的潜力，能够确保数据驱动环境中获得更可信的结果。

（翻译严格遵循学术规范，保持专业术语一致性："hallucinations"统一译为"幻觉"；采用符合中文科技论文的句式结构；通过破折号和括号处理英文特殊标点；专业表述如"fine-tuning"译为行业通用术语"微调"；在保持原文严谨性的同时，对英语长句进行符合中文阅读习惯的拆分重组）
