# CritiqueLLM: Scaling LLM-as-Critic for Effective and Explainable Evaluation of Large Language Model Generation

链接: http://arxiv.org/abs/2311.18702v1

原文摘要:
Since the natural language processing (NLP) community started to make large
language models (LLMs) act as a critic to evaluate the quality of generated
texts, most of the existing works train a critique generation model on the
evaluation data labeled by GPT-4's direct prompting. We observe that these
models lack the ability to generate informative critiques in both pointwise
grading and pairwise comparison especially without references. As a result,
their generated critiques cannot provide fine-grained distinguishability on
generated texts, causing unsatisfactory evaluation performance. In this paper,
we propose a simple yet effective method called Eval-Instruct, which can first
acquire pointwise grading critiques with pseudo references and then revise
these critiques via multi-path prompting to obtain informative evaluation data
in different tasks and settings, including pointwise grading and pairwise
comparison with / without references. After fine-tuning on these data, the
resulting model CritiqueLLM is empirically shown to outperform ChatGPT and all
the open-source baselines and even achieve comparable evaluation performance to
GPT-4 in system-level correlations of pointwise grading. We also demonstrate
that our generated critiques can act as scalable feedback to further improve
the generation quality of strong LLMs like ChatGPT.

中文翻译:
自自然语言处理（NLP）领域开始利用大语言模型（LLMs）作为生成文本质量的评估者以来，现有研究大多基于GPT-4直接提示标注的评估数据来训练评论生成模型。我们发现这些模型在单点评分和成对比较（尤其在无参考文本时）场景下缺乏生成信息量丰富的评论能力，导致其生成的评论无法对文本质量进行细粒度区分，造成评估效果欠佳。本文提出一种简单有效的方法Eval-Instruct：首先生成带有伪参考文本的单点评分评论，继而通过多路径提示修订这些评论，最终获得适用于不同任务与场景（含带/无参考文本的单点评分及成对比较）的高信息量评估数据。基于这些数据微调得到的CritiqueLLM模型在实证研究中表现优于ChatGPT及所有开源基线模型，在单点评分的系统级相关性指标上甚至达到与GPT-4相当的评估性能。我们还证明生成的评论可作为可扩展反馈，进一步提升如ChatGPT等强语言模型的生成质量。

（翻译说明：采用学术论文摘要的标准句式结构，保留术语一致性如"pseudo references"译为"伪参考文本"；将"multi-path prompting"意译为"多路径提示"以保持技术准确性；处理长句时拆分英文被动语态为中文主动句式；"scalable feedback"译为"可扩展反馈"既符合中文表达习惯又准确传递技术内涵；通过"实证研究""系统级相关性"等措辞确保学术严谨性。）
