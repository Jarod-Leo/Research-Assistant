# TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation

链接: http://arxiv.org/abs/2401.14373v1

原文摘要:
The recent advances in natural language processing have predominantly favored
well-resourced English-centric models, resulting in a significant gap with
low-resource languages. In this work, we introduce the language model TURNA,
which is developed for the low-resource language Turkish and is capable of both
natural language understanding and generation tasks. TURNA is pretrained with
an encoder-decoder architecture based on the unified framework UL2 with a
diverse corpus that we specifically curated for this purpose. We evaluated
TURNA with three generation tasks and five understanding tasks for Turkish. The
results show that TURNA outperforms several multilingual models in both
understanding and generation tasks, and competes with monolingual Turkish
models in understanding tasks. TURNA is made available at
https://huggingface.co/boun-tabi-LMG/TURNA .

中文翻译:
近期自然语言处理领域的进展主要集中于资源丰富的英语核心模型，导致低资源语言存在显著差距。本研究针对低资源语言土耳其语开发了兼具自然语言理解与生成能力的TURNA语言模型。该模型基于UL2统一框架的编码器-解码器架构进行预训练，并采用我们为此专门构建的多类型语料库。我们在土耳其语的三个生成任务和五个理解任务上对TURNA进行评估，结果表明：无论在理解还是生成任务上，TURNA均优于多个多语言模型；在理解任务方面，其性能可与土耳其语单语模型相媲美。该模型已发布于https://huggingface.co/boun-tabi-LMG/TURNA。

（翻译说明：
1. 专业术语处理："encoder-decoder architecture"译为"编码器-解码器架构"，"multilingual models"译为"多语言模型"
2. 长句拆分：将原文复合句拆分为符合中文表达习惯的短句结构
3. 被动语态转换："is pretrained"译为主动态的"进行预训练"
4. 概念显化："diverse corpus"译为"多类型语料库"以明确其特性
5. 学术规范：保留技术框架名称"UL2"及模型名称"TURNA"原文
6. 链接处理：完整保留原始URL以确保可访问性
7. 逻辑连接：通过"无论...还是..."等连接词保持论证逻辑的连贯性）
