# Developing and Utilizing a Large-Scale Cantonese Dataset for Multi-Tasking in Large Language Models

链接: http://arxiv.org/abs/2503.03702v1

原文摘要:
High-quality data resources play a crucial role in learning large language
models (LLMs), particularly for low-resource languages like Cantonese. Despite
having more than 85 million native speakers, Cantonese is still considered a
low-resource language in the field of natural language processing (NLP) due to
factors such as the dominance of Mandarin, lack of cohesion within the
Cantonese-speaking community, diversity in character encoding and input
methods, and the tendency of overseas Cantonese speakers to prefer using
English. In addition, rich colloquial vocabulary of Cantonese, English
loanwords, and code-switching characteristics add to the complexity of corpus
collection and processing. To address these challenges, we collect Cantonese
texts from a variety of sources, including open source corpora, Hong
Kong-specific forums, Wikipedia, and Common Crawl data. We conduct rigorous
data processing through language filtering, quality filtering, content
filtering, and de-duplication steps, successfully constructing a high-quality
Cantonese corpus of over 2 billion tokens for training large language models.
We further refined the model through supervised fine-tuning (SFT) on curated
Cantonese tasks, enhancing its ability to handle specific applications. Upon
completion of the training, the model achieves state-of-the-art (SOTA)
performance on four Cantonese benchmarks. After training on our dataset, the
model also exhibits improved performance on other mainstream language tasks.

中文翻译:
高质量数据资源对大型语言模型（LLM）的学习具有关键作用，尤其对粤语这类低资源语言而言更是如此。尽管粤语拥有超过8500万母语者，但由于普通话的强势地位、粤语社区内部缺乏凝聚力、字符编码与输入法的多样性，以及海外粤语人群更倾向使用英语等因素，其在自然语言处理（NLP）领域仍被视为低资源语言。此外，粤语丰富的口语词汇、英语借词及语码转换特性，进一步增加了语料收集与处理的复杂度。为应对这些挑战，我们从开源语料库、香港本地论坛、维基百科和Common Crawl数据等多渠道收集粤语文本，通过语言过滤、质量筛选、内容过滤及去重等严格的数据处理步骤，成功构建了包含超过20亿词元的高质量粤语语料库，用于训练大型语言模型。我们进一步在精选的粤语任务上进行监督微调（SFT），优化模型处理特定应用的能力。训练完成后，该模型在四项粤语基准测试中达到最先进（SOTA）水平。经本数据集训练后，模型在其他主流语言任务上也展现出性能提升。

（翻译说明：  
1. 专业术语保留英文缩写并首次出现时标注全称（如LLM/Large Language Model译为"大型语言模型"并标注"LLM"）  
2. 文化负载词采用意译+补充说明策略（如"code-switching"译为"语码转换"）  
3. 长句按中文表达习惯拆分重组（如将原文复合从句分解为多个短句）  
4. 计量单位本地化处理（"85 million"译为"8500万"符合中文数字表达习惯）  
5. 被动语态转为主动句式（如"is still considered"译为"被视为"）  
6. 技术概念采用业界通用译法（如"SFT"统一译为"监督微调"））
