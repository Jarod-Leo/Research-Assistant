# Efficient Long-Range Transformers: You Need to Attend More, but Not Necessarily at Every Layer

链接: http://arxiv.org/abs/2310.12442v1

原文摘要:
Pretrained transformer models have demonstrated remarkable performance across
various natural language processing tasks. These models leverage the attention
mechanism to capture long- and short-range dependencies in the sequence.
However, the (full) attention mechanism incurs high computational cost -
quadratic in the sequence length, which is not affordable in tasks with long
sequences, e.g., inputs with 8k tokens. Although sparse attention can be used
to improve computational efficiency, as suggested in existing work, it has
limited modeling capacity and often fails to capture complicated dependencies
in long sequences. To tackle this challenge, we propose MASFormer, an
easy-to-implement transformer variant with Mixed Attention Spans. Specifically,
MASFormer is equipped with full attention to capture long-range dependencies,
but only at a small number of layers. For the remaining layers, MASformer only
employs sparse attention to capture short-range dependencies. Our experiments
on natural language modeling and generation tasks show that a decoder-only
MASFormer model of 1.3B parameters can achieve competitive performance to
vanilla transformers with full attention while significantly reducing
computational cost (up to 75%). Additionally, we investigate the effectiveness
of continual training with long sequence data and how sequence length impacts
downstream generation performance, which may be of independent interest.

中文翻译:
以下是符合要求的学术中文翻译：

预训练Transformer模型已在各类自然语言处理任务中展现出卓越性能。这类模型通过注意力机制捕捉序列中的长程与短程依赖关系。然而，（全）注意力机制会导致高昂的计算成本——其复杂度随序列长度呈平方级增长，这对于长序列任务（如包含8k个标记的输入）难以承受。尽管现有研究表明稀疏注意力能提升计算效率，但其建模能力有限，往往难以捕捉长序列中的复杂依赖关系。为解决这一挑战，我们提出MASFormer——一种易于实现的混合注意力跨度Transformer变体。具体而言，MASFormer仅在少量层级配备全注意力以捕获长程依赖，其余层级则仅采用稀疏注意力处理短程依赖。在自然语言建模与生成任务上的实验表明：仅含13亿参数的纯解码器MASFormer模型，在显著降低计算成本（最高达75%）的同时，能达到与全注意力标准Transformer相当的性能。此外，我们还探究了长序列数据持续训练的有效性，以及序列长度对下游生成任务性能的影响机制，这些发现可能具有独立研究价值。

翻译说明：
1. 专业术语处理：
- "attention mechanism"统一译为"注意力机制"
- "sparse/full attention"分别译为"稀疏/全注意力"
- "decoder-only"译为"纯解码器"
- "vanilla transformers"译为"标准Transformer"（避免直译"香草"）

2. 句式重构：
- 将英文长句拆分为符合中文表达习惯的短句（如原文第一句拆分为两个语义单元）
- 被动语态转换（如"is equipped with"译为主动式"配备"）
- 添加连接词保持逻辑连贯（如"此外"）

3. 学术规范：
- 保留技术指标（"1.3B参数"、"8k tokens"）
- 专业表述（如"平方级增长"而非"二次方"）
- 术语一致性（全篇统一"长程/短程依赖"表述）

4. 文化适配：
- "easy-to-implement"译为"易于实现"而非字面直译
- "of independent interest"译为"具有独立研究价值"符合中文论文惯用表述
