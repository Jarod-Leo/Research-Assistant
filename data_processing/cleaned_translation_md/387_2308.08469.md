# LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs

链接: http://arxiv.org/abs/2308.08469v1

原文摘要:
Multivariate time-series forecasting is vital in various domains, e.g.,
economic planning and weather prediction. Deep train-from-scratch models have
exhibited effective performance yet require large amounts of data, which limits
real-world applicability. Recently, researchers have leveraged the
representation learning transferability of pre-trained Large Language Models
(LLMs) to handle limited non-linguistic datasets effectively. However,
incorporating LLMs with time-series data presents challenges of limited
adaptation due to different compositions between time-series and linguistic
data, and the inability to process multi-scale temporal information. To tackle
these challenges, we propose LLM4TS, a framework for time-series forecasting
with pre-trained LLMs. LLM4TS consists of a two-stage fine-tuning strategy: the
time-series alignment stage to align LLMs with the nuances of time-series data,
and the forecasting fine-tuning stage for downstream time-series forecasting
tasks. Furthermore, our framework features a novel two-level aggregation method
that integrates multi-scale temporal data within pre-trained LLMs, enhancing
their ability to interpret time-specific information. In experiments across 7
time-series forecasting datasets, LLM4TS is superior to existing
state-of-the-art methods compared with trained-from-scratch models in full-shot
scenarios, and also achieves the highest rank in few-shot scenarios. In
addition, evaluations compared with different unsupervised representation
learning approaches highlight LLM4TS's effectiveness with representation
learning in forecasting tasks. Ablation studies further validate each
component's contribution to LLM4TS and underscore the essential role of
utilizing LLM's pre-trained weights for optimal performance. The code is
available at 