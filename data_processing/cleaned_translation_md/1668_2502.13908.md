# Judging the Judges: A Collection of LLM-Generated Relevance Judgements

链接: http://arxiv.org/abs/2502.13908v1

原文摘要:
Using Large Language Models (LLMs) for relevance assessments offers promising
opportunities to improve Information Retrieval (IR), Natural Language
Processing (NLP), and related fields. Indeed, LLMs hold the promise of allowing
IR experimenters to build evaluation collections with a fraction of the manual
human labor currently required. This could help with fresh topics on which
there is still limited knowledge and could mitigate the challenges of
evaluating ranking systems in low-resource scenarios, where it is challenging
to find human annotators. Given the fast-paced recent developments in the
domain, many questions concerning LLMs as assessors are yet to be answered.
Among the aspects that require further investigation, we can list the impact of
various components in a relevance judgment generation pipeline, such as the
prompt used or the LLM chosen.
  This paper benchmarks and reports on the results of a large-scale automatic
relevance judgment evaluation, the LLMJudge challenge at SIGIR 2024, where
different relevance assessment approaches were proposed. In detail, we release
and benchmark 42 LLM-generated labels of the TREC 2023 Deep Learning track
relevance judgments produced by eight international teams who participated in
the challenge. Given their diverse nature, these automatically generated
relevance judgments can help the community not only investigate systematic
biases caused by LLMs but also explore the effectiveness of ensemble models,
analyze the trade-offs between different models and human assessors, and
advance methodologies for improving automated evaluation techniques. The
released resource is available at the following link:
https://llm4eval.github.io/LLMJudge-benchmark/

中文翻译:
利用大语言模型（LLMs）进行相关性评估为改进信息检索（IR）、自然语言处理（NLP）及相关领域提供了重要机遇。当前IR实验人员通过LLMs构建评估数据集，仅需投入传统人工标注所需的小部分工作量，这将有助于处理知识积累有限的新兴主题，并缓解低资源场景下排名系统评估的难题——这类场景往往难以招募人工标注者。鉴于该领域近期快速发展，关于LLMs作为评估者的诸多问题仍有待探索，包括提示词设计、模型选择等相关性判断生成流程中关键组件的影响机制。

本文基于SIGIR 2024大会"LLMJudge挑战赛"的大规模自动相关性评估实验，对42组由8支国际团队生成的TREC 2023深度学习赛道相关性标注进行基准测试与结果分析。这些自动生成的多样性标注不仅有助于研究LLMs引发的系统性偏差，还可推动以下研究方向：集成模型效能验证、不同模型与人工评估者的权衡分析，以及自动化评估技术的优化方法论。相关资源已发布于：https://llm4eval.github.io/LLMJudge-benchmark/


5. 保留专业术语缩写（LLMs/IR/NLP）并首次出现时标注全称
6. 长难句拆分重组（如最后复合句拆解为三个研究方向的并列列举）
